---
title: "Data Camp Insights"
author: "davegoblue"
date: "April 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
## Background and Overview  
DataCamp offer interactive courses related to R Programming.  While some is review, it is helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  

This document is currently split between _v003 and v_003_a and v_003_b due to the need to keep the number of DLL that it opens below the hard-coded maximum.  This introductory section needs to be re-written, and the contents consolidated, at a future date.
  
* R Programming (Introductory R, Intermediate R, Writing Functions in R, Object Oriented Programming in R, Introduction to Tidyverse)  
* Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
* Data Manipulation (dplyr, data.table, xts/zoo, dplyr joins, cases for EDA/Time Series/Pitch Data)  
* Data Visualization (base, ggplot2 parts I/II/III, ggvis, geospatial)  
* Statistics (8 refresher modules)  
* Machine Learning (3 modules + 1 text mining case)  
* R Studio (2 parts) and R Markdown (1 module)  
  
The original DataCamp_Insights_v001 and DataCamp_Insights_v002 documents have been split for this document:  
  
* This DataCamp_Insights_v003 document contains evolving sections on R Programming, Machine Learning, and RStudio / R Markdown  
* Importing and Cleaning Data components have been moved to DataCamp_ImportClean_v002  
* Data Manipulation components have been moved to DataCamp_DataManipulation_v002  
* Visualization components have been moved to DataCamp_Visualization_v002  
* Statistics components have been moved to DataCamp_Statistics_v002  
  

###_Working with Dates and Times in R_  
  
Chapter 1 - Dates and Times in R  
  
Introduction to dates - including the built-in methods for R:  
  
* Differences in M-D-Y and D-M-Y  
* ISO8601 is a standard for dates - components should be decreasing such as YYYY-MM-DD  
	* The numbers should all be padded with leading zeroes  
    * A separator is not required, but it must be a dash (-) if used  
* R will generally require input using as.Date(<isoFormattedString>)  
* Some functions that read in data will automatically recognize and parse dates in a variety of formats  
    * In particular the import functions, like read_csv(), in the readr package will recognize dates in a few common formats  
    * There is also the anytime() function in the anytime package whose sole goal is to automatically parse strings as dates regardless of the format  
  
Why use dates?  
  
* Behind the scenes, dates are stored as the number of days since 1970-01-01  
	* Can compare dates, take differences of dates, use dates for plotting, and the like  
* R releases have a major, minor, and patch  
	* Patch starts at zero with a new minor and increments by 1  
    * Minor starts at zero with a new major and incerements by 1  
  
What about times?  
  
* R also has the built-in capability to handle datetimes  
* ISO8601 has standards for datetimes also - YYYYMMDD HH:MM:SS  
* Two capabilities for storing times in R  
	* POSIXlt - list with named components  
    * POSIXct - seconds since 1970-01-01 00:00:00 (typically better for data frames, and focus of this module)  
* Can convert to POSIXct using as.POSIXct(<ISOString>)  
* Can pass a timezone, and the default assumption is local time  
    * If the string is passed as YYYYMMDDTHH:MM:SSZ then the assumption is made of Zulu (UTC) time  
* One drawback is that as.POSIXct() does not naturally recognize the timezones, so some additional work is required to properly enter a datetime  
  
Why lubridate?  
  
* The lubridate package is designed to make it easier to work with dates and times  
	* Part of the tidyverse - designed for humans, and integrates nicely to data analysis pipelines  
    * Consistent behavior regardless of the underlying objects  
* Easier to use, and more forgiving of formats  
* Has capability for time spans (time between two times, such as time for reign of monarchs)  
  
Example code includes:  
```{r}

library(dplyr)
library(ggplot2)


# The date R 3.0.0 was released
x <- "2013-04-03"

# Examine structure of x
str(x)

# Use as.Date() to interpret x as a date
x_date <- as.Date(x)

# Examine structure of x_date
str(x_date)

# Store April 10 2014 as a Date
april_10_2014 <- as.Date("2014-04-10")


# Load the readr package
library(readr)

# Use read_csv() to import rversions.csv
releases <- read_csv("./RInputFiles/rversions.csv")

# Examine the structure of the date column
str(releases$date)

# Load the anytime package
library(anytime)

# Various ways of writing Sep 10 2009
sep_10_2009 <- c("September 10 2009", "2009-09-10", "10 Sep 2009", "09-10-2009")

# Use anytime() to parse sep_10_2009
anytime(sep_10_2009)


# Set the x axis to the date column
ggplot(releases, aes(x = date, y = type)) +
  geom_line(aes(group = 1, color = factor(major)))

# Limit the axis to between 2010-01-01 and 2014-01-01
ggplot(releases, aes(x = date, y = type)) +
  geom_line(aes(group = 1, color = factor(major))) +
  xlim(as.Date("2010-01-01"), as.Date("2014-01-01"))

# Specify breaks every ten years and labels with "%Y"
ggplot(releases, aes(x = date, y = type)) +
  geom_line(aes(group = 1, color = factor(major))) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y")


# Find the largest date
last_release_date <- max(releases$date)

# Filter row for last release
last_release <- filter(releases, date == last_release_date)

# Print last_release
last_release

# How long since last release?
Sys.Date() - last_release_date


# Use as.POSIXct to enter the datetime 
as.POSIXct("2010-10-01 12:12:00")

# Use as.POSIXct again but set the timezone to `"America/Los_Angeles"`
as.POSIXct("2010-10-01 12:12:00", tz = "America/Los_Angeles")

# Use read_csv to import rversions.csv
releases <- read_csv("./RInputFiles/rversions.csv")

# Examine structure of datetime column
str(releases$datetime)


# Import "cran-logs_2015-04-17.csv" with read_csv()
logs <- read_csv("./RInputFiles/cran-logs_2015-04-17.csv")

# Print logs
logs

# Store the release time as a POSIXct object
release_time <- as.POSIXct("2015-04-16 07:13:33", tz = "UTC")

# When is the first download of 3.2.0?
logs %>% 
  filter(r_version == "3.2.0")

# Examine histograms of downloads by version
ggplot(logs, aes(x = datetime)) +
  geom_histogram() +
  geom_vline(aes(xintercept = as.numeric(release_time)))+
  facet_wrap(~ r_version, ncol = 1)

```
  
  
  
***
  
Chapter 2 - Parsing and Manipulating Dates with lubridate  
  
Parsing dates with lubridate:  
  
* lubridate::ymd() will manage dates in format ymd, even if they are not properly ISO formatted (have separators, English abbreviations, and the like)  
	* Analogous behaviors from ydm(), mdy(), myd(), dmy(), dym(), day_hm()  
    * Assumes UTC unless otherwise specified  
    * All the functions with y, m and d in any order exist  
    * If your dates have times as well, you can use the functions that start with ymd, dmy, mdy or ydm and are followed by any of _h, _hm or _hms  
    * To see all the functions available look at ymd() for dates and ymd_hms() for datetimes  
* lubridate::parse_date_time(x=, orders=)  
	* The orders = argument is a sequence of characters, reflecting the order in the input  
    * y-year with century, Y-year without century, m-month, d-day, H-hours (24-hour), M-minutes, S-seconds, and many others  
    * a-abbreviated weekday, A-full weekday, b-abbreviate month, B-full month, I-hours (12-hour), p-AM/PM, z-timezone (offset in minutes/seconds from UTC)  
    * Can pass a vector of sequences to orders=, such as orders=c("ymd", "dmy"), if some of the dates are formatted differently than others  
  
Weather in Auckland (data from Weather Underground, METAR from Auckland airport):  
  
* Data are available in akl_weather_daily.csv and akl_weather_hourly_2016.csv  
* The lubridate::make_date(year, month, date) will produce a date from its components (these components can be vectors, such as columns in a frame  
	* There is also a lubridate::make_datetime(year, month, day, hour, min, sec)  
  
Extracting parts of a datetime:  
  
* The lubridate::year() will pull out the year from a datetime object  
	* month(), day(), hour(), minute(), second() will do the same  
    * wday() is the weekday (1-7), while yday() is the Julian date (1-366) and tz() is the timezone  
* The extractors can also be used to set a component of the datetime object  
* Several functions return booleans, more or less answers to "is this a" questions  
	* leap_year(), am(), pm(), dst(), quarter() will return 1-4, semester() will return 1-2  
    * Months of course are different lengths so we should really correct for that, take a look at days_in_month() for helping with that  
  
Rounding datetimes:  
  
* The lubridate::floor_date(unit=) will round-down to the requested unit, such as "hour"  
	* round_date() for nearest  
    * ceiling_date() for round-up  
* Units can be specified as "second", "minute", "hour", "day", "week", "month", "bimonth", "quarter", "halfyear", "year"  
  
Example code includes:  
```{r}

library(lubridate)
library(readr)
library(dplyr)
library(ggplot2)
library(ggridges)
library(stringr)


# Parse x 
x <- "2010 September 20th" # 2010-09-20
ymd(x)

# Parse y 
y <- "02.01.2010"  # 2010-01-02
dmy(y)

# Parse z 
z <- "Sep, 12th 2010 14:00"  # 2010-09-12T14:00
mdy_hm(z)


# Specify an order string to parse x
x <- "Monday June 1st 2010 at 4pm"
parse_date_time(x, orders = "AmdyIp")

# Specify order to include both "mdy" and "dmy"
two_orders <- c("October 7, 2001", "October 13, 2002", "April 13, 2003", 
  "17 April 2005", "23 April 2017")
parse_date_time(two_orders, orders = c("mdy", "dmy"))

# Specify order to include "dOmY", "OmY" and "Y"
short_dates <- c("11 December 1282", "May 1372", "1253")
parse_date_time(short_dates, orders = c("dOmY", "OmY", "Y"))


# Import CSV with read_csv()
akl_daily_raw <- read_csv("./RInputFiles/akl_weather_daily.csv")

# Print akl_daily_raw
akl_daily_raw

# Parse date 
akl_daily <- akl_daily_raw %>%
  mutate(date = ymd(date))

# Print akl_daily
akl_daily

# Plot to check work
ggplot(akl_daily, aes(x = date, y = max_temp)) +
  geom_line() 


# Import "akl_weather_hourly_2016.csv"
akl_hourly_raw <- read_csv("./RInputFiles/akl_weather_hourly_2016.csv")

# Print akl_hourly_raw
akl_hourly_raw

# Use make_date() to combine year, month and mday 
akl_hourly  <- akl_hourly_raw  %>% 
  mutate(date = make_date(year = year, month = month, day = mday))

# Parse datetime_string 
akl_hourly <- akl_hourly  %>% 
  mutate(
    datetime_string = paste(date, time, sep = "T"),
    datetime = ymd_hms(datetime_string)
  )

# Print date, time and datetime columns of akl_hourly
akl_hourly %>% select(date, time, datetime)

# Plot to check work
ggplot(akl_hourly, aes(x = datetime, y = temperature)) +
  geom_line()


# Examine the head() of release_time
releases <- read_csv("./RInputFiles/rversions.csv")
release_time <- releases %>% pull(datetime)
head(release_time)

# Examine the head() of the months of release_time
head(month(release_time))

# Extract the month of releases 
month(release_time) %>% table()

# Extract the year of releases
year(release_time) %>% table()

# How often is the hour before 12 (noon)?
mean(hour(release_time) < 12)

# How often is the release in am?
mean(am(release_time))


# Use wday() to tabulate release by day of the week
wday(releases$datetime) %>% table()

# Add label = TRUE to make table more readable
wday(releases$datetime, label=TRUE) %>% table()

# Create column wday to hold labelled week days
releases$wday <- wday(releases$datetime, label=TRUE)

# Plot barchart of weekday by type of release
ggplot(releases, aes(x=wday)) +
  geom_bar() +
  facet_wrap(~ type, ncol = 1, scale = "free_y")


# Add columns for year, yday and month
akl_daily <- akl_daily %>%
  mutate(
    year = year(date),
    yday = yday(date),
    month = month(date, label=TRUE))

# Plot max_temp by yday for all years
ggplot(akl_daily, aes(x = yday, y = max_temp)) +
  geom_line(aes(group = year), alpha = 0.5)

# Examine distribtion of max_temp by month
ggplot(akl_daily, aes(x = max_temp, y = month, height = ..density..)) +
  geom_density_ridges(stat = "density")


# Create new columns hour, month and rainy
akl_hourly <- akl_hourly %>%
  mutate(
    hour = hour(datetime),
    month = month(datetime, label=TRUE),
    rainy = (weather == "Precipitation")
  )

# Filter for hours between 8am and 10pm (inclusive)
akl_day <- akl_hourly %>% 
  filter(hour >= 8, hour <= 22)

# Summarise for each date if there is any rain
rainy_days <- akl_day %>% 
  group_by(month, date) %>%
  summarise(
    any_rain = any(rainy)
  )

# Summarise for each month, the number of days with rain
rainy_days %>% 
  summarise(
    days_rainy = sum(any_rain)
  )


r_3_4_1 <- ymd_hms("2016-05-03 07:13:28 UTC")

# Round down to day
floor_date(r_3_4_1, unit = "day")

# Round to nearest 5 minutes
round_date(r_3_4_1, unit = "5 minutes")

# Round up to week 
ceiling_date(r_3_4_1, unit = "week")

# Subtract r_3_4_1 rounded down to day
r_3_4_1 - floor_date(r_3_4_1, unit = "day")


# Create day_hour, datetime rounded down to hour
akl_hourly <- akl_hourly %>%
  mutate(
    day_hour = floor_date(datetime, unit = "hour")
  )

# Count observations per hour  
akl_hourly %>% 
  count(day_hour) 

# Find day_hours with n != 2  
akl_hourly %>% 
  count(day_hour) %>%
  filter(n != 2) %>% 
  arrange(desc(n))


```
  
  
  
***
  
Chapter 3 - Arithmetic with Dates and Times  
  
Taking differences of datetimes:  
  
* Pure subtraction will give the days between two datetimes, reported on the command line as "Time difference of x days"  
	* The difftime(day1, day2, units=) function is the same as day1 - day2, but with additional control of being able to request units (secs, mins, hours, days, weeks)  
* The today() function gives you today's date as a Date object  
* The now() function gives you the current date-time as a POSIXct object  
  
Time spans - difficult because they do not have a constant meaning (e.g., impact of daylight savings time):  
  
* The lubridate package manages time spans as EITHER period or duration  
	* The period is the way a human thinks about it - 1 day means same exact hour-minute-second tomorrow  
    * The duration is the way a stopwatch thinks about it - 1 day means 24 hours from now  
* The period time span in lubridate is called by adding an "s" to the end of the relevant function  
	* For example, days(x=1) will be exactly +1 in the days category only (all other units untouched)  
* The duration in lubridate is called by adding a "d" to the front of the relevant period function  
	* For example, ddays(x=1) will add 24 hours to the datetime  
* There was an eclipse over North America on 2017-08-21 at 18:26:40  
	* It's possible to predict the next eclipse with similar geometry by calculating the time and date one Saros in the future  
    * A Saros is a length of time that corresponds to 223 Synodic months, a Synodic month being the period of the Moon's phases, a duration of 29 days, 12 hours, 44 minutes and 3 seconds  
* What should ymd("2018-01-31") + months(1) return? Should it be 30, 31 or 28 days in the future? Try it  
	* In general lubridate returns the same day of the month in the next month, but since the 31st of February doesn't exist lubridate returns a missing value, NA  
    * There are alternative addition and subtraction operators: %m+% and %m-% that have different behavior  
    * Rather than returning an NA for a non-existent date, they roll back to the last existing date  
    * But use these operators with caution, unlike + and -, you might not get x back from x %m+% months(1) %m-% months(1)  
    * If you'd prefer that the date was rolled forward check out add_with_rollback() which has roll_to_first argument  
  
Intervals - third option in lubridate for storing times:  
  
* Can find length, whether an object is in the interval, whether various intervals overlap, and the like  
	* Intervals can be created either by using interval(datetime1, datetime2) or datetime1 %--% datetime2  
* There are many lubridate functions for working with intervals  
	* int_start() and int_end() will give back the start and end date for the interval  
    * int_length() will give back the interval length in seconds  
    * as.period() will return the interval length as a period, while as.duration() will return the interval length as a duration  
    * aDateTime %within% anInterval will return a boolean that answers the question  
    * The int_overlaps(int1, int2) will return a boolean for whether there is any overlap  
* Intervals tend to be best when you have a specific start and end date  
	* Otherwise, use periods for human purposes and durations for technical purposes  
* The operator %within% tests if the datetime (or interval) on the left hand side is within the interval of the right hand side  
	* int_overlaps() performs a similar test, but will return true if two intervals overlap at all  
  
Example code includes:  
```{r}

# The date of landing and moment of step
date_landing <- mdy("July 20, 1969")
moment_step <- mdy_hms("July 20, 1969, 02:56:15", tz = "UTC")

# How many days since the first man on the moon?
difftime(today(), date_landing, units = "days")

# How many seconds since the first man on the moon?
difftime(now(), moment_step, units = "secs")


# Three dates
mar_11 <- ymd_hms("2017-03-11 12:00:00", 
  tz = "America/Los_Angeles")
mar_12 <- ymd_hms("2017-03-12 12:00:00", 
  tz = "America/Los_Angeles")
mar_13 <- ymd_hms("2017-03-13 12:00:00", 
  tz = "America/Los_Angeles")

# Difference between mar_13 and mar_12 in seconds
difftime(mar_13, mar_12, units = "secs")

# Difference between mar_12 and mar_11 in seconds
difftime(mar_12, mar_11, units = "secs")


# Add a period of one week to mon_2pm
mon_2pm <- dmy_hm("27 Aug 2018 14:00")
mon_2pm + weeks(1)

# Add a duration of 81 hours to tue_9am
tue_9am <- dmy_hm("28 Aug 2018 9:00")
tue_9am + dhours(81)

# Subtract a period of five years from today()
today() - years(5)

# Subtract a duration of five years from today()
today() - dyears(5)


# Time of North American Eclipse 2017
eclipse_2017 <- ymd_hms("2017-08-21 18:26:40")

# Duration of 29 days, 12 hours, 44 mins and 3 secs
synodic <- ddays(29) + dhours(12) + dminutes(44) + dseconds(3)

# 223 synodic months
saros <- 223 * synodic

# Add saros to eclipse_2017
eclipse_2017 + saros


# Add a period of 8 hours to today
today_8am <- today() + hours(8)

# Sequence of two weeks from 1 to 26
every_two_weeks <- 1:26 * weeks(2)

# Create datetime for every two weeks for a year
today_8am + every_two_weeks


jan_31 <- ymd("2018-01-31")
# A sequence of 1 to 12 periods of 1 month
month_seq <- 1:12 * months(1)

# Add 1 to 12 months to jan_31
jan_31 + month_seq

# Replace + with %m+%
jan_31 %m+% month_seq

# Replace + with %m-%
jan_31 %m-% month_seq


# Create monarchs
mNames <- c('Elizabeth II' ,'Victoria' ,'George V' ,'George III' ,'George VI' ,'George IV' ,'Edward VII' ,'William IV' ,'Edward VIII' ,'George III(also United Kingdom)' ,'George II' ,'George I' ,'Anne' ,'Henry III' ,'Edward III' ,'Elizabeth I' ,'Henry VI' ,'Henry VI' ,'Æthelred II' ,'Æthelred II' ,'Henry VIII' ,'Charles II' ,'Henry I' ,'Henry II(co-ruler with Henry the Young King)' ,'Edward I' ,'Alfred the Great' ,'Edward the Elder' ,'Charles I' ,'Henry VII' ,'Edward the Confessor' ,'Richard II' ,'James I' ,'Edward IV' ,'Edward IV' ,'William I' ,'Edward II' ,'Cnut' ,'Stephen' ,'Stephen' ,'John' ,'Edgar I' ,'Æthelstan' ,'Henry IV' ,'William III(co-ruler with Mary II)' ,'Henry the Young King(co-ruler with Henry II)' ,'William II' ,'Richard I' ,'Eadred' ,'Henry V' ,'Edmund I' ,'Edward VI' ,'Mary II(co-ruler with William III)' ,'Mary I' ,'Anne(also Kingdom of Great Britain)' ,'Eadwig' ,'James II' ,'Edward the Martyr' ,'Harold I' ,'Harthacnut' ,'Richard III' ,'Louis (disputed)' ,'Harold II' ,'Edmund II' ,'Matilda (disputed)' ,'Edward V' ,'Edgar II' ,'Sweyn Forkbeard' ,'Jane (disputed)' ,'James VI' ,'William I' ,'Constantine II' ,'David II' ,'Alexander III' ,'Malcolm III' ,'Alexander II' ,'James I' ,'Malcolm II' ,'James V' ,'David I' ,'James III' ,'Charles II' ,'Charles II' ,'James IV' ,'Mary I' ,'Charles I' ,'Kenneth II' ,'James II' ,'Robert I' ,'Robert II' ,'Alexander I' ,'Macbeth' ,'Robert III' ,'Constantine I' ,'Kenneth MacAlpin' ,'William II' ,'Malcolm IV' ,'Giric(co-ruler with Eochaid?)' ,'Donald II' ,'Malcolm I' ,'Edgar' ,'Kenneth III' ,'Indulf' ,'Duncan I' ,'Mary II' ,'Amlaíb' ,'Anne(also Kingdom of Great Britain)' ,'Dub' ,'Cuilén' ,'Domnall mac Ailpín' ,'James VII' ,'Margaret' ,'John Balliol' ,'Donald III' ,'Constantine III' ,'Áed mac Cináeda' ,'Lulach' ,'Duncan II' ,'Ruaidrí Ua Conchobair' ,'Edward Bruce (disputed)' ,'Brian Ua Néill (disputed)' ,'Gruffudd ap Cynan' ,'Llywelyn the Great' ,'Owain Gwynedd' ,'Dafydd ab Owain Gwynedd' ,'Hywel ab Owain Gwynedd' ,'Llywelyn ap Gruffudd' ,'Owain Glyndŵr (disputed)' ,'Owain Goch ap Gruffydd' ,'Owain Lawgoch (disputed)' ,'Dafydd ap Llywelyn' ,'Dafydd ap Gruffydd')
mDominion <- c('United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'Great Britain' ,'Great Britain' ,'Great Britain' ,'Great Britain' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Ireland' ,'Ireland' ,'Ireland' ,'Gwynedd' ,'Gwynedd' ,'Gwynedd' ,'Gwynedd' ,'Gwynedd' ,'Wales' ,'Wales' ,'Wales' ,'Wales' ,'Wales' ,'Wales')
mFrom <- c('1952-02-06' ,'1837-06-20' ,'1910-05-06' ,'1801-01-01' ,'1936-12-11' ,'1820-01-29' ,'1901-01-22' ,'1830-06-26' ,'1936-01-20' ,'1760-10-25' ,'1727-06-22' ,'1714-08-01' ,'1707-05-01' ,'NA' ,'1327-01-25' ,'1558-11-17' ,'1422-08-31' ,'1470-10-31' ,'978-03-18' ,'1014-02-03' ,'1509-04-22' ,'1649-01-30' ,'1100-08-03' ,'1154-10-25' ,'1272-11-20' ,'871-04-24' ,'899-10-27' ,'1625-03-27' ,'1485-08-22' ,'1042-06-08' ,'1377-06-22' ,'1603-03-24' ,'1461-03-04' ,'1471-04-11' ,'1066-12-12' ,'1307-07-07' ,'1016-11-30' ,'1135-12-22' ,'1141-11-01' ,'1199-04-06' ,'959-10-01' ,'924-08-02' ,'1399-09-29' ,'1689-02-13' ,'1170-06-14' ,'1087-09-09' ,'1189-07-06' ,'946-05-26' ,'1413-03-21' ,'939-10-27' ,'1547-01-28' ,'1689-02-13' ,'1553-07-19' ,'1702-03-08' ,'955-11-23' ,'1685-02-06' ,'975-07-09' ,'1037-11-12' ,'1040-03-17' ,'1483-06-26' ,'1216-06-14' ,'1066-01-05' ,'1016-04-23' ,'1141-04-07' ,'1483-04-09' ,'1066-10-15' ,'1013-12-25' ,'1553-07-10' ,'1567-07-24' ,'1165-12-09' ,'900-01-01' ,'1329-06-07' ,'1249-07-06' ,'1058-03-17' ,'1214-12-04' ,'1406-04-04' ,'1005-03-25' ,'1513-09-09' ,'1124-04-23' ,'1460-08-03' ,'1649-01-30' ,'1660-05-29' ,'1488-06-11' ,'1542-12-14' ,'1625-03-27' ,'971-01-01' ,'1437-02-21' ,'1306-03-25' ,'1371-02-22' ,'1107-01-08' ,'1040-08-14' ,'1390-04-19' ,'862-01-01' ,'843-01-01' ,'1689-05-11' ,'1153-05-24' ,'878-01-01' ,'889-01-01' ,'943-01-01' ,'1097-01-01' ,'997-01-01' ,'954-01-01' ,'1034-11-25' ,'1689-04-11' ,'971-01-01' ,'1702-03-08' ,'962-01-01' ,'NA' ,'858-01-01' ,'1685-02-06' ,'1286-11-25' ,'1292-11-17' ,'1093-11-13' ,'1095-01-01' ,'877-01-01' ,'1057-08-15' ,'1094-05-01' ,'1166-01-01' ,'1315-06-01' ,'1258-01-01' ,'1081-01-01' ,'1195-01-01' ,'1137-01-01' ,'1170-01-01' ,'1170-01-01' ,'1253-01-01' ,'1400-09-16' ,'1246-02-25' ,'1372-05-01' ,'1240-04-12' ,'1282-12-11')
mTo <- c('2018-02-08' ,'1901-01-22' ,'1936-01-20' ,'1820-01-29' ,'1952-02-06' ,'1830-06-26' ,'1910-05-06' ,'1837-06-20' ,'1936-12-11' ,'1801-01-01' ,'1760-10-25' ,'1727-06-11' ,'1714-08-01' ,'1272-11-16' ,'1377-06-21' ,'1603-03-24' ,'1461-03-04' ,'1471-04-11' ,'1013-12-25' ,'1016-04-23' ,'1547-01-28' ,'1685-02-06' ,'1135-12-01' ,'1189-07-06' ,'1307-07-07' ,'899-10-26' ,'924-07-17' ,'1649-01-30' ,'1509-04-21' ,'1066-01-05' ,'1399-09-29' ,'1625-03-27' ,'1470-10-03' ,'1483-04-09' ,'1087-09-09' ,'1327-01-20' ,'1035-11-12' ,'1141-04-07' ,'1154-10-25' ,'1216-10-19' ,'975-07-08' ,'939-10-27' ,'1413-03-20' ,'1702-03-08' ,'1183-06-11' ,'1100-08-02' ,'1199-04-06' ,'955-11-23' ,'1422-08-31' ,'946-05-26' ,'1553-07-06' ,'1694-12-28' ,'1558-11-17' ,'1707-04-30' ,'959-10-01' ,'1688-12-11' ,'978-03-18' ,'1040-03-17' ,'1042-06-08' ,'1485-08-22' ,'1217-09-22' ,'1066-10-14' ,'1016-11-30' ,'1141-11-01' ,'1483-06-26' ,'1066-12-17' ,'1014-02-03' ,'1553-07-19' ,'1625-03-27' ,'1214-12-04' ,'943-01-01' ,'1371-02-22' ,'1286-03-19' ,'1093-11-13' ,'1249-07-06' ,'1437-02-21' ,'1034-11-25' ,'1542-12-14' ,'1153-05-24' ,'1488-06-11' ,'1651-09-03' ,'1685-02-06' ,'1513-09-09' ,'1567-07-24' ,'1649-01-30' ,'995-01-01' ,'1460-08-03' ,'1329-06-07' ,'1390-04-19' ,'1124-04-23' ,'1057-08-15' ,'1406-04-04' ,'877-01-01' ,'858-02-13' ,'1702-03-08' ,'1165-12-09' ,'889-01-01' ,'900-01-01' ,'954-01-01' ,'1107-01-08' ,'1005-03-25' ,'962-01-01' ,'1040-08-14' ,'1694-12-28' ,'977-01-01' ,'1707-04-30' ,'NA' ,'971-01-01' ,'862-04-13' ,'1688-12-11' ,'1290-09-26' ,'1296-07-10' ,'1097-01-01' ,'1097-01-01' ,'878-01-01' ,'1058-03-17' ,'1094-11-12' ,'1193-01-01' ,'1318-10-14' ,'1260-01-01' ,'1137-01-01' ,'1240-04-11' ,'1170-01-01' ,'1195-01-01' ,'1170-01-01' ,'1282-12-11' ,'1416-01-01' ,'1255-01-01' ,'1378-07-01' ,'1246-02-25' ,'1283-10-03')

padMDate <- function(x) { 
    if (is.na(x[1]) | x[1] == "NA") { 
        NA 
    } else { 
        paste0(c(str_pad(x[1], 4, pad="0"), x[2], x[3]), collapse="-") 
    } 
}



monarchs <- tibble::tibble(name=mNames, dominion=mDominion, 
                           from=ymd(sapply(str_split(mFrom, "-"), FUN=padMDate)), 
                           to=ymd(sapply(str_split(mTo, "-"), FUN=padMDate))
                           )

# Print monarchs
monarchs

# Create an interval for reign
monarchs <- monarchs %>%
  mutate(reign = from %--% to) 

# Find the length of reign, and arrange
monarchs %>%
  mutate(length = int_length(reign)) %>% 
  arrange(desc(length)) %>%
  select(name, length, dominion)


# Print halleys
pDate <- c('66-01-26', '141-03-25', '218-04-06', '295-04-07', '374-02-13', '451-07-03', '530-11-15', '607-03-26', '684-11-26', '760-06-10', '837-02-25', '912-07-27', '989-09-02', '1066-03-25', '1145-04-19', '1222-09-10', '1301-10-22', '1378-11-09', '1456-01-08', '1531-08-26', '1607-10-27', '1682-09-15', '1758-03-13', '1835-11-16', '1910-04-20', '1986-02-09', '2061-07-28')
sDate <- c('66-01-25', '141-03-22', '218-04-06', '295-04-07', '374-02-13', '451-06-28', '530-09-27', '607-03-15', '684-10-02', '760-05-20', '837-02-25', '912-07-18', '989-09-02', '1066-01-01', '1145-04-15', '1222-09-10', '1301-10-22', '1378-11-09', '1456-01-08', '1531-08-26', '1607-10-27', '1682-09-15', '1758-03-13', '1835-08-01', '1910-04-20', '1986-02-09', '2061-07-28')
eDate <- c('66-01-26', '141-03-25', '218-05-17', '295-04-20', '374-02-16', '451-07-03', '530-11-15', '607-03-26', '684-11-26', '760-06-10', '837-02-28', '912-07-27', '989-09-05', '1066-03-25', '1145-04-19', '1222-09-28', '1301-10-31', '1378-11-14', '1456-06-09', '1531-08-26', '1607-10-27', '1682-09-15', '1758-12-25', '1835-11-16', '1910-05-20', '1986-02-09', '2061-07-28')

halleys <- tibble::tibble(perihelion_date=ymd(sapply(str_split(pDate, "-"), FUN=padMDate)), 
                          start_date=ymd(sapply(str_split(sDate, "-"), FUN=padMDate)), 
                          end_date=ymd(sapply(str_split(eDate, "-"), FUN=padMDate))
                          )


# New column for interval from start to end date
halleys <- halleys %>%
  mutate(visible = start_date %--% end_date)

# The visitation of 1066
halleys_1066 <- halleys[14, ]

# Monarchs in power on perihelion date
monarchs %>%
  filter(halleys_1066$perihelion_date %within% reign) %>%
  select(name, from, to, dominion)

# Monarchs whose reign overlaps visible time
monarchs %>%
  filter(int_overlaps(halleys_1066$visible, reign)) %>%
  select(name, from, to, dominion)


# New columns for duration and period
monarchs <- monarchs %>%
  mutate(
    duration = as.duration(reign),
    period = as.period(reign))

# Examine results    
monarchs %>% 
    select(name, duration, period) %>%
    head(10) %>%
    print.data.frame()

```
  
  
  
***
  
Chapter 4 - Problems in Practice  
  
Time zones - ways to keep track of times in different locations (can pose analysis challenges):  
  
* Typically captured as an offset from GMT, but specified in R using tz= since the offset to GMT can change during the year (DST for example)  
	* Sys.timezone() gives the timezone on your computer  
    * OlsonNames() gives all the timezones that R is aware of  
    * The OlsonNames() function matches with an international standard as to which cities are included  
    * The lubridate::tz() will extract the timezone from a specific datetime  
* Can change the timezone without changing the underlying clock time components by using lubridate::force_tz()  
	* force_tz(ymd_hm("2017-12-12 12:00", tz="America/Los_Angeles"), tzone="America/Boston") will produce 2017-12-12 12:00 EST (note that the 12:00 is held, with ONLY time-zone changed)  
* Can view the time in a different zone by using lubridate::with_tz()  
	* with_tz(ymd_hm("2017-12-12 12:00", tz="America/Los_Angeles"), tzone="America/Boston") will produce 2017-12-12 15:00 EST (note that 15:00 EST and 12:00 PST are the same)  
* For this entire course, if you've ever had a time, it's always had an accompanying date, i.e. a datetime. But sometimes you just have a time without a date  
	* If you find yourself in this situation, the hms package provides an hms class of object for holding times without dates, and the best place to start would be with as.hms()  
    * readr knows the hms class, so if it comes across something that looks like a time it will use it  
  
Importing and exporting datetimes:  
  
* The parse_date_time() function is designed to be forgiving and flexible, but at the expense of being slow (since it considers many possible formats)
	* The fasttime::fastPOSIXct() is designed to very quickly read a proper ISO formatting of "YYYY-MM-DD"  
    * The lubridate::fast_strptime(x=, format=) is also fast, but it requires a valid strptime format like "%Y-%m-%d" rather than the more flexible/forgiving parse_date_time(x=, order="ymd")  
    * See help for strptime() for the valid strings  
* The readr::write_csv() will write datetime objects in a proper ISO format, making for easy read-in  
* Can also use the lubridate::stamp() capability to build a function that will format things based on an example you provide  
	* my_stamp <- stamp("Tuesday October 10 2017")  
    * my_stamp has been created by lubridate::stamp() as function(x) format(x, format="%A %B %d %Y") to match the example given  
  
Wrap-up:  
  
* Chapter 1: base R objects Date, POSIXct  
	* lubridate, zoo, xts, and the like all work together with each other and these  
* Chapter 2: importing and manipulating datetime obects  
* Chapter 3: challenges of arithmetic with datetimes  
	* periods, durations, intervals  
* Chapter 4: time zones, and import/outputs  
  
Example code includes:  
```{r}

# Game2: CAN vs NZL in Edmonton
game2 <- mdy_hm("June 11 2015 19:00")

# Game3: CHN vs NZL in Winnipeg
game3 <- mdy_hm("June 15 2015 18:30")

# Set the timezone to "America/Edmonton"
game2_local <- force_tz(game2, tzone = "America/Edmonton")
game2_local

# Set the timezone to "America/Winnipeg"
game3_local <- force_tz(game3, tzone = "America/Winnipeg")
game3_local

# How long does the team have to rest?
as.period(game2_local %--% game3_local)


# What time is game2_local in NZ?
with_tz(game2_local, tzone = "Pacific/Auckland")

# What time is game2_local in Corvallis, Oregon?
with_tz(game2_local, tzone = "America/Los_Angeles")

# What time is game3_local in NZ?
with_tz(game3_local, tzone = "Pacific/Auckland")


# Examine datetime and date_utc columns
head(akl_hourly$datetime)
head(akl_hourly$date_utc)
  
# Force datetime to Pacific/Auckland
akl_hourly <- akl_hourly %>%
  mutate(
    datetime = force_tz(datetime, tzone = "Pacific/Auckland"))

# Reexamine datetime
head(akl_hourly$datetime)
  
# Are datetime and date_utc the same moments
table(akl_hourly$datetime - akl_hourly$date_utc)


# Import auckland hourly data 
akl_hourly <- read_csv("./RInputFiles/akl_weather_hourly_2016.csv")

# Examine structure of time column
str(akl_hourly$time)

# Examine head of time column
head(akl_hourly$time)

# A plot using just time
ggplot(akl_hourly, aes(x = time, y = temperature)) +
  geom_line(aes(group = make_date(year, month, mday)), alpha = 0.2)


library(microbenchmark)
library(fasttime)

# Examine structure of dates
dates <- paste0(gsub(" ", "T", as.character(akl_hourly$date_utc)), "Z")

str(dates)

# Use fastPOSIXct() to parse dates
fastPOSIXct(dates) %>% str()

# Compare speed of fastPOSIXct() to ymd_hms()
microbenchmark(
  ymd_hms = ymd_hms(dates),
  fasttime = fastPOSIXct(dates),
  times = 20)


# Head of dates
head(dates)

# Parse dates with fast_strptime
fast_strptime(dates, 
    format = "%Y-%m-%dT%H:%M:%SZ") %>% str()

# Comparse speed to ymd_hms() and fasttime
microbenchmark(
  ymd_hms = ymd_hms(dates),
  fasttime = fastPOSIXct(dates),
  fast_strptime = fast_strptime(dates, 
    format = "%Y-%m-%dT%H:%M:%SZ"),
  times = 20)


finished <- "I finished 'Dates and Times in R' on Thursday, September 20, 2017!"
# Create a stamp based on "Sep 20 2017"
date_stamp <- stamp("September 20, 2017", orders="mdy")

# Print date_stamp
date_stamp

# Call date_stamp on today()
date_stamp(today())

# Create and call a stamp based on "09/20/2017"
stamp("09/20/2017", orders="mdy")(today())

# Use string finished for stamp()
stamp(finished, orders="amdy")(today())

```
  
  
  
***
  
###_Scalable Data Processing in R_  
  
Chapter 1 - Working with Increasingly Large Data Sets  
  
What is scalable data processing?:  
  
* Working with data that is too large for one computer  
* Scalable code lets you work in parallel, and use resources as they become available  
* Data sets are frequently much bigger than available RAM, which is a challenge since R by default runs using R  
    * "R is not well suited to working with data larger than 10%-20% of a computer's RAM" - The R Installation and Administration Manual  
    * When a computer runs out of RAM, it "swaps" to the hard drive, vastly slowing down the calculations  
* A more scalable solution is as follows  
	* Move a subset of data in to RAM  
    * Process the subset  
    * Keep the results and discard the subset  
* Code may be slow due to complexity of calculations  
	* Consider the disk operations needed  
* Benchmarking using microbenchmark() can be critical  
  
Working with "out of core" objects using the Bigmemory Project:  
  
* Package "bigmemory" was written by Kane (instructor for this course) to store, manipulate, and process matrices exceeding RAM  
	* Core object is a big.matrix and it is designed to manage situations where disk space is much greater than RAM  
    * The process of moving data to RAM only when needed is called "out of core" processing  
* By default, a big.matrix keeps data on the disk, only moving the data to RAM as needed  
	* The movements to/from RAM are implicit, which is to say that they are managed by the package  
    * Only a single import is needed  
* The big.matrix is created using big.matrix(nrow=, ncol=, init=, type=, backingfile=, descriptorfile=)  
	* The nrow, ncol are the same as matrix(), while init is the initial value to assign everywhere and type is a quoted type such as "double" or "integer"  
    * The backingfile is a quoted file name that will hold the binary representation of the big.matrix on the disk, with extension .bin  
    * The descriptorfile is a quoted file name that will hold some metadata such as the number of rows/columns, name, and the like  
* Supposing that x is a big,matrix, then the default print(x) obtained by x on the command line is to show a few slots/pointers  
	* To have contents of x printed, use x[ , ]  
    * Assignments can be made using x[myRow, myColumn] <- myValue  
* The read.big.matrix() function is meant to look similar to read.table() but, in addition, needs to know:  
	* what type of numeric values you want to read ("char", "short", "integer", "double")  
    * name of the file that will hold the matrix's data (the backing file)  
    * name of the file to hold information about the matrix (a descriptor file)   
    * Result will be a file on the disk holding the value read in along with a descriptor file which holds extra information (like the number of columns and rows) about the resulting big.matrix object  
* A final advantage to using big.matrix is that if you know how to use R's matrices, then you know how to use a big.matrix  
	* You can subset columns and rows just as you would a regular matrix, using a numeric or character vector and the object returned is an R matrix  
    * Likewise, assignments are the same as with R matrices and after those assignments are made they are stored on disk and can be used in the current and future R sessions  
    * One thing to remember is that $ is not valid for getting a column of either a matrix or a big.matrix  
  
References vs. Copies:  
  
* Can subset and make assignments to a big.matrix much like a matrix  
* There are a few key differences between a big.matrix and a matrix  
	* big.matrix is stored on the disk (persists across R sessions, can be shared across R sessions)  
    * R typically makes copies during assignment, which is why changing a variable inside a function (playing with the copy) has no impact on the variable outside the function  
    * However, some objects such as environments are not copied, so modifying them inside a function modified them globally (outside the function) also  
    * The big.matrix is not copied, and is instead a reference object; thus, you have to explicitly request a copy, which means 1) you have more control, but 2) you need to be more careful  
* The reference vs. copy for big.matrix objects seems in some ways similar to Python  
	* a <- b will set a to reference the same data as b; changing a or changing b means changing both  
    * a <- deepcopy() will produce a copy of a and assign it to b; much like a = b[:] in Python  
  
Example code includes:  
```{r}

# Load the microbenchmark package
library(microbenchmark)

# Compare the timings for sorting different sizes of vector
mb <- microbenchmark(
  # Sort a random normal vector length 1e5
  "1e5" = sort(rnorm(1e5)),
  # Sort a random normal vector length 2.5e5
  "2.5e5" = sort(rnorm(2.5e5)),
  # Sort a random normal vector length 5e5
  "5e5" = sort(rnorm(5e5)),
  "7.5e5" = sort(rnorm(7.5e5)),
  "1e6" = sort(rnorm(1e6)),
  times = 10
)

# Plot the resulting benchmark object
plot(mb)


# Load the bigmemory package
library(bigmemory)

# Create the big.matrix object: x
x <- read.big.matrix("./RInputFiles/mortgage-sample.csv", header = TRUE, 
                     type = "integer", 
                     backingfile = "mortgage-sample.bin", 
                     descriptorfile = "mortgage-sample.desc")
    
# Find the dimensions of x
dim(x)


# Attach mortgage-sample.desc
mort <- attach.big.matrix("mortgage-sample.desc")

# Find the dimensions of mort
dim(mort)

# Look at the first 6 rows of mort
head(mort)


# Create mort
mort <- attach.big.matrix("mortgage-sample.desc")

# Look at the first 3 rows
mort[1:3, ]

# Create a table of the number of mortgages for each year in the data set
table(mort[, "year"])

a <- getLoadedDLLs()
length(a)

R.utils::gcDLLs()

a <- getLoadedDLLs()
length(a)

# Load the biganalytics package (error in loading to Knit file, works OK otherwise)
library(biganalytics)

# Get the column means of mort
colmean(mort)

# Use biganalytics' summary function to get a summary of the data
summary(mort)


# Use deepcopy() to create first_three
first_three <- deepcopy(mort, cols = 1:3, 
                        backingfile = "first_three.bin", 
                        descriptorfile = "first_three.desc")

# Set first_three_2 equal to first_three
first_three_2 <- first_three

# Set the value in the first row and first column of first_three to NA
first_three[1, 1] <- NA

# Verify the change shows up in first_three_2
first_three_2[1, 1]

# but not in mort
mort[1, 1]

```
  
  
  
***
  
Chapter 2 - Processing and Analyzing Data with bigmemory  
  
The Bigmemory Suite of Packages:  
  
* Many packages have been designed to work together with a big.matrix object  
	* biganalytics - summarizing  
    * bigtabulate - split and tabulate (includes the bigtable(x, quotedColumnVector))  
    * bigalgebra - linear algenra  
    * bigpca - PCA  
    * bigFastLM - linear regressions  
    * biglasso - lasso regressions  
    * bigrf - random forests  
  
* FHFA Dataset has data about millions of mortgages - difference in ownership rates, defaults, etc.  
	* Course will use a 70,000 record subset  
    * Raw data (full 2.5 GB dataset) available at FHFA (fhfa.gov)  
    * Code works the same on subsets and full data sets  
  
Split-Apply-Combine (aka Split-Compute-Combine), run in this course using split() Map() Reduce():  
  
* The split() function partitions the data, whether randomly or based on a factor variable  
	* split(myData, myFactor) will produce a list, with each element of the list containing the requested data (one per myFactor)  
* The Map() function processes each of the partitions  
	* Map(myFunction, mySplitList) will apply the myFunction to each of the items in the mySplitList, with the output a list named like mySplitList  
* The Reduce() function combines the (typically processed) data from a list  
	* Reduce(myFunction, myMapList) will apply the myFunction while combining the items in myMapList  
    * A common function might be rbind or '+' (add them up)  
  
Visualize results using tidyverse:  
  
* The pipe (%>%) operator works well with many of the big.matrix functions, since the first argument is a dataset  
* Can combine some of big.matrix processing outputs with standard packages like dplyr and tidyr and ggplot  
  
Limitations of bigmemory - process is useful for dense, numeric matrices that can be stored on hard disk:  
  
* Underlying structures are compatible with low-level linear algebra libraries for fast fitting  
* If you have different column types, you can try the ff package (similar to bigmemory but includes structures like a data.frame)  
* The bigmemory object is said to be "random access", which means it is equally easy to get access to any specific component  
* There are some big drawbacks to the "random access" capabilities, however  
	* Cannot add rows or columns - need to create an entirely new object and port over the relevant data  
    * Need enough disk space to hold the entire matrix in a block  
    * Can instead use other tools to process data using a "continuous chunks" approach - discussed in the next chapter  
  
Example code includes:  
```{r}

library(bigtabulate)
library(tidyr)
library(ggplot2)
library(biganalytics)
library(dplyr)


race_cat <- c('Native Am', 'Asian', 'Black', 'Pacific Is', 'White', 'Two or More', 'Hispanic', 'Not Avail')

# Call bigtable to create a variable called race_table
race_table <- bigtable(mort, "borrower_race")

# Rename the elements of race_table
names(race_table) <- race_cat
race_table


# Create a table of the borrower race by year
race_year_table <- bigtable(mort, c("borrower_race", "year"))

# Convert rydf to a data frame
rydf <- as.data.frame(race_year_table)

# Create the new column Race
rydf$Race <- race_cat

# Let's see what it looks like
rydf


female_residence_prop <- function(x, rows) {
    x_subset <- x[rows, ]
    # Find the proporation of female borrowers in urban areas
    prop_female_urban <- sum(x_subset[, "borrower_gender"] == 2 & 
                                 x_subset[, "msa"] == 1) / 
        sum(x_subset[, "msa"] == 1)
    # Find the proporation of female borrowers in rural areas
    prop_female_rural <- sum(x_subset[, "borrower_gender"] == 2 & 
                                 x_subset[, "msa"] == 0) / 
        sum(x_subset[, "msa"] == 0)
    
    c(prop_female_urban, prop_female_rural)
}

# Find the proportion of female borrowers in 2015
female_residence_prop(mort, mort[, "year"] == 2015)


# Split the row numbers of the mortage data by year
spl <- split(1:nrow(mort), mort[, "year"])

# Call str on spl
str(spl)


# For each of the row splits, find the female residence proportion
all_years <- Map(function(rows) female_residence_prop(mort, rows), spl)

# Call str on all_years
str(all_years)


# Collect the results as rows in a matrix
prop_female <- Reduce(rbind, all_years)

# Rename the row and column names
dimnames(prop_female) <- list(names(all_years), c("prop_female_urban", "prop_femal_rural"))

# View the matrix
prop_female


# Convert prop_female to a data frame
prop_female_df <- as.data.frame(prop_female)

# Add a new column Year
prop_female_df$Year <- row.names(prop_female_df)

# Call gather on prop_female_df
prop_female_long <- gather(prop_female_df, Region, Prop, -Year)

# Create a line plot
ggplot(prop_female_long, aes(x = Year, y = Prop, group = Region, color = Region)) + 
    geom_line()


# Call summary on mort
summary(mort)

bir_df_wide <- bigtable(mort, c("borrower_income_ratio", "year")) %>% 
    as.data.frame() %>% 
    tibble::rownames_to_column() %>% 
    filter(rowname %in% c(1, 2, 3)) %>% 
    select(-rowname) %>%
    # Create a new column called BIR with the corresponding table categories
    mutate(BIR = c(">=0,<=50%", ">50, <=80%", ">80%"))

bir_df_wide

bir_df_wide %>% 
    # Transform the wide-formatted data.frame into the long format
    gather(Year, Count, -BIR) %>%
    # Use ggplot to create a line plot
    ggplot(aes(x = Year, y = Count, group = BIR, color = BIR)) + 
    geom_line()

```
  
  
  
***
  
Chapter 3 - Working with iotools  
  
Introduction to chunk-wise processing - solution to challenges from bigmemory:  
  
* The iotools allows for processing the data in "chunks", allowing for data frames, data across many machines, and the like  
* Can process chunks either sequentially (keep as needed after each chunk runs) or independently  
	* Independent processing is typically harder to code (final result must be combined), but allows for parallel processing  
* Sometimes Split-Apply-Combine cannot be made to work, such as trying to find a median (even keeping some extra data per chunk -- such as sum and count when end goal is mean -- will not work)  
	* Fortunately, most regressions can be successfully run using the Split-Apply-Combine methodology  
* An operation that gives the same answer whether you apply it to an entire data set or to chunks of a data set and then on the results on the chunks is sometimes called foldable  
	* The max() and min() operations are an example of this  
  
First look at iotools: Importing data:  
  
* Basic components of chunk-wise processing include 1) load pieces of data, 2) convert to native objects, 3) perform computation and store results, and 4) repeated as needed until finished  
* Loading data often takes more time than processing the data (retrieval from disk and conversion to readable formats)  
* The iotools package is designed to separate the physical loading of data and the parsing of data in to R objects for better flexibility and performance  
	* readAsRaw() reads the entire data in to a raw vector  
    * read.chunk() reads the data in chunks in to a raw vector  
* The iotools can then parse the data in to either a matrix or a data frame  
	* mstrsplit() converts raw data in to a matrix  
    * dstrsplit() converts raw data in to a data frame  
    * read.delim.raw() = readAsRaw() + dstrsplit()  
* Processing contiguous chunks means there is no need to have read all the data in advance (such as to create the spl vector by 1:nrows by myVar)  
* When processing a sequence of contiguous chunks of data on a hard drive, iotools can turn a raw object into a data.frame or matrix while - at the same time - retrieving the next chunk of data  
    * These optimizations allow iotools to quickly process very large files  
  
Using chunk.apply - effectively moves away from what is functionally a "for loop" to allow better parallel processing:  
  
* iotools is the basis of hmr which allows for running R on TB of data using Hadoop  
* The general usage is chunk.apply(myFile=, myFunction=, CH.MAX.SIZE=)  # this will apply myFunction across chunks of size CH.MAX.SIZE in myFile  
	* Output will be a matrix where each row is one of the chunks and each column is one of (or the only) output from myFunction for that chunk  
    * There is an optional parallel= option; the argument supplied is the number of parallel clusters to be used  
* By default, chunk.apply() aggregates the processed data using the rbind() function  
	* This means that you can create a table from each of the chunks and then add up the rows of the resulting matrix to get the total counts for the table  
* When the parallel parameter is set to a value greater than one on Linux and Unix machine (including the Mac) multiple processes read and process data at the same time thereby reducing the execution time  
	* On Windows the parallel parameter is ignored  
  
Example code includes:  
```{r}

foldable_range <- function(x) {
  if (is.list(x)) {
    # If x is a list then reduce it by the min and max of each element in the list
    c(Reduce(min, x), Reduce(max, x))
  } else {
    # Otherwise, assume it's a vector and find it's range
    range(x)
  }
}

# Verify that foldable_range() works on the record_number column
foldable_range(mort[, "record_number"])


# Split the mortgage data by year
spl <- split(1:nrow(mort), mort[, "year"])

# Use foldable_range() to get the range of the record numbers
foldable_range(Map(function(s) foldable_range(mort[s, "record_number"]), spl))


# Load the iotools and microbenchmark packages
library(iotools)
library(microbenchmark)

# Time the reading of files
microbenchmark(
    # Time the reading of a file using read.delim five times
    read.delim("./RInputFiles/mortgage-sample.csv", header = FALSE, sep = ","),
    # Time the reading of a file using read.delim.raw five times
    read.delim.raw("./RInputFiles/mortgage-sample.csv", header = FALSE, sep = ","),
    times = 5
)


# Read mortgage-sample.csv as a raw vector
raw_file_content <- readAsRaw("./RInputFiles/mortgage-sample.csv")

# Convert the raw vector contents to a matrix
mort_mat <- mstrsplit(raw_file_content, sep = ",", type = "integer", skip = 1)

# Look at the first 6 rows
head(mort_mat)

# Convert the raw file contents to a data.frame
mort_df <- dstrsplit(raw_file_content, sep = ",", col_types = rep("integer", 16), skip = 1)

# Look at the first 6 rows
head(mort_df)


# We have created a file connection fc to the "mortgage-sample.csv" file and read in the first line to get rid of the header.
# Define the function to apply to each chunk
make_table <- function(chunk) {
    # Read each chunk as a matrix
    x <- mstrsplit(chunk, type = "integer", sep = ",")
    # Create a table of the number of borrowers (column 3) for each chunk
    table(x[, 3])
}

# Create a file connection to mortgage-sample.csv
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")

# Read the first line to get rid of the header
(col_names <- readLines(fc, n = 1))
(col_names <- lapply(str_split(col_names, '\\",\\"'), FUN=function(x) { str_replace(x, '\\"', '') })[[1]])

# Read the data in chunks
counts <- chunk.apply(fc, make_table, CH.MAX.SIZE = 1e5)

# Close the file connection
close(fc)

# Print counts
counts

# Sum up the chunks
colSums(counts)


msa_map <- c("rural", "urban")
# Define the function to apply to each chunk
make_msa_table <- function(chunk) {
    # Read each chunk as a data frame
    x <- dstrsplit(chunk, col_types = rep("integer", length(col_names)), sep = ",")
    # Set the column names of the data frame that's been read
    colnames(x) <- col_names
    # Create new column, msa_pretty, with a string description of where the borrower lives
    x$msa_pretty <- msa_map[x$msa + 1]
    # Create a table from the msa_pretty column
    table(x$msa_pretty)
}

# Create a file connection to mortgage-sample.csv
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")

# Read the first line to get rid of the header
readLines(fc, n = 1)

# Read the data in chunks
counts <- chunk.apply(fc, make_msa_table, CH.MAX.SIZE = 1e5)

# Close the file connection
close(fc)

# Aggregate the counts as before
colSums(counts)


iotools_read_fun <- function(parallel) {
    fc <- file("./RInputFiles/mortgage-sample.csv", "rb")
    readLines(fc, n = 1)
    chunk.apply(fc, make_msa_table,
                CH.MAX.SIZE = 1e5, parallel = parallel)
    close(fc)
}

# Benchmark the new function
microbenchmark(
    # Use one process
    iotools_read_fun(1), 
    # Use three processes
    iotools_read_fun(3), 
    times = 20
)


```
  
  
  
***
  
Chapter 4 - Case Study: Preliminary Analysis of Housing Data  
  
Overview of types of analysis for this chapter:  
  
* Compare proportions of people receiving mortgages  
* Amount of "missingness" in the data  
* Changes in 1) mortgage demographic proportions over time, and 2) city vs. rural mortgages, and 3) proportions of federally insured loans  
  
Are the data missing at random?  
  
* Missing data is pervasive, including in this housing dataset  
* Three components of missing data  
	* Missing Completely at Random (MCAR) - no way to predict where/what, meaning rows with missing data can just be dropped  
    * Missing at Random (MAR) - missingness is dependent on variables in the dataset, meaning that multiple imputation can be successful  
    * Missing Not At Random (MNAR) - typically due to deterministic relationships between missing data and other variables, beyond the scope of this course  
* Assumption for this exercise will be that data are checked for MAR and assumed to be MCAR if they are not MAR  
	* For each column, recode the column as a 1/0 for missing, then run a logit on all the other variables  
    * If the other variables have a statistically significant prediction effect on the 1/0 column, then that column is MAR rather than MCAR  
    * Need to have a smart p-value for significance depending on number of regressions that have been run  
  
Analyzing the Housing Data:  
  
* Adjusted counts - adjusting group sizes allows you to compare different groups as though they were the same size  
* Proportional change can show growth (or decline) of groups over time  
  
Borrower Lending Trends: City vs. Rural:  
  
* Looking at city (MSA == 1) vs rural  
* Looking at federally guaranteed loans  
	* Can use Borrower Income Ratio (borrower income divided by median income in the area)  
  
Wrap up:  
  
* Split-Compute-Combine (aka Split-Apply-Combine) as enabled by bigmemory and iotools  
* Operations can be run on a single machine in series, a single machine in parallel, or across multiple machines  
* Summary of the bigmemory approach  
    * Good for dense, large matrices that might otherwise overhwlem RAM  
    * Looks like a regular R matrix  
* Summary of the iotools approach:  
    * Good for much larger data that can be processed in sequential chunks  
    * More flexible than bigmemory in that it can handle data frames and files saved on multiple disks  
  
Example code includes:  
```{r}

# Create a table of borrower_race column
race_table <- bigtable(mort, "borrower_race")

# Rename the elements
names(race_table) <- race_cat[as.numeric(names(race_table))]

# Find the proportion
race_table[1:7] / sum(race_table[1:7])

mort_names <- col_names

# Create table of the borrower_race 
race_table_chunks <- chunk.apply(
    "./RInputFiles/mortgage-sample.csv", function(chunk) { 
        x <- mstrsplit(chunk, sep = ",", type = "integer") 
        colnames(x) <- mort_names 
        table(x[, "borrower_race"])
}, CH.MAX.SIZE = 1e5)

# Add up the columns
race_table <- colSums(race_table_chunks)

# Find the proportion
borrower_proportion <- race_table[1:7] / sum(race_table[1:7])

pop_proportion <- c(0.009, 0.048, 0.126, 0.002, 0.724, 0.029, 0.163)
names(pop_proportion) <- race_cat[1:7]
# Create the matrix
matrix(c(pop_proportion, borrower_proportion), byrow = TRUE, nrow = 2,
  dimnames = list(c("Population Proportion", "Borrower Proportion"), race_cat[1:7]))


# Create a variable indicating if borrower_race is missing in the mortgage data
borrower_race_ind <- mort[, "borrower_race"] == 9

# Create a factor variable indicating the affordability
affordability_factor <- factor(mort[, "affordability"])

# Perform a logistic regression
summary(glm(borrower_race_ind ~ affordability_factor, family = binomial))


# Open a connection to the file and skip the header
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")
readLines(fc, n = 1)

# Create a function to read chunks
make_table <- function(chunk) {
    # Create a matrix
    m <- mstrsplit(chunk, sep = ",", type = "integer")
    colnames(m) <- mort_names
    # Create the output table
    bigtable(m, c("borrower_race", "year"))
}

# Import data using chunk.apply
race_year_table <- chunk.apply(fc, make_table)

# Close connection
close(fc)

# Cast it to a data frame
rydf <- as.data.frame(race_year_table)

# Create a new column Race with race/ethnicity
rydf$Race <- race_cat


# Note: We removed the row corresponding to "Not Avail".
# View rydf
rydf <- 
    rydf %>% 
    filter(Race !="Not Avail")
rydf 

# View pop_proportion
pop_proportion

# Gather on all variables except Race
rydfl <- gather(rydf, Year, Count, -Race)

# Create a new adjusted count variable
rydfl$Adjusted_Count <- rydfl$Count / pop_proportion[rydfl$Race]

# Plot
ggplot(rydfl, aes(x = Year, y = Adjusted_Count, group = Race, color = Race)) + 
    geom_line()


# View rydf
rydf

# Normalize the columns
for (i in seq_len(nrow(rydf))) {
  rydf[i, 1:8] <- rydf[i, 1:8] / rydf[i, 1]
}

# Convert the data to long format
rydf_long <- gather(rydf, Year, Proportion, -Race)

# Plot
ggplot(rydf_long, aes(x = Year, y = Proportion, group = Race, color = Race)) + 
    geom_line()


# Open a connection to the file and skip the header
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")
readLines(fc, n = 1)

# Create a function to read chunks
make_table <- function(chunk) {
    # Create a matrix
    m <- mstrsplit(chunk, sep = ",", type = "integer")
    colnames(m) <- mort_names
    # Create the output table
    bigtable(m, c("msa", "year"))
}

# Import data using chunk.apply
msa_year_table <- chunk.apply(fc, make_table)

# Close connection
close(fc)

# Convert to a data frame
df_msa <- as.data.frame(msa_year_table)

# Rename columns
df_msa$MSA <- c("rural", "city")

# Gather on all columns except Year
df_msa_long <- gather(df_msa, Year, Count, -MSA)

# Plot 
ggplot(df_msa_long, aes(x = Year, y = Count, group = MSA, color = MSA)) + 
    geom_line()


# Tabulate borrower_income_ratio and federal_guarantee
ir_by_fg <- bigtable(mort, c("borrower_income_ratio", "federal_guarantee"))

# Label the columns and rows of the table
income_cat <- c('0 <= 50', '50 < 80', '> 80', 'Not Applicable')
guarantee_cat <- c('FHA/VA', 'RHS', 'HECM', 'No Guarantee')
dimnames(ir_by_fg) <- list(income_cat, guarantee_cat)

# For each row in ir_by_fg, divide by the sum of the row
for (i in seq_len(nrow(ir_by_fg))) {
  ir_by_fg[i, ] = ir_by_fg[i, ] / sum(ir_by_fg[i, ])
}

# Print
ir_by_fg


# Quirky fix so that the files can be used again later
rm(mort)
rm(x)
rm(first_three)
rm(first_three_2)
gc()

```
  
  
  
***
  
###_Working with Web Data in R_  
  
Chapter 1 - Downloading Files and Using API Clients  
  
Introduction: Working with Web Data in R:  
  
* Methods for getting data from the internet in to R - frequently automatic, such as giving an internet address to read.csv()  
* Using the httr package (tidyverse) to query API using GET() and POST()  
* Using JSON and XML formats (nested data structures)  
* CSS (cascading style sheets) for extracts  
* Can use download.file() so that there is no need for repeatedly querying the same remote files  
* You could use write.table(), but then you have to worry about accidentally writing out data in a format R can't read back in  
	* An easy way to avoid this risk is to use saveRDS() and readRDS(), which save R objects in an R-specific file format, with the data structure intact  
    * That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in  
    * saveRDS() takes two arguments, object, pointing to the R object to save and file pointing to where to save it to  
    * readRDS() expects file, referring to the path to the RDS file to read in  
  
Understanding Application Programming Interfaces (API) - automatically handling data changes:  
  
* Data are frequently made available by way of API  
	* "websites, but for machines", allowing you to query/download data automatically  
* R has several API interaction capabilities  
	* Native interfaces to API  
    * Hides API complexity  
    * Allows for reading data as R object  
* Can find R packages for API by googling CRAN - packages frequently exist already  
	* Example is library(pageviews) to get pageview counts  
  
Access tokens and API:  
  
* API cients (by way of R packages) abstract away the complications of getting the data  
* The API owner frequently does care how your API client interacts with it, though  
	* Overwhelming API causes problems for owner and many users  
    * Access tokens are sometimes used to monitor and throttle usage  
* Getting access tokens is frequently straightforward  
	* Usually requires registering an e-mail address  
    * Sometimes requires an explanation  
    * Example is www.wordnik.com, which can be accessed by way of library(bidnik)  
  
Example code includes:  
```{r cache=TRUE}

# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"

# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)

# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)

# Examine the objects with head()
head(csv_data)
head(tsv_data)


# Download the file with download.file()
download.file(url = csv_url, destfile = "./RInputFiles/feed_data.csv")

# Read it in with read.csv()
csv_data <- read.csv("./RInputFiles/feed_data.csv")


# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight ** 2

# Save it to disk with saveRDS()
saveRDS(csv_data, "./RInputFiles/modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS("./RInputFiles/modified_feed_data.RDS")

# Examine modified_feed_data
str(modified_feed_data)


# Load pageviews
# library(pageviews)

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- pageviews::article_pageviews(project = "en.wikipedia", "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)


# Load birdnik
# library(birdnik)

# Get the word frequency for "vector", using api_key to access it
# vector_frequency <- word_frequency(api_key, "vector")


```
  
  
  
***
  
Chapter 2 - Using httr to interact with API Directly  
  
GET and POST requests in theory - https and web requests in theory:  
  
* Interactions on the internet can be though of as the client-server communication  
* The most common request is "GET", which is the client request for something from the server  
	* The parallel is "POST", which is asking the server to accept something from the client  
    * HEAD is similar to head()  
    * DELETE is a request to the server to get rid of something - typically not needed  
* The httr package enables basic communication in R  
	* response <- httr::GET(url=) # will get that url  
    * httr::content(response)  # will tell you about the response  
    * response <- httr::POST(url=) is for posting, and the recipient can figure out what to do with the data  
  
Graceful httr - code that responds appropriately and constructs its own url:  
  
* Error handling - all httr requests come back with an error code (status)  
	* Status: 200 (completed) - starts with 2/3 is usually fine  
    * Status: 404 (no clue where to look) - starts with 4 is usually error in your code  
    * Status: starts with 5 is usually error in their code  
    * Can check for bad codes with http_error()  
* URL construction - frequently most of the text stays the same, with just the occasional change in other components that do  
	* Directory based url are based on / and can be created using paste(sep="/") - very common, and very easy to create  
    * Parameter based url use text like https://fakeurl.com/api.php?a=1&b=2 and can be created using GET() with its named list of parameters  
    * GET("fakeurl.com/api.php", query = list(fruit = "peaches", day = "thursday"))  
  
Respectful API Usage - usage that works for the API owners as well as the clients:  
  
* User agents - bits of text that ID your browser, give the server some idea of what you are trying to do, can be set with user_agent(), add an e-mail address, etc.  
* Many API have rate-limiter capability - exceed and you will be blocked  
	* Keep an interval between requests, such as having a sleep (or similar) capability between requests using Sys.sleep()  
  
Example code includes:  
```{r cache=TRUE}

# Load the httr package
library(httr)

# Make a GET request to http://httpbin.org/get
get_result <- GET("http://httpbin.org/get")

# Print it to inspect it
# get_result


# Make a POST request to http://httpbin.org/post with the body "this is a test"
# post_result <- POST(url="http://httpbin.org/post", body="this is a test")

# Print it to inspect it
# post_result


url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102"
# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)


fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
    warning("The request failed")
} else {
    content(request_result)
}


# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", 1, sep = "/")

# Make a GET call with it
result <- GET(directory_url)


# Create list with nationality and country elements
query_params <- list(nationality = "americans", 
    country = "antigua")
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)

# Print parameter_response
parameter_response


# Do not change the url
# url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"

# Add the email address and the test sentence inside user_agent()
# server_response <- GET(url, user_agent("my@email.address this is a test"))


# Construct a vector of 2 URLs
urls <- c("http://fakeurl.com/api/1.0/", "http://fakeurl.com/api/2.0/")

for(url in urls){
    # Send a GET request to url
    result <- GET(url)
    # Delay for 5 seconds between requests
    Sys.sleep(1)
}


get_pageviews <- function(article_title){
    
    url <- paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents", article_title, "daily/2015100100/2015103100", sep = "/") 
    
    response <- GET(url, user_agent("my@email.com this is a test")) 
    
    if(http_error(response)){ 
        stop("the request failed" ) 
    } else { 
        result <- content(response) 
        return(result) 
    }
}


```
  
  
  
***
  
Chapter 3 - Handling JSON and XML  
  
JSON is a dictionary-like format (plain text) foe sending data on the internet:  
  
* All JSON structures are made up of objects (name-value pairs in parentheses) {"a" : "b" , "c" : "d"} and arrays [1977, 1980]  
	* Values can be "string", number, true, false, null, another object or array  
    * Complicated hierarchy can easily be represented  
* Can find the type of data using httr::http_type(response)  
  
Manipulating JSON - lists are the natural R hierarchy for JSON:  
  
* fromJSON() will return named lists (if key-value pairs) and unnamed lists (if arrays)  
	* The simplifyDataFrame = TRUE argument will pull everything together in to a data frame if possible  
    * Alternately, can run lapply (or similar) over the list that has been returned  
* One way to extract relevant data from that list is to use a package specifically designed for manipulating lists, rlist  
	* rlist provides two particularly useful functions for selecting and combining elements from a list: list.select() and list.stack()  
    * list.select() extracts sub-elements by name from each element in a list  
    * For example using the parsed movies data from the video (movies_list), we might ask for the title and year elements from each element: list.select(movies_list, title, year)  
    * The result is still a list, that is where list.stack() comes in. It will stack the elements of a list into a data frame: list.stack(list.select(movies_list, title, year))  
  
XML Structure - plain text like JSON, but with a very different structure:  
  
* Consists of markup (tags) and struture (data)  
	* Tags begin with < and end with >  
    * Typically <tag> some stuff </tag>  
    * Can privide attributes inside of tags, such as <tag myValue = myInput> more stuff </tag>  
    * There is no formal standard, though attributes are usually used only for metadata  
* XML is a hierarchical structure, and includes everything between the start tag and the end tag  
	* Each element can contain many other elements  
    * Sub-elements are considered to be "children" of the "parent" element they are part of; "children" of the same "parent" are called "sibling" tags  
* Just like JSON, you should first verify the response is indeed XML with http_type() and by examining the result of content(r, as = "text")  
	* Then you can turn the response into an XML document object with read_xml()  
    * One benefit of using the XML document object is the available functions that help you explore and manipulate the document  
    * For example xml_structure() will print a representation of the XML document that emphasizes the hierarchical structure by displaying the elements without the data  
  
XPATH - language for specifying nodes in an XML document:  
  
* XPATH looks a lot like file.path, since it uses forward slash / to find the requested sub-nodes  
* xml_find_all(x=, xpath=) # x is the object such as movies_xml and path is the xpath such as "/movies/movie/title"; will return a "node set"  
	* xml_text() run on a "node set" will return the data in an easier to digest format  
    * The // means "any node at any level below", so "//title" will grab any node, from any path, that is tagged as "title"  
    * The @ means to extract an attribute; so, //movie/@episode will create a node set of the episodes under the movie tags  
* Alternate ways to extract attributes include xml_attr() and xml_attrs()  
	* xml_attrs() takes a nodeset and returns all of the attributes for every node in the nodeset  
    * xml_attr() takes a nodeset and an additional argument attr to extract a single named argument from each node in the nodeset  
  
Example code includes:  
```{r cache=TRUE, eval=FALSE}

rev_history <- function(title, format = "json"){
  if (title != "Hadley Wickham") {
    stop('rev_history() only works for `title = "Hadley Wickham"`')
  }
  
  if (format == "json"){
    resp <- readRDS("had_rev_json.rds")
  } else if (format == "xml"){
    resp <- readRDS("had_rev_xml.rds")
  } else {
    stop('Invalid format supplied, try "json" or "xml"')
  }
  resp  
}

test_json <- "{\"continue\":{\"rvcontinue\":\"20150528042700|664370232\",\"continue\":\"||\"},\"query\":{\"pages\":{\"41916270\":{\"pageid\":41916270,\"ns\":0,\"title\":\"Hadley Wickham\",\"revisions\":[{\"user\":\"214.28.226.251\",\"anon\":\"\",\"timestamp\":\"2015-01-14T17:12:45Z\",\"comment\":\"\",\"contentformat\":\"text/x-wiki\",\"contentmodel\":\"wikitext\",\"*\":\"'''Hadley Mary Helen Wickham III''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"},{\"user\":\"73.183.151.193\",\"anon\":\"\",\"timestamp\":\"2015-01-15T15:49:34Z\",\"comment\":\"\",\"contentformat\":\"text/x-wiki\",\"contentmodel\":\"wikitext\",\"*\":\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"},{\"user\":\"FeanorStar7\",\"timestamp\":\"2015-01-24T16:34:31Z\",\"comment\":\"/* External links */ add LCCN and cats\",\"contentformat\":\"text/x-wiki\",\"contentmodel\":\"wikitext\",\"*\":\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"},{\"user\":\"KasparBot\",\"timestamp\":\"2015-04-26T19:18:17Z\",\"comment\":\"authority control moved to wikidata\",\"contentformat\":\"text/x-wiki\",\"contentmodel\":\"wikitext\",\"*\":\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"},{\"user\":\"Spkal\",\"timestamp\":\"2015-05-06T18:24:57Z\",\"comment\":\"/* Bibliography */  Added his new book, R Packages\",\"contentformat\":\"text/x-wiki\",\"contentmodel\":\"wikitext\",\"*\":\"'''Hadley Wickham''' is a  [[statistician]] from [[New Zealand]] who is currently Chief Scientist at [[RStudio]]<ref>{{cite web|url=http://washstat.org/wss1310.shtml |title=Washington Statistical Society October 2013 Newsletter |publisher=Washstat.org |date= |accessdate=2014-02-12}}</ref><ref>{{cite web|url=http://news.idg.no/cw/art.cfm?id=F66B12BB-D13E-94B0-DAA22F5AB01BEFE7 |title=60+ R resources to improve your data skills ( - Software ) |publisher=News.idg.no |date= |accessdate=2014-02-12}}</ref> and an [[Professors_in_the_United_States#Adjunct_professor|adjunct]] [[Assistant Professor]] of statistics at [[Rice University]].<ref name=\\\"about\\\">{{cite web|url=http://www.rstudio.com/about/ |title=About - RStudio |accessdate=2014-08-13}}</ref> He is best known for his development of open-source statistical analysis software packages for [[R (programming language)]] that implement logics of [[data visualisation]] and data transformation. Wickham completed his undergraduate studies at the [[University of Auckland]] and his PhD at [[Iowa State University]] under the supervision of Di Cook and Heike Hoffman.<ref>{{cite web|URL=http://blog.revolutionanalytics.com/2010/09/the-r-files-hadley-wickham.html |title= The R-Files: Hadley Wickham}}</ref> In 2006 he was awarded the [[John_Chambers_(statistician)|John Chambers]] Award for Statistical Computing for his work developing tools for data reshaping and visualisation.<ref>{{cite web|url=http://stat-computing.org/awards/jmc/winners.html |title=John Chambers Award Past winners|publisher=ASA Sections on Statistical Computing, Statistical Graphics,|date= |accessdate=2014-08-12}}</ref>\\n\\nHe is a prominent and active member of the [[R (programming language)|R]] user community and has developed several notable and widely used packages including [[ggplot2]], plyr, dplyr, and reshape2.<ref name=\\\"about\\\" /><ref>{{cite web|url=http://www.r-statistics.com/2013/06/top-100-r-packages-for-2013-jan-may/ |title=Top 100 R Packages for 2013 (Jan-May)! |publisher=R-statistics blog |date= |accessdate=2014-08-12}}</ref>\"}]}}}}"

# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")

# Check http_type() of resp_json
http_type(resp_json)

# Examine returned text with content()
content(resp_json, as="text")

# Parse response with content()
content(resp_json, as="parsed")

# Parse returned text with fromJSON()
library(jsonlite)
fromJSON(content(resp_json, as="text"))


# Load rlist
library(rlist)

# Examine output of this code
str(content(resp_json), max.level = 4)

# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract the user element
user_time <- list.select(revs, user, timestamp)

# Print user_time
user_time

# Stack to turn into a data frame
list.stack(user_time)


# Load dplyr
library(dplyr)

# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract user and timestamp
revs %>%
  bind_rows() %>%           
  select(user, timestamp)


# Load xml2
library(xml2)

# Get XML revision history
resp_xml <- rev_history("Hadley Wickham", format = "xml")

# Check response is XML 
http_type(resp_xml)

# Examine returned text with content()
rev_text <- content(resp_xml, as="text")
rev_text

# Turn rev_text into an XML document
rev_xml <- read_xml(rev_text)

# Examine the structure of rev_xml
str(rev_xml)


# Load xml2
library(xml2)

# Get XML revision history
resp_xml <- rev_history("Hadley Wickham", format = "xml")

# Check response is XML 
http_type(resp_xml)

# Examine returned text with content()
rev_text <- content(resp_xml, as="text")
rev_text

# Turn rev_text into an XML document
rev_xml <- read_xml(rev_text)

# Examine the structure of rev_xml
xml_structure(rev_xml)


# Find all nodes using XPATH "/api/query/pages/page/revisions/rev"
xml_find_all(rev_xml, "/api/query/pages/page/revisions/rev")

# Find all rev nodes anywhere in document
rev_nodes <- xml_find_all(rev_xml, "//rev")

# Use xml_text() to get text from rev_nodes
xml_text(rev_nodes)


# All rev nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")

# The first rev node
first_rev_node <- xml_find_first(rev_xml, "//rev")

# Find all attributes with xml_attrs()
xml_attrs(first_rev_node)

# Find user attribute with xml_attr()
xml_attr(first_rev_node, attr="user")

# Find user attribute for all rev nodes
xml_attr(rev_nodes, attr="user")

# Find anon attribute for all rev nodes
xml_attr(rev_nodes, attr="anon")


get_revision_history <- function(article_title){
  # Get raw revision response
  rev_resp <- rev_history(article_title, format = "xml")
  
  # Turn the content() of rev_resp into XML
  rev_xml <- read_xml(content(rev_resp, "text"))
  
  # Find revision nodes
  rev_nodes <- xml_find_all(rev_xml, "//rev")

  # Parse out usernames
  user <- xml_attr(rev_nodes, attr="user")
  
  # Parse out timestamps
  timestamp <- readr::parse_datetime(xml_attr(rev_nodes, "timestamp"))
  
  # Parse out content
  content <- xml_text(rev_nodes)
  
  # Return data frame 
  data.frame(user = user,
    timestamp = timestamp,
    content = substr(content, 1, 40))
}

# Call function for "Hadley Wickham"
get_revision_history(article_title = "Hadley Wickham")

```
  
  
  
***
  
Chapter 4 - Web Scraping with XPATH  
  
Web scraping 101 - sometimes a website does not have an API, so a different approach is required:  
  
* Web scraping is the process of grabbling the full html and then parsing the data as needed  
* The "selector" plug-in for a browser can be helpful for finding IDs associated with examples of interest  
* There is a package "rvest" that helps to simplify the process of web scraping  
	* rvest::read_html(url=)  # returns an XML document  
    * html_node() will extract contents with XPATH (???) - the argument to html_node should be the returned XML document from the previous step  
  
HTML structure - basically, content within tags, much like XML:  
  
* For example <p> This is a test </p> requests that "This is a test" be available in paragraph form  
* Attributes can be stored also, such as <a href="https://en.wikipedia.org/"> this is a test </a>  
* Parameters can incorporate formatting, style, and the like  
* The rvest package has the means for extracting the data from html  
	* html_text(x=) for text contents  
    * html_attr(x=, name=) to get a specific attribute  
    * html_name(x=) to get the tag name  
  
Reformatting data (especially to a rectangular format such as a data frame):  
  
* Turning html tables (tables are a structure in html) in to data frames  
	* They can be identified in raw html from <table> </table>  
    * They can be turned in to tables using html_table()  
    * Can assign column names using colnames() as per normal R  
* Turning html non-tables in to data frames  
	* Use data.frame() with the vectors of text or names or attributes or the like  
  
Example code includes:  
```{r cache=TRUE}

# Load rvest
library(rvest)

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Print test_xml
test_xml


test_node_xpath <- "//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]"
# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`
node <- html_node(x = test_xml, xpath = test_node_xpath)

# Print the first element of the result
node[1]


# The first thing we'll grab is a name, from the first element of the previously extracted table (now stored as table_element)
table_element <- node

# Extract the name of table_element
element_name <- html_name(table_element)

# Print the name
element_name


second_xpath_val <- "//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"fn\", \" \" ))]"
# Extract the element of table_element referred to by second_xpath_val and store it as page_name
page_name <- html_node(x = table_element, xpath = second_xpath_val)

# Extract the text from page_name
page_title <- html_text(page_name)

# Print page_title
page_title


# Turn table_element into a data frame and assign it to wiki_table
wiki_table <- html_table(table_element)

# Print wiki_table
wiki_table


# Rename the columns of wiki_table
colnames(wiki_table) <- c("key", "value")

# Remove the empty row from wiki_table
cleaned_table <- subset(wiki_table, !(key == ""))

# Print cleaned_table
cleaned_table

```
  
  
  
***
  
Chapter 5 - CSS Web Scraping and Final Case Study  
  
CSS (cascading style sheets) web scraping in theory:  
  
* CSS is for style, formatting, and the like  
* Groups of CSS commands are associated to a class, allowing the class to be used in multiple areas  
	* .class_a { color: black; }  
    * .class_b { color: red; }  
    * Specific html can then be addressed using <a class = "class_a" href=myHREFText> This is black </a>  
* CSS scraping is the concept of finding the class groups  
	* Works much like XPATH but will often grab many items rather than just a single element  
    * It's more common with CSS selectors to use html_nodes()  
    * To select elements with a certain class, you add a . in front of the class name  
    * If you need to select an element based on its id, you add a # in front of the id name  
    * For example if this element was inside your HTML document:  
    * <h1 class = "heading" id = "intro">  
    * Introduction  
    * </h1>  
    * You could select it by its class using the CSS selector ".heading", or by its id using the CSS selector "#intro"  
  
Final case study: Introduction:  
  
* Extracting an infobox from a Wikipedia page  
	1.  Get XML by way of API  
    2.  Extract infobox from the page  
    3.  Clean up and convert to data frame  
    4.  Wrap in a function for reproducibility  
  
Wrap up:  
  
* Downloading and reading flat files  
* Designing and using API clients  
* Web scraping using XPATHs and CSS  
  
Example code includes:  
```{r}

library(rvest)

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Print test_xml
test_xml

# Select the table elements
html_nodes(test_xml, css = "table")

# Select elements with class = "infobox"
html_nodes(test_xml, css = ".infobox")

# Select elements with id = "firstHeading"
html_nodes(test_xml, css = "#firstHeading")


# Extract element with class infobox
infobox_element <- html_nodes(test_xml, css = ".infobox")

# Get tag name of infobox_element
element_name <- html_name(infobox_element)

# Print element_name
element_name


# Extract element with class fn
page_name <- html_node(x = infobox_element, css=".fn")

# Get contents of page_name
page_title <- html_text(page_name)

# Print page_title
page_title


# Load httr
library(httr)

# The API url
base_url <- "https://en.wikipedia.org/w/api.php"

# Set query parameters
query_params <- list(action="parse", 
  page="Hadley Wickham", 
  format="xml")

# Get data from API
resp <- GET(url = "https://en.wikipedia.org/w/api.php", query = query_params)
    
# Parse response
resp_xml <- content(resp)


# Load rvest
library(rvest)

# Read page contents as HTML
page_html <- read_html(xml_text(resp_xml))

# Extract infobox element
infobox_element <- html_node(page_html, css=".infobox")

# Extract page name element from infobox
page_name <- html_node(infobox_element, css=".fn")

# Extract page name as text
page_title <- html_text(page_name)


# Your code from earlier exercises
wiki_table <- html_table(infobox_element)
colnames(wiki_table) <- c("key", "value")
cleaned_table <- subset(wiki_table, !key == "")

# Create a dataframe for full name
name_df <- data.frame(key = "Full name", value = page_title)

# Combine name_df with cleaned_table
wiki_table2 <- rbind(name_df, cleaned_table)

# Print wiki_table
wiki_table2


library(httr)
library(rvest)
library(xml2)

get_infobox <- function(title){
  base_url <- "https://en.wikipedia.org/w/api.php"
  
  # Change "Hadley Wickham" to title
  query_params <- list(action = "parse", 
    page = title, 
    format = "xml")
  
  resp <- GET(url = base_url, query = query_params)
  resp_xml <- content(resp)
  
  page_html <- read_html(xml_text(resp_xml))
  infobox_element <- html_node(x = page_html, css =".infobox")
  page_name <- html_node(x = infobox_element, css = ".fn")
  page_title <- html_text(page_name)
  
  wiki_table <- html_table(infobox_element)
  colnames(wiki_table) <- c("key", "value")
  cleaned_table <- subset(wiki_table, !wiki_table$key == "")
  name_df <- data.frame(key = "Full name", value = page_title)
  wiki_table <- rbind(name_df, cleaned_table)
  
  wiki_table
}

# Test get_infobox with "Hadley Wickham"
get_infobox(title = "Hadley Wickham")

# Try get_infobox with "Ross Ihaka"
get_infobox(title = "Ross Ihaka")

# Try get_infobox with "Grace Hopper"
get_infobox(title = "Grace Hopper")

```
  
  
  
***
  
###_Data Visualization in R with lattice_  
  
Chapter 1 - Basic plotting with lattice  
  
Introduction - general objectives:  
  
* Visualization may be for EDA or for reporting results  
* Three basic graphing capabilities in R  
	* Base - powerful but not flexible  
    * lattice - based on "Trellis graphics" (Cleveland)  
    * ggplot2 - based on "Grammar of Graphics" (Wilkinson)  
* This course will cover lattice graphics for both EDA and reporting  
* Focus will be on the USCancerRates dataset, with exploration of variance by gender and location  
	* histogram(~ x, data=) # lattice for make a histogram (default appears to be RELATIVE frequency by bin)  
    * xyplot(y ~ x, data=) # lattice for make an xy plot  
    * The modeling calls are similar to what would be seen in an lm()  
  
Optional arguments:  
  
* Plotting functions in lattice frequently require two arguments - formula and data set  
* Additional options are available and can be supplied to certain functions  
	* For example, histogram(~ x, data=, main=, xlab=) # will give the plot title "main" and the X-axis label "xlab"  
    * xyplot can also have a ylab=  
    * histogram can also have nint= (specifies the number of bins)  
    * The grid= argument of xyplot adds a background grid, while abline= adds a line with slope and intercept as specified  
* In the case of histogram(), the optional argument type controls what is plotted on the y-axis. It can take three values:  
	* "percent", the default, gives percentage or relative frequency  
    * "count" gives bin count, which is the default in hist()  
    * "density" gives a density histogram  
* The lattice function densityplot() creates kernel density plots (formula interface is similar to that of histogram())  
	* the formula should be written as ~ x to plot the values of the x column along the x-axis, and the estimated density on the y-axis  
    * A useful optional argument for densityplot() is plot.points, which can take values  
    * TRUE, the default, to plot the data points along the x-axis in addition to the density  
    * FALSE to suppress plotting the data points  
    * "jitter", to plot the points along the y-axis but with some random jittering in the y-direction so that overlapping points are easier to see  
  
Box and whisker plots and reordering elements:  
  
* Box and whisker plots are formed using bwplot(~ x, data=)  
* Can serve a similar purpose as a histogram or density plot, and the formula is correspondingly similar  
	* bwplot(y ~ x, data=) will make box plots for x, split by each level of y (which needs to be a factor/categorical)  
* The function reorder(myFactor, myData, myFunction, … ) will reorder factor variables for plotting  
	* For example, reorder(state, rate.male, median, na.rm=TRUE) will order the factor variable state by median(rate.male) in that state  
* Your task for this exercise is to produce a box-and-whisker plot where the whiskers extend to the data extremes  
    * These calculations are controlled by the coef argument of the R helper function boxplot.stats()  
	* A positive value of coef makes the whiskers extend to no more than coef times the length of the box  
    * The value of coef = 0 makes the whiskers extend to the data extremes  
  
Example code includes:  
```{r}

data(airquality)
str(airquality)

# Load the lattice package
library(lattice)


# Create the histogram 
histogram(~ Ozone, data = airquality)

# Create the histogram
histogram(~ Ozone, data = airquality, 
          # Specify number of bins
          nint = 15,
          # Specify quantity displayed on y-axis
          type = "count")


# Create the scatter plot
xyplot(Ozone ~ Solar.R, data = airquality)

# Create scatterplot
xyplot(Ozone ~ Temp, data = airquality,
       # Add main label
       main = "Environmental conditions in New York City (1973)", 
       # Add axis labels
       ylab = "Ozone (ppb)",
       xlab = "Temperature (Fahrenheit)")


# Create a density plot
densityplot(~ Ozone, data = airquality, 
    # Choose how raw data is shown
    plot.points = "jitter")



data(USCancerRates, package="latticeExtra")
str(USCancerRates)
rn_USCR <- row.names(USCancerRates)

# Create reordered variable
library(dplyr)
USCancerRates <-
    mutate(USCancerRates, 
           state.ordered = reorder(state, rate.female, median, na.rm = TRUE)
           )

# Create box and whisker plot
bwplot(state.ordered ~ rate.female, data = USCancerRates)

# Create box and whisker plot
bwplot(state.ordered ~ rate.female, data = USCancerRates, 
       # Change whiskers extent
       coef = 0)

```
  
  
  
***
  
Chapter 2 - Conditioning and the Formula Interface  
  
Conditioning - identify sources of variability in the data by examining sub-groups:  
  
* Small multiple design - conditioning/faceting approach  
* The conditioning operator in lattice is the single-pipe (|)  
	* xyplot(y ~ x | c, data=)  # co is the conditioning variable in this example  
    * Can use the conditioning operator in any function within the lattice framework  
* The plus (+) operator is another way to condition - means condition on more than one variable  
	* histogram(~ a + b, outer=TRUE, layout=c(1, 2), data=) will put a separate histogram for b below the separate histogram for a, keeping both on the same scale  
    * The outer command determines how to interpret a+b  
    * The layout=c(1, 2) means 1 column and 2 rows - general format is layout=c(ncol, nrow, npages)  
* Since count-based functions tend to have higher variances associated to higher means, the log transform for these can be valuable  
* lattice, unlike ggplot2, allows you to have data in a wide format  
  
Data summary and transformation - grouping:  
  
* Data summarization can be especially valuable for reporting  
	* For example, may want to summarize cancer rates by state (median county) rather than by county  
    * The tapply() function can be valuable for applying a function across a vector  
    * To get both genders on the same plot but in different colors, use xyplot(State ~ Rate, data=, grid=TRUE, groups=Gender)  # will treat the Gender as a separate group with different color on the same plot  
* New concept: groups - interpreted as a factor that defines sub-groups  
	* xyplot() and densityplot() support this, while histogram() does not  
    * Using auto.key = TRUE will add a legend telling which colors are associated to which groups  
* For more detailed control, the auto.key argument can be a list with various sub-components, the most useful of which are  
	* space: which can be "left", "right", "top", or "bottom"  
    * columns: specifies the number of columns in which to divide up the levels  
    * title: specifies a title for the legend  
  
Incorporating external data sources:  
  
* Can potentially split panels in to multiple pages or place multiple plots in the same pane  
	* For eample, could aggregate states by region and report states in the same region together  
    * The layout argument inside a lattice plotting function calls for layout=c(ncol, nrow)  
    * The between argument inside a lattice plotting function calls for spacing - bewteen=list(y=c(0, 0, 1, 0, 0)) will put a space of 1 between the third and fourth items  
* The outer=FALSE makes the conditioning variable in to a grouping variable - more effective visual with multiple plots together on the same pane  
* In a conditioned lattice plot, the panels are by default drawn starting from the bottom-left position, going right and then up  
	* This is patterned on the Cartesian coordinate system where the x-axis increases to the right and the y-axis increases from bottom to top  
* Often, want to change this so that the layout is similar to a matrix or table, where rows start at the top  
	* The layout of any conditioned lattice plot can be changed to follow this scheme by adding the optional argument as.table = TRUE  
  
The trellis object - lattice creates trellis objects rather than directly creating plots (as in base R):  
  
* Can run the class(), summary() and the like, with auto-print and/or print() making the plot visible  
* If you have a trellis object, the update() command can be used to modify the object  
	* In particular, their dimnames() are used as strip labels  
* Can think of the trellis object as being like a matrix, so t(trellisObject) will flip the rows/columns  
* Depending on the amount of space available, a conditioned plot may have too many combinations to be displayed effectively  
	* Such plots can be split into multiple pages using the layout argument  
    * But another convenient way to explore large lattice plots is to subset them like a matrix or array, using the [ indexing operator, to display only parts of the plot at a time  
  
Example code includes:  
```{r}

# The airquality dataset has been pre-loaded
str(airquality)

# Create a histogram
histogram(~ Ozone | factor(Month),
          data = airquality, 
          # Define the layout
          layout=c(2, 3),
          # Change the x-axis label
          xlab="Ozone (ppb)")


# USCancerRates has been pre-loaded
str(USCancerRates)

# Create a density plot
densityplot(~ rate.male + rate.female,
    data = USCancerRates, 
    outer = TRUE,
    # Suppress data points
    plot.points = FALSE,
    # Add a reference line
    ref=TRUE)


# Create a density plot
densityplot(~ rate.male + rate.female,
    data = USCancerRates,
    # Set value of 'outer' 
    outer=FALSE,
    # Add x-axis label
    xlab="Rate (per 100,000)",
    # Add a legend
    auto.key=TRUE,
    plot.points = FALSE,
    ref = TRUE)


xyplot(Ozone ~ Temp, airquality, groups = Month,
       # Complete the legend spec
       auto.key = list(space = "right", 
                       title = "Month", 
                       text = month.name[5:9]))


USCancerRates <- USCancerRates %>%
    mutate(division=state.division[match(state, state.name)])

# Create 'division.ordered' by reordering levels
USCancerRates <- 
  mutate(USCancerRates, 
         division.ordered = reorder(division, 
                                    rate.male + rate.female, 
                                    mean, na.rm = TRUE))

# Create conditioned scatter plot
xyplot(rate.female ~ rate.male | division.ordered,
       data = USCancerRates, 
       # Add reference grid
       grid = TRUE, 
       # Add reference line
       abline = c(0, 1))


# Levels of division.ordered
levels(USCancerRates$division.ordered)

# Specify the as.table argument 
xyplot(rate.female ~ rate.male | division.ordered,
       data = USCancerRates, 
       grid = TRUE, abline = c(0, 1),
       as.table=TRUE)


# Create box-and-whisker plot
bwplot(division.ordered ~ rate.male + rate.female,
       data = USCancerRates, 
       outer = TRUE, 
       # Add a label for the x-axis
       xlab="Rate (per 100,000)",
       # Add strip labels
       strip = strip.custom(factor.levels = c("Male", "Female")))


# Create "trellis" object
tplot <-
    densityplot(~ rate.male + rate.female | division.ordered, 
                data = USCancerRates, outer = TRUE, 
                plot.points = FALSE, as.table = TRUE)

# Change names for the second dimension
dimnames(tplot)[[2]] <- c("Male", "Female")

# Update x-axis label and plot
update(tplot, xlab = "Rate")


# Create "trellis" object
tplot <-
    densityplot(~ rate.male + rate.female | division.ordered, 
                data = USCancerRates, outer = TRUE, 
                plot.points = FALSE, as.table = TRUE)

# Inspect dimension
dim(tplot)
dimnames(tplot)

# Select subset retaining only last three divisions
tplot[7:9, ]

```
  
  
  
***
  
Chapter 3 - Controlling scales and graphical parameters  
  
Combining scales:  
  
* Can use dotplot(y ~ x | c + d, data=, as.table=TRUE) to have a conditioned dot-plot on c and d  
	* Expects a categorical variable on at least one of the axes (typically, but not always, y)  
* The default for axis limits is for them to be common across all the panels - typically, best for interpretation  
	* Can override the default behavior using the scales argument, a list with named components  
    * relation = "same" is the default  
    * relation = "free" allows independence for each panel  
    * relation = "sliced" allows different limits for each panel, but with same range (???)  
* The call to scales is fairly complicated  
	* scales = list(x = list(relation = "free")) # asks for an x-axis scale to be free  
* Some other useful sub-components of the scales argument are:  
	* tick.number: approximate number of tick marks / labels  
    * alternating: 1 puts labels on the left/bottom boundary, 2 on the right/top, and 3 on both sides. The value can be a vector, in which case it applies row-wise or column-wise  
    * rot: angle in degrees to rotate axis labels  
  
Logarithmic scales:  
  
* Can use dotplot(y ~ x | c, data=, groups=d, as.table=TRUE) will use d as a grouping variable with the plots only conditioned on c  
* Can use the log() transform directly on the y variable to help with visualizing the data  
* Alternately, can keep the data the same but just stretch the scales  
	* dotplot(y ~ x | c, data=, groups=d, scales=list(x = list(log = 2, equispaced.log=FALSE)), auto.key=list(columns=2))  
* There is one more component you need to know, equispaced.log  
	* This component indicates if the tick marks are equispaced when log scales are in use  
    * By default, equispaced.log is set to TRUE  
    * Note: If you set equispaced.log = FALSE, you don't have to explicitly specify a base for the log component; just log = TRUE should do the trick!  
  
Graphical parameters:  
  
* A collection of graphical parameters is referred to as a theme, frequently stored globally so it can be easily re-used  
* The trellis.par.set(myTheme) will work to set myTheme as the theme for the upcoming plot  
	* The latticeExtra package has ggplot2like() which will help match up the ggplot2 defaults  
* Can also control graphical parameters by way of calls within a graphin function  
	* For example, pch=15, col=c("red", "blue")  
* Changing the graphical theme using trellis.par.set(), as demonstrated in the preceding video, makes the changes permanent, applying to all subsequent plots, until the theme is reset  
	* If you wish to make changes for a specific plot, an easier alternative is to supply the theme as the optional argument par.settings to a high-level call   
    * In that case, the settings will be associated only to that particular call  
    * In this exercise, you will use this approach to create a dot plot of the WorldPhones data with the ggplot2like() theme  
    * As we saw earlier, changing the theme alone may be insufficient; we also need to change other things like the spacing between panels  
    * Such settings (which are not considered graphical parameters) can also be customized through a list of "options"  
    * To go with the ggplot2like() theme, the latticeExtra package also provides a suitable list of options, produced by ggplot2like.opts()  
* Options can be associated to a particular plot by specifying it as the lattice.options argument in a high-level call, or set more permanently using the lattice.options() function  
  
Using simpleTheme():  
  
* Empty circles are the default plotting symbol  
	* The pch=16 will create filled-in circles  
* Interesting, changing parameters like pch in the function call apply only to the data, not to the legend describing the data  
	* Can instead make changes that apply to everything by specifying (inside the function) par.settings = simpleTheme(pch=16, col=c("red", "blue"))  
    * The simpleTheme() call will only change the requested options, leaving the global theme for everything else  
  
Example code includes:  
```{r}

# The lattice package and the USMortality dataset have been pre-loaded.
Status <- factor(c('Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural', 'Urban', 'Rural'), levels=c("Rural", "Urban")
                 )
Sex <- factor(c('Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female'), levels=c("Female", "Male")
              )
Cause <- factor(c('Heart disease', 'Heart disease', 'Heart disease', 'Heart disease', 'Cancer', 'Cancer', 'Cancer', 'Cancer', 'Lower respiratory', 'Lower respiratory', 'Lower respiratory', 'Lower respiratory', 'Unintentional injuries', 'Unintentional injuries', 'Unintentional injuries', 'Unintentional injuries', 'Cerebrovascular diseases', 'Cerebrovascular diseases', 'Cerebrovascular diseases', 'Cerebrovascular diseases', 'Alzheimers', 'Alzheimers', 'Alzheimers', 'Alzheimers', 'Diabetes', 'Diabetes', 'Diabetes', 'Diabetes', 'Flu and pneumonia', 'Flu and pneumonia', 'Flu and pneumonia', 'Flu and pneumonia', 'Suicide', 'Suicide', 'Suicide', 'Suicide', 'Nephritis', 'Nephritis', 'Nephritis', 'Nephritis'), 
                levels=c('Alzheimers', 'Cancer', 'Cerebrovascular diseases', 'Diabetes', 'Flu and pneumonia', 'Heart disease', 'Lower respiratory', 'Nephritis', 'Suicide', 'Unintentional injuries')
                )
Rate <- c(210.2, 242.7, 132.5, 154.9, 195.9, 219.3, 140.2, 150.8, 44.5, 62.8, 36.5, 46.9, 49.6, 71.3, 24.7, 37.2, 36.1, 42.2, 34.9, 42.2, 19.4, 21.8, 25.5, 30.6, 24.9, 29.5, 17.1, 21.8, 17.7, 20.8, 12.9, 16.3, 19.2, 26.3, 5.3, 6.2, 15.7, 18.3, 10.7, 13.9)
SE <- c(0.2, 0.6, 0.2, 0.4, 0.2, 0.5, 0.2, 0.4, 0.1, 0.3, 0.1, 0.2, 0.1, 0.3, 0.1, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 0.1, 0.1, 0.2, 0, 0.1, 0.1, 0.2, 0, 0.1)

USMortality <- data.frame(Status=Status, Sex=Sex, Cause=Cause, Rate=Rate, SE=SE)


# Specify upper bound to exclude Heart disease and Cancer
x_limits <- c(0, 100)

# Draw the plot
dotplot(Cause ~ Rate | Sex + Status, data = USMortality, as.table = TRUE, 
        xlim = x_limits)


dotplot(Cause ~ Rate | Sex + Status, data = USMortality,
        as.table = TRUE,
        scales = list(x = list(relation = "free",
                               # Specify limits for each panel
                               limits = list(c(0, 50), c(0, 80), 
                                             c(0, 50), c(0, 80) ))))


dotplot(Cause ~ Rate | Sex + Status, data = USMortality, 
        as.table = TRUE,
        # Change the number of tick marks
        scales = list(x = list(tick.number = 10, 
                               # Show `Rate` labels on both bottom and top
                               alternating = 3, 
                               # Rotate `Rate` labels by 90 degrees
                               rot = 90),
                      # Rotate `Cause` labels by 45 degrees
                      y = list(rot = 45)))


# Define at as 2^3 up to 2^8
x_ticks_at <- 2 ** (3:8)

dotplot(Cause ~ Rate | Sex, data = USMortality,
        groups = Status, auto.key = list(columns = 2),
        scales = list(x = list(log = 2, 
                               # A numeric vector with 
                               # values 2^3, 2^4, ..., 2^8
                               at = x_ticks_at, 
                               # A character vector, 
                               # "8" for 2^3, "16" for 2^4, etc.
                               labels = x_ticks_at)))


# Create the dot plot
dotplot(Cause ~ Rate | Status, data = USMortality,
        groups = Sex, auto.key = list(columns = 2),
        scales = list(x = list(log = TRUE, 
                      equispaced.log = FALSE)), 
        # Provide pch values for the two groups
        pch = c(3, 1))


dotplot(Cause ~ Rate | Status, data = USMortality,
        groups = Sex, auto.key = list(columns = 2),
        par.settings = simpleTheme(pch = c(3, 1)),
        scales = list(x = list(log = 2, equispaced.log = FALSE)))


# The WorldPhones matrix is already provided, with the first row removed so you only need consider consecutive years
data(WorldPhones)
WorldPhones <- WorldPhones[row.names(WorldPhones) != 1951, ]
WorldPhones

names(dimnames(WorldPhones)) <- c("Year", "Region")

# Transform matrix data to data frame
WorldPhonesDF <- as.data.frame(
                   # Intermediate step: convert to table
                   as.table(WorldPhones), 
                   responseName = "Phones")

# Create the dot plot
dotplot(Year ~ Phones | Region, 
        data = WorldPhonesDF, 
        as.table = TRUE,
        # Log-transform the x-axis
        scales = list(x = list(log = TRUE,
                               equispaced.log = FALSE, 
                               # Set x-axis relation to "sliced"
                               relation = "sliced")))


# Load latticeExtra package for ggplot2like()
library(latticeExtra)

# Transform matrix data to data frame
names(dimnames(WorldPhones)) <- c("Year", "Region")
WorldPhonesDF <- 
  as.data.frame(as.table(WorldPhones[-1, ]), 
                responseName = "Phones")

# Create the dot plot
dotplot(Year ~ Phones | Region,
        data = WorldPhonesDF, 
        as.table = TRUE,
        scales = list(x = list(log = TRUE,
                               equispaced.log = FALSE, 
                               relation = "sliced")),
        # Fill in suitable value of par.settings
        par.settings = ggplot2like(),
        # Fill in suitable value of lattice.options
        lattice.options = ggplot2like.opts())



# Create factor variable
airquality$Month.Name <- 
  factor(airquality$Month, levels = 1:12, 
         labels = month.name[1:12])
         
# Create histogram of Ozone, conditioning on Month
histogram(~ Ozone | Month.Name,
          data = airquality, as.table = TRUE,
          # Set border to be transparent
          border = "transparent", 
          # Set fill color to be mid-gray
          col = "grey50")


# Create factor variable
airquality$Month.Name <- 
  factor(airquality$Month, levels = 1:12, 
         labels = month.name)
levels(airquality$Month.Name)

# Drop empty levels
airquality$Month.Name <- droplevels(airquality$Month.Name)
levels(airquality$Month.Name)

# Obtain colors from RColorBrewer
library(RColorBrewer)
my.colors <- brewer.pal(n = 5, name = "Set1")

# Density plot of ozone concentration grouped by month
densityplot(~ Ozone, data = airquality, groups = Month.Name,
            plot.points = FALSE,
            auto.key = list(space = "right"),
            # Fill in value of col
            par.settings = simpleTheme(col = my.colors, 
                                       # Fill in value of lwd
                                       lwd = 2))

```
  
  
  
***
  
Chapter 4 - Customizing plots using panel functions  
  
Panel functions:  
  
* Declarative approach (you provide specifications, system figures out requirements) is used by ggplot2  
* Procedural approach (you provide step-by-step) is used by lattice and base R  
* Custom displays in lattice cannot be created directly by faceting; instead, function build-up is needed  
	* panel.histdens <- function(x, …) { panel.histogram(x, …) ; panel.lines(density(x, na.rm=TRUE)) }  # overlay a density with a density histogram  
    * The panel.histdens is then used inside the function, such as panel=panel.histdens inside a call to histogram()  
    * The base R functions like lines and points need to be replaced by their lattice equivalents like panel.lines and panel.points for the lattice code to work  
* Rather than customizing a default display, sometimes you may want to replace it entirely  
	* An example of this is a violin plot, which is structured like a box and whisker plot, but instead of the boxes and whiskers, it uses kernel density estimates to summarize a distribution  
    * The resulting plot retains the compactness of a box and whisker plot, but also shows features like bimodality  
    * The built-in function panel.violin() in the lattice package implements the display of violin plots  
* When there are a large number of points in the data, there may be substantial overplotting in a standard scatter plot  
	* Another built-in panel function available in the lattice package that can serve as a replacement for panel.xyplot() in such cases is panel.smoothScatter()
    * Instead of plotting the points directly, it uses a color gradient to show a 2-D kernel density estimate obtained from the data  
    * xyplot(rate.female ~ rate.male, data = USCancerRates, panel = panel.smoothScatter, scales = list(log = TRUE, equispaced.log = FALSE), main = "County-wise deaths due to cancer")  
  
Prepanel Functions to control limits:  
  
* Controlling the x/y axis limits is enabled within lattice  
	* prepanel.histdens.1 <- function(x, ...) { d <- density(x, na.rm = TRUE); list(ylim = c(0, max(d$y))) }  
    * histogram(~ rate.male + rate.female, USCancerRates, type = "density", scales = list(x = list(log = 10)), xlab = "Rate", panel = panel.histdens, prepanel = prepanel.histdens.1)  
  
Optional arguments of default panel functions:  
  
* Some optional arguments are common to all high-level functions in lattice - xlab, ylab, main, layout, between, scales  
* Some optional arguments are specific to a single high-level function  
	* nint, type - histogram()  
    * plot.points, ref - densityplot()  
    * grad, abline - xyplot()  
    * col, cwd, cex, pch  
* The high-level functions will handle the general arguments, while sweeping up all the others for passage to the panel functions  
	* For example, passing grid=TRUE in xyplot() passes the argument to panel.xyplot()  
* The type argument for xyplot adds a number of arguments  
	* "p" - points  
    * "l" - lines  
    * "r" - regresion by way of panel.lmline()  
    * "smooth" - LOESS smooth by way of panel.loess()  
    * "a" - join average y values for each unique x value by way of panel.average()  
    * Multiple types can be specified as a vector  
* Note a few following features for an xyplot() 
	* grid = list(h=-1, v=0) # draws horizontal reference lines  
    * type = c("p", "a") draws the points and a line connecting their averages  
    * jitter.x = TRUE will apply a jitter on the x-axis only  
* The default panel function for bwplot() has two additional arguments that you have not used before:  
	* pch = "|" replaces the black dot representing the median inside the box by a line segment dividing the box into two smaller rectangles  
    * notch = TRUE puts "notches" on the side of the boxes that indicate a confidence interval for median  
    * the overlapping of notches for two subgroups suggests that the true medians of the two subgroups are not significantly different  
* For the last exercise in this chapter, your task is to recreate a grouped dot plot you have seen before, but replace the plotting characters by emoji images  
	* To do so, you will use the panel.xyimage() function in the latticeExtra package, which is similar to the panel.xyplot() function,  
    * except that plotting symbols are replaced by images whose locations (file names or URLs of JPEG or PNG image files) are specified as the pch argument  
  
Example code includes:  
```{r}

panel.xyrug <- function(x, y, ...)
{
  # Reproduce standard scatter plot
  panel.xyplot(x, y, ...)
  
  # Identify observations with x-value missing
  x.missing <- is.na(x)
  
  # Identify observations with y-value missing
  y.missing <- is.na(y)
  
  # Draw rugs along axes
  panel.rug(x = x[y.missing], y = y[x.missing])
}

airquality$Month.Name <- 
    factor(month.name[airquality$Month], levels = month.name)
    
xyplot(Ozone ~ Solar.R | Month.Name, data = airquality,
       panel = panel.xyrug, as.table = TRUE)


# Create factor variable with month names
airquality$Month.Name <- 
  factor(month.name[airquality$Month], levels = month.name)

# Create box-and-whisker plot
bwplot(Month.Name ~ Ozone + Temp, airquality, 
       # Specify outer
       outer=TRUE, 
       # Specify x-axis relation
       scales = list(x = list(relation="free")),
       # Specify layout
       layout=c(2, 1),
       # Specify x-axis label
       xlab="Measured value")

# Create violin plot
bwplot(Month.Name ~ Ozone + Temp, airquality, 
       # Specify outer
       outer = TRUE, 
       # Specify x-axis relation
       scales = list(x = list(relation="free")),
       # Specify layout
       layout=c(2, 1),
       # Specify x-axis label
       xlab="Measured value",
       # Replace default panel function
       panel = panel.violin)


# Create panel function
panel.ss <- function(x, y, ...) {
  # Call panel.smoothScatter()
  panel.smoothScatter(x, y, ...)
  # Call panel.loess()
  panel.loess(x, y, col = "red")
  # Call panel.abline()
  panel.abline(0, 1)
}

# Create plot
xyplot(rate.female ~ rate.male, data = USCancerRates,
       panel = panel.ss,
       main = "County-wise deaths due to cancer")


# Define prepanel function
prepanel.histdens.2 <- function(x, ...) {
    h <- prepanel.default.histogram(x, ...)
    d <- density(x, na.rm = TRUE)
    list(xlim = quantile(x, c(0.005, 0.995), na.rm = TRUE),
         # Calculate upper y-limit
         ylim = c(0, max(d$y, h$ylim[2])))
}

panel.histdens <- function(x, ...) {
    panel.histogram(x, ...)
    panel.lines(density(x, na.rm = TRUE))
}

# Create a histogram of rate.male and rate.female
histogram(~ rate.male + rate.female,
          data = USCancerRates, outer = TRUE,
          type = "density", nint = 50,
          border = "transparent", col = "lightblue",
          # The panel function: panel.histdens
          panel = panel.histdens, 
          # The prepanel function: prepanel.histdens.2
          prepanel = prepanel.histdens.2,
          # Ensure that the x-axis is log-transformed
          # and has relation "sliced"
          scales = list(x = list(log = TRUE,
                                 equispaced.log = FALSE,
                                 relation = "sliced")),
          xlab = "Rate (per 100,000)")


# Create the box and whisker plot
bwplot(division.ordered ~ rate.male, 
       data = USCancerRates,
       # Indicate median by line instead of dot
       pch = "|", 
       # Include notches for confidence interval
       notch = TRUE,
       # The x-axis should plot log-transformed values
       scales = list(x = list(log=TRUE, equispaced.log=FALSE)),
       xlab = "Death Rate in Males (per 100,000)")


# Load the 'latticeExtra' package
library(latticeExtra)

# Create summary dataset
USCancerRates.state <- 
   with(USCancerRates, {
     rmale <- tapply(rate.male, state, median, na.rm = TRUE)
     rfemale <- tapply(rate.female, state, median, na.rm = TRUE)
     data.frame(Rate = c(rmale, rfemale),
                State = rep(names(rmale), 2),
                Gender = rep(c("Male", "Female"), 
                             each = length(rmale)))
  })

# Reorder levels
library(dplyr)
USCancerRates.state <- 
   mutate(USCancerRates.state, State = reorder(State, Rate))
head(USCancerRates.state)

# URLs for emojis
emoji.man <- "https://twemoji.maxcdn.com/72x72/1f468.png"
emoji.woman <- "https://twemoji.maxcdn.com/72x72/1f469.png"

# Create dotplot
# dotplot(State ~ Rate, data = USCancerRates.state, 
        # Specify grouping variable
#         groups = Gender, 
        # Specify panel function
#         panel = panel.xyimage, 
        # Specify emoji URLs
#         pch = c(emoji.woman, emoji.man),
        # Make symbols smaller
#         cex = 0.75)

```
  
  
  
***
  
Chapter 5 - Extensions and the lattice ecosystem  
  
New methods - lattice is used by many packages because it is highly extensible:  
  
* High-level lattice functions are "generic functions", and the first argument need not be a formula  
* For example, dotplot() can be applied directly to a table  
	* For example, dotPlot(worldPhones[-1, ], scales=list(x=list(log=2)), groups=FALSE, layout=c(1, NA),  strip=FALSE, strip.left=TRUE)  
* The xyplot() function has a suitable method for time series objects  
	* The function to create the time-series plot is simply xyplot()  
    * Instead of a formula and a data frame, the only mandatory argument is a time series object, which must be the first argument  
    * The default value of type is "l", so that data points are joined by lines  
    * The argument superpose, which can take values TRUE or FALSE, is used to control whether multiple time series are plotted within the same panel or in separate panels, respectively
    * The default is to plot them separately  
    * The argument cut, which should be a list of the form list(number = , overlap = ), is used to produce so-called "cut-and-stack" plots, by splitting the time axis into multiple overlapping periods which are then used to condition  
    * This makes it easier to see parts of a long series  
* One innovative display design for time series data, known as horizon graphs, is implemented in the panel.horizonplot() function in the latticeExtra package  
	* Horizon plots allow you to visualize many time series in a small amount of space  
    * The main motivation for this design is to reduce the vertical space occupied by a single time series, without the loss of resolution that would result from simply flattening the usual line graph display  
    * This is achieved in two ways. First, negative values are mirrored to lie above the x-axis, but distinguished from positive values by shading using different colors  
    * Second, values are divided into bands with progressively higher saturation, and the bands are collapsed to wrap them around lower bands  
  
New high-level functions can be created:  
  
* Completely new high-level functions are built when the panel options are insufficient  
	* The horizonplot() for above is one example  
    * The chloropleth (colored map) is another - see mapplot() in the latticeExtra() package  
* Since the earth is three dimensional but the plot is two dimensional, a projection is required to reduce the number of dimensions  
	* The list of available projections is given in the Details section of the mapproject() help page  
* Map plots are drawn in two stages. First, a map object is created using the map() function from the maps package with plot = FALSE  
	* the_map <- map("a_map_dataset", plot = FALSE, projection = "some_projection")  
* Second, mapplot() is called with a formula, a data frame, and a map  
	* mapplot(region ~ value, data, map = the_map)  
* It is common to have statistical estimates in the form of confidence intervals in addition to point estimates  
    * Such data can be displayed using segment plots via the segplot() function in the latticeExtra package  
	segplot(
    * categories ~ lower_limit + upper_limit, data = some_data, centers = point_estimates)  
    * Notice that the categories are displayed on the y-axis, and the confidence intervals are displayed on the x-axis  
    * The point estimates, usually a mean or median value for that category, are specified using the centers argument, not the formula  
    * An optional argument, draw.bands, let's you choose between confidence bands and confidence intervals  
    * This argument is passed to the default panel function panel.segplot()  
* One common approach is to plot some form of bivariate density estimate instead of the raw data, as is done with histograms and kernel density plots for univariate data  
	* Hexagonal binning and plotting is implemented in the R package hexbin, which also includes the high-level function hexbinplot() for creating conditional hexbin plots using the lattice framework  
    * The formula and data argument in a hexbinplot() call is interpreted in the same way as xyplot()  
    * The type argument can be set to "r" to add a regression line  
    * The trans argument can be a function that is applied to the observed counts before creating bands for different colors  
    * By default, the range of counts is divided up evenly into bands, but taking the square root of the counts, for example, emphasizes differences in the lower range of counts more  
    * The inv argument gives the inverse function of trans, so that transformed counts can be converted back before being shown in the legend  
  
Manipulation (extension) of trellis objects:  
  
* latticeExtra::useOuterStrip(latticeObject) will make the strips show only on the top and the left  
* The directlabels package tackles an interesting problem: instead of having a separate legend associating graphical parameters and levels of a grouping variable, it tries to indicate the grouping by placing text labels within the panel  
	* This is generally a tricky thing to do automatically. directlabels relies on heuristics, and also allows the user to provide their own heuristics. It works with both lattice and ggplot2 plots  
* Once a lattice plot object is created, it can be modified using the update() method  
    * Among other things, a new panel function can be provided as the panel argument, to change or enhance the panel display  
	* Specifying the display in the form of a function can be cumbersome, especially for minor changes  
    * An alternative approach, implemented in the latticeExtra package, is to add so-called layers to the existing display. This is modeled on the approach used by the ggplot2 package  
* There are two kinds of layers  
	* Layers that go below the default display (i.e., are drawn before it) are created by the layer_() function  
    * Those that go above are created using layer()  
    * There are also corresponding versions glayer_() and glayer() for grouped displays  
    * A layer is created by putting a function call, as it would appear inside a panel function, inside a call to layer_() or layer()  
* Suppose you want to create a layer with a call to panel.grid that goes under the display, and a call to panel.lmline() that goes above, and then add it to an existing lattice plot p  
	* under_layer <- layer_(panel.grid())
    * over_layer <- layer(panel.lmline(x, y))
    * p + under_layer + over_layer
* Layers are added to a plot using the + operator  
  
Example code includes:  
```{r}

# Use 'EuStockMarkets' time series data
data(EuStockMarkets)
str(EuStockMarkets)

# Create time series plot
xyplot(EuStockMarkets, 
       # Plot all series together
       superpose = TRUE,
       # Split up the time axis into parts
       cut = list(number = 3, overlap = 0.25))


# Create time series plot
xyplot(EuStockMarkets,
       # Specify panel function
       panel=panel.horizonplot,
       # Specify prepanel function
       prepanel=prepanel.horizonplot)


# Load required packages
library(maps)


# Create map object for US counties
county.map <- map("county", plot = FALSE, fill = TRUE, 
                  # Specify projection
                  projection = "sinusoidal")

# Create choropleth map
row.names(USCancerRates) <- rn_USCR

mapplot(row.names(USCancerRates) ~ log10(rate.male) + log10(rate.female), 
        data = USCancerRates, 
        xlab = "", scales = list(draw = FALSE),
        # Specify map
        map = county.map)


# Create subset for Louisiana
LACancerRates1 <- filter(USCancerRates, state == "Louisiana")
str(LACancerRates1)

# Reorder levels of county
LACancerRates2 <- 
    mutate(LACancerRates1, 
           county = reorder(county, rate.male))

# Draw confidence intervals
segplot(county ~ LCL95.male + UCL95.male,
        data = LACancerRates2,
        # Add point estimates
        centers = rate.male,
        # Draw segments rather than bands
        draw.bands = FALSE)


# The 'USCancerRates' dataset
str(USCancerRates)

# Load the 'hexbin' package 
library(hexbin)

# Create hexbin plot
hexbinplot(rate.female ~ rate.male, 
           data = USCancerRates, 
           # Add a regression line
           type = "r",
           # function to transform counts
           trans = sqrt,
           # function to invert transformed counts
           inv = function(x) x^2
           )


# Load the 'directlabels' package
library(directlabels)

# Use the 'airquality' dataset
str(airquality)

# Create factor variable
airquality$Month.Name <- 
    factor(month.name[airquality$Month], levels = month.name)

# Create density plot object
tplot2 <- 
    densityplot(~ Ozone + Temp, data = airquality, 
                # Variables should go in different panels
                outer = TRUE,
                # Specify grouping variable
                groups = Month.Name,
                # Suppress display of data points
                plot.points = FALSE, 
                # Add reference line
                ref = TRUE,
                # Specify layout
                layout = c(2, 1),
                # Omit strip labels
                strip = FALSE,
                # Provide column-specific x-axis labels
                xlab = c("Ozone (ppb)", "Temperature (F)"),
                # Let panels have independent scales 
                scales = list(relation="free"))

# Produce plot with direct labels
direct.label(tplot2)


# 'USCancerRates' is pre-loaded
str(USCancerRates)

# Create scatter plot
p <- xyplot(rate.female ~ rate.male, data = USCancerRates, 
            # Change plotting character
            pch = 16, 
            # Make points semi-transparent
            alpha = 0.25)

# Create layer with reference grid
l0 <- layer_(panel.grid())

# Create layer with reference line
l1 <- layer(panel.abline(0, 1))

# Create layer with regression fit
l2 <- layer(panel.smoother(x, y, method="lm"))

# Combine and plot
p + l0 + l1 + l2

```
  
  
  
***
  
###_Visualizing Time Series Data in R_  
  
Chapter 1 - R Time Series Visualization Tools  
  
Refresher on xts and the plot() function:  
  
* With a time series plot, each element is associated to a specific time  
* The xts objects is typically the storage mechanism for times series data in R  
	* Time Index (Date, POSIXct, or the like) + Matrix  
* The plot() call can be used on xts objects and will call plot.xts() to achieve this purpose  
	* Many of the calls are similar to a normal plot() - for example, can overwrite using lines()  
  
Other useful visualizing functions:  
  
* Can use lines() to add a line to an existing time series plot  
* Can use axis(side=, at=) # 1 bottom, 2 left, 3 top, 4 right ; can use at=pretty(existingPlotData)  
* Can add legends using legend(x=<psn>, legend=, col=, lty=)  
* Can add lines to a plot using abline(v=, h=)  
* The PerformanceAnalytics package allows for better highlighting portions of the plot  
* To highlight a specific period in a time series, you can display it in the plot in a different background color  
	* The chart.TimeSeries() function in the PerformanceAnalytics package offers a very easy and flexible way of doing this  
    * chart.TimeSeries(R, period.areas, period.color)  
    * R is an xts, time series, or zoo object of asset returns  
    * period.areas are shaded areas specified by a start and end date in a vector of xts date ranges like c("1926-10/1927-11")  
    * period.color draws the shaded region in whichever color is specified  
  
Example code includes:  
```{r}

library(xts)

# data is a 504x4 xts object of Yahoo, Microsoft, Citigroup, and Dow
tmpData <- readr::read_delim("./RInputFiles/dataset_1_1.csv", delim=" ")
data <- xts::xts(tmpData[, -1], order.by=as.POSIXct(tmpData$Index))


# Display the first few lines of the data
head(data)

# Display the column names of the data
colnames(data)

# Plot yahoo data and add title
plot(data[, "yahoo"], main="yahoo")

# Replot yahoo data with labels for X and Y axes
plot(data[, "yahoo"], main="yahoo", xlab="date", ylab="price")


# Note that type="h" is for bars
# Plot the second time series and change title
plot(data[, 2], main="microsoft")

# Replot with same title, add subtitle, use bars
plot(data[, 2], main="microsoft", sub="Daily closing price since 2015", type="h")

# Change line color to red
lines(data[, 2], col="red")


# Plot two charts on same graphical window
par(mfrow = c(2, 1))
plot(data[, 1], main="yahoo")
plot(data[, 2], main="microsoft")

# Replot with reduced margin and character sizes
par(mfrow = c(2, 1), mex=0.6, cex=0.8)
plot(data[, 1], main="yahoo")
plot(data[, 2], main="microsoft")

par(mfrow = c(1, 1), mex=1, cex=1)


# Plot the "microsoft" series
plot(data[, "microsoft"], main="Stock prices since 2015")

# Add the "dow_chemical" series in red
lines(data[, "dow_chemical"], col="red")

# Add a Y axis on the right side of the chart
axis(side=4, at=pretty(data[, "dow_chemical"]))

# Add a legend in the bottom right corner
legend("bottomright", legend=c("microsoft", "dow_chemical"), col=c("black", "red"), lty=c(1, 1))


# Plot the "citigroup" time series
plot(data[, "citigroup"], main="Citigroup")

# Create vert_line to identify January 4th, 2016 in citigroup
vert_line <- which(index(data[, "citigroup"]) == as.POSIXct("2016-01-04"))

# Add a red vertical line using vert_line
abline(v = .index(data[, "citigroup"])[vert_line], col = "red")

# Create hori_line to identify average price of citigroup
hori_line <- mean(data[, "citigroup"])

# Add a blue horizontal line using hori_line
abline(h = hori_line, col = "blue")


# Create period to hold the 3 months of 2015
period <- c("2015-01/2015-03")

# Highlight the first three months of 2015 
PerformanceAnalytics::chart.TimeSeries(data[, "citigroup"], period.areas=period)

# Highlight the first three months of 2015 in light grey
PerformanceAnalytics::chart.TimeSeries(data[, "citigroup"], period.areas=period, period.color="lightgrey")


# Plot the microsoft series
plot(data[, "microsoft"], main="Dividend date and amount")

# Add the citigroup series
lines(data[, "citigroup"], col="orange", lwd=2)

# Add a new y axis for the citigroup series
axis(side=4, at=pretty(data[, "citigroup"]), col="orange")


micro_div_date <- "15 Nov. 2016"
citi_div_date <- "13 Nov. 2016"
micro_div_value <- "$0.39"
citi_div_value <- "$0.16"
# Same plot as the previous exercise
plot(data$microsoft, main = "Dividend date and amount")
lines(data$citigroup, col = "orange", lwd = 2)
axis(side = 4, at = pretty(data$citigroup), col = "orange")

# Create the two legend strings
micro <- paste0("Microsoft div. of ", micro_div_value," on ", micro_div_date)
citi <- paste0("Citigroup div. of ", citi_div_value," on ", citi_div_date)

# Create the legend in the bottom right corner
legend(x = "bottomright", legend = c(micro, citi), col = c("black", "orange"), lty = c(1, 1))

data_1_1_old <- data

```
  
  
  
***
  
Chapter 2 - Univariate Time Series  
  
Univariate time series analysis - deals with only a single variable:  
  
* Location, Dispersion, Distribution - frequently presented by way of histograms  
* Time series typically need to be transformed prior to these calculations, since their data is in the wrong format otherwise  
	* For example, it is often more helpful to get the distribution of price change (and/or percentage return) rather than just the stock price  
* In finance, price series are often transformed to differenced data, making it a return series   
	* In R, the ROC() (which stands for "Rate of Change") function from the TTR package does this automatically to a price or volume series x  
  
Other visualization tools:  
  
* Can create histograms of stock returns  
* Can use boxplot() to see the box-and-whisker of the stock returns  
	* The argument horizontal=TRUE will display the block horizontally  
* Can run acf() to see the autocorrelation of the returns  
* Can run qqnorm() and qqline() to see whether the data are normally distributed  
  
Combining everything so far:  
  
* The histogram helps with understanding both central tendencies and outliers  
	* The box and whiskers plot helps in a similar manner - also helps to show investment riskiness  
* The autocorrelation plot helps with understanding the linkages between today and days in the future  
* The QQ plot helps to assess whether methods/tests that rely on normality can be safely used on the dataset  
  
Example code includes:  
```{r}

tmpData <- readr::read_delim("./RInputFiles/dataset_2_1.csv", delim=" ")
names(tmpData) <- c("Index", "apple")
data <- xts::xts(tmpData[, -1], order.by=as.Date(tmpData$Index))
# indexClass(data) <- c("POSIXt", "POSIXlt")

# Plot Apple's stock price 
plot(data[, "apple"], main="Apple stock price")

# Create a time series called rtn
rtn <- TTR::ROC(data[, "apple"])

# Plot Apple daily price and daily returns 
par(mfrow=c(1, 2))
plot(data[, "apple"], main="Apple stock price")
plot(rtn)
par(mfrow=c(1, 1))


dim(rtn)
rtn <- rtn[complete.cases(rtn), ]
dim(rtn)

# Create a histogram of Apple stock returns
hist(rtn, main="Apple stock return distribution", probability=TRUE)

# Add a density line
lines(density(rtn[complete.cases(rtn), ]))

# Redraw a thicker, red density line
lines(density(rtn[complete.cases(rtn), ]), col="red", lwd=2)


rtnRaw <- as.double(rtn$apple)

# Draw box and whisker plot for the Apple returns
boxplot(rtnRaw)

# Draw a box and whisker plot of a normal distribution
boxplot(rnorm(1000))

# Redraw both plots on the same graphical window
par(mfrow=c(2, 1))
boxplot(rtnRaw, horizontal=TRUE)
boxplot(rnorm(1000), horizontal=TRUE)
par(mfrow=c(1, 1))


# Draw autocorrelation plot
acf(rtn, main="Apple return autocorrelation")

# Redraw with a maximum lag of 10
acf(rtn, main="Apple return autocorrelation", lag.max=10)


# Create q-q plot
qqnorm(rtn, main="Apple return QQ-plot")

# Add a red line showing normality
qqline(rtn, col="red")


par(mfrow=c(2, 2))

hist(rtn, probability=TRUE)
lines(density(rtn), col="red")
boxplot(rtnRaw)
acf(rtn)
qqnorm(rtn)
qqline(rtn, col="red")

par(mfrow=c(1, 1))

```
  
  
  
***
  
Chapter 3 - Multivariate Time Series  
  
Dealing with higher dimensions - visualization challenges with larger numbers of series:  
  
* Might want to compare stock prices vs interest rate changes  
* Cannot easily visualize even 10 time series, let alone 100 time series  
* One solution is to plot both time series as barcharts. There are two types:  
	* Grouped barchart: for a single period, there are as many bars as time series  
    * Stacked bar chart: for each period, there is a single bar, and each time series is represented by a portion of the bar proportional to the value of the time series at this date (i.e. the total at each period adds up to 100%)  
  
Multivariate time series:  
  
* To create a stacked chart, use barchart(myFrame, col=c(), main=)  # can specify the desired colors in the barchart or use the defaults  
* Can create the correlation matrix using cor(myMatrix, digit=)  
	* Several types of correlations exist but the most used ones are:  
    * Pearson correlation: measures the linear relationship between 2 variables  
    * Spearman rank correlation: measures the statistical dependency between the ranking of 2 variables (not necessarily linear)  
* Can create the pair chart using pairs(myFrame, lower.panel=NULL, main=)  # the lower.panel=NULL shows only the diagonal and the upper-right of the pairs plot  
* Can create a correlation plot using corrplot(myMatrix, method="number", type="upper")  # type="upper" shows only the upper-right of the diagonal  
  
Higher dimension time series:  
  
* Can display a correlation matrix as a heat map  
	* corrplot(myMatrix, method="color", type="upper")  
  
Example code includes:  
```{r}

# You are provided with a dataset (portfolio) containing the weigths of stocks A (stocka) and B (stockb) in your portfolio for each month in 2016
stockA <- c(0.1, 0.4, 0.5, 0.5, 0.2, 0.3, 0.7, 0.8, 0.7, 0.2, 0.1, 0.2)
stockB <- c(0.9, 0.6, 0.5, 0.5, 0.8, 0.7, 0.3, 0.2, 0.3, 0.8, 0.9, 0.8)
pDates <- as.Date(c('2016-01-01', '2016-02-01', '2016-03-01', '2016-04-01', '2016-05-01', '2016-06-01', '2016-07-01', '2016-08-01', '2016-09-01', '2016-10-01', '2016-11-01', '2016-12-01'))
portfolio <- xts(data.frame(stocka=stockA, stockb=stockB), order.by=pDates)

# Plot stacked barplot
barplot(portfolio)

# Plot grouped barplot
barplot(portfolio, beside=TRUE)


tmpData <- readr::read_delim("./RInputFiles/data_3_2.csv", delim=",")
# names(tmpData) <- c("Index", "apple")
my_data <- xts::xts(tmpData[, -1], order.by=as.Date(tmpData$Index))


citi <- as.numeric(my_data$citigroup)
sp500 <- as.numeric(my_data$sp500)

# Draw the scatterplot
plot(y=citi, x=sp500)

# Draw a regression line
abline(reg=lm(citi ~ sp500), col="red", lwd=2)


# my_data containing the returns for 5 stocks: ExxonMobile, Citigroup, Microsoft, Dow Chemical and Yahoo
# Create correlation matrix using Pearson method
cor(my_data)

# Create correlation matrix using Spearman method
cor(my_data, method="spearman")


# Create scatterplot matrix
pairs(as.data.frame(my_data))

# Create upper panel scatterplot matrix
pairs(as.data.frame(my_data), lower.panel=NULL)


cor_mat <- cor(my_data)

# In this exercise, you will use the provided correlation matrix cor_mat
# Create correlation matrix
corrplot::corrplot(cor_mat)

# Create correlation matrix with numbers
corrplot::corrplot(cor_mat, method="number")

# Create correlation matrix with colors
corrplot::corrplot(cor_mat, method="color")

# Create upper triangle correlation matrix
corrplot::corrplot(cor_mat, method="number", type="upper")


# Draw heatmap of cor_mat
corrplot::corrplot(cor_mat, method="color")

# Draw upper heatmap
corrplot::corrplot(cor_mat, method="color", type="upper")

# Draw the upper heatmap with hclust
corrplot::corrplot(cor_mat, method="color", type="upper", order="hclust")

```
  
  
  
***
  
Chapter 4 - Case Study: Stock Picking for Portfolios  
  
Case study presentation:  
  
* Suppose you have a portfolio of Apple, Microsoft, and Yahoo  
* Suppose also that you can add just a single extra stock with some spare cash  
* Examine the correlations of new stocks to the existing portfolio  
	* Starting point assumption is capital protection - low correlation to the existing portfolio  
* The PerformanceAnalytics package has some helpful tools for this analysis  
  
New stocks:  
  
* Goal is to choose the best new stock for the portfolio  
* The PerformanceAnalytics package provides additional tools to get a finer view of your portfolio  
	* In particular, the charts.PerformanceSummary() function provides a quick and easy way to display the portfolio value, returns and periods of poor performance, also known as drawdowns  
  
Course conclusion:  
  
* xts, plot()  
* Univariate  
* Multivariate  
* Case study  
  
Example code includes:  
```{r}

# In this exercise, you are provided with a dataset data containing the value and the return of the portfolio over time, in value and return, respectively.

tmpData <- readr::read_delim("./RInputFiles/data_4_1.csv", delim=",")
# names(tmpData) <- c("Index", "apple")
data <- xts::xts(tmpData[, -1], order.by=as.Date(tmpData$Index))
# indexClass(data) <- c("POSIXt", "POSIXlt")


# Plot the portfolio value
plot(data$value, main="Portfolio Value")

# Plot the portfolio return
plot(data$return, main="Portfolio Return")

# Plot a histogram of portfolio return 
hist(data$return, probability=TRUE)

# Add a density line
lines(density(data$return), col="red", lwd=2)

tmpPortfolioData <- data


# The new dataset data containing four new stocks is available in your workspace: Goldman Sachs (GS), Coca-Cola (KO), Walt Disney (DIS), Caterpillar (CAT)

tmpData <- readr::read_delim("./RInputFiles/data_4_3.csv", delim=",")
# names(tmpData) <- c("Index", "apple")
data <- xts::xts(tmpData[, -1], order.by=as.Date(tmpData$Index))
# indexClass(data) <- c("POSIXt", "POSIXlt")


# Plot the four stocks on the same graphical window
par(mfrow=c(2, 2), mex=0.8, cex=0.8)
plot(data[, 1])
plot(data[, 2])
plot(data[, 3])
plot(data[, 4])
par(mfrow=c(1, 1), mex=1, cex=1)


# In this exercise, you are provided with four individual series containing the return of the same four stocks:
# gs, ko, dis, cat
# Solution makes absolutely no sense


portfolio <- as.numeric(tmpPortfolioData$return)
gs <- as.numeric(TTR::ROC(data[, "GS"]))[-1]
ko <- as.numeric(TTR::ROC(data[, "KO"]))[-1]
dis <- as.numeric(TTR::ROC(data[, "DIS"]))[-1]
cat <- as.numeric(TTR::ROC(data[, "CAT"]))[-1]


# Draw the scatterplot of gs against the portfolio
plot(y=portfolio, x=gs)

# Add a regression line in red
abline(reg=lm(gs ~ portfolio), col="red", lwd=2)


# Plot scatterplots and regression lines to a 2x2 window
par(mfrow=c(2, 2))

plot(y=portfolio, x=gs)
abline(reg=lm(gs ~ portfolio), col="red", lwd=2)

plot(y=portfolio, x=ko)
abline(reg=lm(ko ~ portfolio), col="red", lwd=2)

plot(y=portfolio, x=dis)
abline(reg=lm(dis ~ portfolio), col="red", lwd=2)

plot(y=portfolio, x=cat)
abline(reg=lm(cat ~ portfolio), col="red", lwd=2)

par(mfrow=c(1, 1))


# In this exercise, you are given a dataset old.vs.new.portfolio with the following self-explanatory columns: old.portfolio.value, new.portfolio.value, old.portfolio.rtn, new.portfolio.rtn
tmpData <- readr::read_delim("./RInputFiles/old.vs.new.portfolio.csv", delim=",")
# names(tmpData) <- c("Index", "apple")
old.vs.new.portfolio <- xts::xts(tmpData[, -1], order.by=as.Date(tmpData$Index))
# indexClass(data) <- c("POSIXt", "POSIXlt")


# Plot new and old portfolio values on same chart
plot(old.vs.new.portfolio$old.portfolio.value)
lines(old.vs.new.portfolio$new.portfolio.value, col = "red")

# Plot density of the new and old portfolio returns on same chart
plot(density(old.vs.new.portfolio$old.portfolio.rtn))
lines(density(old.vs.new.portfolio$new.portfolio.rtn), col ="red")


# Draw value, return, drawdowns of old portfolio
PerformanceAnalytics::charts.PerformanceSummary(old.vs.new.portfolio[, "old.portfolio.rtn"])

# Draw value, return, drawdowns of new portfolio
PerformanceAnalytics::charts.PerformanceSummary(old.vs.new.portfolio[, "new.portfolio.rtn"])

# Draw both portfolios on same chart
# Draw value, return, drawdowns of new portfolio
PerformanceAnalytics::charts.PerformanceSummary(old.vs.new.portfolio[, c("old.portfolio.rtn", "new.portfolio.rtn")])

```
  
  
  
***
  
###_Communicating with the Tidyverse_  
  
Chapter 1 - Custom ggplot2 themes  
  
Introduction to the data - finding stories in datasets:  
  
* Communication is the final step in the tidyverse workflow  
* This course will create a production-level plot from Swiss public radio regarding hours worked in Europe, using ggplot2  
* Will also create a report of the findings using R Markdown  
* This course will work with two datasets from the ILO (International Labor Organization)  
	* ilo_working_hours - country-year-working_hours  
    * ilo_hourly_compensation - coutry-year-hourly_compensation  
* Begin by integrating the data using dplyr::inner_join  
* Usually, categorical variables like country in this example should be converted to factors before plotting them  
	* You can do so using as.factor(). In your data set, two columns are still of type "character" – use mutate() to turn them into factors  
  
Filtering and plotting the data:  
  
* The filter() function can be used to maintain only the European countries - best for the key years of interest  
	* The %in% operator will be valuable for this, given a vector of countries in Europe  
* Will look at histograms, scatter-plots, titling, and the like  
* Will use group_by() and summarize() also for looking at tabular results  
  
Custom ggplot2 themes - providing a custom look to a chart:  
  
* Custom looks can make it easier to highlight key data - colors, emphasis, shading, etc.  
* The theme() function is added to a function just like anything else in a ggplot  
	* text=element_text(family=, color=)  # to make a specific family and color available for all the labels and text  
* Can also add default ggplot2 themes to a plot  
* Can chain themes, including a default theme followed by several overrides  
	* theme_classic() + theme(text=element_text(family=, color=)  
* Can get an overview of all the possible options by using ?theme  
* There are four key members of the element_* function family  
	* element_text()  
    * element_rect()  
    * element_line()  
    * element_blank() - makes plot elements disappear  
  
Example code includes:  
```{r}

library(ggplot2)

load("./RInputFiles/ilo_hourly_compensation.RData")
load("./RInputFiles/ilo_working_hours.RData")


# Join both data frames
ilo_data <- ilo_hourly_compensation %>%
  inner_join(ilo_working_hours, by = c("country", "year"))

# Count the resulting rows
ilo_data  %>% 
    count()

# Examine ilo_data
ilo_data


# Turn year into a factor
ilo_data <- ilo_data %>%
  mutate(year = as.factor(as.numeric(year)))

# Turn country into a factor
ilo_data <- ilo_data %>%
  mutate(country = as.factor(country))


# Examine the European countries vector
european_countries <- c('Finland', 'France', 'Italy', 'Norway', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'Belgium', 'Ireland', 'Luxembourg', 'Portugal', 'Netherlands', 'Germany', 'Hungary', 'Austria', 'Czech Rep.')
european_countries

# Only retain European countries
ilo_data <- ilo_data %>%
  filter(country %in% european_countries)

# Examine the structure of ilo_data
str(ilo_data)


# Group and summarize the data
ilo_data %>%
  group_by(year) %>%
  summarize(mean_hourly_compensation = mean(hourly_compensation),
            mean_working_hours = mean(working_hours))


# Filter for 2006
plot_data <- ilo_data %>%
  filter(year == 2006)
  
# Create the scatter plot
ggplot(plot_data) +
  geom_point(aes(x = working_hours, y = hourly_compensation))


# Create the plot
ggplot(plot_data) +
  geom_point(aes(x = working_hours, y = hourly_compensation)) +
  # Add labels
  labs(
    x = "Working hours per week",
    y = "Hourly compensation",
    title = "The more people work, the less compensation they seem to receive",
    subtitle = "Working hours and hourly compensation in European countries, 2006",
    caption = "Data source: ILO, 2017"
  )


# Save your current plot into a variable: ilo_plot
ilo_plot <- ggplot(plot_data) +
  geom_point(aes(x = working_hours, y = hourly_compensation)) +
  labs(
    x = "Working hours per week",
    y = "Hourly compensation",
    title = "The more people work, the less compensation they seem to receive",
    subtitle = "Working hours and hourly compensation in European countries, 2006",
    caption = "Data source: ILO, 2017"
  )
  
# Try out theme_minimal
ilo_plot +
  theme_minimal()
    
# Try out any other possible theme function
ilo_plot +
  theme_linedraw()

windowsFonts(Bookman=windowsFont("Bookman Old Style"))

ilo_plot <- ilo_plot +
  theme_minimal() +
  # Customize the "minimal" theme with another custom "theme" call
  theme(
    text = element_text(family = "Bookman"),
    title = element_text(color = "gray25"),
    plot.subtitle = element_text(size=12),
    plot.caption = element_text(color = "gray30")
  )

# Render the plot object
ilo_plot


ilo_plot +
  # "theme" calls can be stacked upon each other, so this is already the third call of "theme"
  theme(
    plot.background = element_rect(fill = "gray95"),
    plot.margin = unit(c(5, 10, 5, 10), units = "mm")
  )

```
  
  
  
***
  
Chapter 2 - Creating Custom and Unique Visualization  
  
Visualizing aspects of data with facets:  
  
* The facet_grid() function builds on the facet_wrap() concept, allowing for further control  
	* facet_grid(rowVar ~ colVar)  
    * Note that facet_grid(. ~ year) will give the same output as facet_wrap(~ year)  
* Theme options are available for faceted plots - strip.backgroumd, strip.text, etc.  
* Can also create your own theme functions, such as:  
	* theme_green <- function(){ theme( plot.background = element_rect(fill = "green"), panel.background = element_rect(fill = "lightgreen") ) }  
  
Custom plot to emphasize change:  
  
* The dot plot is useful for comparing change over time  
	* Dot for starting point, arrow pointint towards ending point, text labels at start and end of arrow, arranged so that country (or whatever) is along the y-axis  
* The default geom_dotplot() is NOT what is needed - this is a histogram using dots rather than bars  
* Instead, the geom_path() is available for connecting observations in the order in which they appear in the data (so, proper ordering of the data frame is VERY important!)  
	* The geom_path(aes(x=, y=), arrow=arrow()) will expect at least one numeric variable, and one variable (y) that is either numeric or factor  
    * The arrow() is a function that allows for calling a specific type of arrow, arrow head, and the like  
	
Polishing the dot plot:  
  
* Ordering the factors can help make things much clearer in the ggplot - ggplot defaults to using the factor levels  
* The library(forcats) is great for working with factor variables, and is part of the tidyverse  
	* fct_drop for dropping levels  
    * fct_rev for reversing factor levels  
    * fct_reorder for reordering factor levels  
* The arguments for fct_reorder(factorVar, dataVar, FUN) - frequently applied by way of a mutate() call  
* Can further use the hjust and vjust aesthetics to nudge the labels for better readability  
	* These are added inside the aes() call for geom_text() and can be like aes(…, hjust=ifelse(year == 2006, 1.4, -0.4))  
  
Finalizing plots for different audiences and devices:  
  
* Changing the viewport (zooming or repositioning) can be managed in any of two manners
	* coord_cartesian(xlim=c(), ylim=c()) is the default ggplot2 mechanism  
    * The difference with using coord_cartesian rather than direct +xlim() + ylim() is that coord_cartesian() will prevent clipping, which is generally preferred  
* Need to customize the plot for mobile devices  
	* Can be helpful to have the plot available in 16:9 aspect ratio, which nicely fits most smartphones  
    * Can also be helpful to kill off axes, and put any labels needed directly in to the data  
* In this exercise, you're going to encounter something that is probably new to you  
	* New data sets can be given to single geometries like geom_text(), so these geometries don't use the data set given to the initial ggplot() call  
    * In this exercise, you are going to need this because you only want to add one label to each arrow  
    * If you were to use the original data set ilo_data, two labels would be added because there are two observations for each country in the data set, one for 1996 and one for 2006  
  
Example code includes:  
```{r}

# Filter ilo_data to retain the years 1996 and 1996
ilo_data <- ilo_data %>%
  filter(year == 1996 | year == 2006)


# Again, you save the plot object into a variable so you can save typing later on
ilo_plot <- ggplot(ilo_data, aes(x = working_hours, y = hourly_compensation)) +
  geom_point() +
   labs(
    x = "Working hours per week",
    y = "Hourly compensation",
    title = "The more people work, the less compensation they seem to receive",
    subtitle = "Working hours and hourly compensation in European countries, 2006",
    caption = "Data source: ILO, 2017"
  ) +
  # Add facets here
  facet_grid(facets = . ~ year)
 
ilo_plot


# For a starter, let's look at what you did before: adding various theme calls to your plot object
ilo_plot +
  theme_minimal() +
  theme(
    text = element_text(family = "Bookman", color = "gray25"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(color = "gray30"),
    plot.background = element_rect(fill = "gray95"),
    plot.margin = unit(c(5, 10, 5, 10), units = "mm")
  )

# Define your own theme function below
theme_ilo <- function() {
  theme_minimal() +
  theme(
    text = element_text(family = "Bookman", color = "gray25"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(color = "gray30"),
    plot.background = element_rect(fill = "gray95"),
    plot.margin = unit(c(5, 10, 5, 10), units = "mm"))
}


# Apply your theme function
ilo_plot <- ilo_plot + theme_ilo()

# Examine ilo_plot
ilo_plot

ilo_plot +
  # Add another theme call
  theme(
    # Change the background fill to make it a bit darker
    strip.background = element_rect(fill = "gray60", color = "gray95"),
    # Make text a bit bigger and change its color to white
    strip.text = element_text(size = 11, color = "white")
  )


# Create the dot plot
ggplot(ilo_data) +
    geom_path(aes(x=working_hours, y=country))


ggplot(ilo_data) +
  geom_path(aes(x = working_hours, y = country),
  # Add an arrow to each path
            arrow = arrow(length = unit(1.5, "mm"), type = "closed"))


ggplot(ilo_data) +
  geom_path(aes(x = working_hours, y = country),
            arrow = arrow(length = unit(1.5, "mm"), type = "closed")) +
  # Add a geom_text() geometry
  geom_text(
          aes(x = working_hours,
              y = country,
              label = round(working_hours, 1))
        )


library(forcats)

# Reorder country factor levels
ilo_data <- ilo_data %>%
  # Arrange data frame
  arrange(country, year) %>%
  # Reorder countries by working hours in 2006
  mutate(country = fct_reorder(country,
                               working_hours,
                               last))

# Plot again
ggplot(ilo_data) +
  geom_path(aes(x = working_hours, y = country),
            arrow = arrow(length = unit(1.5, "mm"), type = "closed")) +
    geom_text(
          aes(x = working_hours,
              y = country,
              label = round(working_hours, 1))
          )


# Save plot into an object for reuse
ilo_dot_plot <- ggplot(ilo_data) +
  geom_path(aes(x = working_hours, y = country),
            arrow = arrow(length = unit(1.5, "mm"), type = "closed")) +
    # Specify the hjust aesthetic with a conditional value
    geom_text(
          aes(x = working_hours,
              y = country,
              label = round(working_hours, 1),
              hjust = ifelse(year == "2006", 1.4, -0.4)
            ),
          # Change the appearance of the text
          size = 3,
          family = "Bookman",
          color = "gray25"
          )

ilo_dot_plot


# Reuse ilo_dot_plot
ilo_dot_plot <- ilo_dot_plot +
  # Add labels to the plot
  labs(
    x = "Working hours per week",
    y = "Country",
    title = "People work less in 2006 compared to 1996",
    subtitle = "Working hours in European countries, development since 1996",
    caption = "Data source: ILO, 2017"
  ) +
  # Apply your theme
  theme_ilo() +
  # Change the viewport
  coord_cartesian(xlim = c(25, 41))
  
# View the plot
ilo_dot_plot


# Compute temporary data set for optimal label placement
median_working_hours <- ilo_data %>%
  group_by(country) %>%
  summarize(median_working_hours_per_country = median(working_hours)) %>%
  ungroup()

# Have a look at the structure of this data set
str(median_working_hours)

ilo_dot_plot +
  # Add label for country
  geom_text(data = median_working_hours,
            aes(y = country,
                x = median_working_hours_per_country,
                label = country),
            vjust = -0.5,
            size=3,
            family = "Bookman",
            color = "gray25") +
  # Remove axes and grids
  theme(
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    panel.grid = element_blank(),
    # Also, let's reduce the font size of the subtitle
    plot.subtitle = element_text(size = 9)
  )

```
  
  
  
***
  
Chapter 3 - Introduction to R Markdown  
  
What is R Markdown?  
  
* R Markdown is a framework for converting R code in to a wide range of outputs - html, PDF, etc.  
	* R Markdown -> knitr -> md -> pandoc -> final output  
* The biggest benefit of R Markdown is the full reproducibility of the analysis  
	* Other people or yourself (if there is new data)  
* The code needs to be executable on other people's machines, and the data should either be included (or have a link to it)  
	* These are the minimum standards for reproducibility  
    * Fuller standards would include software/package systems, run date/time, operating system, etc.  
  
Formatting with R Markdown:  
  
* Markdown is an example of a mark-up language (like html, which is a hyper-text mark-up language)  
	* Markdown was created to have a quick means of creating valid html code that could be published on the internet  
* The hash signs (#) are for levels of header - # (first), ## (second), etc.  
* The asterisk will make italics with singles (*myItalicText*) and bold with doubles (**myBoldText**)  
* Links can be introduced and named using the target name inside square brackets followed by the target link inside parentheses [myLinkName](myValidAddressLink)  
* The R Markdown document is a mix of R code and Markdown code  
	* R Markdown uses pandoc markdown, part of the markdown family  
    * Can use the pandoc markdown reference guide, available at R Studio  
  
R Code in R Markdown Documents:  
  
* Adding R chunks is as simple as adding triple back-ticks, followed by {r}, ended with triple back-ticks    
* Can also add R code inline such as back-tick r 2 + 2 back-tick, such as 2 + 2 equals `r 2+2`  
* There are many options available for R code chunks including  
	* include=FALSE  # execute the R code but do not quote it or print any output  
    * echo=FALSE # do not show the R code, but show its output  
    * message=FALSE # do not show messages  
    * warning=FALSE # do not show warnings  
    * eval=FALSE # do not evaluate the R chunk (but do print it provided the default echo=TRUE is set)  
* R code chunks can also be named  
	* This makes sense in large documents, especially if there is an error inside a chunk  
    * While knitting a document in RStudio, for example, the error can then be located in an easier fashion  
    * Chunk options are added after the name of the chunk and a comma, like so: {r name, option=value}  
  
Images in R Markdown Files:  
  
* Images resulting from code are responsive, which is to say that they will change with the page size  
	* Sometimes, the default options that go with a figure are sub-optimal (wrong aspect ratio or the like)  
    * Can add options like fig.height=6 inside the ```{r} command to address these - default unit is inches  
    * Also can use fig.width= (inches) and fig.align= (any of "right", "left", "center")  
* Can also load external images in to Markdown  
	* exclamation-mark square-brackets-containing-name parentheses-containing-image-location  
  
Example code is contained in the summary Excel worksheet.
  

    
***
  
Chapter 4 - Customizing R Markdown Reports  
  
Advanced YAML Settings (YAML is a recursive name meaning YAML and Markup Language):  
  
* YAML documents typically start and end with three hyphens (---) with value: key pairs  
	* Indentations suggest sub-family relationships; spacing does not matter, but everything of the same level must be indented the same  
* All R Markdown documents begin with a YAML header, which can be customized and enhanced  
* You add a table with toc: true and specify whether it should be floating (= whether it moves along as you scroll) with toc_float  
	* The depth of your TOC is set with toc_depth  
* Before you dig deeper into custom stylesheets, let's enable code folding with code_folding: ....   
	* This will allow your readers to completely hide all code chunks or show them – all at once or individually  
  
Custom stylesheets - creating a unique theme for a report:  
  
* Can refer to any CSS (cascading style sheet) in the YAML header  
* Can use any of the html tags that would be created by the document, and enhance the properties they will have for this html document output  
	* h2 { font-family: "Bookman", serif; }  
    * Conclude each rule with a semicolon!  
    * body, h1, h2 { font-family: "Bookman", serif; }  
    * Separate the html tags with commas  
    * a { color: #0000FF; font-weight: bold; }  
    * Separate the commands with a semicolon (same as used to end the commands)  
* There are some further customization possibilities  
	* strong { color: "blue"; }  # this will make everything of tag "strong" blue  
    * strong.red { color: "red"; }  # can create a strong.red tag that will be "red" even while the rest of them are "blue"  
* It is also possible to specify combinators in CSS, which is to say that tags within another tag only are impacted  
	* div strong { color: "green"; }  # strong tags subordinate at any number of levels to div tags will be colored green  
    * div > strong { color: "red"; }  # strong tags directly subordinate (pure child-parent relationship) to div will be colored red  
* Mozilla Developer Network has a lot of style tag  ideas  
  
Beautiful tables:  
  
* By default, R Markdown renders tables exactly as they would be rendered to the R console  
* Can add the df_print: key-value under the html_document: (or whatever) area  
	* Typically key-value are either df_print: kable or df_print: paged  
* Alternately, to just change a single table, pipe the output to knitr::kable()  
	* myData %>% group_by(myFactor) %>% summarize(myEquations) %>% knitr::kable()  
* Tables can also be styled using html tags - basic anatomy of a table includes  
	* <table> <thead> … </thead><tbody> … </tbody></table>  
    * Each of the header and the body will have one or more rows, each depicted using <tr> … </tr>  
    * Each row of the header is <th>Column1</th><th>Column 2<th> . . .   
    * Each row of the body is <td>Cell1</td><td>Cell 2<td> . . .   
* Add %>% pull(n) (from dplyr) to the inline R statement in the "Data" section, so its output is not rendered as a table  
	* pull() extracts single columns out of data frames  
  
Summary:  
  
* Course summarized the final component of the tidyverse process - communication is key!  
* Switzerland demographic map  
* Can show population density using geom_line()  
  
Example code is contained in the summary Excel worksheet.
  
  
  
***
  
###_Foundations of Probability in R_  
  
Chapter 1 - Binomial Distribution  
  
Flipping coins in R - for example, rbinom(1, 1, 0.5) - 1 draw of 1 coint with 50% of being heads:  
  
* Generally, interpretation of 1 is heads  
* rbinom(nDraws, nPerDraw, pPerDraw) - can generate multiple simulations at the same time  
* Frequent focus in this course will be on biased coins - pPerDraw != 0.5  
  
Density and cumulative density:  
  
* Histogram on a simulation can be a helpful way for understanding densities and likelihoods  
* With a known distribution, can get the exact answer using dbinom(nHit, nDraw, pPerDraw)  
* The "cumulative density" is the probability of getting this value or less  
	* pbinom(nHit, nDraw, pPerDraw)  # gives the cumulative probability of nHit or ferwer hits when making nDraw draws each at probability pPerDraw  
  
Expected value and variance:  
  
* Two interesting characteristics are the expected value and the variance of the distribution  
* The theoretical mean for the binomial is easy to calculate from the parameters  
	* mean = n * p  
* The theroretical variance (mean-squared distance from the mean) for the binomial is also easy to calculate from the parameters  
	* variance = n * p * (1 - p)  
  
Example code includes:  
```{r}

# Generate 10 separate random flips with probability .3
rbinom(10, 1, 0.3)

# Generate 100 occurrences of flipping 10 coins, each with 30% probability
rbinom(100, 10, 0.3)


# Calculate the probability that 2 are heads using dbinom
dbinom(2, 10, 0.3)

# Confirm your answer with a simulation using rbinom
mean(rbinom(10000, 10, 0.3) == 2)

# Calculate the probability that at least five coins are heads
1 - pbinom(4, 10, 0.3)

# Confirm your answer with a simulation of 10,000 trials
mean(rbinom(10000, 10, 0.3) >= 5)


# Here is how you computed the answer in the last problem
mean(rbinom(10000, 10, .3) >= 5)

# Try now with 100, 1000, 10,000, and 100,000 trials
mean(rbinom(100, 10, .3) >= 5)
mean(rbinom(1000, 10, .3) >= 5)
mean(rbinom(10000, 10, .3) >= 5)
mean(rbinom(100000, 10, .3) >= 5)


# Calculate the expected value using the exact formula
25 * 0.3

# Confirm with a simulation using rbinom
mean(rbinom(10000, 25, 0.3))


# Calculate the variance using the exact formula
25 * 0.3 * (1 - 0.3)

# Confirm with a simulation using rbinom
var(rbinom(10000, 25, 0.3))

```
  
  
  
***
  
Chapter 2 - Laws of Probability  
  
Probability of Event A and Event B:  
  
* Suppose there are two independent events, possibly with different probabilities, A and B  
	* P(A and B) = P(A) * P(B)  # assuming A and B are independent, as assumed throughout this chapter  
* If there are two boolean vectors, A and B, then A & B will give a single boolean vector that is the "and" on each pair of elements  
  
Probability of A or B:  
  
* P(A or B) = P(A) + P(B) - P(A and B)  
	* Alternately, P(A or B) = 1 - P(notA and notB)  
    * Can also use the mean(A | B) assuming that A and B are boolean vectors of the same length  
  
Multiplying random variables:  
  
* Suppose that you already have a variable X with a known mean and variance  
	* mean(a * X) = a * mean(X)  
    * var(a * X) = a^2 * var(X)  
  
Adding random variables:  
  
* Suppose that you already have random variables X and Y with known means and variances  
	* mean(X + Y) = mean(X) + mean(Y)  # does not require independence  
    * var(X + Y) = var(X) + var(Y)  # requires independence  
  
Example code includes:  
```{r}

# Simulate 100,000 flips of a coin with a 40% chance of heads
A <- rbinom(100000, 1, 0.4)

# Simulate 100,000 flips of a coin with a 20% chance of heads
B <- rbinom(100000, 1, 0.2)

# Estimate the probability both A and B are heads
mean(A & B)


# You've already simulated 100,000 flips of coins A and B
A <- rbinom(100000, 1, .4)
B <- rbinom(100000, 1, .2)

# Simulate 100,000 flips of coin C (70% chance of heads)
C <- rbinom(100000, 1, .7)

# Estimate the probability A, B, and C are all heads
mean(A & B & C)


# Simulate 100,000 flips of a coin with a 60% chance of heads
A <- rbinom(100000, 1, 0.6)

# Simulate 100,000 flips of a coin with a 10% chance of heads
B <- rbinom(100000, 1, 0.1)

# Estimate the probability either A or B is heads
mean(A | B)


# Use rbinom to simulate 100,000 draws from each of X and Y
X <- rbinom(100000, 10, 0.6)
Y <- rbinom(100000, 10, 0.7)

# Estimate the probability either X or Y is <= to 4
mean((X <= 4) | (Y <= 4))

# Use pbinom to calculate the probabilities separately
prob_X_less <- pbinom(4, 10, 0.6)
prob_Y_less <- pbinom(4, 10, 0.7)

# Combine these to calculate the exact probability either <= 4
prob_X_less + prob_Y_less - prob_X_less * prob_Y_less


# Simulate 100,000 draws of a binomial with size 20 and p = .1
X <- rbinom(100000, 20, 0.1)

# Estimate the expected value of X
mean(X)

# Estimate the expected value of 5 * X
mean(5 * X)

# Estimate the variance of X
var(X)

# Estimate the variance of 5 * X
var(5 * X)


# Simulate 100,000 draws of X (size 20, p = .3) and Y (size 40, p = .1)
X <- rbinom(100000, 20, 0.3)
Y <- rbinom(100000, 40, 0.1)

# Estimate the expected value of X + Y
mean(X + Y)

# Find the variance of X + Y
var(X + Y)

# Find the variance of 3 * X + Y
var(3 * X + Y)

```
  
  
  
***
  
Chapter 3 - Bayesian Statistics  
  
Updating with evidence:  
  
* Probability of A given B -> P(A | B) = P(A and B) / P(B)  
  
Prior probability - may not be equal odds prior to seeing any evidence:  
  
* The prior probability is the belief in the probabilities prior to seeing any evidence  
* Can just simulate the relative sizes - for example, if there is a 9:1 prior, simulate 90,000 vs. 10,000 before finding conditional probability  
  
Bayes theorem:  
  
* Basically multiply prior probability for A with likelihood of seeing event (density) if A  
	* Repeat for B, C, …  
    * Scale multiplied probabilities to add to one, and those are the posterior probabilities  
    * Pr(A|B) = P(A and B) / P(B)  
* The more generalized Bayes theory is  
	* Numer = P(B|A) * P(A)  
    * Denom = P(B|A) * P(A) + P(B | notA) * P(notA)  
    * P(A|B) = Numer / Denom  
  
Example code includes:  
```{r}

# Simulate 50000 cases of flipping 20 coins from fair and from biased
fair <- rbinom(50000, 20, 0.5)
biased <- rbinom(50000, 20, 0.75)

# How many fair cases, and how many biased, led to exactly 11 heads?
fair_11 <- sum(fair == 11)
biased_11 <- sum(biased == 11)

# Find the fraction of fair coins that are 11 out of all coins that were 11
fair_11 / (fair_11 + biased_11)


# How many fair cases, and how many biased, led to exactly 16 heads?
fair_16 <- sum(fair == 16)
biased_16 <- sum(biased == 16)

# Find the fraction of fair coins that are 16 out of all coins that were 16
fair_16 / (fair_16 + biased_16)


# Simulate 8000 cases of flipping a fair coin, and 2000 of a biased coin
fair_flips <- rbinom(8000, 20, 0.5)
biased_flips <- rbinom(2000, 20, 0.75)

# Find the number of cases from each coin that resulted in 14/20
fair_14 <- sum(fair_flips == 14)
biased_14 <- sum(biased_flips == 14)

# Use these to estimate the posterior probability
fair_14 / (fair_14 + biased_14)


# Simulate 80,000 draws from fair coin, 10,000 from each of high and low coins
flips_fair <- rbinom(80000, 20, 0.5)
flips_high <- rbinom(10000, 20, 0.75)
flips_low <- rbinom(10000, 20, 0.25)

# Compute the number of coins that resulted in 14 heads from each of these piles
fair_14 <- sum(flips_fair == 14)
high_14 <- sum(flips_high == 14)
low_14 <- sum(flips_low == 14)

# Compute the posterior probability that the coin was fair
fair_14 / (fair_14 + high_14 + low_14)


# Use dbinom to calculate the probability of 11/20 heads with fair or biased coin
probability_fair <- dbinom(11, 20, 0.5)
probability_biased <- dbinom(11, 20, 0.75)

# Calculate the posterior probability that the coin is fair
probability_fair / (probability_fair + probability_biased)


# Find the probability that a coin resulting in 14/20 is fair
probability_fair <- dbinom(14, 20, .5)
probability_biased <- dbinom(14, 20, .75)
probability_fair / (probability_fair + probability_biased)

# Find the probability that a coin resulting in 18/20 is fair
probability_fair <- dbinom(18, 20, .5)
probability_biased <- dbinom(18, 20, .75)
probability_fair / (probability_fair + probability_biased)


# Use dbinom to find the probability of 16/20 from a fair or biased coin
probability_16_fair <- dbinom(16, 20, 0.5)
probability_16_biased <- dbinom(16, 20, 0.75)

# Use Bayes' theorem to find the posterior probability that the coin is fair
(probability_16_fair * 0.99) / (probability_16_fair * 0.99 + probability_16_biased * 0.01)

```
  
  
  
***
  
Chapter 4 - Related Distributions  
  
Normal distribution - symmetrical bell curve, Gaussian:  
  
* The normal distribution can be defined by mean and standard deviation (or mean and variance)  
* Can simulate from the normal distribution with rnorm(n, mean, sd)  
  
Poisson distribution - approximates the binomial under the assumption of a large number of trials each with a low probability:  
  
* The Poisson distribution is described only by its mean, lambda  
	* Basically, lambda is nDraw * pPerDraw  
    * The variance of the Poisson distribution is equal to the mean  
* The Poisson distribution is best for modeling rare events where you really just care about counts (not proportions of a total potential universe)  
* One of the useful properties of the Poisson distribution is that when you add multiple Poisson distributions together, the result is also a Poisson distribution  
  
Geometric distribution - example of flipping a coin with probability p and assessing when the first success occurs:  
  
* The replicate() function is basically a wrapper to sapply() and can be helpful for simulations like this  
	* replicate(10, which(binom(100, 1, 0.1) == 1)[1])  
* The rgeom(nDraws, prob) will give back the geometric distribution  
    * The mean will be 1/prob - 1 since it is is the number of trials "before" the first success  
    * The mean would be 1/prob if instead the question is the number of trials to get the first success  
  
Example code includes:  
```{r}

compare_histograms <- function(variable1, variable2) {
  x <- data.frame(value = variable1, variable = "Variable 1")
  y <- data.frame(value = variable2, variable = "Variable 2")
  ggplot(rbind(x, y), aes(value)) +
    geom_histogram() +
    facet_wrap(~ variable, nrow = 2)
}


# Draw a random sample of 100,000 from the Binomial(1000, .2) distribution
binom_sample <- rbinom(100000, 1000, 0.2)

# Draw a random sample of 100,000 from the normal approximation
normal_sample <- rnorm(100000, 200, sqrt(160))

# Compare the two distributions with the compare_histograms function
compare_histograms(binom_sample, normal_sample)


# Use binom_sample to estimate the probability of <= 190 heads
mean(binom_sample <= 190)

# Use normal_sample to estimate the probability of <= 190 heads
mean(normal_sample <= 190)

# Calculate the probability of <= 190 heads with pbinom
pbinom(190, 1000, 0.2)

# Calculate the probability of <= 190 heads with pnorm
pnorm(190, 200, sqrt(160))


# Draw a random sample of 100,000 from the Binomial(10, .2) distribution
binom_sample <- rbinom(100000, 10, 0.2)

# Draw a random sample of 100,000 from the normal approximation
normal_sample <- rnorm(100000, 2, sqrt(1.6))

# Compare the two distributions with the compare_histograms function
compare_histograms(binom_sample, normal_sample)


# Draw a random sample of 100,000 from the Binomial(1000, .002) distribution
binom_sample <- rbinom(100000, 1000, 0.002)

# Draw a random sample of 100,000 from the Poisson approximation
poisson_sample <- rpois(100000, 2)

# Compare the two distributions with the compare_histograms function
compare_histograms(binom_sample, poisson_sample)


# Find the percentage of simulated values that are 0
mean(poisson_sample == 0)

# Use dpois to find the exact probability that a draw is 0
dpois(0, 2)


# Simulate 100,000 draws from Poisson(1)
X <- rpois(100000, 1)

# Simulate 100,000 draws from Poisson(2)
Y <- rpois(100000, 2)

# Add X and Y together to create Z
Z <- X + Y

# Use compare_histograms to compare Z to the Poisson(3)
compare_histograms(Z, rpois(100000, 3))


# Simulate 100 instances of flipping a 20% coin
flips <- rbinom(100, 1, 0.2)

# Use which to find the first case of 1 ("heads")
which(flips == 1)[1]


# Existing code for finding the first instance of heads
which(rbinom(100, 1, .2) == 1)[1]

# Replicate this 100,000 times using replicate()
replications <- replicate(100000, which(rbinom(100, 1, .2) == 1)[1])

# Histogram the replications with qplot
qplot(replications)


# Generate 100,000 draws from the corresponding geometric distribution
geom_sample <- rgeom(100000, 0.2)

# Compare the two distributions with compare_histograms
compare_histograms(replications, geom_sample)


# Find the probability the machine breaks on 5th day or earlier
pgeom(4, 0.1)

# Find the probability the machine is still working on 20th day
1 - pgeom(19, 0.1)


# Calculate the probability of machine working on day 1-30
still_working <- 1 - pgeom(0:29, 0.1)

# Plot the probability for days 1 to 30
qplot(1:30, still_working)

```
  
  
  
***
  
###_Inference for Numerical Data_  
  
Chapter 1 - Bootstrapping for Parameter Estimates  
  
Introduction - beginning with bootstrapping approach:  
  
* Example of 20 random apartment rents available in Manhattan  
	* Median is the best statistic  
* Bootstrap comes from the phrase "pulling yourself up by the bootstraps" (doing the impossible without any help)  
	* Take many random samples with replacement of the same length as the sample data, take their medians, and find the summary statistics about the median  
    * The bootstrap distribution is like multiple samples from the sample population  
* Can run bootstraps from the infer package, for example  
	* myData %>% infer::specify(response=) %>% infer::generate(reps=, type="bootstrap") %>% infer::calculate(stat="")  
  
Percentile and standard error methods:  
  
* Sampling with replacement allows for each item in the sample to potentially be in the population many more times  
* Can describe a bootstrap statistic using a CI, such as the 95th percentile  
* A more accurate calculation is typically to use the standard error approach  
	* sample statistic +/- t(df=n-1) * SEboot  
  
Re-centering bootstrap distributions for hypothesis testing:  
  
* Simulation methods to test whether a bootstrap parameter is less than, different than, or greater than a critical value  
* There is a multi-step process that includes  
	* Bootstrap distribution is centered around the same statistics to begin with  
    * Since we are now assuming Ho to be true, we shift the bootstrap distribution right/left as needed so that this default is true  
    * The p-value is then the number of observations that are at least as favorable to the alternate hypothesis as the observed sample statistic  
  
Example code includes:  
```{r}

manhattan <- readr::read_csv("./RInputFiles/manhattan.csv")

# Will need to either call library(infer) or add infer:: to this code
library(infer)

# Generate bootstrap distribution of medians
rent_ci_med <- manhattan %>%
  # Specify the variable of interest
  specify(response = rent) %>%  
  # Generate 15000 bootstrap samples
  generate(reps = 15000, type = "bootstrap") %>% 
  # Calculate the median of each bootstrap sample
  calculate(stat = "median")

# View the structure of rent_ci_med
str(rent_ci_med)

# Plot a histogram of rent_ci_med
ggplot(rent_ci_med, aes(x=stat)) +
  geom_histogram(binwidth=50)


# Percentile method
rent_ci_med %>%
  summarize(l = quantile(stat, 0.025),
            u = quantile(stat, 0.975))

# Standard error method

# Calculate observed median
rent_med_obs <- manhattan %>%
  # Calculate observed median rent
  summarize(median(rent)) %>%     
  # Extract numerical value
  pull()

# Determine critical value
t_star <- qt(0.975, df = nrow(manhattan) - 1)

# Construct interval
rent_ci_med %>%
  summarize(boot_se = sd(rent_ci_med$stat)) %>%
  summarize(l = rent_med_obs - t_star * boot_se,
            u = rent_med_obs + t_star * boot_se)


data(ncbirths, package="openintro")
str(ncbirths)

# Remove NA visits
ncbirths_complete_visits <- ncbirths %>%
  filter(!is.na(visits))
  
# Generate 15000 bootstrap means
visit_ci_mean <- ncbirths_complete_visits %>%
  specify(response=visits) %>%
  generate(reps=15000, type="bootstrap") %>%
  calculate(stat="mean")
  
# Calculate the 90% CI via percentile method
visit_ci_mean %>%
  summarize(l = quantile(stat, 0.05),
            u = quantile(stat, 0.95))


# Calculate 15000 bootstrap SDs
visit_ci_sd <- ncbirths_complete_visits %>%
  specify(response=visits) %>%
  generate(reps=15000, type="bootstrap") %>%
  calculate(stat="sd")

# Calculate the 90% CI via percentile method
visit_ci_sd %>%
  summarize(l = quantile(stat, 0.05),
            u = quantile(stat, 0.95))


# Generate 15000 bootstrap samples centered at null
rent_med_ht <- manhattan %>%
  specify(response = rent) %>%
  hypothesize(null = "point", med = 2500) %>% 
  generate(reps = 15000, type = "bootstrap") %>% 
  calculate(stat = "median")
  
# Calculate observed median
rent_med_obs <- manhattan %>%
  summarize(median(rent)) %>%
  pull()

# Calculate p-value
rent_med_ht %>%
  filter(stat > rent_med_obs) %>%
  summarize(n() / 15000)


# Generate 1500 bootstrap means centered at null
weight_mean_ht <- ncbirths %>%
  specify(response = weight) %>%
  hypothesize(null = "point", mu = 7) %>% 
  generate(reps=1500, type="bootstrap") %>% 
  calculate(stat="mean")
  
# Calculate observed mean
weight_mean_obs <- ncbirths %>%
  summarize(mean(weight)) %>%
  pull()

# Calculate p-value
weight_mean_ht %>%
  filter(stat > weight_mean_obs) %>%
  summarize((n()/1500) * 2)

```
  
  
  
***
  
Chapter 2 - Introducing the t-distribution  
  
The t-distribution - especially useful when the population standard deviation is unknown (as is typically the case):  
  
* The t-distribution is like the normal distribution, but with thicker tails  
	* Observations are more likely to be 2+ SD from the mean using the t-distribution than with the normal distribution  
    * The t-distribution is always centered at zero, and has a single parameter, degrees of freedom  
* As the degrees of freedom go to infinite, the t-distribution becomes the normal distribution  
	* Can always use the t-distribution, though  
* We can use the pt function to find probabilities under the t-distribution  
	* For a given cutoff value q and a given degrees of freedom df, pt(q, df) gives us the probability under the t-distribution with df degrees of freedom for values of t less than q  
    * In other words, P(tdf<T)P(tdf<T) = pt(q = T, df)  
* We can use the qt() function to find cutoffs under the t-distribution  
	* For a given probability p and a given degrees of freedom df, qt(p, df) gives us the cutoff value for the t-distribution with df degrees of freedom for which the probability under the curve is p  
    * In other words, if P(tdf<T)=pP(tdf<T)=p, then TT = qt(p, df)  
    * For example, if TT corresponds to the 95th percentile of a distribution, p=0.95p=0.95  
  
Estimating a mean with a t-interval:  
  
* Quantifying the expected variability of sample means - theory (CLM)  
* The Central Limit Theorem (CLM) states that the sample mean will be normal with population mean and appropriate standard error (population sigma divided by sqrt(n) where n is the sample size)  
	* Since we do not have the original population, we never really have the population sigma  
    * However, the standard error is frequently estimated as the sample standard deviation divided by the square root of the sample size  
    * We use a t-distribution with df=n-1 to account for the extra uncertainty  
* The CLM has some key assumptions that must be validated first  
	* Independence of observations - hard to check, but typically assumed when the sampling methodology is appropriate  
    * The more skewed the original population, the larger sample size that is needed  
* The function t.test(myVar, conf.level=) will generate a confidence interval for the mean of myVar, as well as a p-value for the mean being non-zero  
  
The t-interval for paired data:  
  
* Examples would be same student taking two tests - this means the data are NOT independent, but instead they are paired  
	* Can be helpful in these cases to create a variable diff which is the difference in test scores by student  
    * Can then just run the normal t-test on the differences  
  
Testing a mean with a t-test:  
  
* Can run t.test(myVar, mu=myNullValue, alternative="two.sided") to run a two-sided t-test for mean(myVar) != myNullValue  
    * Will provide a p-value as well as a 95% CI for the mean of myVar  
  
Example code includes:  
```{r}

# P(T < 3) for df = 10
(x <- pt(3, df = 10))

# P(T > 3) for df = 10
(y <- 1 - pt(3, df=10))

# P(T > 3) for df = 100
(z <- 1 - pt(3, df=100))

# Comparison
y == z
y > z
y < z


# 95th percentile for df = 10
(x <- qt(0.95, df = 10))

# upper bound of middle 95th percent for df = 10
(y <- qt(0.975, df = 10))

# upper bound of middle 95th percent for df = 100
(z <- qt(0.975, df = 100))

# Comparison
y == z
y > z
y < z


data(acs12, package="openintro")

# Subset for employed respondents
acs12_emp <- acs12 %>%
  filter(employment == "employed")

# Construct 95% CI for avg time_to_work
t.test(acs12_emp$time_to_work, conf.level=0.95)

t.test(acs12_emp$hrs_work, conf.level=0.95)


data(textbooks, package="openintro")

# 90% CI
t.test(textbooks$diff, conf.level = 0.9)

# 95% CI
t.test(textbooks$diff, conf.level = 0.95)

# 99% CI
t.test(textbooks$diff, conf.level = 0.99)

# Conduct HT
t.test(textbooks$diff, mu=0, alternative="two.sided", conf.level=0.95)


# Calculate 15000 bootstrap means
textdiff_med_ci <- textbooks %>%
  specify(response = diff) %>%
  generate(reps=15000, type="bootstrap") %>%
  calculate(stat = "median")
  
# Calculate the 95% CI via percentile method
textdiff_med_ci %>%
  summarize(l=quantile(stat, 0.025), 
            u=quantile(stat, 0.975))


data(hsb2, package="openintro")

# Calculate diff
hsb2 <- hsb2 %>%
  mutate(diff = math - science)
  
# Generate 15000 bootstrap means centered at null
scorediff_med_ht <- hsb2 %>%
  specify(response=diff) %>%
  hypothesize(null="point", mu=0) %>% 
  generate(reps=15000, type="bootstrap") %>% 
  calculate(stat="median")
  
# Calculate observed median of differences
scorediff_med_obs <- hsb2 %>%
  summarize(median(diff)) %>%
  pull()

# Calculate p-value
scorediff_med_ht %>%
  filter(stat > scorediff_med_obs) %>%
  summarize(p_val = (n() / 15000) * 2)

```
  
  
  
***
  
Chapter 3 - Inference for Difference in Two Parameters  
  
Hypothesis testing for comparing two means:  
  
* Data stem.cell are available in the openintro package - question of whether stem cells help with heart recovery in sheep  
	* Question is the impact of test vs. control, with each sheep having change measured, but only some having the stem cell therapy  
* For the hacker statistics approach, can randomly assign the sheep (multiple times) as test vs. control, and plot the ECDF (or similar) of the changes  
	* Can then compare how extreme our actual sample is relative to the hacker statitistics simulation  
* The library(infer) is built to help with problems like this  
	* library(infer) diff_ht_mean <- stem.cell %>%  
    * specify(__) %>% # y ~ x  
    * hypothesize(null = __) %>% # "independence" or "point"  
    * generate(reps = __, type = __) %>% # "bootstrap", "permute", or "simulate"  
    * calculate(stat="diff in means") %>%  
    * …  
* For problems like this, the null hypothesis is "independence" and the generation type is "permute"  
  
Bootstrap CI for difference in two means:  
  
* Take a bootstrap sample from each of the two groups  
* Calculate the bootstrap statistic of interest  
* Repeat as needed to calculate a bootstrap interval  
  
Comparing means with a t-test:  
  
* Looking at the average hourly rate vs. citizenship from the ACS data  
	* t.test(hrly_rate ~ citizen, data=acs12, null=0, alternative="two.sided")  
* Review of conditions required for the t-test  
	* Independence of observations (usually assumed with proper randomization and a sample size that is small relative to the population)  
    * Independence of observations across the samples (not paired)  
    * Skewed samples require larger sample sizes for the normality approximations to be valid  
  
Example code includes:  
```{r}

data(stem.cell, package="openintro")
str(stem.cell)


# Calculate difference between before and after
stem.cell <- stem.cell %>%
  mutate(change = after - before)

# Calculate observed difference in means
diff_mean <- stem.cell %>%
  # Group by treatment group
  group_by(trmt) %>%       
  # Calculate mean change for each group
  summarize(mean_change = mean(change)) %>%
  # Extract
  pull() %>% 
  # Calculate difference
  diff()                      


# Generate 1000 differences in means via randomization
diff_ht_mean <- stem.cell %>%
  # y ~ x
  specify(change ~ trmt) %>% 
  # Null = no difference between means
  hypothesize(null = "independence") %>% 
  # Shuffle labels 1000 times
  generate(reps = 1000, type = "permute") %>% 
  # Calculate test statistic
  calculate(stat = "diff in means", order=rev(levels(stem.cell$trmt)))

# Calculate p-value
diff_ht_mean %>%
  # Identify simulated test statistics at least as extreme as observed
  filter(stat > diff_mean) %>%
  # Calculate p-value
  summarize(p_val = (n() / 1000))


# Remove subjects with missing habit
ncbirths_complete_habit <- ncbirths %>%
  filter(!is.na(habit))

# Calculate observed difference in means
diff_mean <- ncbirths_complete_habit %>%
  # Group by habit group
  group_by(habit) %>%
  # Calculate mean weight for each group
  summarize(mean_weight = mean(weight)) %>%
  # Extract
  pull() %>%
  # Calculate difference
  diff()                             
  
# Generate 1000 differences in means via randomization
diff_ht_mean <- ncbirths_complete_habit %>%
  # y ~ x
  specify(weight ~ habit) %>%
  # Null = no difference between means
  hypothesize(null = "independence") %>%  
  # Shuffle labels 1000 times
  generate(reps = 1000, type = "permute") %>%
  # Calculate test statistic
  calculate(stat = "diff in means", order=rev(levels(ncbirths_complete_habit$habit)))

# Calculate p-value
diff_ht_mean %>%
  # Identify simulated test statistics at least as extreme as observed
  filter(stat < diff_mean) %>%
  # Calculate p-value
  summarize(p_val = (n()/1000) * 2)


# Generate 1500 bootstrap difference in means
diff_mean_ci <- ncbirths_complete_habit %>%
  specify(weight ~ habit) %>%
  generate(reps = 1500, type = "bootstrap") %>%
  calculate(stat = "diff in means", order=rev(levels(ncbirths_complete_habit$habit)))

# Calculate the 95% CI via percentile method
diff_mean_ci %>%
  summarize(l=quantile(stat, 0.025), 
            u=quantile(stat, 0.975))


# Remove subjects with missing habit and weeks
ncbirths_complete_habit_weeks <- ncbirths %>%
  filter(!is.na(habit) & !is.na(weeks))

# Generate 1500 bootstrap difference in medians
diff_med_ci <- ncbirths_complete_habit_weeks %>%
  specify(weeks ~ habit) %>%
  generate(reps = 1500, type = "bootstrap") %>%
  calculate(stat="diff in medians", order=rev(levels(ncbirths_complete_habit_weeks$habit)))

# Calculate the 92% CI via percentile method
diff_med_ci %>%
  summarize(l=quantile(stat, 0.04), 
            u=quantile(stat, 0.96))


# Create hrly_pay and filter for non-missing hrly_pay and citizen
acs12_complete_hrlypay_citizen <- acs12 %>%
  mutate(hrly_pay = income / (hrs_work * 52)) %>%
  filter(
    !is.na(hrly_pay),
    !is.na(citizen)
  )

# Calculate percent missing
new_n <- nrow(acs12_complete_hrlypay_citizen)
old_n <- nrow(acs12)
(perc_missing <- (old_n - new_n) / old_n) 

# Calculate summary statistics
acs12_complete_hrlypay_citizen %>%
  group_by(citizen) %>%
  summarize(
    x_bar = mean(hrly_pay),
    s = sd(hrly_pay),
    n = n()
  )

# Plot the distributions
ggplot(data = acs12_complete_hrlypay_citizen, mapping = aes(x = hrly_pay)) +
  geom_histogram(binwidth = 5) +
  facet_grid(. ~ citizen, labeller = labeller(citizen = c(no  = "Non citizen", 
                                                          yes = "Citizen"))) 

# Construct 95% CI
t.test(hrly_pay ~ citizen, data=acs12_complete_hrlypay_citizen, null=0, alternative="two.sided")

```
  
  
  
***
  
Chapter 4 - Comparing Many Means  
  
Vocabulary score vary between social class:  
  
* Data set includes wordsum (vocabular score) and class (lower, working, middle, upper)  
  
ANOVA - Analysis of Variance:  
  
* Example of runners in a marathon finsihing in different times based on many different factors  
* Suppose that we are interested in a specific variable X (perhaps training time)  
	* Variability in finishing time due to X  
    * Variability in finishing time due to all factors other than X  
* The null hypothesis is that the means are the same across all of the groups, while the alternate hypothesis is that at least one mean is different  
* Can assess the total variability of vocabulary scores as follows  
	* Variability between groups  
    * Variability within groups  
* Running aov(x ~ y, data=z) will run ANOVA and report on  
	* myVar - between groups df, sumsq, and the like  
    * Residuals - within groups df, sumsq, and the like  
    * Can also calculate the percentage of variability explained  
    * The F-statistic is the key test statistic for this type of analysis  
  
Conditions for ANOVA:  
  
* Independence - within groups (samples observations must be independent) and across groups (must be non-paired)  
	* Generally assumed to be OK with a properly stratified and randomized sample that is reasonably small relative to the population  
    * The between groups pairing can be handled with techniques not covered during this course  
* Approximate normality within each group  
* Equal variance within each group  
	* Especially important when sample sizes are significantly different across groups  
  
Post-hoc testing - determining which of the means are different:  
  
* Can run t-tests for each group comparison, though this will epxlode the Type I error rate  
	* Can instead use a modified significance level for each individual test to maintain the desired overall Type I error rate  
    * The Bonferroni correction is common - newAlpha = tgtAlpha / K where K = k * (k-1) / 2 with k being the number of groups  
* Since there has been an assumption of constant variance, can use a consistent standard error and degrees of freedom for all the tests  
  
Wrap-up:  
  
* Simulation-based and CLM-based inference  
* Single variables and bivariate variables  
* Two levels and multiple levels  
  
Example code includes:  
```{r}

gss <- readr::read_csv("./RInputFiles/gss_wordsum_class.csv")
str(gss)


ggplot(gss, aes(x=wordsum)) +
  geom_histogram(binwidth=1) +
  facet_grid(class ~ .)


aov_wordsum_class <- aov(wordsum ~ class, data=gss)
broom::tidy(aov_wordsum_class)


gss %>%
  group_by(class) %>%
  summarize(s = sd(wordsum))


# Conduct the pairwise.t.test with p.adjust = "none" option (we'll adjust the significance level, not the p-value). The first argument is the response vector and the second argument is the grouping vector.
pairwise.t.test(gss$wordsum, gss$class, p.adjust = "none") %>%
  broom::tidy()

```
  
  
  
***
  
###_Introduction to Statistics with R: Correlation and Linear Regression_  
  
Chapter 1 - Introduction to Correlation Coefficients  

How are correlation coefficients calculated?  
  
* Can be calculated using the raw-score formula or the Z-score formula  
* The general formula for calculating the correlation coefficient between two variables is  
	* r=cov(A,B) / [sA * sB]  
    * where cov(A,B) is the covariance between A and B, while sA and sB are the standard deviations  
* The covariance is defined as follows  
	* diff_A = A - mean(A)  
    * diff_B = B - mean(B)  
    * cov(A, B) = sum(diff_A * diff_B) / (length(A) - 1)  # A and B need to be of the same length, so length(A) or length(B) will do  
* The standard deviation is defined as the sample standard deviation, so (length(A) - 1) is in the denominator prior to the square root being taken  
	* sd_A = sqrt( sum(diff_A ** 2) / (length(A) - 1) )  
  
Usefulness of correlation coefficients:  
  
* Correlation can range between +1 (perfect positive correlation), -1 (perfect negative correlation), and 0 (no linear relationship)  
* When variables are strongly correlated, knowing one variable can help you predict another variable  
	* Working memory capacity is strongly correlated with intelligence and IQ  
  
Points of caution:  
  
* Correlation does not imply causation  
* The magnitude of correlation depends on many factors - sampling (full random vs. targeted population), measurement (reliable and valid), etc.  
	* Attenuation of correlation due to restriction of range - correlation on college graduates only may not work well  
* Correlation coefficient is a sample statistic, just like the mean  
  
Example code includes:  
```{r cache=TRUE}

PE <- read.table("http://assets.datacamp.com/course/Conway/Lab_Data/Stats1.13.Lab.04.txt", header=TRUE)

```


```{r}

# Take a quick peek at both vectors
(A <- c(1, 2, 3))
(B <- c(3, 6, 7))

# Save the differences of each vector element with the mean in a new variable
diff_A <- A - mean(A)
diff_B <- B - mean(B)

# Do the summation of the elements of the vectors and divide by N-1 in order to acquire the covariance between the two vectors
cov <- sum(diff_A*diff_B)/ (length(A)-1)


# Square the differences that were found in the previous step
sq_diff_A <- diff_A ** 2
sq_diff_B <- diff_B ** 2

# Take the sum of the elements, divide them by N-1 and consequently take the square root to acquire the sample standard deviations
sd_A <- sqrt(sum(sq_diff_A)/(length(A)-1))
sd_B <- sqrt(sum(sq_diff_B)/(length(B)-1))


# Combine all the pieces of the puzzle
correlation <- cov / (sd_A * sd_B)
correlation

# Check the validity of your result with the cor() command
cor(A, B)


# Read data from a URL into a dataframe called PE (physical endurance) - moved above to cache
# PE <- read.table("http://assets.datacamp.com/course/Conway/Lab_Data/Stats1.13.Lab.04.txt", header=TRUE)

# Summary statistics
psych::describe(PE)

# Scatter plots
plot(PE$age ~ PE$activeyears)
plot(PE$endurance ~ PE$activeyears)
plot(PE$endurance ~ PE$age)


# Correlation Analysis
round(cor(PE[, !(names(PE) == "pid")]), 2)

# Do some correlation tests. If the null hypothesis of no correlation can be rejected on a significance level of 5%, then the relationship between variables is  significantly different from zero at the 95% confidence level
cor.test(PE$age, PE$activeyears)
cor.test(PE$endurance, PE$activeyears)
cor.test(PE$endurance, PE$age)


# The impact dataset is already loaded in
rawImpactData <- " 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, control, control, control, control, control, control, control, control, control, control, control, control, control, control, control, control, control, control, control, control, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, concussed, 95, 90, 87, 84, 92, 89, 78, 97, 93, 90, 89, 97, 79, 86, 85, 85, 98, 95, 96, 92, 79, 85, 97, 89, 75, 75, 84, 93, 88, 97, 93, 96, 84, 89, 95, 95, 97, 95, 92, 95, 88, 82, 77, 72, 77, 79, 63, 82, 85, 66, 76, 79, 60, 59, 60, 76, 85, 83, 67, 84, 81, 85, 91, 74, 63, 68, 78, 74, 80, 73, 74, 70, 81, 72, 90, 74, 70, 63, 65, 69, 35.29, 31.47, 30.87, 41.87, 33.28, 40.73, 38.09, 31.65, 39.59, 30.53, 33.65, 37.51, 40.39, 32.88, 33.39, 35.13, 38.51, 29.64, 35.32, 27.36, 27.19, 32.66, 26.29, 28.92, 32.77, 32.92, 34.26, 36.08, 31.63, 28.89, 35.81, 33.61, 34.46, 39.18, 33.14, 33.03, 39.01, 35.06, 30.58, 38.45, 0.42, 0.63, 0.56, 0.66, 0.56, 0.81, 0.66, 0.79, 0.68, 0.60, 0.74, 0.51, 0.82, 0.59, 0.82, 0.63, 0.73, 0.57, 0.65, 1.00, 0.57, 0.71, 0.82, 0.61, 0.72, 0.50, 0.54, 0.65, 0.66, 0.71, 0.55, 0.79, 0.48, 0.55, 1.20, 0.73, 0.60, 0.84, 0.60, 0.42, 11,  7,  8,  7,  7,  6,  6, 10,  7, 10,  7,  7, 12,  2,  9, 10, 10,  8,  5, 11,  7,  9,  9,  9,  8,  9,  6, 10,  9,  7,  9,  7,  7, 10, 10, 11, 10,  5,  8, 11, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 97, 86, 90, 85, 87, 91, 90, 94, 91, 93, 92, 89, 84, 81, 85, 87, 96, 93, 95, 93, 63, 79, 91, 85, 74, 72, 80, 59, 75, 90, 66, 85, 72, 82, 80, 59, 74, 62, 67, 66, 86, 80, 79, 70, 77, 85, 60, 72, 83, 68, 72, 79, 67, 71, 61, 72, 78, 85, 67, 80, 75, 79, 80, 72, 56, 66, 74, 69, 79, 73, 69, 61, 79, 66, 80, 70, 62, 54, 57, 63, 35.61, 37.01, 20.15, 33.26, 28.34, 33.47, 44.28, 36.14, 37.42, 25.19, 23.63, 26.32, 43.70, 32.40, 39.32, 35.62, 39.95, 35.62, 30.21, 30.37, 29.23, 44.45, 26.12, 27.98, 60.77, 31.91, 49.62, 35.68, 55.67, 25.70, 35.21, 33.01, 37.46, 53.20, 33.20, 34.59, 39.66, 35.09, 32.30, 44.49, 0.65, 0.49, 0.75, 0.19, 0.59, 0.48, 0.77, 0.90, 0.65, 0.59, 0.55, 0.56, 0.57, 0.69, 0.73, 0.48, 0.43, 0.37, 0.47, 0.50, 0.61, 0.65, 1.12, 0.65, 0.71, 0.79, 0.64, 0.70, 0.68, 0.73, 0.58, 0.97, 0.56, 0.51, 1.30, 0.70, 0.74, 1.24, 0.65, 0.98, 10,  7,  9,  8,  8,  5,  6, 10,  8, 11,  9,  9, 10,  3, 10, 12, 10,  9,  5, 11,  3,  6,  5,  5,  1,  9,  7, 11,  6,  3,  4,  3,  1,  7,  7,  4,  5,  2,  6,  5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0, 26, 34, 27, 22, 26, 35, 43, 31, 39, 25, 31, 38, 14, 16, 33, 13, 27, 15, 19, 39"
rawImpactNames <- c('subject', 'condition', 'vermem1', 'vismem1', 'vms1', 'rt1', 'ic1', 'sym1', 'vermem2', 'vismem2', 'vms2', 'rt2', 'ic2', 'sym2')
splitImpactData <- stringr::str_split(rawImpactData, ",")
impactRawMatrix <- matrix(data=splitImpactData[[1]], ncol=length(rawImpactNames))
colnames(impactRawMatrix) <- rawImpactNames

rawImpactDF <- as.data.frame(impactRawMatrix, stringsAsFactors=FALSE)
for (intCtr in c(1, 3:ncol(rawImpactDF))) { rawImpactDF[, intCtr] <- as.numeric(rawImpactDF[, intCtr]) }
rawImpactDF$condition <- factor(stringr::str_replace_all(rawImpactDF$condition, " ", ""))
impact <- rawImpactDF


# Summary statistics entire dataset
psych::describe(impact)

# Calculate correlation coefficient
entirecorr <- round(cor(impact$vismem2, impact$vermem2), 2)

# Summary statistics subsets
psych::describeBy(impact, impact$condition)

# Create 2 subsets: control and concussed
control <- subset(impact, condition == "control")
concussed <- subset(impact, condition == "concussed")

# Calculate correlation coefficients for each subset
controlcorr <- round(cor(control$vismem2, control$vermem2), 2)
concussedcorr <- round(cor(concussed$vismem2, concussed$vermem2), 2)

# Display all values at the same time
correlations <- cbind(entirecorr, controlcorr, concussedcorr)
correlations

```
  
  
  
***
  
Chapter 2 - Introduction to Linear Regression  
  
Introduction to regression:  
  
* Linear regression uses correlations to make predictions about one variable by knowing other variable(s)  
* Simple regression has only a single predictor variable while multiple regression has many predictor variables  
  
Regression equations and the R-squared value:  
  
* Simple regression (one predictor) vs. multiple regression (2+ predictors)  
	* y = m (intercept) + b (slope) * x + e (error or residual)  
    * y = Bo + B1 * X1 + e (more generlized equation - allows for Bi * Xi for as many i as needed)  
* R is the multiple correlation coefficient, which is the correlation between the predicted outcomes and the actual outcomes  
	* R^2 is the percentage of variance explained by the model  
* The regression equation would then be  
	* sym2=B0+B1(ic2)+e,  
    * where B0 is the intercept, B1 the slope and e the residual error  
* There are five values required for the calculation of a regression line for this model:  
	* The mean of sym2: mean_sym2,  
    * The mean of ic2: mean_ic2,  
    * The standard deviation of sym2: sd_sym2,  
    * The standard deviation of ic2: sd_ic2, and  
    * The correlation coefficient between sym2 and ic2: r  
* The general formula for the slope is:  
	* B1=r*sd(y)/sd(x)  
* The general formula for the intercept is  
	* B0=mean(y)−B1*mean(x)  
  
Multiple linear regression:  
  
* Adding predictor variables (or better predictor variables) can increase the predictive power of the regression  
  
Example code includes:  

```{r}

# Look at the dataset. Note that the variables we are interested in are on the 9th to 14th columns
head(impact)

# Create a correlation matrix for the dataset
correlations <- cor(impact[, 9:14])

# Create the scatterplot matrix for the dataset
corrplot::corrplot(correlations)


# Calculate the required means, standard deviations and correlation coefficient
mean_sym2 <- mean(impact$sym2)
mean_ic2 <- mean(impact$ic2)
sd_sym2 <- sd(impact$sym2)
sd_ic2 <- sd(impact$ic2)
r <- cor(impact$ic2,impact$sym2)

# Calculate the slope
B_1 <- r * ( sd_sym2 )/( sd_ic2 )

# Calculate the intercept
B_0 <- mean_sym2 - B_1 * mean_ic2

# Plot of ic2 against sym2
plot(x=impact$ic2, y=impact$sym2, main = "Scatterplot", ylab = "Symptoms", xlab = "Impulse Control")

# Add the regression line
abline(B_0, B_1, col = "red")


# Construct the regression model
model_1 <- lm(impact$sym2 ~ impact$ic2)

# Look at the results of the regression by using the summary function
summary(model_1)

# Create a scatter plot of Impulse Control against Symptom Score
plot(impact$sym2 ~ impact$ic2, main = "Scatterplot", ylab = "Symptoms", xlab = "Impulse Control")

# Add a regression line
abline(model_1, col = "red")


# Multiple Regression
model_2 <- lm(impact$sym2 ~ impact$ic2 + impact$vermem2)

# Examine the results of the regression
summary(model_2)

# Extract the predicted values
predicted <- fitted(model_2)

# Plotting predicted scores against observed scores
plot(predicted ~ impact$sym2, main = "Scatterplot", xlab = "Observed Scores", ylab = "Predicted Scores")
abline(lm(predicted ~ impact$sym2), col = "green")

```
  
  
  
***
  
Chapter 3 - Linear Regression Models (cont)  
  
Estimation of coefficients - key concept is to minimize the residuals (specifically, residuals-squared):  
  
* Ordinary Least Squares (OLS) is the process of minimizing the sum-squared of the residuals  
* There is a total sum-squared of the dependent variable, and a model sum-squared (portion that is explained by the model)  
  
Estimation of standardized and unstandardized regression coefficients:  
  
* For simple regression, B1 = r * (sdY / sdX)  
	* If X and Y have been standardized, the B1 = r  
* Executing a standardized linear regression in R is very similar to executing an unstandardized linear regression but involves the extra step of standardizing the variables by using the scale() function  
  
Assumptions of linear regression:  
  
* Normal distribution for Y  
* Linear relationship between X and Y  
* Homoscedasticity (constant variance)  
* Reliable/valid/representative measures for both X and Y  
* General process for assessing these is to examine the residuals  
  
Anscombe's quartet:  
  
* Data set with identical correlations and identical sd(x), sd(y) for 4 highly distinct data sets  
	* Same regression line for Y = 3 + 0.5 * X  
* Calculate the residuals and plot them against the x variable(s)  
	* If there is any trend or pattern, then the assumptions for the linear regression have been violated  
  
Example code includes:  
```{r}

# Create a linear regression with `ic2` and `vismem2` as regressors
model_1 <- lm(impact$sym2 ~ impact$ic2 + impact$vismem2)

# Extract the predicted values
predicted_1 <- fitted(model_1)

# Calculate the squared deviation of the predicted values from the observed values
deviation_1 <- (impact$sym2 - predicted_1) ** 2

# Sum the squared deviations
SSR_1 <- sum(deviation_1)
SSR_1


# Create a linear regression with `ic2` and `vermem2` as regressors
model_2 <- lm(impact$sym2 ~ impact$ic2 + impact$vermem2)

# Extract the predicted values
predicted_2 <- fitted(model_2)

# Calculate the squared deviation of the predicted values from the observed values
deviation_2 <- (impact$sym2 - predicted_2) ** 2

# Sum the squared deviations
SSR_2 <- sum(deviation_2)
SSR_2


# Create a standardized simple linear regression
model_1_z <- lm(scale(impact$sym2) ~ scale(impact$ic2))

#Look at the output of this regression model
summary(model_1_z)

# Extract the R-Squared value for this regression
r_square_1 <- summary(model_1_z)$r.square

#Calculate the correlation coefficient
corr_coef_1 <- sqrt(r_square_1)


# Create a standardized multiple linear regression
model_2_z <- lm(scale(impact$sym2) ~ scale(impact$ic2) + scale(impact$vismem2))

# Look at the output of this regression model
summary(model_2_z)

# Extract the R-Squared value for this regression
r_square_2 <- summary(model_2_z)$r.squared

# Calculate the correlation coefficient
corr_coef_2 <- sqrt(r_square_2)


# Extract the residuals from the model
residual <- resid(model_2)

# Draw a histogram of the residuals
hist(residual)

# Extract the predicted symptom scores from the model
predicted <- fitted(model_2)

# Plot the residuals against the predicted symptom scores
plot(residual ~ predicted, main = "Scatterplot", xlab="Model 2 Predicted Scores", ylab="Model 2 Residuals" )
abline(lm(residual ~ predicted), col="red")

```
  
  
  
***
  
###_Inference for Linear Regression_  
  
Chapter 1 - Inferential Ideas  
  
Variability in regression lines:  
  
* Different samples would produce different regression lines; question is the magnitude of the impact of sampling variability  
* Can take many bootstrap samples, calculate the regressions for each, and use broom::tidy() to get the various slopes  
  
Research question - linear modeling for relationships between fat, carbohydrates, and calories in Starbucks food:  
  
* Can look at either a one-sided or two-sided test of relationships among any two of the variables  
* The standard error gives a sense for the uncertainty of the least-squares point estimate  
* The p-value reported in the lm defaults to being a two-sided t-test for the intercept and coefficient estimates  
	* Can divide the p-value by 2 for a 1-sided test (which is only appropriate if the original research question was one-sided)  
  
Variability of coefficients:  
  
* Using the BikeTrail data from masiacData, can look at bicycle volume as a function of high temperature  
* There is frequently variability in the intercept/coefficient estimates based on differences between the sample and the population  
	* The "tighter" the data, generally the tighter the standard errors for the intercept/coefficient estimates  
    * When there is less variability along the x-axis, generally the uncertainty in the slope increases  
  
Example code includes:  
```{r}

# Load the mosaicData package and the RailTrail data
library(mosaicData)
data(RailTrail)

# Fit a linear model
ride_lm <- lm(volume ~ hightemp, data=RailTrail)

# View the summary of your model
summary(ride_lm)

# Print the tidy model output
ride_lm %>% broom::tidy()


expData1 <- c(-4.3, 0.19, -2.59, -0.43, 0.59, -2.74, 3.09, 3.51, 0.56, 5.89, 0.36, -0.01, 2.59, 1.51, 2.89, -8.26, -0.46, 3.28, 4.85, 1.16, 3.03, 2.24, 1.78, -0.26, 4.29, 6.92, -6.34, 0.49, 3.4, 3.08, 2.1, -1.93, 3.72, 0.52, -4.65, 4.24, -1.21, 5.15, -10.43, 6.46, -2.78, 0.7, 2.93, -4.84, -7.08, -3.98, 8.27, -4.51, -5.22, -2.17, 2.32, 0.37, -2.53, 3.2, -8.02, -1.82, -6.17, 1.45, -0.19, -0.91, -2.02, 1.13, 11.2, 4.43, 0.88, -0.28, -9.29, 0.18, -6.9, 0.44, -9.1, -1.21, 11.32, -3.3, 3.56, 1.28, 5.76, -2.73, -9.69, -4.43, 5.71, 1.09, -8.28, -7.12, -0.33, -4.3, 4.16, 4.83, -0.29, -3.78, 5.03, 12.3, 4.79, 0.69, -11.06, 3.73, -6.64, -0.24, 5.08, -0.48, 0.68, 4.43, 2.11, 1.8, 2.98, -4.84, -3.9, 4.1, 0.05, -7.43, -2.41, 1.14, -1.87, 11.12, 6.26, 1.29, -4.54, 5.38, 3.09, -4.59, 8.55, -4.21, -0.92, 0.79, -3.48, -6.13, 3.58, 4.54, -4.83, -13.5, 1.58, -1.03, 1.34, -1.46, 5.53, -4.23, -6.95, 6.17, -0.89, 9.95, -4.12, 0.08, 2.49, -8.42, -2.4, -6.96, 7.92, -5.04, -0.25, -0.63, 8.4, 4.18, -4.86, 0.99, -5.54, -4.23, -2.23, 2.21, -0.05, -2.67, -1.14, 3.3, -5.48, 3.86, 2.1, 4.81, -1.09, -10.97, -16.68, -8.58, 3.78, 5.94, 0.35, 0.14, -8.6, -3.44, -5.14, -6.65, -0.49, -1.99, 3.54, 4.7, -0.61, 8.69, 0.91, 0.71, 3.6, -3.1, -2.99, 5.82, 3.84, 0.82, -2.74, -6.27, -3.03, 1.29, 1.58, 1.76, 4.64, -7.24, 1.54, 0.83, -0.6, -0.29, 0.78, -8.42, 9.76, 14.35, -1.09, -13.42, -1.72, 4.49, -0.02, -0.47, 8.93, 5.27, -6.06, 12.66, 0.53, -3.08, 0.52, -0.71, -0.39, -1.11, -1.72, 8.66, -1.41, 2.77, 1.03, -6.97, 7.57, -10.75, -0.88, -2.53, 1.64, 6.48, -1.61, -1.98, -5.91, 7.25, -1.67, 4.26, -7.22, 6.03, 2.92, 4.08, 9.65, -12.34, 1.24, 3.76, 3.25, -9.13, -3.23, 0.51, -1.52, -3.44, 6.75, -0.18, -3.92, -4.14, 1.14, 6.44, -0.32, 5.91, -3.55, -8.99, -6.38, -2.64, -1.47, -3.91, 12.07, 5.55, -7.94, 10.98, -6.57, -3.43, -1.13, 9.51, 11.19, -3.21, -3.19, -7.94, 2.4)
expData2 <- c(-4.59, 6.5, 3.8, -6.42, 0.78, -2.4, -2.55, -3.2, -6.3, 4.69, -0.05, 5.71, 6.5, 3.69, -4.75, 4.87, -2.42, -5.04, 3.75, 1.69, -0.19, 8.33, 2.8, -0.09, 6.24, -3.73, -2.64, 8.11, -4.43, 4.42, 3.46, -6.71, -5.47, 6.84, 4.94, 2.23, 0.92, 1.56, -3.52, -5.42, -1.04, -4.33, -0.63, -1.72, -5.42, -8.92, -4.8, -6.53, 3.33, 3.39, 4.08, -3.03, -5.11, 7.04, -0.93, -2.56, -1.45, 8.75, -4.01, -5.87, 3.36, 5.83, 1.13, -1.25, -0.04, 0.23, 0.95, 3.16, -7.17, 12.37, -9.98, -9.73, 1.55, -8.56, 13.58, 0.56, 6.39, 2.34, -5.11, 6.48, -1.62, -1.16, -6.37, 7.48, 3.51, 4.82, 1.73, -0.48, -0.84, 2.58, -3.24, -1.33, 4.69, -0.99, 9.78, -16.75, -2.92, 10.15, -4.64, 5.66, 0.89, 2.11, 1.66, 3.78, 3.43, -1.09, -1.43, -10.07, -0.87, 4.41, -3.55, -1.66, 8.28, 8.3, 1.03, 6.42, -0.33, -2.63, -4.12, 6.68, -1.32, 10.69, 7.11, -3.75, 1.16, 5.19, -4.41, -4.13, -3.32, -8.24, -3.19, 1.1, 5.45, 2.19, -10.27, -0.87, -1.32, -2.77, 7.39, -14.48, -2.06, -3.46, -4.21, -6.55, -1.59, -0.44, -3.11, -4.21, -8.38, 0.01, 10.58, 3.05, 3.67, -2.52, 2.05, -2, 7.04, -0.42, -12.23, -0.44, -1.66, -1.31, -0.16, 1.72, -3.25, 2.56, -0.21, -1.59, 2.35, -2.5, 0.44, 8.61, 2.83, 10.75, 1.1, -0.89, 4.89, -0.91, 1.83, 3.2, -1.16, -3.23, 0.96, 2.59, 6.36, -0.53, -4.2, -1.13, 2.37, -1.06, -3.69, -0.25, 8.21, -5.84, -5.53, -3.03, -0.79, -0.72, -0.67, 3.23, -6.51, 2.06, -0.4, 0.75, -2.39, -2.27, -3.65, -7.56, 3.24, -4.05, -4.2, -5.91, 5.24, -11.65, -4.16, -5.99, 1.22, 1.32, -3.63, -0.9, -3.52, -5.25, 8.05, 4.09, -3.22, 5.71, 0.67, -5.46, -5.24, -1.7, -6.4, 0.48, 4.49, 15.97, -1.42, 2.41, -1.75, 4.77, -4.45, 0.88, 0.24, 11.64, -0.51, 1.58, 4.18, -3.51, 2.32, -2.15, -5.42, 5.6, 4.18, -4.82, -1.41, -5.32, 0.58, 1.23, -5.35, -5.88, 0.76, -2.81, 0.59, -2.26, 4.05, 0.32, 5.97, 4.22, -1.79, 3.28, -4.16, -4.88, -1.24, -7.38, -2.67, -4.56, 2.45, 4.92, 1.84, -1.6, 4.79, -4.02, -9.2, 6.78, -8.21, -0.18, -4.02, 4.84, 2.81, -2.65, -4.72, -0.83, -4.69, 7.94, 3.53, 4.25, 5.06, 7.88, -1.08, -0.78, 3.41, -10.45, 0.16, 0.13, -0.6, 1.82, 5.68, 5.7, 4.66, -5.4, 7.12, -2.49, 1.5, 1.27, -8.26, 0.58, 0.04, 3.17, -3.23, -0.66, -3.2, 1.59, -4, -1.96, -3.48, -3.4, -3.95, 4.52, 2.5, -3.37, -14.81, -3.22, -3.57, 2.44, 0.17, 4.8, -6.15, -3.4, -4.1, -2.68, 5.86, 2.92, -0.19, -4.64, 9.4, 6.49, -5.84, -6.62, 2.86, -3.56, -4.6, -4.87, 7.32, 3.82, 8.99, -1.46, 4.98, -1.41, -5.89, -8.86, 6.87, -7.25, 2.67, 2.81, -0.22, -3.37, 6.74, 3.33, 4.72, 1.02, -3.02, -4.9, 2, 3.41, 0.5, -7.36, 6.36, 4, 2.24, -6.75, -5.62, -8.14, 1.82, 6.23, -0.18, 10.71, -0.57, 1.38, 9.5, 1.12, 3.08, 0.08, -4.75, 4.23, -2.23, -0.82, 1.84, -1.15, 4.12, -5.86, -0.16, -6.5, 4.86)
expData3 <- c(-0.62, 1.5, 5.44, -1.68, -10.04, 11.49, 1.48, -1.82, 1.57, 3.06, -2.36, -7.98, 0.25, -1.77, 3.32, 1.72, -7.55, 7.24, 2.78, -4.41, -2.55, -1.3, -1.49, 2.78, -4.37, -4.41, 0.57, 0.7, -0.56, 0.17, -2.52, 0.5, 2.46, -5.55, 2.98, 0.51, -0.28, 3.97, 6.74, 0.14, 3.54, 0.38, -2.69, 1.59, 3.09, -2.73, 4.93, 7.43, 1.76, 0.77, 4.54, 3.69, 5.75, -2.68, -1.01, 6.47, 1.91, -3.48, -2.91, 3.62, -3.72, 2.09, 0.63, -6.95, -0.66, -8.25, 6.6, -3.02, 3.51, 11.77, -1.78, -1.57, 5.58, -0.44, 3.07, -2.54, -3.1, 3.77, 8.05, -2.44, -0.95, 3.73, 1.64, 7.64, 3.63, 3.39, 1.71, -6.25, -3.47, 1.6, 3.49, 0.94, 0.18, -4.29, -2.62, 14.57, -1.73, 1.79, 2.54, -2.94, -0.56, 6.87, -4.81, 6.45, 4.2, 1.65, 8.4, 7.45, 7.11, 5.56, 1.06, -8.52, -7.68, -6.63, -4.09, 0.16, -6.08, -5.78, -4.46, -1.35, 3.34, -0.51, -3.65, -3.82, 0.64, 8.2, 14.07, -0.87, 3.3, 1.7, -3.17, -0.57, -1.06, 5.74, 0.79, -5.42, -2.22, 3.72, 2.88, -5.73, 0.82, -3.04, 6.11, 7.04, 2.84, 0.29, -2.37, 4.49, -5, -4.09, 0.33, 0.34, 0.81, 2.11, -1.55, -0.75, -7.49, 6.03, 0.14, 3.58, -0.67, 7.74, 5.55, 5.44, -8.21, 8.48, 2.15, 0.04, -3.68, 6.09, 4.06, 2.85, 2.47, -1.37, 3.66, 0.63, 0.46, -1.82, -7.6, 0.05, -3.03, -7.56, 1.56, 2.44, -2.56, -9.01, -0.19, -5.88, -7.51, -5.84, 3.79, -18, 5.33, -4.15, -6.26, 0.53, 15.21, 4.85, 1.98, -1.25, -1.12, -5.65, -0.96, 11.19, 2.76, -2.89, 0.49, -1.83, -2.52, -1.03, -1.54, -1.22, 4.27, -2.39, 0, -3.61, 0.93, -8.6, -4.41, 5.23, -3.77, 0.99, -6.99, -1.57, 3, -0.47, -3.44, 10.14, 2.8, 1.28, -0.16, 8, 4.47, 1.46, 0.86, -3.14, 1.47, 2.22, -0.05, -1.66, 3.6, -2.25, -5.84, 5.91, 2.39, 4.85, 5.07, -2.37, 0.86, -4.37, -3.32, 2.24, -3.78, 1.35, 0.01, -1.53, -3.88, 2.32, -4.27, -1.08, 4.45, 3.55, 1.82, 11.33, 1.49, -1.67, 0.49, -3.35, 0.26, -2.57, -2.51, -13.35, 6.11, 8.47, 3.94, -4.56, -3.28, 3.92, 5.81, -3.57, -1.75, -4.77, 4, -3.46, -2.25, -0.94, -4.16, -11.13, 5.81, -3.29, 5.69, 10.75, 4.29, -0.21, -0.38, 6.03, -1.97, -4.57, 7.61, -5.07, 3.82, 1.73, 8.15, 5.79, 0.19, 1.28, 3.23, -5.88, -10.91, 9.61, -0.47, 6.15, -6.18, -0.29, 1.76, 0.34)

respData1 <- c(27.8, 39.19, 39.31, 42.6, 46.38, 40.21, 44.47, 46.24, 34.38, 69.78, 47.47, 53.41, 52.07, 47.09, 49.82, 13.05, 37.2, 54.27, 42.01, 31.94, 46.56, 10.89, 34.58, 44.43, 51.34, 44.57, 23.28, 46.32, 38.84, 50.78, 34.92, 35.1, 59.31, 40.65, 26.79, 37.85, 45.41, 52.6, 19.58, 36.63, 17.9, 63.94, 51.59, 19.05, 14.15, 37.03, 66.97, 15.58, 29.71, 43.78, 47.02, 37.27, 30.03, 43.11, 36.41, 32.44, 42.13, 46.72, 39.5, 32.4, 45.52, 26.89, 72.18, 51.75, 46.9, 41.5, 22.07, 46.82, 17.87, 50.12, 18.44, 28.21, 68.83, 24.07, 49.43, 43.31, 53.94, 26.36, 7.55, 17.13, 64.75, 36.93, 8.65, 21.06, 44.15, 40.35, 27.84, 42.75, 38.86, 21.84, 52.34, 63.13, 43.23, 38.48, 25.56, 37.81, 19.7, 32.33, 51.69, 40.01, 35.01, 60.59, 47.98, 32.92, 62.64, 15.48, 28.79, 46.04, 60.79, 32.6, 55.21, 41.05, 33.99, 58.24, 50.12, 43.4, 38.2, 58.34, 40.5, 25.68, 60.69, 44.46, 25.28, 40.56, 42.48, 32.15, 52.42, 56.78, 31.09, 18.29, 53.15, 30.62, 43.09, 35.78, 56.31, 20.42, 23.26, 48.99, 26.23, 61, 41.67, 41.04, 11.61, 41.64, 50.24, 18.98, 48.7, 17.97, 38, 50.85, 63.39, 57.49, 19.51, 54.11, 18.01, 33.74, 19.89, 44.66, 23.09, 42.45, 47.84, 39.38, 26.44, 25.24, 46.74, 33.03, 35.28, 35.73, -2.21, 20.49, 54.54, 50.42, 34.82, 47.67, 13.75, 44.62, 33.73, 31.53, 42.63, 36.64, 55.48, 49.84, 41.98, 69.24, 48.39, 39.12, 40.55, 41.95, 29.31, 34.22, 32.13, 33.6, 14.66, 23.75, 31.9, 35.76, 29, 50.02, 51.85, 13, 43.69, 45.67, 39.06, 43.92, 47.04, 11.32, 66.35, 56.47, 46.27, -0.56, 58.99, 57.85, 50.48, 24.43, 62.28, 49.07, 29.16, 63.71, 35.43, 25.9, 27.7, 40.02, 36.33, 43.11, 28.98, 51.88, 45.73, 48.29, 44.55, 28.76, 60.29, 14.26, 38.09, 43.13, 47.68, 60.55, 47.78, 29.85, 26.43, 62.71, 31.78, 38.87, 28.99, 56.19, 17.08, 44.7, 51.59, 9.56, 39.35, 48.91, 35.22, 26.53, 36.73, 43.78, 50.2, 32.55, 53.92, 33.67, 32.58, 34.44, 41.82, 51.16, 21.73, 53.09, 35.07, 6.84, 30.26, 33.74, 54.12, 32.41, 57.36, 52.16, 22, 64.32, 42.23, 51.91, 44.38, 47.45, 57.47, 48.04, 28.02, 21.08, 52.87, 30.48, 52.76, 51.07, 21.07, 37.38, 27.2, 35.8, 35.36, 50.08, 61.72, 27.94, 54.76, 57.88, 50.57, 38.38, 58.27, 13.5, 26.88, 33.78, 67.2, 31.6, 43.14, 43.19, 43.39, 43.09, 30.85, 34.1, 72.42, 15.42, 66.27, 43.71, 28.42, 13.3, 46.63, 35.42, 52.04, 55.89, 49.6, 44.02, 24.67, 46.79, 37.98, 39.42, 23.08, 26.36, 30.27, 57.94, 22.79, 60.07, 36.51, 49.22, 22.53, 29.96, 62.01, 42.3, 30.05, 55.57, 51.68, 23.05, 28.9, 47.02, 63.76, 42.48, 56.21, 44.29, 28.75, 43.53, 29.18, 14.44, 67.68, 16.92, -1, 51.54, 24.68, 77.5, 72.04, 51.44, 59.89, 5.48, 65.2, 35.03, 29.17, 25.99, 65.76, 54.67, 51.44, 51.34, 25.71, 25.48, 45.49, 37.31, 25.55, 59.4, 23.38, 46.47, 5.27, 48.51, 59.98, 34.85, 48.4, 62.56, 27.1, 41.17, 60.38, 57.21, 17.7, 39.84, 9.25, 39.82, 60.86, 53.29, 33.74, 66.61, 66.06, 50.5, 67.98, 21.79, 25.02, 48.24, 69.45, 35.39, 67.24, 53.73, 25.21, 43.4, 50.39, 30.88, 44.33, 6.28)
respData2 <- c(18.26, 36.11, 25.88, 46.64, 38.22, -2.89, 24.17, 32.85, 37.09, 52.34, 12.35, 50.37, 31.17, 22.07, 42.49, 51.39, 34.57, 25.83, 28.45, 37.21, 41.36, 71.02, 46.11, 39.2, 36.16, 41.46, 40.83, 59.73, 30.75, 1.55, 26.67, 49.85, 35.61, 50.58, 39.23, 40.37, 45.63, 35.29, 46.06, 44.51, 36.47, 51.52, 46.69, 36.55, 53.82, 66.62, 47.33, 54.24, 27.13, 45.48, 53.06, 21.14, 52.55, 51.62, 47.59, 40.92, 27.66, 34.75, 30.73, 61.73, 36.3, 48.03, 34.31, 61.52, 27.52, 32.32, 45.33, 31.56, 32.85, 33.09, 44.52, 15.36, 48.63, 37.8, 45.67, 42.7, 40.2, 13.56, 29.57, 48.77, 29.23, 34.8, 26.91, 50.36, 29.04, 27.91, 0.98, 44.37, 29.11, 36.25, 37.73, 36.64, 18.95, 41.73, 54.34, 34.53, 56.36, 44.06, 39.32, 21.44, 53.32, 27.81, 49.95, 49.67, 68.74, 31.31, 57.06, 33.94, 39.64, 7.46, 52.8, 34.4, 70.42, 32.35, 49.45, 47.13, 34.96, 42.26, 38.28, 28.98, 37.15, 49.47, 29.23, 31.53, 28.17, 35.08, 32.34, 33.13, 33.98, 31.66, 29.63, 38.07, 49.42, 48.03, 45.81, 42.76, 63.01, 31.79, 36.26, 35.28, 34.19, 24.96, 9.13, 30.69, 36.83, 22.96, 52.03, 52.85, 49.96, 54.53, 31.88, 14.11, 51.02, 28.36, 40.92, 53.8, 63.55, 49.42, 16.49, 26.25, 34.56, 34.24, 29.5, 56.65, 33.47, 57.91, 54.78, 40.52, 41.14, 43.87, 28.43, 25.15, 38.2, 52.35, 40.83, 58.92, 37.48, 50.09, 33.76, 46.91, 30.51, 52.1, 45.28, 25.65, 28.95, 43.69, 49.32, 32.96, 34.64, 45.21, 30.77, 43.83, 45.89, 27.21, 38.51, 23.67, 37.26, 49.04, 42.06, 7.7, 36.93, 20.52, 29.9, 30.13, 55.83, 18.76, 35.06, 36.68, 34.92, 59.64, 41.81, 22.45, 28.44, 77.59, 59.44, 19.26, 34.14, 63.37, 24.93, 24.94, 16.79, 38.96, 34.77, 55, 43.88, 43.47, 29, 38.99, 20.2, 59.86, 42.71, 52.67, 47.27, 42.85, 15.67, 54, 68.67)
respData3 <- c(52.15, 49.27, 49.14, 40.6, 50.46, 58.2, 45.66, 20.46, 49.97, 42.83, 25.73, 28.75, 27.69, 22.29, 53.53, 52.18, 41.86, 69.86, 37.78, 31.75, 46.9, 32.98, 52.79, 35.54, 34.51, 26.63, 39.69, 27, 47.79, 19.5, 45.63, 26.92, 44.14, 16.93, 42.78, 56.02, 47.01, 46.89, 19.48, 25.95, 61.74, 47.83, 45.1, 41.11, 64.28, 27.7, 8.05, 49.37, 43.05, 49.23, 33.99, 25.97, 60.66, 44.42, 37.06, 40.95, 21.97, 18.88, 34.68, 41.47, 35.65, 36.49, 31.45, 36.02, 38.67, 28.87, 47.33, 48.99, 39.26, 59.34, 57.07, 39.02, 35.12, 50.94, 48.7, 59.07, 36.1, 41.1, 48.53, 36.84, 20.57, 67.63, 52.86, 28.83, 47.47, 43.59, 63.2, 59.74, 23.63, 44.15, 49.71, 41.84, 28.01, 24.4, 36.04, 46.16, 39.15, 50.76, 27.61, 11.67, 27.04, 39.95, 19.58, 31.08, 70.48, 41.18, 21.73, 51.49, 45.3, 61.05, 18.11, 26.11, 41.49, 51.15, 45.32, 35.03, 45.41, 48.5, 41.7, 56.59, 44.21, 51.48, 22.45, 38.89, 42.35, 47.44, 44.4, 47.43, 38.98, 15.87, 73.93, 57.64, 55.71, 56.87, 40.79, 35.31, 56.81, 31.64, 51.35, 40.61, 44.84, 63.09, 56.03, 36.39, 54.85, 35.81, 5.16, 25.41, 26.6, 39.91, 18.4, 30.47, 28.81, 35.49, 21.98, 54.58, 37.43, 45.14, 26.94, 42.56, 61.86, 63.86, 22.83, 41.93, 43.3, 35.72, 30.3, 23.57, 62.44, 28.81, 45.11, 38.81, 37.09, 35.89, 28.37, 25.34, 20.95, 37.1, 50.87, 44.03, 25.36, 46.32, 40.63, 13.77, 33.89, 35.41, 42.71, 57.71, 46.37, 48.02, 42.58, 20.81, 64.81, 42.34, 44.93, 24.07, 71.44, 49.04, 65.73, 30.5, 56.97, 77.54, 37.53, 49.04, 52.37, 44.21, 49.64, 41.62, 33.15, 26.12, 59.01, 50.39, 12.17, 26.18, 51.92, 35.1, 45.64, 62.54, 47.18, 23.61, 13.8, 48.7, 22.64, 23.25, 26.07, 65.36, -3.6, 44.56, 20.4, 28.77, 44.92, 52.98, 38.6, 46.51, 37.8, 41.54, 14.45, 40.69, 61.11, 55.09, 30.34, 39.57, 32.23, 48.52, 44.47, 27.24, 40.09, 48.87, 31.78, 44.45, 50.95, 36.98, 24.06, 22, 39.89, 33.79, 52.91, 18.36, 32.73, 65.36, 39.55, 36.56, 61.26, 70.61, 48.07, 41.77, 83.77, 62.77, 36.37, 38.26, 31.61, 46.1, 58.54, 24.53, 39.71, 58.49, 36, 28.21, 41.23, 31.57, 31.77, 42.54, 22.47, 48.5, 46.3, 30.97, 53.55, 23.35, 60.6, 41.07, 46.19, 22.14, 52.2, 29.76, 42.34, 43.52, 38.48, 49.56, 59.15, 29.13, 20.9, 40.13, 25.61, 56.45, 35.77, 34.6, 27.61, 37.08, 42.26, 36.76, 23.48, 27.94, 43.68, 49.03, 54.34, 57.83, 45.74, 54, 41.25, 36.96, 56.99, 25.3, 37.1, 54.32, 39.65, 59.93, 53.08, 32.52, 25.58, 34.33, 50.6, 49.97, 23.38, 66.08, 39.08, 47.11, 54.24, 55.54, 45.4, 44.52, 36.92, 45.72, 29.69, 13.78, 41.59, 27.96, 40.98, 19.86, 37.85, 23.43, 41.28)

popdata <- data.frame(explanatory=c(expData1, expData2, expData3), 
                      response=c(respData1, respData2, respData3)
                      )
str(popdata)


# Plot the whole dataset
ggplot(popdata, aes(x = explanatory, y = response)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 


# Take 2 samples of size 50
set.seed(4747)
sample1 <- popdata %>% sample_n(50)
sample2 <- popdata %>% sample_n(50)

# Plot sample1
plot1 <- ggplot(sample1, aes(x = explanatory, y = response)) + 
  geom_point(color = "blue") + 
  geom_smooth(method = "lm", se = FALSE, color = "blue")

plot1 

# Plot sample2 over sample1
plot1 + geom_point(data = sample2, 
                   aes(x = explanatory, y = response),
                   color = "red") +
  geom_smooth(data = sample2, 
              aes(x = explanatory, y = response), 
              method = "lm", 
              se = FALSE, 
              color = "red")


# Repeatedly sample the population
manysamples <- infer::rep_sample_n(popdata, size=50, reps=100)

# Plot the regression lines
ggplot(manysamples, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 


# Fit and tidy many linear models
manylms <- manysamples %>% 
  group_by(replicate) %>% 
  do(lm(response ~ explanatory, data=.)  %>% 
     broom::tidy()) %>%
  filter(term=="explanatory")

# Plot a histogram of the slope coefficients
ggplot(manylms, aes(x=estimate)) +
  geom_histogram()


# Take 100 samples of size 50
manysamples1 <- infer::rep_sample_n(popdata, size=50, reps=100)

# Plot the regression line for each sample
ggplot(manysamples1, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 


# Take 100 samples of size 10
manysamples2 <- infer::rep_sample_n(popdata, size=10, reps=100)

# Plot the regression line for each sample
ggplot(manysamples2, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 


# In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, reducing the variance associated with the response variable around the line changes the variability associated with the slope statistics.
# The new popdata is already loaded in your workspace.
# Take 100 samples of size 50
oldPopData <- popdata

popdata$response <- (oldPopData$response - mean(oldPopData$response)) / sd(oldPopData$response)
popdata$response <- 40 + popdata$response * 11.152

manysamples <- infer::rep_sample_n(popdata, size=50, reps=100)

# Plot a regression line for each sample
ggplot(manysamples, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 


```
  
  
  
***
  
Chapter 2 - Simulation Based Inference for Slope Parameters  
  
Simulation-based inference - using the twins study from the 1920s (one twin was raise by their parents and the other in a foster home):  
  
* The regression analysis looks at Foster vs Biological on an IQ test  
* Can instead permute the response variable (Foster) and see what baseline variability in slopes would be  
	* Can then assess the likelihood of seeing the actual regression slope in the data; that becomes the p-value  
* Can run the analyses using the "infer" package; specifically  
	* twins %>%   
    * specify(Foster ~ Biological) %>%  
    * hypothesize(null = "independence") %>%  
    * generate(reps = 10, type = "permute") %>%  
    * calculate(stat = "slope")  
* Typically, to do inference, you will need to know the sampling distribution of the slope under the hypothesis that there is no relationship between the explanatory and response variables  
	* In most situations, you don't know the population from which the data came, so the null sampling distribution must be derived from only the original dataset  
    * In this exercise you'll use the pull() function. This function takes a data frame and returns a selected column as a vector (similar to $)  
  
Simulation-based inference for slope - can also be calculated using bootstrap for CI (as opposed to testing a null-hypothesis):  
  
* The bootstrap will count some sets of (x, y) 2+ times, and some sets of (x, y) 0 times; there is no permuting of the data, though  
* While the permuted slopes were all centered around zero (as null hypotheses), the resamples slopes will be centered around the test-statistic slope (as confidence intervals)  
* Can run the analyses using the "infer" package; specifically  
	* twins %>%   
    * specify(Foster ~ Biological) %>%   
    * generate(reps = 100, type = "bootstrap") %>%   
    * calculate(stat = "slope")  
  
Example code includes:  
```{r}

# Load the infer package
library(infer)

twins <- readr::read_csv("./RInputFiles/twins.csv")
str(twins)


# Calculate the observed slope
obs_slope <- lm(Foster ~ Biological, data=twins) %>%
  broom::tidy() %>%   
  filter(term == "Biological") %>%
  pull(estimate)

# Simulate 10 slopes with a permuted dataset
set.seed(4747)
perm_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 10, type = "permute") %>%
  calculate(stat = "slope") 

# Print the observed slope and the 10 permuted slopes
obs_slope
perm_slope


# Make a dataframe with replicates and plot them!
set.seed(4747)
perm_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 500, type = "permute") %>%
  calculate(stat = "slope") 

ggplot(perm_slope, aes(x=stat)) +
  geom_density()

# Calculate the mean and the standard deviation of the slopes
mean(perm_slope$stat)
sd(perm_slope$stat)


# Calculate the absolute value of the slope
abs_obs_slope <- lm(Foster ~ Biological, data=twins) %>%
  broom::tidy() %>%   
  filter(term == "Biological") %>%
  pull(estimate) %>%
  abs()

# Compute the p-value  
perm_slope %>% 
  mutate(abs_perm_slope=abs(stat)) %>%
  summarize(p_value = mean(abs_perm_slope > abs_obs_slope))


# Calculate 1000 bootstrapped slopes
set.seed(4747)
BS_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope") 

# Look at the head of BS_slope  
head(BS_slope)


# Create a confidence interval
BS_slope %>% 
summarize(lower = mean(stat) - 2 *sd(stat),
          upper = mean(stat) + 2 *sd(stat))

# Set alpha
alpha <- 0.05

# Create a confidence interval  
BS_slope %>% 
summarize(low = quantile(stat, alpha/2), 
          high = quantile(stat, 1 - alpha/2))

```
  
  
  
***
  
Chapter 3 - t-Based Inference for the Slope Parameter  
  
Mathematical approximation for testing and estimating slope parameters (based on the t-distribution):  
  
* Continuing with the Starbucks data for the relationships between fat and calories  
* Can look at the histogram of 1) simulation slopes for the permuted null hypothesis, and 2) t-distribution for df=n-2; should see very good overlaps  
	* The dt function is the density of the t-distribution; this is used by R behind the scenese to select the p-value  
* Sometimes, the t-distribution is not such a good match to the histograms of the null hypothesis  
	* R will use the t-distribution even if it is not a perfect fit; further reinforcement of the value of plotting the data  
    * The differences between the mathematical model and the data is only really important in the edge cases (minor, low-power impact)  
* In thinking about the scientific research question, if IQ is caused only by genetics, then we would expect the slope of the line between the two sets of twins to be 1  
	* Testing the hypothesized slope value of 1 can be done by making a new test statistic which evaluates how far the observed slope is from the hypothesized value of 1  
    * newt=(slope−1) / SE  
    * If the hypothesis that the slope equals one is true, then the new test statistic will have a t-distribution which we can use for calculating a p-value  
* When technical conditions (see next chapter) hold, the inference from the randomization test and the t-distribution test should give equivalent conclusions  
	* They will not provide the exact same answer because they are based on different methods  
    * But they should give p-values and confidence intervals that are reasonably close  
  
Intervals in regression - estimating the coefficients by way of confidence intervals (CI):  
  
* CI = point-estimate +/- critical_value * SE_estimate  
* Can use the tidy() call to get the CI, such as tidy(lm(…), conf.int=TRUE, conf.level=1-alpha)  
  
Different types of intervals - often of interest to know the variability in the predicted value, not just the parameter estimates:  
  
* There is greater uncertainty in predicted values as you get further away from the "center of mass" of the x-data  
* Can use the broom::augment() call to get the range of predicted values for a new set of data  
	* alpha <- .05  
    * crit_val <- qt((1-alpha/2), df = nrow(starbucks) - 2)  
    * newfood <- data.frame(Fat = c(0,10,20,30))  
    * augment(lm(Calories ~ Fat, data=starbucks), newdata = newfood) %>%  
    * mutate(lowMean = .fitted - crit_val*.se.fit, upMean = .fitted + crit_val*.se.fit)  
* Can create plots for the population estimate error using ggplot  
	* ggplot(predMeans, aes(x = Fat, y = Calories)) +  
    * geom_point() +  
    * stat_smooth(method = "lm", se = FALSE) +   # can alternately set se=TRUE to get a similar plot  
    * geom_ribbon(aes(ymin = lowMean, ymax = upMean), alpha=.2)  
* The prediction interval can also be requested to give the CI on the individual (rather than population mean) members of the population given a specific X variable  
	* This is a combination of both the error in the population mean predictions AND the natural variability in the population on the y-metric  
    * FatCal_pred <- augment(FatCal_lm) %>%  
    * mutate(.se.pred = sqrt(FatCal_sig^2 + .se.fit^2))  
    * predResp <- FatCal_pred %>%  
    * mutate(lowResp = .fitted - crit_val*.se.pred, upResp = .fitted + crit_val*.se.pred)  
    * ggplot(predResp, aes(x = Fat, y = Calories)) +  
    * geom_point() +  
    * stat_smooth(method = "lm", se = FALSE) +  
    * geom_ribbon(aes(ymin = lowResp, ymax = upResp), alpha = .2)  
  
Example code includes:  
```{r}

# twins_perm <- twins %>%
#   specify(Foster ~ Biological) %>%
#   hypothesize(null="independence") %>%
#   generate(reps = 10, type = "permute") %>%
#   calculate(stat = "slope") 

# The randomized slopes are given in the twins_perm dataframe
# Look at the head of the data
# head(twins_perm)

# Plot the histogram with the t distribution
# twins_perm %>%
#   filter(term == "Biological_perm") %>%
#   ggplot(aes(x=statistic)) + 
#   geom_histogram(aes(y = ..density..), bins = 50) + 
#   stat_function(fun = dt, color = "red", args = list(df=nrow(twins)-2))


# Tidy the model
lm(Foster ~ Biological, data=twins) %>% broom::tidy()

# Create a one-sided p-value
lm(Foster ~ Biological, data=twins) %>%
  broom::tidy() %>% 
  filter(term == "Biological") %>%
  select(p.value) %>%
  mutate(p_value_1side = p.value/2)


# Test the new hypothesis
lm(Foster ~ Biological, data = twins) %>% 
  broom::tidy() %>% 
  filter(term == "Biological") %>%
  mutate(statistic_test1 = (estimate - 1) / std.error, 
      p_value_test1 = 2 * pt(abs(statistic_test1), df=nrow(twins)-2, lower.tail=FALSE))


# Find the p-value
# perm_slope %>%
#   mutate(abs_perm_slope = abs(stat)) %>%
#   summarize(p_value = mean(abs_perm_slope > abs(obs_slope)))


# Set alpha
alpha <- 0.05

# Find the critical value
crit_val <- qt(0.975, df = nrow(twins)-2)

# Tidy the model with the confidence level alpha
lm(Foster ~ Biological, data=twins) %>% 
   broom::tidy(conf.int=TRUE, conf.level=1-alpha)

# Find the lower and upper bounds of the confidence interval
lm(Foster ~ Biological, data=twins) %>%
    broom::tidy() %>%
    mutate(lower = estimate - crit_val * std.error,
           upper = estimate + crit_val * std.error)


# Create the bootstrap confidence interval
BS_slope %>% 
    summarize(low = quantile(stat, alpha/2), 
              high = quantile(stat, 1 - alpha/2))


# Set alpha
alpha <- 0.05

# Find the critical value
crit_val <- qt(1-alpha/2, nrow(twins)-2)


# Create a dataframe of new observations
newtwins <- data.frame(Biological = c(80, 90, 100, 110))

# Find prediction intervals
lm(Foster ~ Biological, data=twins) %>% 
  broom::augment(newdata = newtwins) %>%
  mutate(lowMean = .fitted - crit_val * .se.fit,
         upMean = .fitted + crit_val * .se.fit)


# Set alpha and find the critical value
alpha <- 0.05
crit_val <- qt(1-alpha/2, df=nrow(twins)-2)

# Find confidence intervals for the response
predMeans <- lm(Foster ~ Biological, data=twins) %>%
  broom::augment() %>%  
  mutate(lowMean = .fitted - crit_val*.se.fit,
      upMean = .fitted + crit_val*.se.fit) 

# Examine the intervals
head(predMeans)

# Plot the data with geom_ribbon()
ggplot(predMeans, aes(x=Biological, y=Foster)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE) + 
  geom_ribbon(aes(ymin = lowMean, ymax = upMean), alpha=.2)

# Plot the data with stat_smooth()
ggplot(twins, aes(x = Biological, y = Foster)) + 
  geom_point() +
  stat_smooth(method="lm", se=TRUE) 


# Set alpha and find the critical value
alpha <- 0.05
crit_val <- qt(1-alpha/2, nrow(twins)-2)

# Fit a model and use glance to find sigma
twin_lm <- lm(Foster ~ Biological, data=twins)
twin_gl <- broom::glance(twin_lm)

# Pull sigma
twin_sig <- pull(twin_gl, sigma)

# Augment the model to find the prediction standard errors
twin_pred <- broom::augment(twin_lm) %>%
  mutate(.se.pred = sqrt(twin_sig ** 2 + .se.fit ** 2))

# Create prediction intervals  
predResp <- twin_pred %>%
  mutate(lowResp = .fitted - crit_val * .se.pred,
      upResp = .fitted + crit_val * .se.pred)

# Plot the intervals using geom_ribbon()
ggplot(predResp, aes(x=Biological, y=Foster)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE) + 
  geom_ribbon(aes(ymin = lowResp, ymax = upResp), alpha = .2) +
  geom_ribbon(data = predMeans, aes(ymin = lowMean, ymax = upMean), alpha = .2, fill = "red")

```
  
  
  
***
  
Chapter 4 - Technical Conditions in Linear Regression  
  
Technical conditions for linear regression:  
  
* Inferential regression requires that the data follow the LINE  
	* L - linear model  
    * I - independent observations  
    * N - points normally distributed around the line  
    * E - equal variability around the line at all values for X  
* The broom::augment() will give the variables .fitted and .resid, which can be plotted against each other  
* The residual vs. fitted plot frequently makes the technical violations clearer and easier to spot  
* So far, you have implemented two approaches for performing inference assessment to a linear model  
* The first way is given by the standard R output (lm) and is based on the t-distribution  
	* The derivation of the t-distribution is based on the theory (i.e., the LINE conditions)  
* The second method uses a randomization test which assumes that the observations are exchangeable under the null hypothesis  
	* That is, when the null hypothesis (X is independent of Y) is true, the Y values can be swapped among the X values  
    * The technical conditions in the randomization setting are linear relationship, independent observations, and equal variances  
    * However, the normality assumption is not needed  
  
Effect of an outlier - can have unintended impact on the inferential conclusions:  
  
* Removing data points should never be done just to get a "more desired" fit or conclusion  
* The main reasons to exclude data are 1) known incorrect data, or 2) desire to model a subset of the data  
  
Moving forward when model assumptions are violated:  
  
* Removing outlier points on a whim will not make for a good model  
	* There is a big difference in removing points that are not of interest; it is important to document in a final document WHY these points are not of interest  
    * It is common to be interested only in a bounded population, and to have inferential statistics that apply only within that bounded population  
* There is sometimes a need for a transformation of X to make the linear assumptions hold - sqrt(), log(), ** 2, etc.  
	* The model is still considered to be a linear model, even if the transformed terms are no longer linear  
  
Example code includes:  
```{r}

# A dataset containing well behaved observations has been preloaded and is called hypdata_nice. There are two variables in the dataset which are aptly named explanatory and response.
expl <- c(2.14, 3.04, 2.48, 2.91, 3.12, 2.45, 3.62, 3.7, 3.11, 4.18, 3.07, 3, 3.52, 3.3, 3.58, 1.35, 2.91, 3.66, 3.97, 3.23, 3.61, 3.45, 3.36, 2.95, 3.86, 4.38, 1.73, 3.1, 3.68, 3.62, 3.42, 2.61, 3.74, 3.1, 2.07, 3.85, 2.76, 4.03, 0.91, 4.29, 2.44, 3.14, 3.59, 2.03, 1.58, 2.2, 4.65, 2.1, 1.96, 2.57, 3.46, 3.07, 2.49, 3.64, 1.4, 2.64, 1.77, 3.29, 2.96, 2.82, 2.6, 3.23, 5.24, 3.89, 3.18, 2.94, 1.14, 3.04, 1.62, 3.09, 1.18, 2.76, 5.26, 2.34, 3.71, 3.26, 4.15, 2.45, 1.06, 2.11, 4.14, 3.22, 1.34, 1.58, 2.93, 2.14, 3.83, 3.97, 2.94, 2.24, 4.01, 5.46, 3.96, 3.14, 0.79, 3.75, 1.67, 2.95, 4.02, 2.9, 3.14, 3.89, 3.42, 3.36, 3.6, 2.03, 2.22, 3.82, 3.01, 1.51, 2.52, 3.23, 2.63, 5.22, 4.25, 3.26, 2.09, 4.08, 3.62, 2.08, 4.71, 2.16, 2.82, 3.16, 2.3, 1.77, 3.72, 3.91, 2.03, 0.3, 3.32, 2.79, 3.27, 2.71, 4.11, 2.15, 1.61, 4.23, 2.82, 4.99, 2.18, 3.02, 3.5, 1.32, 2.52, 1.61, 4.58, 1.99, 2.95, 2.87, 4.68, 3.84, 2.03, 3.2, 1.89, 2.15, 2.55, 3.44, 2.99, 2.47, 2.77, 3.66, 1.9, 3.77, 3.42, 3.96, 2.78, 0.81, -0.34, 1.28, 3.76, 4.19, 3.07, 3.03, 1.28, 2.31, 1.97, 1.67, 2.9, 2.6, 3.71, 3.94, 2.88, 4.74, 3.18, 3.14, 3.72, 2.38, 2.4, 4.16, 3.77, 3.16, 2.45, 1.75, 2.39, 3.26, 3.32, 3.35, 3.93, 1.55)
resp <- c(19.04, 21.44, 19.21, 20.63, 21.66, 15.99, 26.76, 28.85, 20.9, 19.16, 20.53, 22.79, 22.55, 21.72, 26.31, 18.15, 18.3, 28.03, 24.12, 20.47, 23.03, 22.06, 21.91, 20.4, 22.89, 28.62, 16.63, 22.41, 23.45, 20.06, 25.29, 15.54, 22.88, 20.3, 18.87, 26.13, 19.63, 23.3, 12.38, 27.77, 18.67, 23.12, 19.87, 20.51, 17.92, 20.25, 29.82, 13.36, 18.36, 21.2, 23.69, 17.57, 18.19, 23.12, 15.58, 18.53, 20, 21.8, 19.32, 18.8, 20.24, 24.26, 27.59, 26.02, 20.1, 17.24, 12.87, 20.05, 16.27, 19.7, 20.37, 22.49, 24.61, 23.41, 20.51, 20.4, 24, 23.17, 19.66, 17.06, 23.15, 18.48, 16.99, 14.89, 23.4, 19.94, 20.93, 24.21, 19.86, 17.71, 22.74, 25.86, 25.75, 21.4, 16.65, 25.84, 18.49, 18.95, 26, 19.74, 19.39, 25.16, 22.94, 22.01, 26.12, 19.21, 18.62, 25.96, 19.53, 15.48, 22.79, 19.91, 21.65, 29.06, 22.07, 19.59, 21.01, 26.21, 23.75, 18.61, 26.75, 17.06, 18.28, 21.06, 17.18, 17.07, 22.46, 21.55, 14.53, 10.98, 19.34, 21.72, 23.16, 21.76, 23.11, 16.42, 19.64, 24.33, 19.44, 26.39, 22.03, 19.44, 20.14, 17.29, 21.89, 17.28, 25.25, 17.96, 20.94, 21, 27.3, 20.64, 23.03, 17.6, 13.78, 19.08, 16.24, 27.76, 21.19, 21.96, 21.25, 20.94, 20.3, 22.67, 21.8, 21.34, 23.33, 15.82, 12.92, 16.54, 23.08, 24.23, 22.24, 19.79, 15.31, 20.81, 17.52, 20.92, 14.01, 18.63, 27.18, 21.96, 22.9, 26.57, 22.39, 22.09, 24.67, 20.51, 18.77, 23.92, 19.28, 21.14, 21.12, 15.82, 18.52, 25.09, 25.27, 22.47, 26.35, 16.52)

hypdata_nice <- data.frame(response=resp, explanatory=expl)
str(hypdata_nice)


# Plot the data
ggplot(hypdata_nice, aes(x=explanatory, y=response)) + 
  geom_point()

# Create and augmented model
nice_lm <- lm(response ~ explanatory, data=hypdata_nice) %>%
  broom::augment()

# Print the head of nice_lm
head(nice_lm)

# Plot the residuals
ggplot(nice_lm, aes(x=.fitted, y=.resid)) + 
  geom_point()


# A dataset containing poorly behaved observations has been preloaded and is called hypdata_poor. There are two variables in the dataset which are aptly named explanatory and response
resp <- c(19.08, 21.62, 19.15, 20.57, 21.84, 15.23, 29.92, 33.73, 20.66, 13.32, 20.16, 23.68, 22.54, 21.6, 29.13, 17.46, 17.2, 32.23, 24.34, 19.71, 23.2, 21.85, 21.8, 20.19, 22.25, 32.75, 16.7, 23.01, 23.8, 17.81, 27.43, 14.22, 22.57, 19.74, 18.89, 28.52, 19.39, 22.5, 13.66, 31.09, 18.52, 24.1, 17.58, 20.55, 17.67, 20.41, 34.94, 13.12, 18.35, 21.63, 24.64, 15.61, 17.88, 23.29, 15.76, 18.09, 19.68, 21.75, 18.56, 18.12, 20.38, 25.84, 27.38, 28.25, 19.27, 15.54, 13.97, 19.5, 16.38, 18.85, 18.39, 23.33, 19.43, 24.15, 18.26, 19.53, 23.52, 24.03, 17.56, 16.99, 21.78, 16.54, 16.67, 15.28, 24.62, 20.05, 18.58, 24.52, 19.41, 17.59, 21.46, 21.5, 27.59, 21.39, 15.27, 28.11, 18.25, 18.05, 27.96, 19.3, 18.25, 26.58, 23.42, 21.95, 28.78, 19.23, 18.62, 28.23, 18.78, 15.74, 23.63, 18.82, 22.2, 31.28, 19.05, 18.21, 21.14, 28.26, 24.47, 18.63, 27.6, 16.95, 17.39, 20.82, 16.92, 17.1, 21.87, 19.48, 14.47, 12.61, 17.62, 22.25, 24.02, 22.34, 21.83, 16.26, 19.09, 23.91, 19.02, 25.52, 22.33, 18.63, 18.39, 16.83, 22.5, 17.19, 24.61, 17.96, 20.98, 21.17, 29, 18.01, 23.1, 15.21, 13.99, 19.13, 15.29, 31.68, 21.3, 22.55, 21.61, 19.24, 20.18, 22.09, 21.47, 18.84, 24.5, 14.98, 10.67, 16.3, 22.91, 23.86, 22.79, 19.12, 15.5, 21.1, 17.53, 20.27, 10.99, 18.28, 30.65, 20.17, 23.9, 27.05, 22.89, 22.47, 25.97, 20.77, 18.68, 23.3, 15.71, 20.94, 21.52, 16, 18.39, 27.17, 27.45, 22.74, 28.83, 16.55)
expl <- c(2.14, 3.04, 2.48, 2.91, 3.12, 2.45, 3.62, 3.7, 3.11, 4.18, 3.07, 3, 3.52, 3.3, 3.58, 1.35, 2.91, 3.66, 3.97, 3.23, 3.61, 3.45, 3.36, 2.95, 3.86, 4.38, 1.73, 3.1, 3.68, 3.62, 3.42, 2.61, 3.74, 3.1, 2.07, 3.85, 2.76, 4.03, 0.91, 4.29, 2.44, 3.14, 3.59, 2.03, 1.58, 2.2, 4.65, 2.1, 1.96, 2.57, 3.46, 3.07, 2.49, 3.64, 1.4, 2.64, 1.77, 3.29, 2.96, 2.82, 2.6, 3.23, 5.24, 3.89, 3.18, 2.94, 1.14, 3.04, 1.62, 3.09, 1.18, 2.76, 5.26, 2.34, 3.71, 3.26, 4.15, 2.45, 1.06, 2.11, 4.14, 3.22, 1.34, 1.58, 2.93, 2.14, 3.83, 3.97, 2.94, 2.24, 4.01, 5.46, 3.96, 3.14, 0.79, 3.75, 1.67, 2.95, 4.02, 2.9, 3.14, 3.89, 3.42, 3.36, 3.6, 2.03, 2.22, 3.82, 3.01, 1.51, 2.52, 3.23, 2.63, 5.22, 4.25, 3.26, 2.09, 4.08, 3.62, 2.08, 4.71, 2.16, 2.82, 3.16, 2.3, 1.77, 3.72, 3.91, 2.03, 0.3, 3.32, 2.79, 3.27, 2.71, 4.11, 2.15, 1.61, 4.23, 2.82, 4.99, 2.18, 3.02, 3.5, 1.32, 2.52, 1.61, 4.58, 1.99, 2.95, 2.87, 4.68, 3.84, 2.03, 3.2, 1.89, 2.15, 2.55, 3.44, 2.99, 2.47, 2.77, 3.66, 1.9, 3.77, 3.42, 3.96, 2.78, 0.81, -0.34, 1.28, 3.76, 4.19, 3.07, 3.03, 1.28, 2.31, 1.97, 1.67, 2.9, 2.6, 3.71, 3.94, 2.88, 4.74, 3.18, 3.14, 3.72, 2.38, 2.4, 4.16, 3.77, 3.16, 2.45, 1.75, 2.39, 3.26, 3.32, 3.35, 3.93, 1.55)

hypdata_poor <- data.frame(response=resp, explanatory=expl)
str(hypdata_poor)


# Plot the data
ggplot(hypdata_poor, aes(x=explanatory, y=response)) + 
  geom_point()

# Create an augmented model
poor_lm <- lm(response ~ explanatory, data=hypdata_poor) %>%
  broom::augment()

# Plot the residuals
ggplot(poor_lm, aes(x=.fitted, y=.resid)) + 
  geom_point()


# The data provided in this exercise (hypdata_out) has an extreme outlier. You will run the linear model with and without the outlying point to see how one observation can affect the estimate of the line.
expl <- c(2.14, 3.04, 2.48, 2.91, 3.12, 2.45, 3.62, 3.7, 3.11, 4.18, 3.07, 3, 3.52, 3.3, 3.58, 1.35, 2.91, 3.66, 3.97, 3.23, 3.61, 3.45, 3.36, 2.95, 3.86, 4.38, 1.73, 3.1, 3.68, 3.62, 3.42, 2.61, 3.74, 3.1, 2.07, 3.85, 2.76, 4.03, 0.91, 4.29, 2.44, 3.14, 3.59, 2.03, 1.58, 2.2, 4.65, 2.1, 1.96, 2.57)
resp <- c(23.06, 21.85, 14.4, 27.13, 5.31, 15.71, 10.51, 26, 20.96, 22.71, 17.17, 23.26, 44.96, 30.77, 24.49, 15.47, 2.14, 23.32, 10.12, 22.58, 4.63, 19.91, 44.7, 14.24, 30.7, 27.72, 28.72, 15.84, 3.65, 13.99, 33.68, 22.02, 6.66, 7.07, 17.56, 14.94, 28.6, 33.76, 14.16, 17.31, 29.39, 46.02, 32.34, 19.49, -5.37, 26.08, 500, 17.81, 28.03, 18.73)

hypdata_out <- data.frame(response=resp, explanatory=expl)
str(hypdata_out)


# Plot the data and a linear model
ggplot(hypdata_out, aes(x=explanatory, y=response)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE)

# Remove the outlier
hypdata_noout <- hypdata_out %>%
  filter(explanatory < 4.6)

# Plot all the data and both models
ggplot(hypdata_out, aes(x=explanatory, y=response)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  stat_smooth(data=hypdata_noout, method="lm", se=FALSE, color="red")


# Examine the tidy model
lm(response ~ explanatory, data=hypdata_out) %>%
  broom::tidy()

# Examine the new tidy model
lm(response ~ explanatory, data=hypdata_noout) %>%
  broom::tidy()


# The data frames perm_slope_out and perm_slope_noout are also in your workspace. These data frames hold the permuted slopes for each of the original datsets
# Finally, the observed values are stored in the variables obs_slope_out and obs_slope_noout.
# Calculate the p-value with the outlier
# perm_slope_out %>% 
#   mutate(abs_perm_slope = abs(stat)) %>%
#   summarize(p_value = mean(abs_perm_slope > abs(obs_slope_out)))

# Calculate the p-value without the outlier
# perm_slope_noout %>% 
#   mutate(abs_perm_slope = abs(stat)) %>%
#   summarize(p_value = mean(abs_perm_slope > abs(obs_slope_noout)))


# The dataset data_nonlin has been preloaded.
expl <- c(0.19, 0.34, 0.51, 0.45, 0.3, 0.92, 0.47, 0.93, 0.55, 0.93, 0.29, 0, 0.73, 0.69, 0.76, 0.86, 0.54, 0.95, 0.88, 0.64, 0.53, 0.39, 0.5, 0.53, 0.7, 0.11, 0.62, 0.69, 0.72, 0.27, 0.05, 0.45, 0.46, 0.51, 0.74, 0.86, 0.83, 0.17, 0.59, 0.45, 0.73, 0.31, 0.67, 0.83, 0.64, 0.35, 0.48, 0.2, 0.8, 0.83, 0.92, 0.34, 0.1, 0.91, 0.54, 0.08, 0.75, 0.45, 0.73, 0.11, 0.66, 0.31, 0.35, 0.18, 0.77, 0.96, 0.54, 0.59, 0.18, 0.15, 0.8, 0.21, 0.4, 0.27, 0.85, 0.64, 0.02, 0.84, 0.9, 0.42, 0.29, 0.83, 0.56, 0.78, 0.72, 0.51, 0.17, 0.18, 0.08, 0.71, 0.21, 0.97, 0.95, 0.64, 0.18, 0.55, 0.15, 0.72, 0.33, 0.73)
resp <- c(11.62, 12.81, 15.01, 15.05, 10.67, 25.18, 13.41, 25.96, 16.09, 25.48, 11.75, 10.24, 22.46, 20.02, 21.09, 23.65, 14.23, 26.45, 22.89, 18.12, 13.96, 13.25, 17.5, 15.18, 20.11, 10.79, 18.75, 18.65, 17.96, 10.99, 11.33, 14.58, 12.96, 14.1, 20.47, 22.88, 23.77, 11.89, 16.97, 13.6, 21.14, 14.87, 19.76, 23.05, 15.82, 13.64, 13.56, 11.11, 23.14, 22.78, 25.49, 13.64, 10.88, 25.64, 16.58, 9.34, 19.95, 15.15, 20.22, 9.04, 18.09, 12.59, 12.51, 13.24, 22.49, 26.83, 15.11, 18.06, 11.61, 9.84, 23.75, 10.41, 13.49, 12.02, 22.65, 16.75, 10.78, 24.02, 23.93, 11.28, 12.44, 22.55, 16.57, 21.24, 21.07, 14.67, 9.53, 12.24, 10.15, 21.68, 10.5, 27.13, 26.91, 16.33, 10.58, 14.71, 12.36, 18.83, 12.6, 20)

data_nonlin <- data.frame(response=resp, explanatory=expl)
str(data_nonlin)


# Create an augmented model using the non-linear data
lm_nonlin <- lm(response ~ explanatory, data=data_nonlin) %>%
  broom::augment()

# Plot the residuals
ggplot(lm_nonlin, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_abline(slope = 0, intercept = 0)

# Create a second augmented model
lm2_nonlin <- lm(response ~ explanatory + I(explanatory^2), data=data_nonlin) %>%
  broom::augment()

# Plot the second set of residuals
ggplot(lm2_nonlin, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_abline(slope = 0, intercept = 0)


# In this next example, it appears as though the variance of the response variable increases as the explanatory variable increases
# Note that the fix in this exercise has the effect of changing both the variability as well as modifying the linearity of the relationship
# The dataset data_nonequalvar has been preloaded
expl <- c(48.9, 78.2, 39.5, 42.9, 79.9, 57.9, 35.1, 50.7, 62.6, 63.3, 38.1, 75.8, 43.6, 48.8, 78, 44.8, 29.2, 57.7, 60.6, 63.3, 47.3, 54.3, 67.3, 54.8, 52.9, 50.6, 78, 62.2, 45.8, 40.5, 52.1, 58.7, 31.1, 61.8, 52.6, 63.7, 37.8, 42.5, 29.5, 51.3, 57.2, 70.3, 46.1, 91.2, 74.9, 47.3, 34.8, 29.3, 42, 57.9, 48.1, 35, 56.7, 71.9, 47.9, 49.9, 36.3, 53.6, 50.3, 60, 53.6, 67.1, 55.6, 44.2, 56.4, 41.5, 30.4, 84.1, 48.8, 59.6, 50.8, 59.3, 94.6, 70.5, 45.4, 44.8, 36.2, 87.6, 68.9, 55.8, 64.9, 33.2, 89.9, 37.9, 54.7, 64.6, 71.6, 65.8, 48.3, 67.5, 62.1, 63.6, 67.8, 54.2, 55.6, 65.4, 55.4, 50.2, 81.3, 57.9, 62.1, 55.3, 75.5, 65, 65.6, 53.1, 71.3, 53.1, 63.3, 45.3, 61, 54, 44.8, 66.5, 55.2, 67.8, 43.2, 46.9, 57.1, 92.2, 70.1, 49.7, 46.2, 67, 29.8, 40.8, 62.6, 60.4, 86.4, 42.2, 42.9, 69.5, 63.1, 46.2, 38.5, 43.7, 53.3, 60.3, 32.6, 72.5)
resp <- c(127.15, 45.06, 15.54, 26.25, 17.78, 47.81, 14.22, 104.44, 134.4, 3.21, 138.4, 59.52, 26.54, 4.43, 65.15, 21.73, 8.6, 132.58, 84.87, 242.71, 23.36, 21.16, 29.59, 100.78, 135.44, 21.39, 90.71, 12.22, 34.61, 104.3, 102.54, 9.25, 32.13, 17.37, 22.74, 20.3, 99.88, 33.7, 26.17, 9.67, 2.23, 173.31, 46.49, 339.71, 110.22, 82.22, 4.93, 6.09, 12.88, 37.66, 59.45, 5.12, 37.84, 67.36, 30.94, 30.22, 12.6, 14.14, 106.09, 52.13, 4.72, 35.19, 7.49, 35.67, 28.08, 56.13, 66.75, 69.87, 65.66, 9.08, 89.92, 20.81, 43.22, 59.37, 21.8, 34.34, 1.65, 92.08, 36.89, 63.7, 23.8, 15.55, 79.21, 35.77, 74.66, 55.85, 58.33, 41.08, 53.43, 47.58, 46.57, 23.1, 305.41, 51.99, 39.4, 49.44, 116.64, 110, 120.17, 41.52, 60.48, 26.31, 121.42, 111.76, 33.76, 43.43, 150.36, 31.19, 30.25, 74.32, 132.18, 34.32, 20.45, 106.13, 47.9, 110.07, 66.47, 19.96, 42.72, 361.12, 281.52, 139.26, 22.22, 26.9, 7.66, 3.78, 52.8, 47.61, 81.24, 80.17, 19.48, 7.72, 43.5, 51.48, 37.18, 15.6, 36.02, 6.85, 15.42, 214.95)

data_nonequalvar <- data.frame(response=resp, explanatory=expl)
str(data_nonequalvar)


# Create an augmented model
lm_nonequalvar <- lm(response ~ explanatory, data=data_nonequalvar) %>%
  broom::augment()

# Plot the residuals
ggplot(lm_nonequalvar, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_abline(slope = 0, intercept = 0)

# Create an augmented model using the log of the response
lm2_nonequalvar <- lm(log(response) ~ explanatory, data=data_nonequalvar) %>%
  broom::augment()

# Plot the log of the resoponse
ggplot(data_nonequalvar, aes(x=explanatory, y=log(response))) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE)
  
# Plot the second set of residuals
ggplot(lm2_nonequalvar, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_abline(slope = 0, intercept = 0)


# In this last example, it appears as though the points are not normally distributed around the regression line
# Again, note that the fix in this exercise has the effect of changing both the variability as well as modifying the linearity of the relationship
# The dataset data_nonnorm has been preloaded
resp <- c(190.58, 187.28, 172.34, 291.5, 43.66, 315.81, 94.42, 417.19, 234.56, 343.66, 127.73, 119.66, 690.5, 416.69, 334.43, 337.93, 64.21, 386.13, 176.61, 280.16, 64.13, 167.81, 578.18, 160.62, 393.2, 147.78, 432.51, 216.36, 88.97, 83.47, 226.85, 235.78, 64.11, 89.33, 295.13, 230.52, 469.54, 241.46, 246.09, 131.25, 453.87, 527.73, 422.67, 356.65, 56.87, 272, 89.89, 138.36, 488.88, 321.64, 388.48, 287.1, 161.48, 423.7, 315.97, 47.3, 207.29, 314.94, 300.95, 26.88, 215.97, 196.3, 144.08, 428.02, 516.09, 423.95, 138.73, 408.28, 202.76, 60.61, 617.6, 75.43, 177.11, 176.66, 246.48, 131.29, 170.23, 485.36, 229.54, 11.85, 200.46, 304.64, 276.42, 277.58, 468.18, 138.25, 37.26, 279.1, 101.36, 628.17, 78.01, 391.71, 462.21, 92.89, 98.36, 97.8, 317.43, 172.27, 172.11, 281.36)
expl <- c(0.19, 0.34, 0.51, 0.45, 0.3, 0.92, 0.47, 0.93, 0.55, 0.93, 0.29, 0, 0.73, 0.69, 0.76, 0.86, 0.54, 0.95, 0.88, 0.64, 0.53, 0.39, 0.5, 0.53, 0.7, 0.11, 0.62, 0.69, 0.72, 0.27, 0.05, 0.45, 0.46, 0.51, 0.74, 0.86, 0.83, 0.17, 0.59, 0.45, 0.73, 0.31, 0.67, 0.83, 0.64, 0.35, 0.48, 0.2, 0.8, 0.83, 0.92, 0.34, 0.1, 0.91, 0.54, 0.08, 0.75, 0.45, 0.73, 0.11, 0.66, 0.31, 0.35, 0.18, 0.77, 0.96, 0.54, 0.59, 0.18, 0.15, 0.8, 0.21, 0.4, 0.27, 0.85, 0.64, 0.02, 0.84, 0.9, 0.42, 0.29, 0.83, 0.56, 0.78, 0.72, 0.51, 0.17, 0.18, 0.08, 0.71, 0.21, 0.97, 0.95, 0.64, 0.18, 0.55, 0.15, 0.72, 0.33, 0.73)

data_nonnorm <- data.frame(response=resp, explanatory=expl)
str(data_nonnorm)


# Create an augmented model of the data
lm_nonnorm <- lm(response ~ explanatory, data=data_nonnorm) %>%
  broom::augment()

# Plot the residuals
ggplot(lm_nonnorm, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_abline(slope = 0, intercept = 0)

# Create the second augmented model
lm2_nonnorm <- lm(sqrt(response) ~ explanatory, data=data_nonnorm) %>%
  broom::augment()

# Plot the square root of the response
ggplot(data_nonnorm, aes(x=explanatory, y=sqrt(response))) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE)

# Plot the second set of residuals
ggplot(lm2_nonnorm, aes(x=.fitted, y=.resid)) +
  geom_point() +
  geom_abline(slope = 0, intercept = 0)

```
  
  
  
***
  
Chapter 5 - Building on Inference in Simple Regression  
  
Inference on transformed variables - interpretation of the coefficients is no longer just slope for y ~ x:  
  
* With non-linear X or non-linear Y, be sure to give the right interpretation based on the transformations that were run prior to the linear regression  
  
Multicollinearity - process of some or more of the predictor variables being correlated:  
  
* The most common example would be when X3 = a*X1 + b*X2 + c  
* Interpreting the coefficients can be difficult to impossible when there is multicollinearity  
  
Multiple linear regression:  
  
* Due to multicollinearity, coefficients can change signs and be highly misleading  
* The significance of the coefficients will also change in multiple linear regression  
	* In simple regression, the p-value is the likelihood of no linear relationship between X and Y  
    * In multiple regression, the p-value for X2 is the likelihood of no linear relationship between X2 and Y GIVEN that we already have X1  
  
Summary:  
  
* Models attempt to describe populations  
* Check for the key technical assumptions of LINE  
* Models can be approached either as hypothesis tests or as confidence intervals  
* Can use either the mathematical models, or the permutation / bootstrap approaches  
  
Example code includes:  
```{r}

LAhomes <- readr::read_csv("./RInputFiles/LAhomes.csv")
str(LAhomes, give.attr=FALSE)

restNYC <- readr::read_csv("./RInputFiles/restNYC.csv")
str(restNYC, give.attr=FALSE)

# Using tidy output, run an lm analysis on price versus sqft for the LAhomes dataset.
# Run one more analysis, but this time on transformed variables: log(price) versus log(sqft).
# Create a tidy model
lm(price ~ sqft, data=LAhomes) %>%
  broom::tidy()

# Create a tidy model using the log of both variables
lm(log(price) ~ log(sqft), data=LAhomes) %>%
  broom::tidy()

# Output the tidy model
lm(log(price) ~ log(sqft) + log(bath), data=LAhomes) %>%
  broom::tidy()


# Using the NYC Italian restaurants dataset (compiled by Simon Sheather in A Modern Approach to Regression with R), restNYC, 
# you will investigate the effect on the significance of the coefficients when there are multiple variables in the model
# Recall, the p-value associated with any coefficient is the probability of the observed data given that the particular variable is independent of the response AND given that all other variables are included in the model.
# Output the first model
lm(Price ~ Service, data=restNYC) %>%
  broom::tidy()

# Output the second model
lm(Price ~ Service + Food + Decor, data=restNYC) %>%
  broom::tidy()

```
  
  
  
***
  
###_Multiple and Logistic Regression_  
  
Chapter 1 - Parallel Slopes  
  
What if you have two groups?  
  
* Example of fuel efficiency vs. displacement for cars popular in 1999 through 2008  
	* Measurements only taken in EITHER 1999 OR 2008, and overall improvement in fuel efficiency could be driving the observed results  
* The parallel slopes model is an example of a model with one numeric explanatory variable, and one categorical explanatory variable  
	* Can model as lm(response ~ continuous + factor(categorical))  
  
Visualizing parallel slopes models:  
  
* Can define a binary value based on the year (such as newer = (year == 2008))  
* A model like lm(hwy ~ displ + factor(newer)) will help to tease out the overall change in efficiency based on 2008 vs 1999, independent of displacements  
	* The lines will be parallel (same slope) but with different intercepts  
    * The broom::augment() returns a data frame with all the observations and their key descriptive statistics variables such as .fitted  
  
Interpreting parallel slopes coefficients:  
  
* The intercept is often a rather theoretical interpretation, since X=0 may be far outside the scope of the data (or even feasible)  
	* The coefficient of the factor variable is the change in intercept if that factor level is present  
    * Be careful about which factor level is considered to be the reference - that is captured already in the overall model intercept  
* The slope coefficient is the estimate given to the numerical variable, and will be constant for both groups of the factor variable  
	* There is only ONE slope even though there are two explanatory variables  
  
Three ways to describe a model - Mathematical, Geometric, Syntactic:  
  
* Mathematical includes an Equation, Residuals with normality assumption, and Coefficients  
* Geometric is the plotting, especially for data that reside in 2D or 3D (2D plus a factor)  
	* This is the hardest of the three to scale to multiple regression, given human limits in creating and visualizing complex multi-dimensional plots  
* Syntactic is the format for communicating desired models to R  
  
Example code includes:  
```{r}

# In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used
# A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable
# Explore the data
data(marioKart, package="openintro")
glimpse(marioKart)

# fit parallel slopes
(mod <- lm(totalPr ~ wheels + cond, data=marioKart))

# The parallel slopes model mod relating total price to the number of wheels and condition is already in your workspace.
# Augment the model
augmented_mod <- broom::augment(mod)
glimpse(augmented_mod)

# scatterplot, with color
data_space <- ggplot(augmented_mod, aes(x = wheels, y = totalPr, color = cond)) + 
  geom_point()
  
# single call to geom_line()
data_space + 
  geom_line(aes(y = .fitted))


# The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960--1967
# We would like to build a model for birthweight as a function of the mother's age and whether this child was her first (parity == 0)
# birthweight=β0+β1⋅age+β2⋅parity+ϵ
data(babies, package="openintro")
str(babies)

# build model
lm(bwt ~ age + parity, data=babies)

# build model
lm(bwt ~ gestation + smoke, data=babies)

```
  
  
  
***
  
Chapter 2 - Evaluating and Extending Parallel Slopes  
  
Model fit, residuals, and prediction:  
  
* Model fits is evaluated based on the residuals (distance in y from the fitted line), specifically the mean residual-squared  
* The coefficient of determination (R-squared) is defined as 1 - SSE/SST where SSE is the sum-squared error and SST is the sum-squared total (known prior to any modeling  
* Since the R-squared will always improve with additional variables, the adjusted R-squared is frequently reported also  
	* Adj R^2 = 1 - (SSE / SST) * (n - 1) / (n - p - 1) where n is the number of data points and p is the number of predictor variables  
* Can gain the fitted values in either of two ways  
	* predict(myLM) will return the predicted values  
    * broom::augment(myLM) will return a data frame with many variables, one of which is .fitted  
    * Note that broom::augment() is considered to be part of the tidyverse and is preferred for this course  
* Can make predictions for observations that are out of sample  
	* predict(myLM, newdata=myFrame) where myFrame is a data frame of 1+ observations with the same variables as were used for the model creation  
    * broom::augment(myLM, newdata=myFrame) will return a data frame including .fitted and .se.fit  
  
Understanding interaction - idea that the model might have both different slopes and different intercepts:  	  
* The interaction term is the product of two (or more) variables - for example Y ~ X1 + X2 + X1:X2  
    * The R syntax colon(:) means X1*X2, or ther interaction between X1 and X2  
* Interaction terms can change the intepretation of the model, and also of the components of the model  
* Including an interaction term in a model is easy---we just have to tell lm() that we want to include that new variable. An expression of the form  
	* lm(y ~ x + z + x:z, data = mydata)  
	* The use of the colon (:) here means that the interaction between x and z will be a third term in the model  
* Interaction models are easy to visualize in the with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable  
	* In this case, new and used MarioKarts each get their own regression line  
    * To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color  
  
Simpson's Paradox:  
  
* Example of moderating variable, such as percentage taking the SAT significantly driving average SAT scores  
	* SAT_wbin <- SAT %>%  
    * mutate(sat_bin = cut(sat_pct, 3))  
    * mod <- lm(formula = total ~ salary + sat_bin, data = SAT_wbin)  
* When Simpson's paradox is present the overall slope may be of different sign (positive vs begative) than the slope within any of the properly cut sub-groups  
  
Example code includes:  
```{r}

mario_kart <- marioKart %>% filter(totalPr <= 75)

# fit parallel slopes
(mod <- lm(totalPr ~ wheels + cond, data=mario_kart))

# R^2 and adjusted R^2
summary(mod)

# add random noise
mario_kart_noisy <- mario_kart %>%
  mutate(noise = rnorm(n=n()))
  
# compute new model
mod2 <- lm(totalPr ~ wheels + cond + noise, data=mario_kart_noisy)

# new R^2 and adjusted R^2
summary(mod2)


# return a vector
predict(mod)

# return a data frame
broom::augment(mod)


# include interaction
lm(totalPr ~ cond + duration + cond:duration, data=mario_kart)


# interaction plot
ggplot(mario_kart, aes(x=duration, y=totalPr, color=cond)) + 
  geom_point() + 
  geom_smooth(method="lm", se=FALSE)


slr <- ggplot(mario_kart, aes(y = totalPr, x = duration)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = 0)

# model with one slope
lm(totalPr ~ duration, data=mario_kart)

# plot with two slopes
slr + aes(color=cond)

```
  
  
  
***
  
Chapter 3 - Multiple Regression  
  
Adding a numerical explanatory variable - regressions with 2+ numerical variables:  
  
* The mathematical expression is very simple - just another variable and coefficient added to the model  
* The syntax for fitting the model is very simple also - Y ~ X1 + X2 rather than Y ~ X1  
* The geometric (visualization) expression is nowhere near as simple  
	* ggplot does not handle a z-variable at all!  
* Can run an approach known as "tiling the plane"  
	* grid <- babies %>%  
    * data_grid( gestation = seq_range(gestation, by = 1), age = seq_range(age, by = 1) )  
    * mod <- lm(bwt ~ gestation + age, data = babies)  
    * bwt_hats <- augment(mod, newdata = grid)  
    * data_space +  
    * geom_tile(data = bwt_hats, aes(fill = .fitted, alpha = 0.5)) +  
    * scale_fill_continuous("bwt", limits = range(babies$bwt))  
* Can also run the visuals using the plot_ly() call  
	* plot_ly(data = babies, z = ~bwt, x = ~gestation, y = ~age, opacity = 0.6) %>%  # somewhat similar syntax to ggplot2  
    * add_markers(text = ~case, marker = list(size = 2)) %>% # draw points  
    * add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE, cmax = 1, surfacecolor = color1, colorscale = col1)  # draw plane  
  
Conditional interpretation of coefficients:  
  
* Since a line cannot have two slopes, the coefficients from the multiple regression are defining a plane (two slopes) rather than a line  
* The coefficients can instead be interpreted as meaning "given constant values for all the other predictor variables"  
	* Often referred to as "impact on Y of X1 while holding <variables> constant"  
* Recall that each coefficient is in the same unit as the base data in the model; a coefficient being of larger magnitude does not mean that the coefficient is of larger importance  
  
Adding a third (categorical) variable:  
  
* Can add a third categorical variable to an existing model with two numerical variables already included  
* Recall the geometry of the various models  
	* 1 numeric + 1 categorical: parallel slopes  
    * 2 numeric: plane  
    * 2 numeric + 1 categorical: parallel planes  
* Can use the plotly::plot_ly() to visually represent the parallel planes  
  
Higher dimensions:  
  
* There are no mathematical or syntactical barrier to adding as many explanatory variables as needed in R  
	* There is, however, a geometric (visualization) barrier - parallel hyper-planes are not easy to visualize or interpret  
    * Can map to colors and project in to lower-dimensional spaces, but it does not always enhance understanding  
* Can use the dot operator to mean "everything" and the minus operator to mean "except"  
	* lm(Y ~ . - A) means regress Y and against everything, excluding A  
    * The key is to interpret each coefficient as "holding all other variables equal"  
    * The numeric variable coefficients are interpreted as slopes, while the factor variable coefficients are interpreted as changes to the intercept  
  
Example code includes:  
```{r}

# Fit the model using duration and startPr
(mod <- lm(totalPr ~ duration + startPr, data=mario_kart))


# One method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables
# This heatmap will illustrate how the model output changes over different combinations of the explanatory variables
# This is a multistep process
# First, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We've done this for you and stored the result as a data frame called grid
# Use augment() with the newdata argument to find the y-hat corresponding to the values in grid
# Add these to the data_space plot by using the fill aesthetic and geom_tile()
# add predictions to grid
grid <- expand.grid(duration=1:10, startPr=seq(0.01, 69.95, by=0.01))

price_hats <- broom::augment(mod, newdata=grid)

# tile the plane
data_space <- mario_kart %>% filter(totalPr <= 75) %>% 
    ggplot(aes(x=duration, y=startPr)) + 
    geom_point(aes(col=totalPr))

data_space + 
  geom_tile(data = price_hats, aes(fill=.fitted), alpha=0.5)


# An alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package
# We have created three objects that you will need
# x: a vector of unique values of duration
# y: a vector of unique values of startPr
# plane: a matrix of the fitted values across all combinations of x and y
# draw the 3D scatterplot
p <- plotly::plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%
  plotly::add_markers() 
  
# draw the plane
x <- c(1, 1.13, 1.261, 1.391, 1.522, 1.652, 1.783, 1.913, 2.043, 2.174, 2.304, 2.435, 2.565, 2.696, 2.826, 2.957, 3.087, 3.217, 3.348, 3.478, 3.609, 3.739, 3.87, 4, 4.13, 4.261, 4.391, 4.522, 4.652, 4.783, 4.913, 5.043, 5.174, 5.304, 5.435, 5.565, 5.696, 5.826, 5.957, 6.087, 6.217, 6.348, 6.478, 6.609, 6.739, 6.87, 7, 7.13, 7.261, 7.391, 7.522, 7.652, 7.783, 7.913, 8.043, 8.174, 8.304, 8.435, 8.565, 8.696, 8.826, 8.957, 9.087, 9.217, 9.348, 9.478, 9.609, 9.739, 9.87, 10)
y <- c(0.01, 1.024, 2.037, 3.051, 4.064, 5.078, 6.092, 7.105, 8.119, 9.133, 10.146, 11.16, 12.173, 13.187, 14.201, 15.214, 16.228, 17.242, 18.255, 19.269, 20.282, 21.296, 22.31, 23.323, 24.337, 25.351, 26.364, 27.378, 28.391, 29.405, 30.419, 31.432, 32.446, 33.46, 34.473, 35.487, 36.5, 37.514, 38.528, 39.541, 40.555, 41.569, 42.582, 43.596, 44.609, 45.623, 46.637, 47.65, 48.664, 49.678, 50.691, 51.705, 52.718, 53.732, 54.746, 55.759, 56.773, 57.787, 58.8, 59.814, 60.827, 61.841, 62.855, 63.868, 64.882, 65.896, 66.909, 67.923, 68.936, 69.95)
grid <- expand.grid(duration=x, startPr=y)
predPr <- broom::augment(mod, newdata=grid)
plane <- matrix(data=predPr$.fitted, nrow=70, ncol=70, byrow=FALSE)

p <- p %>%
  plotly::add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)

# Commented due to inability to use in html
# p

# draw the 3D scatterplot
# p <- plotly::plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%
#   plotly::add_markers(color = ~cond) 
  
# draw two planes
# p %>%
#   add_surface(x = ~x, y = ~y, z = ~plane0, showscale = FALSE) %>%
#   add_surface(x = ~x, y = ~y, z = ~plane1, showscale = FALSE)

```
  
  
  
***
  
Chapter 4 - Logistic Regression  
  
What is logistic regression?  
  
* When graphing a categorical variable as the response, geom_jitter() can be much more helpful than geom_point()  
* With a categorical variable as a response, it is important to convert the variable to a numeric  
* A straight linear regression with a numeric predictor and a categorical response can make non-sensical predictions  
	* Might predict a -20% chance of death or a 120% chance of death even for reasonable values of the predictor variable, for example  
* The GLM (Generalized Linear Model) formalizes a modeling technique for predicting categorical variables  
	* The logistic regression (logit) is frequently used to model a binary response  
    * The basic idea is that a link function is added - logit in the case of the logistic regression  
* Fitting a GLM in R is very similar to fitting an LM  
	* glm(myFactor ~ myNumeric, family=binomial)  # will run the logistic regression  
* Note that the mathematical model is now:  
	* log(y / (1−y))=β0+β1⋅x+ϵ  
    * where ϵ is the error term  
  
Visualizing logistic regression:  
  
* Can use geom_smooth(method="glm", se=0, color="red", method.args=list(family="binomial")) to add a logit smooth to a plot  
* The logistic regression can never precisely predict the categorical variable, since the data are all 0 or 1 while the logit is between 0 and 1  
* Can bin the underlying data, look at aggregate probabilities (mean x, mean y) by bin, and compare with the logit outcomes  
* Here we are plotting y as a function of x, where that function is  
	* y = exp(β^0+β^1⋅x) / (1+exp(β^0+β^1⋅x))  
  
Three scales approach to visualization:  
  
* Generating the predicted values comes from myGLM %>% augment(type.predict="response") %>% mutate(y_hat = .fitted)  
* Can no longer interpret the coefficients as slopes  
	* odds = y-hat / (1 - y-hat) = exp(B0 + B1 * X1)  
    * log(odds) = log( (y-hat / (1 - y-hat)) ) = B0 + B1 * X1  
    * So, the coefficients do represent the slope of the log-odds function  
* Comparison of the three potential scales for visualizing a logistic regression  
	* Probability - positives (intuitive and easy to interpret scale) and negatives (non-linear and challenging to interpret function)  
    * Odds - a bit of a middle ground between Probability and Log-Odds  
    * Log-Odds - positive (linear, easy-to-interpret function) and negative (very hard to interpret scale)  
* Can also calculate the odds-ratio, as well as the change in odds-ratio with respect to a key variable  
	* OR = odds(y | x + 1) / odds(y | x) = exp(B1)  
    * If the exp(coef(myGLM)) has a coefficient greater than 1, then the odds increase with that variable; otherwise, they decrease  
  
Using a logistical model - objective is to gain better understanding in to the underlying process:  
  
* Can augment the GLM to add a variable for heart transplant - will be very meaningful!  
* Note that the default for the augment() function is to give back .fitted that represents log-odds scale  
	* Can instead set the type.predict = "response" to get back the normal probability scale  
* Additional goal is to make out-of-sample predictions based on the results of the model  
	* Since the study was performed in 1973, it is likely that the coefficients have changed  
* Can also convert the predictions by rounding the probabilities (less than 0.5 is death, greater than 0.5 is life) and then running the confusion matrix  
	* Can experiment with different rounding thresholds than 0.5, and see whether the confusion matrix is improved  
  
Example code includes:  
```{r}

# To see this in action, we'll fit a linear regression model to data about 55 students who applied to medical school
# We want to understand how their undergraduate GPAGPA relates to the probability they will be accepted by a particular school (Acceptance)
# The medical school acceptance data is loaded in your workspace as MedGPA
# scatterplot with jitter
# tmpSAT <- readr::read_csv("./RInputFiles/SAT.csv")

tmpAccept <- c(0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0)
tmpGPA <- c(3.62, 3.84, 3.23, 3.69, 3.38, 3.72, 3.89, 3.34, 3.71, 3.89, 3.97, 3.49, 3.77, 3.61, 3.3, 3.54, 3.65, 3.54, 3.25, 3.89, 3.71, 3.77, 3.91, 3.88, 3.68, 3.56, 3.44, 3.58, 3.4, 3.82, 3.62, 3.09, 3.89, 3.7, 3.24, 3.86, 3.54, 3.4, 3.87, 3.14, 3.37, 3.38, 3.62, 3.94, 3.37, 3.36, 3.97, 3.04, 3.29, 3.67, 2.72, 3.56, 3.48, 2.8, 3.44)
MedGPA <- data.frame(Acceptance=tmpAccept, GPA=tmpGPA)
str(MedGPA)

data_space <- ggplot(MedGPA, aes(x=GPA, y=Acceptance)) + 
  geom_jitter(width = 0, height = 0.05, alpha = 0.5)

# linear regression line
data_space + 
  geom_smooth(method="lm", se=FALSE)


# filter
MedGPA_middle <- MedGPA %>%
  filter(GPA >= 3.375, GPA <= 3.77)

# scatterplot with jitter
data_space <- ggplot(MedGPA_middle, aes(x=GPA, y=Acceptance)) + 
  geom_jitter(width = 0, height = 0.05, alpha = 0.5)

# linear regression line
data_space + 
  geom_smooth(method="lm", se=FALSE)


# fit model
(mod <- glm(Acceptance ~ GPA, data = MedGPA, family = binomial))


# scatterplot with jitter
data_space <- ggplot(MedGPA, aes(x=GPA, y=Acceptance)) + 
  geom_jitter(width=0, height=0.05, alpha = .5)

# add logistic curve
data_space +
  geom_smooth(method="glm", se=FALSE, method.args=structure(list(family = "binomial"), .Names = "family"))


# We have created a data.frame called MedGPA_binned that aggregates the original data into separate bins for each 0.25 of GPA. It also contains the fitted values from the logistic regression model
MedGPA$bin <- round(MedGPA$GPA*4, 0) / 4
str(MedGPA)

MedGPA_binned <- MedGPA %>% 
    group_by(bin) %>% 
    summarize(mean_GPA=mean(GPA), acceptance_rate=mean(Acceptance), ct=n())
MedGPA_binned

# binned points and line
data_space <- ggplot(MedGPA_binned, aes(x=mean_GPA, y=acceptance_rate)) + 
  geom_point() + 
  geom_line()

# augmented model
MedGPA_plus <- broom::augment(mod, type.predict="response")

# logistic model on probability scale
data_space +
  geom_line(data = MedGPA_plus, aes(x=GPA, y=.fitted), color = "red")


# The MedGPA_binned data frame contains the data for each GPA bin, while the MedGPA_plus data frame records the original observations after being augment()-ed by mod
# compute odds for bins
MedGPA_binned <- MedGPA_binned %>%
  mutate(odds = acceptance_rate / (1 - acceptance_rate))

# plot binned odds
data_space <- ggplot(MedGPA_binned, aes(x=mean_GPA, y=odds)) + 
  geom_point() + 
  geom_line()

# compute odds for observations
MedGPA_plus <- MedGPA_plus %>%
  mutate(odds_hat = .fitted / (1 - .fitted))

# logistic model on odds scale
data_space +
  geom_line(data=MedGPA_plus, aes(x=GPA, y=odds_hat), color = "red")


# compute log odds for bins
MedGPA_binned <- MedGPA_binned %>%
  mutate(log_odds = log(acceptance_rate / (1 - acceptance_rate)))

# plot binned log odds
data_space <- ggplot(MedGPA_binned, aes(x=mean_GPA, y=log_odds)) + 
  geom_point() + 
  geom_line()

# compute log odds for observations
MedGPA_plus <- MedGPA_plus %>%
  mutate(log_odds_hat = log(.fitted / (1 - .fitted)))

# logistic model on log odds scale
data_space +
  geom_line(data=MedGPA_plus, aes(x=GPA, y=log_odds_hat), color = "red")


# create new data frame
new_data <- data.frame(GPA = 3.51)

# make predictions
broom::augment(mod, newdata=new_data, type.predict="response")


# data frame with binary predictions
tidy_mod <- broom::augment(mod, type.predict="response") %>%
  mutate(Acceptance_hat = round(.fitted))
  
# confusion matrix
tidy_mod %>%
  select(Acceptance, Acceptance_hat) %>%
  table()

```
  
  
  
***
  
Chapter 5 - Case Study: Italian Restaurants in NYC  
  
Italian restaurants in NYC - factors that contribute to the price of a meal:  
  
* Will use the numeric data available from Zagat, using the scores that range from 0-30  
* The dataset "nyc" has Price and several other (potential) explanatory variables  
	* Can run pairs(nyc) to see some basic relationships in the variables for this dataset  
  
Incorporating another variable:  
  
* Fifth Island divides Manhattan, and historically the East side has been the pricier side of the island  
	* Question is whether, all else equal, there is an East-side premium in the restaurant pricing  
* Additional question is whether people are willing to pay more for better service, holding the quality of the food constant  
  
Higher dimensions - adding the décor dimension to the existing Zagat analysis:  
  
* Objective is to predict Price, with explanatory variables including Food, Service, Décor, and East (categorical)  
* Can introduce collinearity problems since many of the variables are potentially correlated with each other  
	* Can also have collinearity across multiple variable, such as Y ~ A + B + C when A = B + C  
    * The main problem with collinearity is that it makes the coefficients extremely unreliable (non-robust)  
    * Note that multi-collinearity does NOT hurt the explanatory power of the model - R-squared is still OK  
  
Wrap up:  
  
* Mathematical, Geometric, Syntactical modeling  
* Focus was on descriptive statistics rather than inferential statistics  
* Prior to inferential statistics, only conclusions and descriptions about the sample can be made  
  
Example code includes:  
```{r}

nyc <- readr::read_csv("./RInputFiles/nyc.csv")
str(nyc, give.attr=FALSE)


# Price by Food plot
ggplot(nyc, aes(x=Food, y=Price)) + 
  geom_point()

# Price by Food model
lm(Price ~ Food, data=nyc)

# fit model
lm(Price ~ Food + Service, data=nyc)

# draw 3D scatterplot
# p <- plot_ly(data = nyc, z = ~Price, x = ~Food, y = ~Service, opacity = 0.6) %>%
#   add_markers() 

# draw a plane
# p %>%
#   add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE) 


# Price by Food and Service and East
lm(Price ~ Food + Service + East, data=nyc)


# draw 3D scatterplot
# p <- plot_ly(data = nyc, z = ~Price, x = ~Food, y = ~Service, opacity = 0.6) %>%
#   add_markers(color = ~factor(East)) 

# draw two planes
# p %>%
#   add_surface(x = ~x, y = ~y, z = ~plane0, showscale = FALSE) %>%
#   add_surface(x = ~x, y = ~y, z = ~plane1, showscale = FALSE)

```
  
  
  
***
  
###_Forecasting Using R_  
  
Chapter 1 - Exploring and Visualizing Time Series in R  
  
Introduction and overview:  
  
* Course contents include  
	* Visualizing time series  
    * Simple benchmarks for forecasting  
    * Exponential smoothing and ARIMA  
    * Advanced forecasting methods  
    * Measuring forecast accuracy  
    * Choosing the best methods  
* Hyndman book "Forecasting Principles and Practice" is freely available online  
* Data in this book will be "regularly spaced" - yearly, hourly, etc.  
* Forecasting is the process of projecting the time-series data forward, and including prediction intervals  
	* Trend, seasonality, other features  
* A time series can be thought of as a vector or matrix of numbers along with some information about what times those numbers were recorded  
	* This information is stored in a ts object in R (in most exercises, you will use time series that are part of existing packages)  
    * If you want to work with your own data, you need to know how to create a ts object in R  
* ts(data, start, frequency, ...)  
	* frequency is set to 4 if the data are quarterly  
    * start is set to the form c(year, period) to indicate the time of the first observation  
    * Here, January corresponds with period 1; likewise, a start date in April would refer to 2, July to 3, and October to 4. Thus, period corresponds to the quarter of the year  
* You can use the autoplot() function to produce a time plot of the data with or without facets, or panels that display different subsets of data:  
	* autoplot(usnim_2002, facets = FALSE)  
* To find the number of observations per unit time, use frequency()  
	* frequency(usnim_2002)  
* Along with time plots, there are other useful ways of plotting data to emphasize seasonal patterns and show changes in these patterns over time  
	* A seasonal plot is similar to a time plot except that the data are plotted against the individual “seasons” in which the data were observed  
    * You can create one using the ggseasonplot() function the same way you do with autoplot()  
    * An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal; to make one, simply add a polar argument and set it to TRUE  
    * A subseries plot comprises mini time plots for each season. Here, the mean for each season is shown as a blue horizontal line  
* One way of splitting a time series is by using the window() function, which extracts a subset from the object x observed between the times start and end  
	* window(x, start = NULL, end = NULL)  
  
Trends, seasonality, and cyclicity:  
  
* Several types of patterns in time series are so common that they get named  
	* Trends are long-term patterns of increase or decrease in the data  
    * Seasonality is a periodic pattern that recurs in the data  
    * Cyclic is a pattern of the data rising or falling (typically over 2+ years) that are not of a fixed period - example being a business cycle  
* Be cautious about cycles vs. trends, and be sure that the window is long enough to make the distinction  
* The lynx population in Canada is an example of a very dramatic cyclic pattern (too many lynx -> not enough food per lynx > most lynx starve to death -> plenty of food per survivor -> breeding -> too many lynx)  
* Another way to look at time series data is to plot each observation against another observation that occurred some time previously by using gglagplot()  
	* For example, you could plot yt against yt-1. This is called a lag plot because you are plotting the time series against lags of itself  
    * The correlations associated with the lag plots form what is called the autocorrelation function (ACF). The ggAcf() function produces ACF plots.  
* When data are either seasonal or cyclic, the ACF will peak around the seasonal lags or at the average cycle length  
  
White noise - simply a time series of independently and identically distributed (iid) data:  
  
* White noise serves as the basis for many types of time series modeling  
	* The samping distributions for white noise ACF are well known, and the dashed lines on ACF plots are frequently the 95% interval for these  
    * ACF that are outside the dashed lines are frequently suggestive that there is information beyond white-noise  
* The Ljung-Box test considers the first h autocorrelation values together  
	* A significant p-value indicates that these data are likely NOT white noise  
    * Appears to be a modified implementation of chi-squared  
    * Box.test(pigs, lag = 24, fitdf = 0, type = "Ljung")  
* There is a well-known result in economics called the "Efficient Market Hypothesis" that states that asset prices reflect all available information  
	* A consequence of this is that the daily changes in stock prices should behave like white noise (ignoring dividends, interest rates and transaction costs)  
    * The consequence for forecasters is that the best forecast of the future price is the current price  
  
Example code includes:  
```{r}

library(forecast)

# Read the data from Excel into R
mydata <- readxl::read_excel("./RInputFiles/exercise1.xlsx")

# Look at the first few lines of mydata
head(mydata)

# Create a ts object called myts
myts <- ts(mydata[, -1], start = c(1981, 1), frequency = 4)


# Plot the data with facetting
autoplot(myts, facets = TRUE)

# Plot the data without facetting
autoplot(myts, facets = FALSE)


# Plot the three series
data(gold, package="forecast")
data(woolyrnq, package="forecast")
data(gas, package="forecast")

str(gold)
str(woolyrnq)
str(gas)

autoplot(gold)
autoplot(woolyrnq)
autoplot(gas)

# Find the outlier in the gold series
goldoutlier <- which.max(gold)

# Look at the seasonal frequencies of the three series
frequency(gold)
frequency(woolyrnq)
frequency(gas)


# In this exercise, you will load the fpp2 package and use two of its datasets
# a10 contains monthly sales volumes for anti-diabetic drugs in Australia. In the plots, can you see which month has the highest sales volume each year? What is unusual about the results in March and April 2008?
# ausbeer which contains quarterly beer production for Australia. What is happening to the beer production in Quarter 4?
# Load the fpp2 package
# library(fpp2)

data(a10, package="fpp2")
data(ausbeer, package="fpp2")

str(a10)
str(ausbeer)

# Create plots of the a10 data
autoplot(a10)
forecast::ggseasonplot(a10)

# Produce a polar coordinate season plot for the a10 data
forecast::ggseasonplot(a10, polar = TRUE)

# Restrict the ausbeer data to start in 1992
beer <- window(ausbeer, start=1992)

# Make plots of the beer data
autoplot(beer)
forecast::ggsubseriesplot(beer)


# In this exercise, you will work with the pre-loaded oil data (available in the package fpp2), which contains the annual oil production in Saudi Arabia from 1965-2013 (measured in millions of tons).
# Create an autoplot of the oil data
data(oil, package="fpp2")
str(oil)

autoplot(oil)

# Create a lag plot of the oil data
forecast::gglagplot(oil)

# Create an ACF plot of the oil data
library(forecast)
ggAcf(oil)


# You will investigate this phenomenon by plotting the annual sunspot series (which follows the solar cycle of approximately 10-11 years) in sunspot.year
# and the daily traffic to the Hyndsight blog (which follows a 7-day weekly pattern) in hyndsight. Both objects have been loaded into your workspace.
# Plot the annual sunspot numbers
data(sunspot.year)
str(sunspot.year)

autoplot(sunspot.year)
ggAcf(sunspot.year)


# Plot the traffic on the Hyndsight blog
data(hyndsight, package="fpp2")
str(hyndsight)

autoplot(hyndsight)
ggAcf(hyndsight)


# You can test this hypothesis by looking at the goog series, which contains the closing stock price for Google over 1000 trading days ending on February 13, 2017. This data has been loaded into your workspace.
# Plot the original series
data(goog, package="fpp2")
str(goog)

autoplot(goog)

# Plot the differenced series
autoplot(diff(goog))

# ACF of the differenced series
ggAcf(diff(goog))

# Ljung-Box test of the differenced series
Box.test(diff(goog), lag = 10, type = "Ljung")

```
  
  
  
***
  
Chapter 2 - Benchmark methods and forecast accuracy  
  
Forecasts and potential futures:  
  
* Can build up a distribution of potential futures based on multiple simulations (NOT forecasts - each is a simulated future)  
	* The point forecast ("forecast") is the average of these multiple simulations  
    * The shaded regions are typically the 80% / 95% CI intervals around the point forecasts  
* The very simplest forecasting method is to use the most recent observation; this is called a naive forecast and can be implemented in a namesake function  
	* This is the best that can be done for many time series including most stock price data  
    * Even if it is not a good forecasting method, it provides a useful benchmark for other forecasting methods  
* This is implemented in the snaive() function, meaning, seasonal naive  
	* For both forecasting methods, you can set the second argument h, which specifies the number of values you want to forecast  
    * naive(y, h = 10)  
    * snaive(y, h = 2 * frequency(x))  
  
Fitted values and residuals:  
  
* Frequent validation method is to forecast using historical data ON TO data that you already know  
	* Allows for both fitted values and residuals to be calculated - residuals should look like white noise in a good forecast  
    * Sometimes, there is some "cheating", since the observed data point was also used in building the forecast model  
* Four basic assumptions that are made about the residuals  
	1.  Residuals should be uncorrelated (essential)  
    2.  Residuals should have zero mean (essential - otherwise the model is biased)  
    3.  Residuals have constant variance (convenirnce)  
    4.  Residuals are normally distributed (convenience)  
* Basically, the request is for the residuals to be Gaussian white noise, which can be checked using checkresiduals(myFCModel)  
	* Checking residuals is an essential step prior to moving forward with the forecast  
  
Training and test sets help to validate the forecasting methodology:  
  
* Forecasts can be built using the training set, and then applied to the test set for validation  
* Checking the forecasts on the test set helps solve the issues of over-fitting  
* Forecast "errors" are the differences between the forecast values and the observed values in the test set  
	* In contrast, residuals are the differences in fitted and actual on the training set  
    * Further, residuals are based only on the one-step forecast (???)  
* There are many metrics used for assessing the quality of the forecast  
	* Mean Absolute Error (MAE)  
    * Mean Squared Error (MSE)  
    * Mean Absolute Percentage Error (MAPE)  
    * Mean Absolute Scaled Error (MASE) = MAE / Q where Q is a scaling constant  
* The accuracy() function calculates all of the key measures above - training set based on residuals, and test set based on errors  
	* Useful for comparing multiple forecast models on the same data  
* One function that can be used to create training and test sets is subset.ts(), which returns a subset of a time series where the optional start and end arguments are specified using index values.  
	* > # To subset observations from 101 to 500  
    * > train <- subset.ts(x, start = 101, end = 500, ...)  
    * > # To subset the first 500 observations  
    * > train <- subset.ts(x, end = 500, ...)  
* The function meanf(), which gives forecasts equal to the mean of all observations  
  
Time series cross-validation attempts to solve some of the problems related to test-train:  
  
* A general process is to make each test set a single point following a training set of variable lengths  
	* Analogous to cross-validation in non-time-series problems  
* The tsCV() function does a lot of the work behind the scenes  
	* myE <- tsCV(myTS, forecastfunction = naïve, h=1)  
    * mean(myE ** 2, na.rm=TRUE)  
* Generally, the MSE will increase as the window (h) increases  
  
Example code includes:  
```{r cache=TRUE}

# Use naive() to forecast the goog series
fcgoog <- naive(goog, h=20)

# Plot and summarize the forecasts
autoplot(fcgoog)
summary(fcgoog)


# Use snaive() to forecast the ausbeer series
fcbeer <- snaive(ausbeer, h=16)

# Plot and summarize the forecasts
autoplot(fcbeer)
summary(fcbeer)


# Check the residuals from the naive forecasts applied to the goog series
goog %>% naive() %>% checkresiduals()

# Do they look like white noise (TRUE or FALSE)
googwn <- TRUE

# Check the residuals from the seasonal naive forecasts applied to the ausbeer series
ausbeer %>% snaive() %>% checkresiduals()

# Do they look like white noise (TRUE or FALSE)
beerwn <- FALSE


# The pre-loaded time series gold comprises daily gold prices for 1108 days. Here, you'll use the first 1000 days as a training set, and compute forecasts for the remaining 108 days
# Create the training data as train
train <- subset(gold, end = 1000)

# Compute naive forecasts and save to naive_fc
naive_fc <- naive(train, h = 108)

# Compute mean forecasts and save to mean_fc
mean_fc <- meanf(train, h = 108)

# Use accuracy() to compute RMSE statistics
accuracy(naive_fc, gold)
accuracy(mean_fc, gold)

# Assign one of the two forecasts as bestforecasts
# bestforecasts <- naive_fc


# Here, you will use the Melbourne quarterly visitor numbers (vn[, "Melbourne"]) to create three different training sets, omitting the last 1, 2 and 3 years, respectively
# Inspect the pre-loaded vn data in your console before beginning the exercise
# This will help you determine the correct value to use for the keyword h (which specifies the number of values you want to forecast) in your forecasting methods
melData <- c(4.865, 4.113, 4.422, 5.171, 5.55, 4.009, 3.986, 3.839, 5.8, 4.229, 4.157, 4.627, 5.691, 4.601, 4.742, 5.733, 5.397, 3.884, 4.996, 5.304, 5.222, 4.765, 4.146, 4.717, 4.88, 4.868, 4.182, 4.214, 5.438, 3.87, 4.394, 4.404, 5.716, 5.291, 4.19, 4.712, 4.709, 4.489, 4.698, 5.193, 5.216, 4.215, 5.042, 5.089, 4.688, 4.393, 4.626, 4.88, 4.844, 4.437, 4.833, 4.622, 5.164, 4.504, 4.976, 4.508, 4.759, 4.835, 5.009, 5.693, 5.224, 4.82, 4.688, 4.918, 5.936, 5.44, 5.134, 5.993, 6.654, 5.342, 5.471, 5.812)
sydData <- c(7.319, 6.13, 6.284, 6.384, 6.602, 5.674, 5.715, 6.564, 6.602, 5.398, 7.172, 8.474, 7.012, 6.388, 6.073, 6.196, 5.633, 5.779, 5.869, 6.002, 6.202, 5.321, 5.161, 5.737, 6.168, 5.709, 5.057, 5.362, 5.902, 4.496, 5.093, 5.253, 6.832, 5.67, 5.008, 5.773, 6.529, 4.911, 4.784, 5.844, 6.252, 5.034, 5.263, 4.714, 5.362, 4.769, 4.125, 5.263, 6, 4.283, 5.256, 5.357, 6.194, 5.102, 5.596, 5.066, 6.684, 4.697, 5.366, 5.075, 5.499, 4.867, 5.71, 6.198, 6.416, 5.284, 5.483, 6.234, 6.938, 6.268, 5.562, 6.016)
vnFrame <- data.frame(Melbourne=melData, Sydney=sydData)
vn <- ts(vnFrame, start=c(1998, 1), frequency=4)
str(vn)

# Create three training series omitting the last 1, 2, and 3 years
train1 <- window(vn[, "Melbourne"], end = c(2014, 4))
train2 <- window(vn[, "Melbourne"], end = c(2013, 4))
train3 <- window(vn[, "Melbourne"], end = c(2012, 4))

# Produce forecasts using snaive()
fc1 <- snaive(train1, h = 4)
fc2 <- snaive(train2, h = 4)
fc3 <- snaive(train3, h = 4)

# Use accuracy() to compare the MAPE of each series
accuracy(fc1, vn[, "Melbourne"])["Test set", "MAPE"]
accuracy(fc2, vn[, "Melbourne"])["Test set", "MAPE"]
accuracy(fc3, vn[, "Melbourne"])["Test set", "MAPE"]


# Compute cross-validated errors for up to 8 steps ahead
e <- matrix(NA_real_, nrow = 1000, ncol = 8)
for (h in 1:8)
  e[, h] <- tsCV(goog, forecastfunction = naive, h = h)
  
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = TRUE)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()

```
  
  
  
***
  
Chapter 3 - Exponential smoothing  
  
Exponentially weighted forecasts (simple exponential smoothing):  
  
* The point estimates are represented as y-hat(i)  
* The point estimate for y-hat(y) = alpha * y-hat(t-1) + alpha * (1 - alpha) * y-hat(t-2) + alpha * (1 - alpha)^2 * y-hat(t-3) + … where the extent of the looking back can be user-specified  
	* Smaller alpha leads to slower decay  
* Can instead think of the forecast as y-hat(t) = alpha * y(t) + (1 - alpha) * y(t-1)  
	* Basically, need to estimate both alpha and the starting point  
* Can run these models using myFC <- ses(myTS, h=)  
	* Behind the scenes, this is a non-linear optimization  
    * The h is the number of years for the forecast to project forwards  
* While a simple method, this typically forms the starting point for future models  
* Process for review for test-train analysis with time series data  
	1.  First, import and load your data. Determine how much of your data you want to allocate to training, and how much to testing; the sets should not overlap  
    2.  Subset the data to create a training set, which you will use as an argument in your forecasting function(s). Optionally, you can also create a test set to use later  
    3.  Compute forecasts of the training set using whichever forecasting function(s) you choose, and set h equal to the number of values you want to forecast, which is also the length of the test set  
    4.  To view the results, use the accuracy() function with the forecast as the first argument and original data (or test set) as the second  
    5.  Pick a measure in the output, such as RMSE or MAE, to evaluate the forecast(s); a smaller error indicates higher accuracy  
  
Exponential smoothing methods with trend:  
  
* Forecast is now level(t) + h * slope(t)  
* Level is now alpha * y(t) + (1 - alpha) * (level(t-1) + slope(t-1))  
* Trend (local linear trend) is allowed to change over time and is Beta * (level(t) - level(t-1)) + (1 - Beta) * slope(t-1)  
	* A smaller Beta means the slope changes just a little, while a higher Beta means the slope can change very quickly  
* There are now four parameters to estimate - alpha and Beta, plus starting conditions level-0 and slope-0  
	* Parameters are again created to minimize SSE, using a methodology first developed by Holt  
    * myTS %>% holt(h=) %>% autoplot()  
* Alternately, can add a damping feature so that the data level off over time - Damped Trend Method  
	* There is an additional parameter 0 <= phi <= 1 where phi determines the damping (phi=1 is Holt)  
    * The damped=TRUE argument to holt() will find the parameter phi, in addition to the other Holt function outputs  
  
Exponential smoothing methods with trend and seasonality (commonly known as the Holt-Winters method):  
  
* There is an additive version and a multiplicative version of the formula  
	* In the additive component, the seasoanl components average to zero  
    * In the multiplicative component, the seasoanl components average to one  
* Can run the models using hw(myTS, seasonal=)  # seasonal can be "additive" or "multiplicative"  
* Can think of a 3x3 grid of the potential forecast models  
	* Trend - N (None), A (Additive), Ad (Additive Damped)  
    * Seasonal - N (None), A (Additive), M (Multiplicative)  
* Model can be thought of as handling various portions of the grid, using(Trend, Seasonal) notation  
	* (N, N) - ses()  
    * (A, N) - holt()  
    * (Ad, N) - hw()  
    * (A, A) - hw()  
    * (A, M) - hw()  
    * (Ad, M) - hw()  
    * The other patterns, especially seasonality absent trend, will be considered later in the course  
* The Holt-Winters method can also be used for daily type of data, where the seasonal pattern is of length 7, and the appropriate unit of time for h is in days  
  
State space models for exponential smoothing:  
  
* There are 18 possible "innovations state space" models - Trend (3) x Seasonal (3) x Error (2)  
	* Error can be (A, M) for Additive or Multiplicative  
    * Multiplicative errors means that the error increases as the value increases (error tends to a fixed percentage rather than a fixed value)  
* Overall, these are referred to as ETS models, for "Error, Trend, Seasonality"  
	* Parameters can be estimated using the likelihhod method, the probability of data arising from the specified model  
    * For additive errors, the goal is to minimize SSE  
    * Can choose the best model by minimizing a corrected version of Akaike's Information Criteria (AICc), roughly the same as cross-validation but much faster  
* By running ets(myTS), all of the work is done behind the scenes, and the function reports the best model based on minimizing AICc  
	* Maximizes likelihood rather than minimizing errors  
    * To produce a forecast, the model created by ets() must be piped to the forecast() function  
    * The biggest advantage of ets() is that the model is chosen for you  
* The second argument for tsCV() must return a forecast object, so you need a function to fit a model and return forecasts. Recall:  
	* args(tsCV)  
    * function (y, forecastfunction, h = 1, ...)  
  
Example code includes:  
```{r cache=TRUE}

# You will also use summary() and fitted(), along with autolayer() for the first time, which is like autoplot() but it adds a "layer" to a plot rather than creating a new plot.
# Here, you will apply these functions to marathon, the annual winning times in the Boston marathon from 1897-2016. The data are available in your workspace.
# Use ses() to forecast the next 10 years of winning times
data(marathon, package="fpp2")
str(marathon)

fc <- ses(marathon, h = 10)

# Use summary() to see the model parameters
summary(fc)

# Use autoplot() to plot the forecasts
autoplot(fc)

# Add the one-step forecasts for the training data to the plot
autoplot(fc) + autolayer(fitted(fc))


# Create a training set using subset.ts()
train <- subset(marathon, end = length(marathon) - 20)

# Compute SES and naive forecasts, save to fcses and fcnaive
fcses <- ses(train, h = 20)
fcnaive <- naive(train, h = 20)

# Calculate forecast accuracy measures
accuracy(fcses, marathon)
accuracy(fcnaive, marathon)

# Save the best forecasts as fcbest
# fcbest <- fcnaive


# Here, you will apply it to the austa series, which contains annual counts of international visitors to Australia from 1980-2015 (in millions). The data has been pre-loaded into your workspace.
# Produce 10 year forecasts of austa using holt()
data(austa, package="fpp2")
str(austa)

fcholt <- holt(austa, h=10)

# Look at fitted model using summary()
summary(fcholt)

# Plot the forecasts
autoplot(fcholt)

# Check that the residuals look like white noise
checkresiduals(fcholt)


# Here, you will apply hw() to a10, the monthly sales of anti-diabetic drugs in Australia from 1991 to 2008. The data are available in your workspace.
# Plot the data
data(a10, package="fpp2")
str(a10)

autoplot(a10)

# Produce 3 year forecasts
fc <- hw(a10, seasonal = "multiplicative", h = 3)

# Check if residuals look like white noise
checkresiduals(fc)
whitenoise <- FALSE

# Plot forecasts
autoplot(fc)


# Here, you will compare an additive Holt-Winters method and a seasonal naive() method for the hyndsight data, which contains the daily pageviews on the Hyndsight blog for one year starting April 30, 2014
# Create training data with subset()
train <- subset(hyndsight, end = length(hyndsight) - 28)

# Holt-Winters additive forecasts as fchw
fchw <- hw(train, seasonal = "additive", h = 28)

# Seasonal naive forecasts as fcsn
fcsn <- snaive(train, h=28)

# Find better forecasts with accuracy()
accuracy(fchw, hyndsight)
accuracy(fcsn, hyndsight)

# Plot the better forecasts
autoplot(fchw)


# Fit ETS model to austa in fitaus
fitaus <- ets(austa)

# Check residuals
checkresiduals(fitaus)

# Plot forecasts
autoplot(forecast(fitaus))

# Repeat for hyndsight data in fiths
fiths <- ets(hyndsight)
checkresiduals(fiths)
autoplot(forecast(fiths))

# Which model(s) fails test? (TRUE or FALSE)
fitausfail <- FALSE
fithsfail <- TRUE


# Function to return ETS forecasts
fets <- function(y, h) {
  forecast(ets(y), h = h)
}

data(qcement, package="fpp2")
str(qcement)

cement <- window(qcement, start=1994)
str(cement)

# Apply tsCV() for both methods
e1 <- tsCV(cement, fets, h = 4)
e2 <- tsCV(cement, snaive, h = 4)

# Compute MSE of resulting errors (watch out for missing values)
mean(e1^2, na.rm=TRUE)
mean(e2^2, na.rm=TRUE)

# Copy the best forecast MSE
bestmse <- mean(e2^2, na.rm=TRUE)


# Computing the ETS does not work well for all series
# Here, you will observe why it does not work well for the annual Canadian lynx population available in your workspace as lynx
# Plot the lynx series
data(lynx)
str(lynx)
autoplot(lynx)

# Use ets() to model the lynx series
fit <- ets(lynx)

# Use summary() to look at model and parameters
summary(fit)

# Plot 20-year forecasts of the lynx series
fit %>% forecast(h=20) %>% autoplot()

```
  
  
  
***
  
Chapter 4 - Forecasting with ARIMA Models  
  
Transformations for variance stabilization:  
  
* Common transformations include powers, logs, inverses, and the like  
* There is a family of transformations for time series data known as the Box-Cox transformations  
	* There is a lambda paremeter  
    * lambda = 1 is no transformation  
    * lambda = 0.5 is like a square root  
    * lambda = 0 is like a natural logarithm  
    * lambda = -1 is like an inverse  
* Can run BoxCox.lambda(myTS) to get a rough approximation of the best parameter to use for further analysis  
* Can add the lambda= argument to the ets() function, and R will then take care of the rest  
	* The forecast() function gets the lambda and other parameters, and thereby puts the forecast back on the normal scale  
* The lambda is not so often used with ets() models since they can handle multiplicative error already, but it is vital for ARIMA models (next chapter)  
* Differencing is a way of making a time series stationary; this means that you remove any systematic patterns such as trend and seasonality from the data  
	* A white noise series is considered a special case of a stationary time series  
    * With non-seasonal data, you use lag-1 differences to model changes between observations rather than the observations directly  
    * You have done this before by using the diff() function  
* With seasonal data, differences are often taken between observations in the same season of consecutive years, rather than in consecutive periods  
	* For example, with quarterly data, one would take the difference between Q1 in one year and Q1 in the previous year  
    * This is called seasonal differencing  
    * Sometimes you need to apply both seasonal differences and lag-1 differences to the same series, thus, calculating the differences in the differences  
  
ARIMA models - AutoRegressive Integrated Moving Average:  
  
* The AR component is a multiple regression against the lagged observations of the model, using the last p observations  
* The MA component is a multiple regression against the lagged errors of the model, using the last q errors  
* Combining these, you have an ARMA(p, q) model  
	* ARMA models only work for stationary data, so it must have been differenced or otherwise transformed prior to running these  
* The ARIMA(p, d, q) is an ARMA(p, q) model with d level differencing included also  
	* Can run auto.arima(myTS) to get R to pick what it believes are the best parameters for p, d, q  
    * The "drift" estimate means the intercept, c, for the AR or the MA portion of the model  
* Note that AICc can only be compared for models of the same class (including the same value for d if the model is ARIMA)  
* Due to the search grid criteria, auto.arima can sometimes get a non-optimal solution, since the optimal solution was not in its search grid  
	* To make auto.arima() work harder to find a good model, add the optional argument stepwise = FALSE to look at a much larger collection of models  
* The Arima() function can be used to select a specific ARIMA model  
	* Its first argument, order, is set to a vector that specifies the values of pp, dd and qq  
    * The second argument, include.constant, is a booolean that determines if the constant cc, or drift, should be included  
* Below is an example of a pipe function that would plot forecasts of usnetelec from an ARIMA(2,1,2) model with drift:
	* usnetelec %>% Arima(order = c(2,1,2), include.constant = TRUE) %>% forecast() %>% autoplot()  
  
Seasonal ARIMA models - just needs a lot more differencing and lags:  
  
* A seasonal ARIMA model is referred to as (p, d, q) with (P, D, Q)m  
	* The lower case letters are the main model  
    * The upper case letters are the seasonal component of the model, with m meaning the seasonal period  
    * ARIMA(0,1,4)(0,1,1)[12]  # (p, d, q) = (0, 1, 4) and (P, D, Q) = (0, 1, 1) and m=12  
* Prior to putting any data in to ARIMA, the variance issues need to have been solved; Box-Cox is the typical transformation  
	* The auto.arima() can handle all the other components  
    * Alternately, the auto.arima(lambda=) can be used  
* The ARIMA models allow for seasoanlity to change, and the more recent seasons are weighted more heavily  
* Because the differencing term is included, the model naturally has a trend even in the absence of a formal constant being included  
* What happens when you want to create training and test sets for data that is more frequent than yearly?  
	* If needed, you can use a vector in form c(year, period) for the start and/or end keywords in the window() function  
    * You must also ensure that you're using the appropriate values of h in forecasting functions  
    * Recall that h should be equal to the length of the data that makes up your test set  
    * If your data spans 15 years, your training set consists of the first 10 years, and you intend to forecast the last 5 years of data, you would use h = 12 * 5 not h = 5 because your test set would include 60 monthly observations  
    * If instead your training set consists of the first 9.5 years and you want forecast the last 5.5 years, you would use h = 66 to account for the extra 6 months  
  
Example code includes:  
```{r}

# Plot the series
autoplot(a10)

# Try four values of lambda in Box-Cox transformations
a10 %>% BoxCox(lambda = 0.0) %>% autoplot()
a10 %>% BoxCox(lambda = 0.1) %>% autoplot()
a10 %>% BoxCox(lambda = 0.2) %>% autoplot()
a10 %>% BoxCox(lambda = 0.3) %>% autoplot()

# Compare with BoxCox.lambda()
BoxCox.lambda(a10)


# In this exercise, you will use the pre-loaded wmurders data, which contains the annual female murder rate in the US from 1950-2004
data(wmurders, package="fpp2")

# Plot the US female murder rate
autoplot(wmurders)

# Plot the differenced murder rate
autoplot(diff(wmurders))

# Plot the ACF of the differenced murder rate
ggAcf(diff(wmurders))


# In this exercise, you will use differencing and transformations simultaneously to make a time series look stationary. The data set here is h02, which contains 17 years of monthly corticosteroid drug sales in Australia
data(h02, package="fpp2")
str(h02)

# Plot the data
autoplot(h02)

# Take logs and seasonal differences of h02
difflogh02 <- diff(log(h02), lag = 12)

# Plot difflogh02
autoplot(difflogh02)

# Take another difference and plot
ddifflogh02 <- diff(difflogh02)
autoplot(ddifflogh02)

# Plot ACF of ddifflogh02
ggAcf(ddifflogh02)


# Fit an automatic ARIMA model to the austa series
fit <- auto.arima(austa)

# Check that the residuals look like white noise
checkresiduals(fit)
residualsok <- TRUE

# Summarize the model
summary(fit)

# Find the AICc value and the number of differences used
AICc <- round(fit$aicc, 2)
d <- 1

# Plot forecasts of fit
fit %>% forecast(h = 10) %>% autoplot()


# Plot forecasts from an ARIMA(0,1,1) model with no drift
austa %>% Arima(order = c(0, 1, 1), include.constant = FALSE) %>% forecast() %>% autoplot()

# Plot forecasts from an ARIMA(2,1,3) model with drift
austa %>% Arima(order = c(2, 1, 3), include.constant = TRUE) %>% forecast() %>% autoplot()

# Plot forecasts from an ARIMA(0,0,1) model with a constant
austa %>% Arima(order = c(0, 0, 1), include.constant = TRUE) %>% forecast() %>% autoplot()

# Plot forecasts from an ARIMA(0,2,1) model with no constant
austa %>% Arima(order = c(0, 2, 1), include.constant = FALSE) %>% forecast() %>% autoplot()


# Set up forecast functions for ETS and ARIMA models
fets <- function(x, h) {
  forecast(ets(x), h = h)
}
farima <- function(x, h) {
  forecast(auto.arima(x), h=h)
}

# Compute CV errors for ETS as e1
e1 <- tsCV(austa, fets, h=1)

# Compute CV errors for ARIMA as e2
e2 <- tsCV(austa, farima, h=1)

# Find MSE of each model class
mean(e1**2, na.rm=TRUE)
mean(e2**2, na.rm=TRUE)

# Plot 10-year forecasts using the best model class
austa %>% farima(h=10) %>% autoplot()


# Check that the logged h02 data have stable variance
h02 %>% log() %>% autoplot()

# Fit a seasonal ARIMA model to h02 with lambda = 0
fit <- auto.arima(h02, lambda=0)

# Summarize the fitted model
summary(fit)

# Record the amount of lag-1 differencing and seasonal differencing used
d <- 1
D <- 1

# Plot 2-year forecasts
fit %>% forecast(h=24) %>% autoplot()


# Find an ARIMA model for euretail
data(euretail, package="fpp2")
str(euretail)
fit1 <- auto.arima(euretail)

# Don't use a stepwise search
fit2 <- auto.arima(euretail, stepwise=FALSE)

# AICc of better model
AICc <- round(min(fit1$aicc, fit2$aicc), 2)

# Compute 2-year forecasts from better model
fit2 %>% forecast(h=8) %>% autoplot()


# In the final exercise for this chapter, you will compare seasonal ARIMA and ETS models applied to the quarterly cement production data qcement
# Because the series is very long, you can afford to use a training and test set rather than time series cross-validation. This is much faster
# Use 20 years of the qcement data beginning in 1988
train <- window(qcement, start = c(1988, 1), end = c(2007, 4))

# Fit an ARIMA and an ETS model to the training data
fit1 <- auto.arima(train)
fit2 <- ets(train)

# Check that both models have white noise residuals
checkresiduals(fit1)
checkresiduals(fit2)

# Produce forecasts for each model
fc1 <- forecast(fit1, h = length(window(qcement, start=2008)))
fc2 <- forecast(fit2, h = length(window(qcement, start=2008)))

# Use accuracy() to find better model based on RMSE
accuracy(fc1, qcement)
accuracy(fc2, qcement)
# bettermodel <- fc2

```
  
  
  
***
  
Chapter 5 - Advanced Methods  
  
Dynamic Regression - could include factors like advertising or competition in a single model:  
  
* In dynamic regression, the error term is an ARIMA series rather than a normal iid  
* Example of forecasting personal consumption using personal income as one of the predictor variables  
* Fitting a dynamic regression model is fairly straightforward with the auto.arima()  
	* fit <- auto.arima(myTS, xreg=myExplanatory)  # note that xreg should be a matrix  
    * The forecast is then based on the predictors - forecast(fit, xreg=myPredictedExplanatory)  # allows for scenario modeling  
  
Dynamic Harmonic Regression - handling periodic seasonality with Fourier terms:  
  
* Fourier terms are of the form sin or cos of (2 * pi * k * t / m)  
	* m is the seasonal component  
    * k drives the harmonic frequencies  
    * Most types of seasonality can be modeled by the sin() and cos() pairing of the Fourier terms, properly tuned  
    * The error terms can then be assumed to be a non-seasonal ARIMA model  
* One difference with the Fourier model is that it requires constant and un-changing seasonality  
	* Can run these models using auto.arima(myTS, xreg = fourier(myTS, K=), seasonal=FALSE, lambda=)  # lambda to solve the variance issue, K to tune how deep the Fourier goes  
    * Can then do forecast(myModel, xreg=fourier(myTS, K=, h=))  # use the same K as for modeling, and use h as desired for time horizon periods (h= tells Fourier to look forward rather than backward)  
* Can add even more terms to the xreg; the main analyst duty is to test various K (never more than half the seasonal period) and pick the best (lowest) AICc  
	* The higher the order (K), the more "wiggly" the seasonal pattern is allowed to be  
    * With K=1, it is a simple sine curve  
    * You can select the value of KK by minimizing the AICc value  
* With weekly data, it is difficult to handle seasonality using ETS or ARIMA models as the seasonal length is too large (approximately 52)  
	* Instead, you can use harmonic regression which uses sines and cosines to model the seasonality  
* Harmonic regressions are also useful when time series have multiple seasonal patterns  
	* For example, taylor contains half-hourly electricity demand in England and Wales over a few months in the year 2000  
    * The seasonal periods are 48 (daily seasonality) and 7 x 48 = 336 (weekly seasonality)  
* auto.arima() would take a long time to fit a long time series such as this one, so instead you will fit a standard regression model with Fourier terms using the tslm() function  
	* This is very similar to lm() but is designed to handle time series  
    * With multiple seasonality, you need to specify the order KK for each of the seasonal periods  
    * # The formula argument is a symbolic description of the model to be fitted  
    * > args(tslm)  
    * function (formula, ...)  
  
TBATS models - combines many models in to a single model:  
  
* Integrated model includes many core features  
	* Trigonometric terms for seasonality  
    * Box-Cox for heterogeneity  
    * ARMA for short-term dynamics  
    * Trend (possibly damped)  
    * Seasonal (including multiple and non-integer periods)  
* These models can be powerful but dangerous - sometimes, the automated choices are not so good  
	* Model will output TBATS(BoxCoxTerm, {p, q}, dampParam, {<mFourier, kFourier>})  
* The models are especially useful when the seasonal components are comples (many of them, interspersed with each other, etc.)  
	* Downside is model run time, and the potential for getting the wrong coefficients  
  
Wrap up:  
  
* Continual practice with methods to gain experience  
	* Book "Forecasting Principles and Practice" by Hyndman is available  
* Try various techniques with various types of time series  
  
Example code includes:  
```{r cache=TRUE}

# In this exercise, you will model sales data regressed against advertising expenditure, with an ARMA error to account for any serial correlation in the regression errors
# The data are available in your workspace as advert and comprise 24 months of sales and advertising expenditure for an automotive parts company
data(advert, package="fma")
str(advert)

# Time plot of both variables
autoplot(advert, facets=TRUE)

# Fit ARIMA model
fit <- auto.arima(advert[, "sales"], xreg = advert[, "advert"], stationary = TRUE)

# Check model. Increase in sales for each unit increase in advertising
salesincrease <- coefficients(fit)[3]

# Forecast fit as fc
fc <- forecast(fit, xreg = rep(10, 6))

# Plot fc with x and y labels
autoplot(fc) + xlab("Month") + ylab("Sales")


data(elecdaily, package="fpp2")
str(elecdaily)

elec <- elecdaily
colnames(elec)[2] <- "Workday"

# Time plots of demand and temperatures
autoplot(elec[, c("Demand", "Temperature")], facets = TRUE)

# Matrix of regressors
xreg <- cbind(MaxTemp = elec[, "Temperature"], 
              MaxTempSq = elec[, "Temperature"] ** 2, 
              Workday = elec[, "Workday"])

# Fit model
fit <- auto.arima(elec[, "Demand"], xreg = xreg)

# Forecast fit one day ahead
forecast(fit, xreg = cbind(20, 20**2, 1))


# The pre-loaded gasoline data comprises weekly data on US finished motor gasoline products
# In this exercise, you will fit a harmonic regression to this data set and forecast the next 3 years
data(gasoline, package="fpp2")
str(gasoline)

# Set up harmonic regressors of order 13
harmonics <- fourier(gasoline, K = 13)

# Fit regression model with ARIMA errors
fit <- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE)

# Forecasts next 3 years
newharmonics <- fourier(gasoline, K = 13, h = 156)
fc <- forecast(fit, xreg = newharmonics)

# Plot forecasts fc
autoplot(fc)


# Fit a harmonic regression using order 10 for each type of seasonality
fit <- tslm(taylor ~ fourier(taylor, K = c(10, 10)))

# Forecast 20 working days ahead
fc <- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10, 10), h = 20 * 48)))

# Plot the forecasts
autoplot(fc)

# Check the residuals of fit
checkresiduals(fit)


# Another time series with multiple seasonal periods is calls, which contains 20 consecutive days of 5-minute call volume data for a large North American bank
# There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845
# The weekly seasonality is relatively weak, so here you will just model daily seasonality. calls is pre-loaded into your workspace
# The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant
# This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series
# The effect of the remaining correlations on the forecasts will be negligible
data(calls, package="fpp2")
calls <- window(calls, start=29.8)
str(calls)


# Plot the calls data
autoplot(calls)

# Set up the xreg matrix
xreg <- fourier(calls, K = c(10, 0))

# Fit a dynamic regression model
fit <- auto.arima(calls, xreg = xreg, seasonal=FALSE, stationary=TRUE)

# Check the residuals
checkresiduals(fit)

# Plot forecasts for 10 working days ahead
fc <- forecast(fit, xreg =  fourier(calls, c(10, 0), h = 10 * 169))
autoplot(fc)


# The gas data contains Australian monthly gas production
# A plot of the data shows the variance has changed a lot over time, so it needs a transformation
# The seasonality has also changed shape over time, and there is a strong trend
# This makes it an ideal series to test the tbats() function which is designed to handle these features
# Plot the gas data
autoplot(gas)

# Fit a TBATS model to the gas data
fit <- tbats(gas)

# Forecast the series for the next 5 years
fc <- forecast(fit, h=60)

# Plot the forecasts
autoplot(fc)

# Record the Box-Cox parameter and the order of the Fourier terms
lambda <- round(as.vector(fc$model$lambda), 3) # 0.082
K <- fc$model$k.vector #5

```
  
  
  
***
  
###_Network Analysis in R_  
  
Chapter 1 - Introduction to Networks  
  
What are social networks?  
  
* Patterns of relationships can be represented as a graph (which can be a database representation or a visualization  
* Vertex (node) refers to individuals, while edges refer to connections between individuals  
* The graph data can be stored in two ways  
	* The adjacency matrix can be thought of as a matrix of all vertices (as rows and columns), with a 1 meaning edge-between, and a 0 meaning no edge  
    * The edge matrix is a two-column matrix containing each pair of vertices that have an edge between them  
* The igraph package in R can be used to manage graphs (social networks)  
	* g <- graph.edgelist(as.matrix(df), directed=FALSE)  # creates graph object g with appropriate edges assuming df is a data-frame containing the edge data  
    * print(g) will return a lot of information - the start will be intA intB where intA=#vertices and intB=#edges  
* There are many functions available for extracting information from the graph  
	* V(g)  # return all the vertices  
    * E(g)  # return all the edges  
    * gorder(g)  # number of vertices  
    * gsize(g) # number of edges  
    * plot(g) # visualization of the graph  
  
Network attributes - may want to add information about vertices and edges:  
  
* Most common edge attribute is "weight" - plots as a thicker edge  
	* For example, frequency of flights or volume of messages or the like  
* Can add data to existing vertices and edges  
	* Can use set_vertex_attr(myGraph, "myName", myValues) and set_edge_attr(myGraph, "myName", myValues) to add to an existing graph  
    * Can see the existing attributes with vertex_attr() or edge_attr()  
* Can create the metadata for the vertices and edges from the raw data  
	* graph_from_data_frame(d=myEdgesDF, vertices=myVerticesDF, directed=FALSE)  
* Can subset the edges or vertices using the [[]] operators  
	* E(g)[[inc("E)]]  # will pull all edges that include E  
    * E(g)[[frequency >= 3]]  # will pull all the edges where frequency attribute is 3+  
* Can create vertex attributes using formula logic, for example, adding a "color" attribute to the vertices  
	* V(g)$color <- ifelse( V(g)$age > 22, "red", "white" )  
    * plot(g, vertex.label.color = "black")  
  
Network visualization principles - many options for creating and customizing the display:  
  
* The best visualization provides immediate insight and information to the viewer  
	* Size, labels, colors, and shape are frequently adjusted as needed to best convey information  
    * Size is helpful for showing more central (important) vertices, as can labels (though risky, can be to much)  
    * Color and shape can be helpful for showing categorical data about vertices  
    * Edge line weights, colors, and shapes can help tease out the relative importance of various types of edges  
* The igraph package includes all of the major ways to display  - some basic rules include  
	* Minimize edge crossing  
    * Do not allow vertices to overlap  
    * Make edge lengths as uniform as possible  
    * Increase symmetry where possible  
    * Keep key vertices (nodes) closer to the center  
* Can add commands to plot(myGraph, layout=)  # one example is layout.fruchterman.reingold(g)  
* You can also stipulate the layout by providing a matrix of (x, y) coordinates for each vertex  
	* Here you use the layout_as_tree() function to generate the matrix m of coordinates  
    * Then pass m to the layout function in plot() to plot  
* Choosing a correct layout can be bewildering  
	* Fortunately igraph has a function layout_nicely() that tries to choose the most appropriate layout function for a given graph object  
* This is done by using delete_edges() which takes two arguments  
	* The first is the graph object and the second is the subset of edges to be removed  
  
Example code includes:  
```{r}

# Load igraph
library(igraph)

friends <- readr::read_csv("./RInputFiles/friends.csv")
str(friends)

# Inspect the first few rows of the dataframe 'friends'
head(friends)

# Convert friends dataframe to a matrix
friends.mat <- as.matrix(friends)

# Convert friends matrix to an igraph object
g <- graph.edgelist(friends.mat, directed = FALSE)

# Make a very basic plot of the network
plot(g)


# Subset vertices and edges
V(g)
E(g)

# Count number of edges
gsize(g)

# Count number of vertices
gorder(g)


# Inspect the objects 'genders' and 'ages'
genders <- c('M', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'M')
ages <- c(18, 19, 21, 20, 22, 18, 23, 21, 22, 20, 20, 22, 21, 18, 19, 20)

# Create new vertex attribute called 'gender'
g <- set_vertex_attr(g, "gender", value = genders)

# Create new vertex attribute called 'age'
g <- set_vertex_attr(g, "age", value = ages)

# View all vertex attributes in a list
vertex_attr(g)

# View attributes of first five vertices in a dataframe
V(g)[[1:5]] 


# View hours
hours <- c(1, 2, 2, 1, 2, 5, 5, 1, 1, 3, 2, 1, 1, 5, 1, 2, 4, 1, 3, 1, 1, 1, 4, 1, 3, 3, 4)

# Create new edge attribute called 'hours'
g <- set_edge_attr(g, "hours", value = hours)

# View edge attributes of graph object
edge_attr(g)

# Find all edges that include "Britt"
E(g)[[inc('Britt')]]  

# Find all pairs that spend 4 or more hours together per week
E(g)[[hours>=4]]  


friends1_nodes <- readr::read_csv("./RInputFiles/friends1_nodes.csv")
str(friends1_nodes)

friends1_edges <- readr::read_csv("./RInputFiles/friends1_edges.csv")
str(friends1_edges)


# Create an igraph object with attributes directly from dataframes
g1 <- graph_from_data_frame(d = friends1_edges, vertices = friends1_nodes, directed = FALSE)

# Subset edges greater than or equal to 5 hours
E(g1)[[hours >= 5]]  

# Plot network and color vertices by gender
V(g1)$color <- ifelse(V(g1)$gender == "F", "orange", "dodgerblue")
plot(g1, vertex.label.color = "black")


# Plot the graph object g1 in a circle layout
plot(g1, vertex.label.color = "black", layout = layout_in_circle(g1))

# Plot the graph object g1 in a Fruchterman-Reingold layout 
plot(g1, vertex.label.color = "black", layout = layout_with_fr(g1))

# Plot the graph object g1 in a Tree layout 
m <- layout_as_tree(g1)
plot(g1, vertex.label.color = "black", layout = m)

# Plot the graph object g1 using igraph's chosen layout 
m1 <- layout_nicely(g1)
plot(g1, vertex.label.color = "black", layout = m1)


# Create a vector of weights based on the number of hours each pair spend together
w1 <- E(g1)$hours

# Plot the network varying edges by weights
m1 <- layout_nicely(g1)
plot(g1, 
        vertex.label.color = "black", 
        edge.color = 'black',
        edge.width = w1,
        layout = m1)

# Create a new igraph object only including edges from the original graph that are greater than 2 hours long 
g2 <- delete_edges(g1, E(g1)[hours < 2])

# Plot the new graph 
w2 <- E(g2)$hours
m2 <- layout_nicely(g2)

plot(g2, 
     vertex.label.color = "black", 
     edge.color = 'black',
     edge.width = w2,
     layout = m2)

```
  
  
  
***
  
Chapter 2 - Identifying Important Vertices in a Network  
  
Directed Networks - arrows represent the from-to relationship, such as e-mail exchanges:  
  
* The first character after IGRAPH will be either "U" (undirected) or "D" (directed)  
	* Can also run is.directed(myGraph) and is.weighted(myGraph) to get the Boolean outcome  
* The simplest measure of vertex influence is degree  
	* In undirected networks, vertices have degree equal to the total number of edges for that vertex  
    * In directed networks, vertices have in-degree equal to the total number of incoming edges and out-degree equal to the total number of outgoing edges  
* Can run several tests in R regarding social networks  
	* myGraph["vert1", "vert2"] will check whether an edge exists between these vertices  
    * If a 1 is returned that indicates TRUE there is an edge. If a 0 is returned that indicates FALSE there is not an edge  
    * incident(myGraph, "myVert", mode=c("all"))  will show all of the edges to or from "myVert"  
    * head_of(g, E(g)) # find the origin of all edges in the network  
  
Relationship between Vertices - overall patterns between networks (neighbors and paths):  
  
* Can use neighbors as one measure of a vertices importance  
	* neighbors(myGraph, "myVertex", mode=c("all"))  # will list all of the neighbors for vertex "myVertex"  
    * If you have x as all the neighbors for x and y as all the neighbors for y, then intersection(x, y) will give any common neighbors of (one-stop paths between) x and y  
* Can consider the path-length as the number of stops needed to connect two points - 1 for neighbors, 2 for one-stop, 3 for two-stop, etc.  
	* The farthest_vertices(myGraph) call will find the longest-distance connection available in the graph  
    * The get_diameter(myGraph) will spell out the longest path  
    * Each of the above functions only returns one output, even if there are many tied for the longest path  
    * The ego(myGraph, maxDistance, "myVertex", mode=c("out")) will return all vertices that can be reached in at most maxDistance using outbound edges only  
* The inter-connectivity of a network can be assessed by examining the number and length of paths between vertices  
	* A path is simply the chain of connections between vertices  
    * The number of intervening edges between two vertices represents the geodesic distance between vertices  
    * Vertices that are connected to each other have a geodesic distance of 1  
    * Those that share a neighbor in common but are not connected to each other have a geodesic distance of 2 and so on  
    * In directed networks, the direction of edges can be taken into account  
    * If two vertices cannot be reached via following directed edges they are given a geodesic distance of infinity  
  
Significant nodes in a network:  
  
* There are many potential measures of the importance of a vertex, including  
	* degree  
    * betweenness  
    * eigenvector centrality  
    * closeness centrality  
    * pagerank centrality  
* As per previous, can calculate out-degree (edges out) and in-degree (edges in) for each vertex  
	* degree(myGraph, mode=c(""))  # where mode can be "all", "in", or "out", and the function returns a named vector  
* Can also calculate betweenness, the question of how often a given vertex lies on the shortest path through the network  
	* Higher betweenness means connecting many parts of the network  
    * Can calculate betweenness(myGraph, directed=, normalized= ) where directed is a boolean for whether it is a directed graph and normalized is a boolean for whether to normalize so they sum to 1  
  
Example code includes:  
```{r}

measles <- readr::read_csv("./RInputFiles/measles.csv")
str(measles)


# Get the graph object
g <- graph_from_data_frame(measles, directed = TRUE)

# is the graph directed?
is.directed(g)

# Is the graph weighted?
is.weighted(g)

# Where does each edge originate from?
table(head_of(g, E(g)))


# Make a basic plot
plot(g, 
     vertex.label.color = "black", 
     edge.color = 'gray77',
     vertex.size = 0,
     edge.arrow.size = 0.1,
     layout = layout_nicely(g))

# Is there an edge going from vertex 184 to vertex 178?
g['184', '178']

# Is there an edge going from vertex 178 to vertex 184?
g['178', '184']

# Show all edges going to or from vertex 184
incident(g, '184', mode = c("all"))

# Show all edges going out from vertex 184
incident(g, '184', mode = c("out"))


# Identify all neighbors of vertex 12 regardless of direction
neighbors(g, '12', mode = c('all'))

# Identify other vertices that direct edges towards vertex 12
neighbors(g, '12', mode = c('in'))

# Identify any vertices that receive an edge from vertex 42 and direct an edge to vertex 124
n1 <- neighbors(g, '42', mode = c('out'))
n2 <- neighbors(g, '124', mode = c('in'))
intersection(n1, n2)


# Which two vertices are the furthest apart in the graph ?
farthest_vertices(g) 

# Shows the path sequence between two furthest apart vertices.
get_diameter(g)  

# Identify vertices that are reachable within two connections from vertex 42
ego(g, 2, '42', mode = c('out'))

# Identify vertices that can reach vertex 42 within two connections
ego(g, 2, '42', mode = c('in'))


# Calculate the out-degree of each vertex
g.outd <- degree(g, mode = c("out"))

# View a summary of out-degree
table(g.outd)

# Make a histogram of out-degrees
hist(g.outd, breaks = 30)

# Find the vertex that has the maximum out-degree
which.max(g.outd)


# Calculate betweenness of each vertex
g.b <- betweenness(g, directed = TRUE)

# Show histogram of vertex betweenness
hist(g.b, breaks = 80)

# Create plot with vertex size determined by betweenness score
plot(g, 
     vertex.label = NA,
     edge.color = 'black',
     vertex.size = sqrt(g.b)+1,
     edge.arrow.size = 0.05,
     layout = layout_nicely(g))


# Make an ego graph
g184 <- make_ego_graph(g, igraph::diameter(g), nodes = '184', mode = c("all"))[[1]]

# Get a vector of geodesic distances of all vertices from vertex 184 
dists <- distances(g184, "184")

# Create a color palette of length equal to the maximal geodesic distance plus one.
colors <- c("black", "red", "orange", "blue", "dodgerblue", "cyan")

# Set color attribute to vertices of network g184.
V(g184)$color <- colors[dists+1]

# Visualize the network based on geodesic distance from vertex 184 (patient zero).
plot(g184, 
     vertex.label = dists, 
     vertex.label.color = "white",
     vertex.label.cex = .6,
     edge.color = 'black',
     vertex.size = 7,
     edge.arrow.size = .05,
     main = "Geodesic Distances from Patient Zero"
     )

```
  
  
  
***
  
Chapter 3 - Characterizing Network Structures  
  
Introduction - Forrest Gump network dataset analysis:  
  
* Each edge indicates that the characters were in a scene together (undirected network)  
* Eigenvector centrality is the concept of 1) being connected to a lot of other vertices, and 2) being connected to vertices that are themselves connected to many other vertices  
	* eigen_centrality(myGraph)$vector  # the $vector pulls the "vector" items from the list that is returned by eigen_centrality()  
* The simplest measure of the overall structure of a network is its density - proportion of edges that exist relative to the maximum number of edges that could exist  
	* edge_density(myGraph)  
* Can also calculate the average path length for a network  
	* mean_distance(myGraph, directed=)  
  
Understanding network structures:  
  
* Random graph technique is frequently used to better understand the structures of a network  
	* One of the simplest forms of the random graph is to have the same number of vertices and a similar density  
    * erdos.renyi.game(n=gorder(myGraph), p.or.m=edge_density(myGraph), type="gnp") will generate this  
    * This technique is especially helpful if you want to gauge whether a property of your original graph is unusual; how does it compare to 1,000 random graphs?  
  
Network Substructures:  
  
* Triangles (triads) in a network are of particular interest - looking at any group of three edges, they can be  
	* Closed - all linked to each other - can be found using triangles(myGraph)  
    * count_triangles(myGraph, vids="myVertex")  # counts all triangles (closed) that include myVertex  
    * transitivity(myGraph, vids="myVertex", type="local")  # closed triangles that include myVertex as a proportion of all triangles that could potentially include myVertex  
    * The denominator assumed in transitivity is n * (n-1) / 2 where n is the number of neighbors that myVertex has in myGraph  
* Cliques are another important network sub-structure - in a clique, every vertex is connected to every other vertex  
	* All triangles made by a clique are closed  
    * The largest_cliques(myGraph) will return information about the largest clique  
    * max_cliques(myGraph) returns a list of cliques of each size  
  
Example code includes:  
```{r}

# In this chapter you will use a social network based on the movie Forrest Gump
# Each edge of the network indicates that those two characters were in at least one scene of the movie together
# Therefore this network is undirected
# To familiarize yourself with the network, you will first create the network object from the raw dataset
# Then, you will identify key vertices using a measure called eigenvector centrality
# Individuals with high eigenvector centrality are those that are highly connected to other highly connected individuals
# You will then make an exploratory visualization of the network

gump <- readr::read_csv("./RInputFiles/gump.csv")
str(gump)


# Inspect Forrest Gump Movie dataset
head(gump)

# Make an undirected network
g <- graph_from_data_frame(gump, directed = FALSE)

# Identify key nodes using eigenvector centrality
g.ec <- eigen_centrality(g)
which.max(g.ec$vector)

# Plot Forrest Gump Network
plot(g, vertex.label.color = "black", vertex.label.cex = 0.6, vertex.size = 25*(g.ec$vector), 
     edge.color = 'gray88', main = "Forrest Gump Network"
     )


# Get density of a graph
gd <- edge_density(g)

# Get the diameter of the graph g
igraph::diameter(g, directed = FALSE)

# Get the average path length of the graph g
g.apl <- mean_distance(g, directed = FALSE)
g.apl


# Create one random graph with the same number of nodes and edges as g
g.random <- erdos.renyi.game(n = gorder(g), p.or.m = edge_density(g), type = "gnp")

g.random

plot(g.random)

# Get density of new random graph `g.random`
edge_density(g.random)

#Get the average path length of the random graph g.random
mean_distance(g.random, directed = FALSE)


g.apl=mean_distance(g)

# Generate 1000 random graphs
gl <- vector('list', 1000)
  
for(i in 1:1000){
    gl[[i]] <- erdos.renyi.game(n = gorder(g), p.or.m = gd, type = "gnp")
}

# Calculate average path length of 1000 random graphs
gl.apl <- lapply(gl, FUN=mean_distance, directed = FALSE)
gl.apls <- unlist(gl.apl)

# Plot the distribution of average path lengths
hist(gl.apls, xlim = range(c(1.5, 6)))
abline(v = g.apl, col = "red", lty = 3, lwd=2)

# Calculate the proportion of graphs with an average path length lower than our observed
sum(gl.apls < g.apl)/1000


# Show all triangles in the network.
matrix(triangles(g), nrow = 3)

# Count the number of triangles that vertex "BUBBA" is in.
count_triangles(g, vids='BUBBA')

# Calculate  the global transitivity of the network.
g.tr <- transitivity(g)
g.tr

# Calculate the local transitivity for vertex BUBBA.
transitivity(g, vids='BUBBA', type = "local")


# One thousand random networks are stored in the list object gl
g.tr <- 0.1918082  # Calculate the proportion of random graphs that have a transitivity higher than the transitivity of Forrest Gump's network, which you previously calculated and assigned to g.tr

# Calculate average transitivity of 1000 random graphs
gl.tr <- lapply(gl, FUN=transitivity)
gl.trs <- unlist(gl.tr)

# Get summary statistics of transitivity scores
summary(gl.trs)

# Calculate the proportion of graphs with a transitivity score higher than Forrest Gump's network.
sum(gl.trs > g.tr)/1000


# Identify the largest cliques in the network
largest_cliques(g)

# Determine all maximal cliques in the network and assign to object 'clq'
clq <- max_cliques(g)

# Calculate the size of each maximal clique.
table(unlist(lapply(clq, length)))



# Often in network visualization you will need to subset part of a network to inspect the inter-connections of particular vertices
# Here, you will create a visualization of the largest cliques in the Forrest Gump network
# In the last exercise you determined that there were two cliques of size 9
# You will plot these side-by-side after creating two new igraph objects by subsetting out these cliques from the main network
# The function subgraph() enables you to choose which vertices to keep in a new network object
# Assign largest cliques output to object 'lc'
lc <- largest_cliques(g)

# Create two new undirected subgraphs, each containing only the vertices of each largest clique.
gs1 <- as.undirected(induced_subgraph(g, lc[[1]]))
gs2 <- as.undirected(induced_subgraph(g, lc[[2]]))

# Plot the two largest cliques side-by-side

par(mfrow=c(1, 2)) # To plot two plots side-by-side

plot(gs1,
     vertex.label.color = "black", 
     vertex.label.cex = 0.9,
     vertex.size = 0,
     edge.color = 'gray28',
     main = "Largest Clique 1",
     layout = layout.circle(gs1)
)

plot(gs2,
     vertex.label.color = "black", 
     vertex.label.cex = 0.9,
     vertex.size = 0,
     edge.color = 'gray28',
     main = "Largest Clique 2",
     layout = layout.circle(gs2)
)

par(mfrow=c(1, 1)) # To return to the defaults

```
  
  
  
***
  
Chapter 4 - Identifying Special Relationships  
  
Close relationships: assortativity and reciprocity:  
  
* Question is often whether vertices randomly associate, or whether they preference similar vertices - "do birds of a feather flock together"  
* Assortativity is the preferential attachment of vertices to other vertices that are similar in numeric or other attributes  
	* assortativity(myGraph, myVertexAttaributeVector) will pull the overall assortativity - need to convert to factor so it is a numeric  
    * The value return will be +1 (only attach to similar) through 0 (no pattern) through -1 (only attach to dissimilar)  
* Can also look at whether vertices with a high degree preferentially attach to other vertices with a high degree  
	* assortativity.degree(myGraph, directed=)  # will return a value from -1 to +1  
* Reciprocity is a measure of a directed network, specifically the number of outbound edges that share an inbound edge  
	* reciprocity(myGraph)  
    * Each direction counts as an edge, so if there are 12 two-way edges (6 two-way connections) and 8 one-way edges then reciprocity is 0.6  
  
Community detection - building on cliques, reciprocity, assortativity:  
  
* General philosophy is similar to k-means - build "communities" where the connections within the community are much stronger than the connections outside the community  
	* Modules, groups, clusters, and communities all refer to the same concept when looking at networks and graphs  
    * The modularity score is used, and is a measure of similarity within to similarity without  
    * Can run fastgreedy.community(myGraph)  # will get a baseline of communities building from ground-level  
    * Can run edge.betweenness.community(myGraph)  # will get a baseline of communities splitting from full network  
* Assuming a community object x exists, plot(x, myGraph) will plot the community as part of the graph  
  
Interactive network visualizations:  
  
* There are many R packages for visualizing networks, each with strengths and weaknesses  
* The threejs library is especially good for visualizing large networks - integrates well with igraph  
	* threejs::graphjs(myGraph)  # will graph the myGraph object  
    * If the vertex has a "color" or "value" attribute, that will be picked up automatically  
    * The network becomes fully interactive  
  
Example code includes:  
```{r}

# Plot the network
plot(g1)

# Convert the gender attribute into a numeric value
values <- as.numeric(factor(V(g1)$gender))

# Calculate the assortativity of the network based on gender
assortativity(g1, values)

# Calculate the assortativity degree of the network
assortativity.degree(g1, directed = FALSE)


# Calculate the observed assortativity
observed.assortativity <- assortativity(g1, values)

# Calculate the assortativity of the network randomizing the gender attribute 1000 times
results <- vector('list', 1000)
for(i in 1:1000){
  results[[i]] <- assortativity(g1, sample(values))
}

# Plot the distribution of assortativity values and add a red vertical line at the original observed value
hist(unlist(results))
abline(v = observed.assortativity, col = "red", lty = 3, lwd=2)


fromData <- c(1, 6, 8, 9, 11, 12, 3, 5, 7, 12, 2, 4, 5, 10, 13, 14, 3, 6, 9, 10, 13, 3, 8, 15, 1, 3, 9, 8, 9, 13, 5, 12, 13, 14, 1, 2, 3, 4, 7, 11, 12, 14, 3, 6, 14, 15, 4, 6, 9, 12, 3, 7, 8, 12, 14, 3, 4, 10, 11)
toData <- c(15, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 15, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 15, 13, 13, 13, 13, 13, 14, 14, 14, 14)
g <- graph_from_data_frame(data.frame(from=fromData, to=toData))

# Make a plot of the chimp grooming network
plot(g,
     edge.color = "black",
     edge.arrow.size = 0.3,
     edge.arrow.width = 0.5)

# Calculate the reciprocity of the graph
reciprocity(g)


# The first community detection method you will try is fast-greedy community detection
# You will use the Zachary Karate Club network
# This social network contains 34 club members and 78 edges
# Each edge indicates that those two club members interacted outside the karate club as well as at the club
# Perform fast-greedy community detection on network graph
fromData <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 9, 9, 9, 10, 14, 15, 15, 16, 16, 19, 19, 20, 21, 21, 23, 23, 24, 24, 24, 24, 24, 25, 25, 25, 26, 27, 27, 28, 29, 29, 30, 30, 31, 31, 32, 32, 33)
toData <- c(2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 18, 20, 22, 32, 3, 4, 8, 14, 18, 20, 22, 31, 4, 8, 28, 29, 33, 10, 9, 14, 8, 13, 14, 7, 11, 7, 11, 17, 17, 31, 33, 34, 34, 34, 33, 34, 33, 34, 33, 34, 34, 33, 34, 33, 34, 26, 28, 33, 34, 30, 26, 28, 32, 32, 30, 34, 34, 32, 34, 33, 34, 33, 34, 33, 34, 34)
g <- graph_from_data_frame(data.frame(from=fromData, to=toData), directed=FALSE)

kc = fastgreedy.community(g)

# Determine sizes of each community
sizes(kc)

# Determine which individuals belong to which community
membership(kc)

# Plot the community structure of the network
plot(kc, g)


# Perform edge-betweenness community detection on network graph
gc = edge.betweenness.community(g)

# Determine sizes of each community
sizes(gc)

# Plot community networks determined by fast-greedy and edge-betweenness methods side-by-side
par(mfrow = c(1, 2)) 
plot(kc, g)
plot(gc, g)
par(mfrow = c(1, 1)) 

# In this course you have exclusively used igraph to make basic static network plots
# There are many packages available to make network plots
# One very useful one is threejs which allows you to make interactive network visualizations
# This package also integrates seamlessly with igraph
# In this exercise you will make a basic interactive network plot of the karate club network using the threejs package
# Once you have produced the visualization be sure to move the network around with your mouse
# You should be able to scroll in and out of the network as well as rotate the network

library(igraph)
library(threejs)

# Set a vertex attribute called 'color' to 'dodgerblue' 
g <- set_vertex_attr(g, "color", value = "dodgerblue")

# Redraw the graph and make the vertex size 1 (ActiveX does not work with browser)
# graphjs(g, vertex.size = 1)


# Create numerical vector of vertex eigenvector centralities 
ec <- as.numeric(eigen_centrality(g)$vector)

# Create new vector 'v' that is equal to the square-root of 'ec' multiplied by 5
v <- 5*sqrt(ec)

# Plot threejs plot of graph setting vertex size to v (ActiveX does not work with browser)
# graphjs(g, vertex.size = v)


# Create an object 'i' containin the memberships of the fast-greedy community detection
i <-  membership(kc)

# Check the number of different communities
sizes(kc)

# Add a color attribute to each vertex, setting the vertex color based on community membership
g <- set_vertex_attr(g, "color", value = c("yellow", "blue", "red")[i])

# Plot the graph using threejs (ActiveX does not work with browser)
# graphjs(g)

```
  
  