---
title: "Data Camp Insights"
author: "davegoblue"
date: "June 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)

```

## Background and Overview  
DataCamp offer interactive courses related to R Programming.  While some is review, it is helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  

This document is currently split between _v003 and _v003_a and _v003_b and _v003_c due to the need to keep the number of DLL that it opens below the hard-coded maximum.  This introductory section needs to be re-written, and the contents consolidated, at a future date.
  
* R Programming (Introductory R, Intermediate R, Writing Functions in R, Object Oriented Programming in R, Introduction to Tidyverse)  
* Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
* Data Manipulation (dplyr, data.table, xts/zoo, dplyr joins, cases for EDA/Time Series/Pitch Data)  
* Data Visualization (base, ggplot2 parts I/II/III, ggvis, geospatial)  
* Statistics (8 refresher modules)  
* Machine Learning (3 modules + 1 text mining case)  
* R Studio (2 parts) and R Markdown (1 module)  
  
The original DataCamp_Insights_v001 and DataCamp_Insights_v002 documents have been split for this document:  
  
* This DataCamp_Insights_v003 document contains evolving sections on R Programming, Machine Learning, and RStudio / R Markdown  
* Importing and Cleaning Data components have been moved to DataCamp_ImportClean_v002  
* Data Manipulation components have been moved to DataCamp_DataManipulation_v002  
* Visualization components have been moved to DataCamp_Visualization_v002  
* Statistics components have been moved to DataCamp_Statistics_v002  
  
  
***
  

###_Hierarchical and Mixed Effects Models_  
  
Chapter 1 - Overview and Introduction  
  
What is a hierarchical model?  
  
* Hierarchical data is nested within itself, and can be analyzed using the lme package  
	* Example of students in a classroom - may not all be independent of each other due to teacher quality, building conditions, etc.  
    * Hierarchical models can help with pooling means across small sample sizes  
    * Repeated measurements (test scores each year) are also a common example of data that are not truly independent  
* Hierarchical models can include nested models and multi-level models  
* Regression frameworks can include pool information and random effects (vs. fixed effects) and mixed-effects and linear mixed-effects  
* Repeated sampling can have repeated measures modeling  
  
Parts of a regression:  
  
* Linear regression and linear model can be used interchangeably for this course - epsilon is the error term, assumed to be normal with zero mean and constant variance  
* The linear model in R is closely related to analysis of variance (ANOVA)  
	* lm (y ~ x, myData)  
    * anova( lm (y ~ x, myData) )  
* The most basic regression has an intercept, a slope, a single predictor, and an error term  
	* The concept can be extended to multiple regression with additional predictors  
* There are some limitations to the multiple regression approach  
	* Parameter estimates can be very sensitive to other variables - Simpson's paradox and the like  
    * Need to note that the regression coefficient is "after controlling for . . . " (all the other variables)  
    * Interaction terms can be important as well  
* Regressions in R for an intercept for every group are called as lm(y ~ x - 1)  
* The interaction term x1*x2 is the same as x1 + x2 + x1:x2  
  
Random effects in regression:  
  
* Nested relationships tend to be hierarchical in nature - students are part of classes are part of schools and the like  
	* Mathematically, this is referred to as a mapping among the distributions  
* The algebraic representation is that y ~ B*x + eps, with B ~ N(mu, sigma**2)  
	* library(lme4) is the best packages for this in R  
    * lme4::lmer(y ~ x + (1|randomGroup), data=myData)  
    * lme4::lmer(y ~ x + (randomSlope|randomGroup), data=myData)  
  
School data:  
  
* Appliciation of multi-level models to school data - influence of sex, teacher training, plotting parameter estmates  
  
Example code includes:  
```{r}

rawStudent <- read.csv("./RInputFiles/classroom.csv")

studentData <- rawStudent %>%
    mutate(sex=factor(sex, labels=c("male", "female")), minority=factor(minority, labels=c("no", "yes")))


# Plot the data
ggplot(data = studentData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Fit a linear model
summary( lm(mathgain ~ housepov , data = studentData))


# I have aggregated the data for you into two new datasets at the classroom- and school-levels (As a side note, if you want to learn how to aggregate data, the dplyr or data.table courses teach these skills)
# We will also compare the model outputs across all three outputs
# Note: how we aggregate the data is important
# I aggregated the data by taking the mean across the student data (in pseudo-code: mean(mathgain) by school or mean(mathgain) by classroom), 
# but another reasonable method for aggregating the data would be to aggregate by classroom first and school second

classData <- studentData %>%
    group_by(schoolid, classid) %>%
    summarize_at(vars(mathgain, mathprep, housepov, yearstea), mean, na.rm=TRUE)
str(classData)

schoolData <- studentData %>%
    group_by(schoolid) %>%
    summarize_at(vars(mathgain, mathprep, housepov, yearstea), mean, na.rm=TRUE)
str(schoolData)


# First, plot the hosepov and mathgain at the classroom-level from the classData data.frame
ggplot(data = classData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Second, plot the hosepov and mathgain at the school-level from the schoolData data.frame
ggplot(data = schoolData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Third, compare your liner regression results from the previous expercise to the two new models
summary( lm(mathgain ~ housepov, data = studentData)) ## student-level data
summary( lm(mathgain ~ housepov, data = classData)) ## class-level data
summary( lm(mathgain ~ housepov, data = schoolData)) ## school-level data


# Plot the means of your data, predictor is your x-variable, response is your y-variable, and intDemo is your data.frame
intDemo <- data.frame(predictor=factor(c(rep("a", 5), rep("b", 5), rep("c", 5))), 
                      response=c(-1.207, 0.277, 1.084, -2.346, 0.429, 5.759, 4.138, 4.18, 4.153, 3.665, 9.046, 8.003, 8.447, 10.129, 11.919)
                      )
str(intDemo)


ggIntDemo <- ggplot(intDemo, aes(x = predictor, y = response) ) +
    geom_point() +
    theme_minimal() + stat_summary(fun.y = "mean", color = "red",
                                   size = 3, geom = "point") +
    xlab("Intercept groups")
print(ggIntDemo)

# Fit a linear model to your data where response is "predicted by"(~) predictor
intModel <- lm( response ~ predictor - 1 , data = intDemo)
summary(intModel)


extractAndPlotResults <- function(intModel){
    intCoefPlot <- broom::tidy(intModel)
    intCoefPlot$term <- factor(gsub("predictor", "", intCoefPlot$term))

    plotOut <- ggIntDemo + geom_point(data = intCoefPlot,
                           aes(x = term, y = estimate),
                           position = position_dodge(width = 0.4),
                           color = 'blue', size = 8, alpha = 0.25)
    print(plotOut)
}


# Run the next code that extracts out the model's coeffiecents and plots them 
extractAndPlotResults(intModel)


multIntDemo <- data.frame(group=factor(c(rep("a", 5), rep("b", 5), rep("c", 5))), 
                          x=rep(0:4, times=3), 
                          intercept=c(4.11, -1.69, 1.09, 1.9, 1.21, 4.63, 10.29, 4.67, 12.06, 4.78, 15.22, 19.15, 4.44, 8.88, 9.47), 
                          response=c(4.11, 2.31, 9.09, 13.9, 17.21, 4.63, 14.29, 12.67, 24.06, 20.78, 15.22, 23.15, 12.44, 20.88, 25.47)
                          )
str(multIntDemo)

plot_output1 <- function(out1){
    ggmultIntgDemo <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', fill = NA, color = 'orange', size = 3)
    print(ggmultIntgDemo)
}

plot_output2 <- function(out2){
    out2Tidy <- broom::tidy(out2)
    out2Tidy$term <- gsub("group", "", out2Tidy$term)
    out2Plot <- data.frame(group = pull(out2Tidy[ -1, 1]),
                           slope = pull(out2Tidy[ 1, 2]),
                           intercept = pull(out2Tidy[ -1, 2])
                           )
    ggmultIntgDemo2 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group))+
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        geom_abline(data = out2Plot,
                    aes(intercept = intercept, slope = slope, color = group))
    print(ggmultIntgDemo2)
}

plot_output3 <- function(out3){
    ggmultIntgDemo3 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', aes(color = group), fill = NA)
    print(ggmultIntgDemo3)
}

# First, run a model without considering different intercept for each group
out1 <- lm( response ~ x, data=multIntDemo )
summary(out1)
plot_output1(out1)

# Considering same slope but different intercepts
out2 <- lm( response ~ x + group - 1, data=multIntDemo )
summary(out2)
plot_output2(out2)

# Consdering different slope and intercept for each group (i.e., an interaction)
out3 <- lm( response ~ x + group - 1 + x:group, multIntDemo)
summary(out3)
plot_output3(out3)


multIntDemo$intercept <- c(-0.87, 3.35, 1.25, 0.88, -1.05, 4.55, 1.22, 3.34, 1.26, 3.75, 7.71, 9.59, 2.28, 1.9, 13.35)
multIntDemo$response <- c(-0.87, 6.35, 7.25, 9.88, 10.95, 4.55, 4.22, 9.34, 10.26, 15.75, 7.71, 12.59, 8.28, 10.9, 25.35)

# Run model
outLmer <- lme4::lmer( response ~ x + ( 1 | group), multIntDemo)

# Look at model outputs 
summary( outLmer )
broom::tidy( outLmer )


extractAndPlotOutput <- function(outLmer, slope=3){
    multIntDemo$lmerPredict <- predict(outLmer)
    ggmultIntgDemo2 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group))+
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        geom_abline(data = multIntDemo,
                    aes(intercept = intercept, slope = slope, color = group))
    outPlot <-  ggmultIntgDemo2 +
                geom_line( data =  multIntDemo,
                      aes(x = x, y = lmerPredict, color = group),
                      linetype = 2)
    print(outPlot)
}


# Extract predictor variables and plot
extractAndPlotOutput(outLmer)


# Random effect slopes
multIntDemo$response <- c(-0.72, 1.5, 4.81, 6.61, 13.62, 10.21, 9.64, 11.91, 16.39, 16.97, 8.76, 14.79, 15.83, 15.27, 17.36)
multIntDemo$intercept <- c(-0.72, -1.5, -1.19, -2.39, 1.62, 10.21, 6.64, 5.91, 7.39, 4.97, 8.76, 11.79, 9.83, 6.27, 5.36)

outLmer2 <- lme4::lmer( response ~ ( x|group ), multIntDemo)
summary(outLmer2)
broom::tidy(outLmer2)


plotOutput <- function(outLmer2){
    multIntDemo$lmerPredict2 <- predict(outLmer2)
    ggmultIntgDemo3 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', aes(color = group), fill = NA)
    plotOut <- ggmultIntgDemo3 +
            geom_line( data =  multIntDemo,
                      aes(x = x, y = lmerPredict2, color = group),
                      linetype = 2)
    print(plotOut)
}


# Extract and plot
plotOutput(outLmer2)


# Mixed effect model
lmerModel <- lme4::lmer(mathgain ~ sex + 
                  mathprep + mathknow + (1|classid) +
                  (1|schoolid), data = studentData, na.action = "na.omit",
                  REML = TRUE)
summary(lmerModel)


extractAndPlot <- function(lmerModel){
    modelOutPlot <- broom::tidy(lmerModel, conf.int = TRUE)
    modelOutPlot <- modelOutPlot[ modelOutPlot$group =="fixed" &
                               modelOutPlot$term != "(Intercept)", ]
    plotOut <- ggplot(modelOutPlot, aes(x = term, y = estimate,
                             ymin = conf.low,
                             ymax = conf.high)) +
            theme_minimal() +
            geom_hline(yintercept = 0.0, color = 'red', size = 2.0) +
            geom_point() +
            geom_linerange() + coord_flip()
    print(plotOut)
}


# Extract and plot 
extractAndPlot(lmerModel)

```
  
  
  
***
  
Chapter 2 - Linear Mixed-Effect Models  
  
Linear mixed effect model - Birth rates data:  
  
* Small populations are highly sensitive to stochastic effects - if the mean is 1, a group of 5 might have 0 or 10  
* Questions about how counties may impact birth rates, over and above other demographic factors  
	* Example of plotting birth rate vs. county - will see both the highest and lowest birth rates in the smallest counties  
* Random effect syntax for the lmer model includes  
	* (1 | group) - random intercept with fixed mean  
    * (1 | g1/g2) - intercepts vary among g1 and g2 within g2  
    * (1 | g1) + (1 | g2) - random intercepts for two variables  
    * x + (x | g) - correlated random slope and intercept  
    * x + (x || g) - uncorrelated random slope and intercept  
    * See lme4 documentation for additional details  
  
Understanding and reporting the outputs of lmer:  
  
* The output from lmer is similar to the output from lm, but with some key differences - if using print(), will see  
	* The method used is REML - restricted maximum likelihood - which tends to solve better than maximum likelihood for these problems  
    * There is an REML convergence criteria, which can be a helpful diagnostic  
    * Can see the standard deviations for both the state and the residual, along with the number of observations  
    * Get the fixed effects coefficients in a similar form as lm()  
* The summary() call on lmer produces several additional outputs  
	* Residuals summary  
    * Fixed effects estimates include SE and t-values (but not p-values)  
    * Correlations of fixed effects  
* Can grab only the fixed effects using fixef(myLMERObject)  
	* Can grab only the random effects using ranef(myLMERObject), though these will not have confidence intervals  
    * The random effects confidence intervals could be estimated using bootstrapping or Bayesian methods per the author of lme4 - but actual random effects are just unobserved random variables rather than parameters  
* Can grab only the confidence intervals using confint(myLMERObject)  
* Need to be careful in reporting the results - figures vs. tables vs. in-line descriptions  
  
Statistical inference with Maryland crime data:  
  
* The Maryland crime data is available on data.gov - interesting for many public and private purposes  
* The null hypothsis test can be used with LMER - frequentist approach  
	* By default, lmer does not provide p-values, as there is ongoing debate as to the degrees of freedom and impact on reported results  
    * Can use lmerTest package to calculate and report on the p-values  
* Can use ANOVA to look at the variability explained by one model versus another model, and the associated degrees of freedom needed  
  
Example code includes:  
```{r}

# Read in births data
rawBirths <- read.csv("./RInputFiles/countyBirthsDataUse.csv")
countyBirthsData <- rawBirths
str(countyBirthsData)


# First, build a lmer with state as a random effect. Then look at the model's summary and the plot of residuals. 
birthRateStateModel <- lme4::lmer(BirthRate ~ (1|State), data=countyBirthsData)
summary(birthRateStateModel)
plot(birthRateStateModel)

# Next, plot the predicted values from the model ontop of the plot shown during the video.
countyBirthsData$birthPredictState <- predict(birthRateStateModel, countyBirthsData)
ggplot() + theme_minimal() +
    geom_point(data =countyBirthsData, aes(x = TotalPopulation, y = BirthRate)) + 
    geom_point(data = countyBirthsData, aes(x = TotalPopulation, y = birthPredictState),
               color = 'blue', alpha = 0.5
               )

# Include the AverageAgeofMother as a fixed effect within the lmer and state as a random effect
ageMotherModel <- lme4::lmer( BirthRate ~ AverageAgeofMother + (1|State), data=countyBirthsData)
summary(ageMotherModel)

# Compare the random-effect model to the linear effect model 
summary(lm(BirthRate ~ AverageAgeofMother, data = countyBirthsData))


# Include the AverageAgeofMother as a correlated random-effect slope parameter
ageMotherModelRandomCorrelated <- lme4::lmer(BirthRate ~ AverageAgeofMother + (AverageAgeofMother|State),
                       countyBirthsData)
summary(ageMotherModelRandomCorrelated)


# Include the AverageAgeofMother as a correlated random-effect slope parameter
ageMotherModelRandomUncorrelated <- lme4::lmer(BirthRate ~ AverageAgeofMother + 
                                                    (AverageAgeofMother || State), data=countyBirthsData
                                               )
summary(ageMotherModelRandomUncorrelated)


out <- ageMotherModelRandomUncorrelated

# Extract the fixed-effect coefficients
lme4::fixef(out)

# Extract the random-effect coefficients
lme4::ranef(out)

# Estimate the confidence intervals 
(ciOut <- confint(out))


# Technical note: Extracting out the regression coefficients from lmer is tricky (see discussion between the lmer and broom authors development)
# Extract out the parameter estimates and confidence intervals and manipulate the data
dataPlot <- data.frame(cbind( lme4::fixef(out), ciOut[ 4:5, ]))
rownames(dataPlot)[1] <- "Intercept"
colnames(dataPlot) <- c("mean", "l95", "u95")
dataPlot$parameter <- rownames(dataPlot)

# Print the new dataframe
print(dataPlot)

# Plot the results using ggplot2
ggplot(dataPlot, aes(x = parameter, y = mean,
                     ymin = l95, ymax = u95)) +
    geom_hline( yintercept = 0, color = 'red' ) +
    geom_linerange() + geom_point() + coord_flip() + theme_minimal()



# Read in crime data
rawCrime <- read.csv("./RInputFiles/MDCrime.csv")
MDCrime <- rawCrime
str(MDCrime)


plot1 <- ggplot(data = MDCrime, aes(x = Year, y = Crime, group = County)) +
    geom_line() + theme_minimal() +
    ylab("Major crimes reported per county")
print(plot1)

plot1 + geom_smooth(method = 'lm')


# Null hypothesis testing uses p-values to see if a variable is "significant"
# Recently, the abuse and overuse of null hypothesis testing and p-values has caused the American Statistical Association to issue a statement about the use of p-values
# Because of these criticisms and other numerical challenges, Doug Bates (the creator of the lme4 package) does not include p-values as part of his package
# However, you may still want to estimate p-values, because p-values are sill commonly used. Several packages exist, including the lmerTest package
# https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf

# Load lmerTest
# library(lmerTest)

# Fit the model with Year as both a fixed and random-effect
lme4::lmer(Crime ~ Year + (1 + Year | County) , data = MDCrime)

# Fit the model with Year2 rather than Year
out <- lme4::lmer(Crime ~ Year2 + (1 + Year2 | County) , data = MDCrime)

# Examine the model's output
summary(out)


## Build the Null model with only County as a random-effect
null_model <- lme4::lmer(Crime ~ (1 | County) , data = MDCrime)

## Build the Year2 model with Year2 as a fixed and random slope and County as the random-effect
year_model <- lme4::lmer(Crime ~ Year2 + (1 + Year2 | County) , data = MDCrime)

## Compare the two models using an anova
anova(null_model, year_model)

```
  
  
  
***
  
Chapter 3 - Generalized Linear Mixed-Effect Models  
  
Crash course on GLMs - relaxing the assumptions around normality of the residuals:  
  
* Non-normal data can be transformed using arcsin or the like  
* However, with advances in methodology, it is possible to more directly model the data using binomial and poisson distributions  
* The basic glm call is glm(y ~ x, family="")  # default is family="gaussian", which same as the lm()  
* The Poisson distribution is frequently best for count data, such as website visitors per hour - mean equals variance (generally best for small counts less than 30; can use normals for large counts)  
* For logistic regression, data can be entered in any of three formats  
	* Binary (y=0 or 1) - glm(y ~ x, family="binomial")  
    * Wilkinson-Rogers - glm(cbind(success, failure) ~ x, family="binomial")  
    * Weighted - glm(y ~ x, weights=weights, family="binomial")  
    * These methods differ primarily in the degrees of freedom (and thus deviance)  
  
Binomial data - modeling data with only two outcomes:  
  
* Traditional method for analysis includes looking at proportion of successes  
* The GLM allows for direct looks at the data - logistic regression (logit)  
* Binomial data can be fit using glmer(y ~ x + (1/group), family="error term")  
* The regression coefficients can be difficult to explain, sometimes leading to the use of odds ratios instead  
	* The odds ratio of 2.0 would mean 2:1 odds for that specific group  
  
Count data:  
  
* Examples like number of events per hour (website hits) or counts per area (birds)  
* The count data differs from the binomial in that there is no pre-specified upper boundary  
* While Chi-squared is often used for goodness of fit or test of association for count data, the Poisson GLM can be a nice alternative  
	* glm(y ~ x, family="poisson")  
    * glmer(y ~ x + (1|group), family="poisson")  
  
Example code includes:  
```{r cache=TRUE}

# In this case study, we will be working with simulated dose-response data
# The response is mortality (1) or survival (0) at the end of a study. During this exercise, we will fit a logistic regression using all three methods described in the video
# You have been given two datasets. dfLong has the data in a "long" format with each row corresponding to an observation (i.e., a 0 or 1)
# dfShort has the data in an aggregated format with each row corresponding to a treatment (e.g., 6 successes, 4 failures, number of replicates = 10, proportion = 0.6)

dfLong <- data.frame(dose=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10), 
                     mortality=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1)
                     )
str(dfLong)

dfShort <- dfLong %>% 
    group_by(dose) %>%
    summarize(mortality=sum(mortality), nReps=n()) %>%
    mutate(survival=nReps-mortality, mortalityP=mortality/nReps)
dfShort


# Fit a glm using data in a long format
fitLong <- glm(mortality ~ dose, data = dfLong, family = "binomial")
summary(fitLong)

# Fit a glm using data in a short format with two columns
fitShort <- glm( cbind(mortality , survival ) ~ dose , data = dfShort, family = "binomial")
summary(fitShort)

# Fit a glm using data in a short format with weights
fitShortP <- glm( mortalityP ~ dose , data = dfShort, weights = nReps , family = "binomial")
summary(fitShortP)


y <- c(0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 5, 1, 1)
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)


# Fit the linear model
summary(lm(y ~ x))

# Fit the generalized linear model
summary(glm(y ~ x, family = "poisson"))


# Often, we want to "look" at our data and trends in our data
# ggplot2 allows us to add trend lines to our data
# The defult lines are created using a technique called local regression
# However, we can specify different models, including GLMs
# During this exercise, we'll see how to plot a GLM

# Plot the data using jittered points and the default stat_smooth
ggplot(data = dfLong, aes(x = dose, y = mortality)) + 
    geom_jitter(height = 0.05, width = 0.1) +
    stat_smooth(fill = 'pink', color = 'red') 

# Plot the data using jittered points and the the glm stat_smooth
ggplot(data = dfLong, aes(x = dose, y = mortality)) + 
    geom_jitter(height = 0.05, width = 0.1) +
    stat_smooth(method = 'glm',  method.args = list(family = "binomial"))


# library(lmerTest)

df <- data.frame(dose=rep(rep(c(0, 2, 4, 6, 8, 10), each=20), times=3), 
                 mortality=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1), 
                 replicate=factor(rep(letters[1:3], each=120))
                 )
str(df)


glmerOut <- lme4::glmer(mortality ~ dose + (1|replicate), family = 'binomial', data = df)
summary(glmerOut)


# library(lmerTest)
# Fit the model and look at its summary 
# modelOut <- lme4::glmer( cbind(Purchases, Pass) ~ friend + ranking + (1|city), data = allData, family = 'binomial')
# summary( modelOut) 

# Compare outputs to a lmer model
# summary(lme4::lmer( Purchases/( Purchases + Pass) ~ friend + ranking + (1|city), data = allData))


# Run the code to see how to calculate odds ratios
# summary(modelOut) 
# exp(fixef(modelOut)[2])
# exp(confint(modelOut)[3, ])


# Load lmerTest
# library(lmerTest)


userGroups <- data.frame(group=factor(rep(rep(LETTERS[1:4], each=10), times=2)), 
                         webpage=factor(rep(c("old", "new"), each=40)), 
                         clicks=c(0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 3, 2, 3, 1, 2, 4, 2, 1, 0, 2, 0, 1, 2, 0, 2, 1, 1, 2, 4, 2, 8, 1, 1, 1, 2, 1, 1, 0, 0, 3, 0, 1, 4, 1, 2, 0, 1, 1, 0, 0, 3, 2, 0, 3, 1, 2, 2, 0, 2, 3, 1, 3, 2, 4, 4, 2, 1, 5, 2)
                         )
str(userGroups)


# Fit a Poisson glmer
summary( lme4::glmer(clicks ~ webpage + (1|group), family = 'poisson', data = userGroups))


# library(lmerTest)


rawIL <- read.csv("./RInputFiles/ILData.csv")
ILdata <- rawIL
str(ILdata)

# Age goes before year
modelOut <- lme4::glmer(count ~ age + year + (year|county), family = 'poisson', data = ILdata)
summary(modelOut)


# Extract out fixed effects
lme4::fixef(modelOut)

# Extract out random effects 
lme4::ranef(modelOut)


# Run code to see one method for plotting the data
ggplot(data = ILdata, aes(x = year, y = count, group = county)) +
    geom_line() +
    facet_grid(age ~ . ) +
    stat_smooth( method = 'glm',
                method.args = list( family = "poisson"), se = FALSE,
                alpha = 0.5) +
    theme_minimal()

```
  
  
  
***
  
Chapter 4 - Repeated Measures  
  
An introduction to repeated measures:  
  
* Sampling the same thing over time is a repeated measure, a specific example of a mixed effects model  
	* Follow the same individual through time - cohorts allow for controlling for individuality  
    * The paired t-test is often used for assessing a repeated measures dataset - t.test(x1, x2, paired=TRUE)  # x1 and x2 need to be the same length and each element needs to be the same individual  
* Repeated measures ANOVA is a conceptual extension of the paired t-test - are the means constant over time  
	* anova(lmer(y ~ time + (1|individual)))  
    * Can be used with glmer() also  
    * Note that degrees of freedom is still an open question - different packages calculate this differently  
  
Sleep study:  
  
* Applying LMER to the sleep study dataset - impact of drugs on sleep patterns for 10 patients followed over time  
	* This is the classic "Student" dataset due to Guinness at the time not allowing its employees to publish  
    * Ho will be that the amount of sleep does not vary with the treatments  
    * Modeling will be done using a linear mixed model  
* Modeling approach - iteratively;  
	* EDA  
    * Simple regression  
    * Model of interest  
    * Extract information from model  
    * Visualize final data  
  
Hate in NY state?  
  
* Change in rate of hate crimes over time by county - available from data.gov for 2010-2016  
* Level of technical details in reporting should vary significantly by audience - blend data in to story for wider audiences, while being reporducible/technical for a scientifc audience  
  
Wrap up:  
  
* Hiearchical data, mixed effects models, case studies  
* Start with the LME4 documentation for additional explorations and details  
  
Example code includes:  
```{r}

y <- c(0.23, 2.735, -0.038, 6.327, -0.643, 1.69, -1.378, -1.228, -0.252, 2.014, -0.073, 6.101, 0.213, 3.127, -0.29, 8.395, -0.33, 2.735, 0.223, 1.301)
treat <- rep(c("before", "after"), times=10)
x <- rep(letters[1:10], each=2)

# Run a standard, non-paired t-test
t.test(y[treat == "before"], y[treat == "after"], paired = FALSE)

# Run a standard, paired t-test
t.test(y[treat == "before"], y[treat == "after"], paired = TRUE)


library(lmerTest)
library(lme4)

# Run the paired-test like before
t.test(y[treat == "before"], y[treat == "after"], paired = TRUE)

# Run a repeated-measures ANOVA
anova(lmer( y ~ treat + (1|x)))


data(sleepstudy, package="lme4")
str(sleepstudy)

# Plot the data
ggplot(data = sleepstudy) +
    geom_line(aes(x = Days, y = Reaction, group = Subject)) +
    stat_smooth(aes(x = Days, y = Reaction), method = 'lm', size = 3, se = FALSE)

# Build a lm 
lm( Reaction ~ Days, data = sleepstudy)

# Build a lmer
(lmerOut <- lmer( Reaction ~ Days + (1|Subject), data = sleepstudy))


# The lmer model you built during the previous exercise has been saved as lmerOut
# During this exercise, you will examine the effects of drug type using both an ANOVA framework and a regression framework

# Run an anova
anova(lmerOut)

# Look at the regression coefficients
summary(lmerOut)


# Read in NY hate data
rawHate <- read.csv("./RInputFiles/hateNY.csv")
hate <- rawHate
str(hate)


ggplot( data = hate, aes(x = Year, y = TotalIncidents, group = County)) +
    geom_line() + 
    geom_smooth(method = 'lm', se = FALSE)


# During this exercise, you will build a glmer
# Because most of the incidents are small count values, use a Poisson (R function family poisson) error term
# First, build a model using the actually year (variable Year, such as 2006, 2007, etc) - this model will fail
# Second, build a model using the rescaled year (variable Year2, such as 0, 1, 2, etc)
# This demonstrates the importance of considering where the intercept is located when building regression models
# Recall that a variable x can be both a fixed and random effect in a lmer() or glmer(): for example lmer(y ~ x + (x| group) demonstrates this syntax

# glmer with raw Year
glmer(TotalIncidents ~ Year + (Year|County), data = hate, family = "poisson")

# glmer with scaled Year, Year2
glmerOut <- glmer(TotalIncidents ~ Year2 + (Year2|County), data = hate, family = "poisson")
summary(glmerOut)


# Extract and manipulate data
countyTrend <- ranef(glmerOut)$County
countyTrend$county <- factor(row.names(countyTrend), levels =row.names(countyTrend)[order(countyTrend$Year2)])

# Plot results 
ggplot(data = countyTrend, aes(x = county, y = Year2)) + geom_point() +
    coord_flip() + 
    ylab("Change in hate crimes per year")  +
    xlab("County")

```
  
  
  
***
  
###_Forecasting Product Demand in R_  
  
Chapter 1 - Forecasting Demand with Time Series  
  
Loading data in to an xts object:  
  
* The xts object will be the buidling block for the course - extensible time series (xts) is an extension of the zoo package - basically, a time index attached to the data matrix  
* Can create dates using dates=seq(as.Date("MM-DD-YYYY"), length=, by="weeks")  # to create weekly data  
	* xts(myData, order.by=dates)  # will create an XTS using dates as the index  
  
ARIMA Time Series 101:  
  
* AR - AutoRegressive (lags help to determine today's values - "long memory models")  
* MA - Moving Average (shocks/errors help to determine today's shocks/errors - "short memory models" due to dissipation)  
* I - Integrated (does the data have a dependency across time, and how long does it last) - make the time series stationary  
	* Stationarity is the idea that effects disipate over time - today has more impact on tomorrow than on time periods in the future  
    * Differencing (monthly, seasonal, etc.) the data can be a useful approach for data with stationarity  
* Begin by creating training dataset and valiadation training dataset  
* The auto.arima() function tries to estimate the best ARIMA for a given data series  
	* ARIMA(p, d, q) is ARIMA(AR, Differencing, MA)  
  
Forecasting and Evaluating:  
  
* Can use the ARIMA data to forecast the data forward - extrapolating the signal (forecasting) and estimating the amount of noise (error or CI)  
* The forecast() function in R simplifies the process - forecast(myModel, h=) which will forecast forward h time periods  
* Two common error measurements include MAE (mean average error) and MAPE (mean average percentage error)  
	* MAPE is better at putting things on a common scale  
  
Example code includes:  
```{r}


# Read in beverages data
rawBev <- read.csv("./RInputFiles/Bev.csv")
bev <- rawBev
str(bev)


# Load xts package 
library(xts)
library(forecast)


# Create the dates object as an index for your xts object
dates <- seq(as.Date("2014-01-19"), length = 176, by = "weeks")

# Create an xts object called bev_xts
bev_xts <- xts(bev, order.by = dates)


# Create the individual region sales as their own objects
MET_hi <- bev_xts[,"MET.hi"]
MET_lo <- bev_xts[,"MET.lo"]
MET_sp <- bev_xts[,"MET.sp"]

# Sum the region sales together
MET_t <- MET_hi + MET_lo + MET_sp

# Plot the metropolitan region total sales
plot(MET_t)


# Split the data into training and validation
MET_t_train <- MET_t[index(MET_t) < "2017-01-01"]
MET_t_valid <- MET_t[index(MET_t) >= "2017-01-01"]

# Use auto.arima() function for metropolitan sales
MET_t_model <- auto.arima(MET_t_train)


# Forecast the first 22 weeks of 2017
forecast_MET_t <- forecast(MET_t_model, h = 22)

# Plot this forecast #
plot(forecast_MET_t)


# Convert to numeric for ease
for_MET_t <- as.numeric(forecast_MET_t$mean)
v_MET_t <- as.numeric(MET_t_valid)

# Calculate the MAE
MAE <- mean(abs(for_MET_t - v_MET_t))

# Calculate the MAPE
MAPE <- 100*mean(abs(for_MET_t - v_MET_t)/v_MET_t)

# Print to see how good your forecast is!
print(MAE)
print(MAPE)


# Convert your forecast to an xts object
for_dates <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_t_xts <- xts(forecast_MET_t$mean, order.by = for_dates)

# Plot the validation data set
plot(for_MET_t_xts, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(MET_t_valid, col = "blue")


# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")

# Convert the limits to xts objects
lower <- xts(forecast_MET_t$lower[, 2], order.by = for_dates)
upper <- xts(forecast_MET_t$upper[, 2], order.by = for_dates)

# Adding confidence intervals of forecast to plot
lines(lower, col = "blue", lty = "dashed")
lines(upper, col = "blue", lty = "dashed")

```
  
  
  
***
  
Chapter 2 - Components of Demand  
  
Price elasticity:  
  
* Price is one of the obvious factors that impacts demand, with the relationship called price elasticity (% dDemand / % dPrice)  
	* Elastic goods have elasticity > 1, meaning demand changes more quickly (percentage wise) than price  
    * Inelastic goods have elasticity < 1, for example gasoline  
    * Unit elastic goods have elasticity = 1, meaning that X% increase in price drives X% decrease in demand  
    * Linear regression can be employed to estimate the elasticity for a given product - the log-log transform helps get the % vs % coefficients  
  
Seasonal/holiday/promotional effects:  
  
* Seasonal products are common - can be bought any time of the year, though certain seasons have higher demand (holidays are a common example)  
* Promotions are attempts by companies to influence demand  
* Linear regression can help determine relationships between demand and many other factors  
	* If an xts vector has been created for key dates, can merge(train, holiday, fill=0) and the holiday column will be 0 wherever there is no match to holiday  
  
Forecasting with regression:  
  
* Forecasting with time series is straightforward due to the lag nature of the models - tomorrow forecasts today and today forecasts tomorrow and etc.  
* Forecasting with regression can be more tricky, particularly since we need the future inputs (such as price) in order to predict the future demand  
	* Even when there are contractually fixed prices, promotions can effectively create a de facto price change anyways  
* Need to have the same column names in the test/validation dataset as were used in the modeling  
	* Then, can use predict(myModel, myData)  
    * May need to exponentiate in case the data are currently on the log scale rather than the absolute scale  
  
Example code includes:  
```{r}

bev_xts_train <- bev_xts[index(bev_xts) < "2017-01-01"]
bev_xts_valid <- bev_xts[index(bev_xts) >= "2017-01-01"]

# Save the prices of each product
l_MET_hi_p <- log(as.vector(bev_xts_train[, "MET.hi.p"]))

# Save as a data frame
MET_hi_train <- data.frame(as.vector(log(MET_hi[index(MET_hi) < "2017-01-01"])), l_MET_hi_p)
colnames(MET_hi_train) <- c("log_sales", "log_price")

# Calculate the regression
model_MET_hi <- lm(log_sales ~ log_price, data = MET_hi_train)


# Plot the product's sales
plot(MET_hi)

# Plot the product's price
MET_hi_p <- bev_xts_train[, "MET.hi.p"]
plot(MET_hi_p)


# Create date indices for New Year's week
n.dates <- as.Date(c("2014-12-28", "2015-12-27", "2016-12-25"))

# Create xts objects for New Year's
newyear <- as.xts(rep(1, 3), order.by = n.dates)

# Create sequence of dates for merging
dates_train <- seq(as.Date("2014-01-19"), length = 154, by = "weeks")

# Merge training dates into New Year's object
newyear <- merge(newyear, dates_train, fill = 0)


# Add newyear variable to your data frame
MET_hi_train <- data.frame(MET_hi_train, newyear=as.vector(newyear))

# Build regressions for the product
model_MET_hi_full <- lm(log_sales ~ log_price + newyear, data = MET_hi_train)


# Subset the validation prices #
l_MET_hi_p_valid <- log(as.vector(bev_xts_valid[, "MET.hi.p"]))

# Create a validation data frame #
MET_hi_valid <- data.frame(l_MET_hi_p_valid)
colnames(MET_hi_valid) <- "log_price"


# Predict the log of sales for your high end product
pred_MET_hi <- predict(model_MET_hi, MET_hi_valid)

# Convert predictions out of log scale
pred_MET_hi <- exp(pred_MET_hi)


# Convert to an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
pred_MET_hi_xts <- xts(pred_MET_hi, order.by = dates_valid)

# Plot the forecast
plot(pred_MET_hi_xts)

# Calculate and print the MAPE
MET_hi_v <- bev_xts_valid[,"MET.hi"]

MAPE <- 100*mean(abs((pred_MET_hi_xts - MET_hi_v)/MET_hi_v))
print(MAPE)

```
  
  
  
***
  
Chapter 3 - Blending Regression with Time Series  
  
Residuals from regression model:  
  
* The residuals from the regression models can be used for further modeling - see if the residuals are related over time, and model them with time series if so  
* Need to start by gathering the residuals and then converting them to an XTS object - explore for patterns in this XTS object  
  
Forecasting residuals:  
  
* When the residuals are related across time, we can use time series to model the residuals - basically, patterns to the errors provide an opportunity for further modeling  
* Can use auto.arima() on the residuals data, to see what the best ARIMA model for the residuals is  
	* Can then forecast the residuals in to the future using forecast(myModel, h=) # h being the time periods to predict forward  
  
Transfer functions and ensembling:  
  
* Techniques for combining forecasts - single model (transfer function) or averaging of models (ensembling)  
* Demand can be based on both regression (modeling external factors) and time series (residuals)  
* Ensembling is a combination (blend) of the forecasts, with simple averaging being the simplest approach  
	* Basically, build a stand-alone time series model and a stand-alone regression model  
    * The ensemble forecast can be better or worse than any of the stand-alone models  
  
Example code includes:  
```{r}

# Calculate the residuals from the model
MET_hi_full_res <- resid(model_MET_hi_full)

# Convert the residuals to an xts object
MET_hi_full_res <- xts(MET_hi_full_res, order.by = dates_train)


# Plot the histogram of the residuals
hist(MET_hi_full_res)

# Plot the residuals over time
plot(MET_hi_full_res)


# Build an ARIMA model on the residuals: MET_hi_arima
MET_hi_arima <- auto.arima(MET_hi_full_res)

# Look at a summary of the model
summary(MET_hi_arima)


# Forecast 22 weeks with your model: for_MET_hi_arima
for_MET_hi_arima <- forecast(MET_hi_arima, h=22)

# Print first 10 observations
head(for_MET_hi_arima$mean, n = 10)


# Convert your forecasts into an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_arima <- xts(for_MET_hi_arima$mean, order.by = dates_valid)

# Plot the forecast
plot(for_MET_hi_arima)


# Convert your residual forecast to the exponential version
for_MET_hi_arima <- exp(for_MET_hi_arima)

# Multiply your forecasts together!
for_MET_hi_final <- for_MET_hi_arima * pred_MET_hi_xts


# Plot the final forecast - don't touch the options!
plot(for_MET_hi_final, ylim = c(1000, 4300))

# Overlay the validation data set
lines(MET_hi_v, col = "blue")


# Calculate the MAE
MAE <- mean(abs(for_MET_hi_final - MET_hi_v))
print(MAE)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_hi_final - MET_hi_v)/MET_hi_v)
print(MAPE)


# Build an ARIMA model using the auto.arima function
MET_hi_model_arima <- auto.arima(MET_hi)

# Forecast the ARIMA model
for_MET_hi <- forecast(MET_hi_model_arima, h = length(MET_hi_v))

# Save the forecast as an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_xts <- xts(for_MET_hi$mean, order.by = dates_valid)

# Calculate the MAPE of the forecast
MAPE <- 100 * mean(abs(for_MET_hi_xts - MET_hi_v) / MET_hi_v)
print(MAPE)


# Ensemble the two forecasts together
for_MET_hi_en <- 0.5 * (for_MET_hi_xts + pred_MET_hi_xts)

# Calculate the MAE and MAPE
MAE <- mean(abs(for_MET_hi_en - MET_hi_v))
print(MAE)

MAPE <- 100 * mean(abs(for_MET_hi_en - MET_hi_v) / MET_hi_v)
print(MAPE)

```
  
  
  
***
  
Chapter 4 - Hierarchical Forecasting  
  
Bottom-Up Hierarchical Forecasting:  
  
* The hierarchical data structuring can be an advantage in forecasting, provided that the data has a natural hierarchy  
* The sum of all the lower-level forecasts should equal the higher-level forecasts  
	* Bottom-up: Forecast at the lowest level and aggregate (easiest but requires the most number of forecasts)  
    * Top-down: Forecast at the top level and the apply downwards  
    * Middle-out: Forecast at the middle levels and then apply both upwards and downwards  
  
Top-Down Hierarchical Forecasting:  
  
* The top-down forecasting process is typically quicker but less accurate than the bottom-up forecasting process  
* Two techniques available for top-down reconciliation  
	* Average of historical proportions - mean percentage that each component contributes to the total (calculated by sub-component such as week)  
    * Proportion of historical averages - mean percentage that each component contributes to the total (calculated by aggregate)  
* Reconciled forecasts at lower levels are typically less accurate than the direct forecast of the lower levels  
  
Middle-Out Hierarchical Forecasting:  
  
* Bottom-up forecasting is higher quality but more time-consuming than top-down forecasting  
* The middle-out forecasting method is a sometimes successful blend of the methods, getting decent accuracy at a lesser time commitment  
  
Wrap up:  
  
* Using time series to forecast demand forward  
* Incorporating external factors using linear regression  
* Blending time series and regression approaches  
* Top-down, bottom-up, middle-out approaches to aggregation and forecasting at various levels (hierarchical)  
* Can extend by looking at cross-elasticities (impact of competitor pricing)  
* Can better forecast proportions using time series analysis  
* Additional demand forecasting models include neural networks, exponential smoothing, etc.  
  
Example code includes:  
```{r}

# Build a time series model 
MET_sp_model_arima <- auto.arima(MET_sp)

# Forecast the time series model for 22 periods
for_MET_sp <- forecast(MET_sp_model_arima, h=22)

# Create an xts object
for_MET_sp_xts <- xts(for_MET_sp$mean, order.by=dates_valid)

MET_sp_v <- MET_sp["2017"]

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_sp_xts - MET_sp_v) / MET_sp_v)
print(MAPE)


MET_sp_train <- bev_xts_train %>%
    transform(log_sales = log(MET.sp), log_price=log(MET.sp.p))
MET_sp_train <- MET_sp_train[, c("log_sales", "log_price")]
MET_sp_train$newyear <- 0
MET_sp_train$valentine <- 0
MET_sp_train$christmas <- 0
MET_sp_train$mother <- 0

MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-12-28", "2015-12-27", "2016-12-25")), "newyear"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-02-09", "2015-02-08", "2016-02-07")), "valentine"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-12-21", "2015-12-20", "2016-12-18")), "christmas"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-05-04", "2015-05-03", "2016-05-01")), "mother"] <- 1


# THE BELOW IS TOTAL NONSENSE
# Build a regression model
model_MET_sp <- lm(log_sales ~ log_price + newyear + valentine + christmas + mother, data = MET_sp_train)


MET_sp_valid <- as.data.frame(bev_xts_valid) %>%
    mutate(log_sales = log(MET.sp), log_price=log(MET.sp.p)) %>%
    select("log_sales", "log_price")
MET_sp_valid$newyear <- 0
MET_sp_valid$valentine <- 0
MET_sp_valid$christmas <- 0
MET_sp_valid$mother <- 0  

MET_sp_valid[7, "valentine"] <- 1
MET_sp_valid[19, "mother"] <- 1
MET_sp_valid$log_sales <- NULL


# Forecast the regression model using the predict function 
pred_MET_sp <- predict(model_MET_sp, MET_sp_valid)

# Exponentiate your predictions and create an xts object
pred_MET_sp <- exp(pred_MET_sp)
pred_MET_sp_xts <- xts(pred_MET_sp, order.by = dates_valid)

# Calculate MAPE
MAPE <- 100*mean(abs((pred_MET_sp_xts - MET_sp_v)/MET_sp_v))
print(MAPE)


# Ensemble the two forecasts
for_MET_sp_en <- 0.5 * (for_MET_sp_xts + pred_MET_sp_xts)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_sp_en - MET_sp_v) / MET_sp_v)
print(MAPE)


# Copy over pred_MET_lo_xts
pred_MET_lo_xts <- xts(c(2960.6, 2974.1, 2943.2, 2948.6, 2915.6, 2736.4, 2953.9, 3199.4, 2934, 2898.7, 3027.7, 3165.9, 3073.1, 2842.7, 2928.7, 3070.2, 2982.2, 3018, 3031.9, 2879.4, 2993.2, 2974.1), order.by=dates_valid)


# Calculate the metropolitan regional sales forecast
for_MET_total <- pred_MET_hi_xts + for_MET_sp_en + pred_MET_lo_xts

# Calculate a validation data set 
MET_t_v <- bev_xts_valid[,"MET.hi"] + bev_xts_valid[,"MET.lo"] + bev_xts_valid[,"MET.sp"]

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_total - MET_t_v) / MET_t_v)
print(MAPE)


# Create the MET_total data
MET_total <- xts(data.frame(MET.hi=c(5942, 5600, 5541, 6892, 5586, 5943, 6329, 6693, 6938, 6138, 6361, 6378, 5423, 5097, 4937, 5496, 6870, 6626, 6356, 5657, 6577, 7202, 7381, 7404, 7204, 6667, 6153, 6035, 5633, 5283, 5178, 4758, 5058, 5254, 5954, 6166, 6247, 6304, 7202, 6662, 6814, 6174, 5412, 5380, 5674, 6472, 6912, 7404, 8614, 8849, 7174, 6489, 7174, 6555, 6402, 7671, 5012, 4790, 5075, 5238, 5615, 6113, 7706, 7811, 7898, 7232, 6585, 5870, 7084, 5125, 5330, 5553, 6349, 6195, 6271, 5851, 5333, 5854, 5609, 5649, 6051, 6409, 5786, 5190, 5085, 4949, 5151, 5147, 5426, 5509, 6956, 7870, 8224, 6685, 6153, 5802, 5244, 5162, 5036, 5025, 8378, 8944, 7109, 7605, 7846, 7598, 8012, 9551, 6102, 5366, 4932, 4962, 5392, 6194, 7239, 7621, 7460, 7097, 6596, 5848, 8306, 5344, 5848, 6341, 7364, 7269, 7053, 6682, 6971, 7521, 7063, 6298, 6003, 5227, 5047, 4877, 4851, 4628, 4516, 4442, 4935, 5181, 5431, 5866, 5919, 5704, 5957, 6019, 5962, 6021, 5880, 5674, 7439, 7415)),
                 order.by=dates_train
                 )

# Build a regional time series model
MET_t_model_arima <- auto.arima(MET_total)

# Calculate a 2017 forecast for 22 periods
for_MET_t <- forecast(MET_t_model_arima, h=22)

# Make an xts object from your forecast
for_MET_t_xts <- xts(for_MET_t$mean, order.by=dates_valid)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_t_xts - MET_t_v) / MET_t_v)
print(MAPE)


# Calculate the average historical proportions
prop_hi <- mean(MET_hi/MET_total)
prop_lo <- mean(MET_lo/MET_total)
prop_sp <- mean(MET_sp/MET_total)

# Distribute out your forecast to each product
for_prop_hi <- prop_hi*for_MET_t_xts
for_prop_lo <- prop_lo*for_MET_t_xts
for_prop_sp <- prop_sp*for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi - MET_hi_v) / MET_hi_v)
print(MAPE_hi)

MET_lo_v <- bev_xts_valid[,"MET.lo"]
MAPE_lo <- 100 * mean(abs(for_prop_lo - MET_lo_v) / MET_lo_v)
print(MAPE_lo)

MAPE_sp <- 100 * mean(abs(for_prop_sp - MET_sp_v) / MET_sp_v)
print(MAPE_sp)


# Calculate the average historical proportions
prop_hi_2 <- mean(MET_hi) / mean(MET_total)
prop_lo_2 <- mean(MET_lo) / mean(MET_total)
prop_sp_2 <- mean(MET_sp) / mean(MET_total)

# Distribute out your forecast to each product
for_prop_hi_2 <- prop_hi_2 * for_MET_t_xts
for_prop_lo_2 <- prop_lo_2 * for_MET_t_xts
for_prop_sp_2 <- prop_sp_2 * for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi_2 - MET_hi_v) / MET_hi_v)
print(MAPE_hi)
MAPE_lo <- 100 * mean(abs(for_prop_lo_2 - MET_lo_v) / MET_lo_v)
print(MAPE_lo)
MAPE_sp <- 100 * mean(abs(for_prop_sp_2 - MET_sp_v) / MET_sp_v)
print(MAPE_sp)


SEC_total <- xts(data.frame(SEC.hi=c(700, 775, 789, 863, 765, 759, 757, 747, 746, 709, 749, 786, 796, 726, 727, 723, 778, 755, 739, 740, 723, 695, 727, 707, 725, 684, 667, 698, 727, 722, 748, 695, 742, 739, 715, 724, 686, 671, 688, 682, 710, 700, 672, 680, 695, 780, 751, 693, 809, 881, 703, 712, 768, 796, 808, 904, 641, 662, 693, 725, 719, 736, 715, 722, 732, 745, 689, 705, 811, 739, 744, 700, 745, 735, 732, 722, 721, 732, 750, 714, 752, 677, 731, 674, 720, 675, 741, 722, 715, 719, 649, 697, 743, 733, 772, 698, 690, 734, 713, 644, 788, 833, 749, 731, 670, 675, 675, 993, 773, 751, 697, 677, 750, 723, 780, 763, 721, 701, 704, 684, 985, 791, 731, 714, 704, 694, 685, 652, 708, 754, 747, 705, 711, 699, 712, 745, 706, 665, 666, 692, 676, 696, 689, 697, 689, 717, 697, 708, 660, 707, 715, 680, 922, 888)), order.by=dates_train
                 )

# Build a time series model for the region
SEC_t_model_arima <- auto.arima(SEC_total)

# Forecast the time series model
for_SEC_t <- forecast(SEC_t_model_arima, h=22)

# Make into an xts object
for_SEC_t_xts <- xts(for_SEC_t$mean, order.by=dates_valid)

SEC_t_v <- bev_xts_valid$SEC.hi + bev_xts_valid$SEC.lo
# Calculate the MAPE
MAPE <- 100 * mean(abs(for_SEC_t_xts - SEC_t_v) / SEC_t_v)
print(MAPE)


SEC_hi <- bev_xts_train[, "SEC.hi"]
SEC_lo <- bev_xts_train[, "SEC.lo"]
SEC_hi_v <- bev_xts_valid[, "SEC.hi"]
SEC_lo_v <- bev_xts_valid[, "SEC.lo"]

# Calculate the average of historical proportions
prop_hi <- mean(SEC_hi / SEC_total)
prop_lo <- mean(SEC_lo / SEC_total)

# Distribute the forecast
for_prop_hi <- prop_hi * for_SEC_t_xts
for_prop_lo <- prop_lo * for_SEC_t_xts

# Calculate a MAPE for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi - SEC_hi_v) / SEC_hi_v)
print(MAPE_hi)

MAPE_lo <- 100 * mean(abs(for_prop_lo - SEC_lo_v) / SEC_lo_v)
print(MAPE_lo)


# Copy over for_M_t_xts data
for_M_t_xts <- xts(c(2207, 2021, 2010, 2052, 2075, 2074, 2065, 2058, 2056, 2055, 2053, 2052, 2050, 2049, 2048, 2047, 2046, 2045, 2044, 2043, 2043, 2042), order.by=dates_valid)

# Calculate the state sales forecast: for_state
for_state = for_SEC_t_xts + for_MET_t_xts + for_M_t_xts

# See the forecasts
for_state

```
  
  
  
***
  
###_HR Analytics in R: Exploring Employee Data_  
  
Chapter 1 - Identifying the Best Recruiting Source  
  
Introduction - Ben Teusch, HR Analytics Consultant:  
  
* HR analytics has many other names - people analytics, workforce analytics, etc.  
* Identify groups for comparison - high vs. low performers, groups with high vs. low turnover, etc.  
	* Exploratory analysis and statistics for each group, including plots of key differences  
* Course is outlines as a series of case studies, with one case per chapter  
  
Recruiting and quality of hire:  
  
* Where are the best hires coming from, and how can you get more of them  
	* Defining quality of hire is challenging - some mix of productivity, satisfaction, retention, performance reviews, etc.  
    * Attrition can be defined as the mean of a 1, 0 vector of "did the person leave in the time period T"  
  
Visualizing recruiting data:  
  
* Helpful for communicating findings to decision makers  
* The geom_col() in ggplot will make a bar chart, with the y aestehtic being the bar height  
  
Example code includes:  
```{r}

# Import the recruitment data
recruitment <- readr::read_csv("./RInputFiles/recruitment_data.csv")

# Look at the first few rows of the dataset
head(recruitment)

# Get an overview of the recruitment data
summary(recruitment)

# See which recruiting sources the company has been using
recruitment %>% 
  count(recruiting_source)


# Find the average sales quota attainment for each recruiting source
avg_sales <- recruitment %>% 
  group_by(recruiting_source) %>% 
  summarize(avg_sales_quota_pct=mean(sales_quota_pct))

# Display the result
avg_sales


# Find the average attrition for the sales team, by recruiting source, sorted from lowest attrition rate to highest
avg_attrition <- recruitment %>%
  group_by(recruiting_source) %>% 
  summarize(attrition_rate=mean(attrition)) %>%
  arrange(attrition_rate)

# Display the result
avg_attrition

# Plot the bar chart
avg_sales %>% ggplot(aes(x=recruiting_source, y=avg_sales_quota_pct)) + geom_col()

# Plot the bar chart
avg_attrition %>% ggplot(aes(x=recruiting_source, y=attrition_rate)) + geom_col()

```
  
  
  
***
  
Chapter 2 - What is driving low employee engagement  
  
Analyzing employee engagement:  
  
* Gallup defines engaged employees as those who are involved in, enthusiastic about, and committed to their workplace  
* Survey data are available in the example case study  
	* Will use both mutate() and ifelse()  
    * The ifelse() is needed for vectors of length > 1 since it can work in a vectorized manner (and is thus OK inside the mutate call)  
  
Visualizing the engagement data:  
  
* Multiple attributes in a single place can make for a more compelling report  
* The tidyr package is part of the tidyverse, and hslps arrange the data properly for plotting  
	* tidyr::gather(columns, key="key", value="value") will be the package used in this example - pull the data from the columns down to the rows  
    * ggplot(survey_gathered, aes(x = key, y = value, fill = department)) + geom_col(position = "dodge")  
    * ggplot(survey_gathered, aes(x = key, y = value, fill = department)) + geom_col(position = "dodge") + facet_wrap(~ key, scales = "free")  
  
Are differences meaningful?  
  
* Can use significance testing to assess likelhood (p-value) that the second sample could have come from the same population as the first sample  
	* This course will use t-test (continuous variables) and chi-squared test (categorical variables)  
    * t.test(tenure ~ is_manager, data = survey)  
    * chisq.test(survey$left_company, survey$is_manager)  # no data= argument is available in the function  
  
Example code includes:  
```{r}

# Import the data
survey <- readr::read_csv("./RInputFiles/survey_data.csv")

# Get an overview of the data
summary(survey)

# Examine the counts of the department variable
survey %>% count(department)


# Output the average engagement score for each department, sorted
survey %>%
  group_by(department) %>%
  summarize(avg_engagement=mean(engagement)) %>%
  arrange(avg_engagement)


# Create the disengaged variable and assign the result to survey
survey_disengaged <- survey %>% 
  mutate(disengaged = ifelse(engagement <= 2, 1, 0)) 

survey_disengaged

# Summarize the three variables by department
survey_summary <- survey_disengaged %>%
  group_by(department) %>%
  summarize(pct_disengaged=mean(disengaged), 
            avg_salary=mean(salary), 
            avg_vacation_taken=mean(vacation_days_taken)
            )

survey_summary


# Gather data for plotting
survey_gathered <- survey_summary %>% 
  gather(key = "measure", value = "value",
         pct_disengaged, avg_salary, avg_vacation_taken)

# Create three bar charts
ggplot(survey_gathered, aes(x=measure, y=value, fill=department)) +
  geom_col(position="dodge") + 
  facet_wrap(~ measure, scales="free")


# Add the in_sales variable
survey_sales <- survey %>%
  mutate(in_sales = ifelse(department == "Sales", "Sales", "Other"), 
         disengaged = ifelse(engagement < 3, 1L, 0L)
         )

# Test the hypothesis using survey_sales
chisq.test(survey_sales$disengaged, survey_sales$in_sales)
t.test(disengaged ~ in_sales, data=survey_sales)


# Test the hypothesis using the survey_sales data
t.test(vacation_days_taken ~ in_sales, data = survey_sales)

```
  
  
  
***
  
Chapter 3 - Are new hires getting paid too much?  
  
Paying new hires fairly:  
  
* Sometimes, current employees get paid less than new employees, which can drive low engagement and turnover  
* Case study will have a simulated pay dataset available for analysis  
* Can use broom::tidy() to return the outputs in a nicely formatted data frame  
	* chisq.test(survey$in_sales, survey$disengaged) %>% tidy()  
  
Omitted variable bias:  
  
* Key assumption of the tests is that the groups are the same, with the exception of the variables being tested  
* Omitted variable bias occurs when both 1) the omitted variable is correlated with the dependent variable, and 2) the omitted variable is correlated with an explanatory variable  
	* Omitted variables are often known as confounders  
    * Plotting can help to identify the issue, particularly with a stacked (to 100%) bar chart  
    * pay %>% ggplot(aes(x = new_hire, fill = department)) + geom_bar(position = "fill")  
    * The geom_bar() object has height that is fully dependent on x, in contrast to geom_col() which has a y-aestehtic  
  
Linear regression helps to test the multivariate impacts of variables:  
  
* lm(salary ~ new_hire, data = pay) %>% tidy()  # single dependent variable  
* lm(salary ~ new_hire + department, data = pay) %>% tidy()  # multiple dependent variables  
* lm(salary ~ new_hire + department, data = pay) %>% summary()  # more detailed summary of the linear regression  
  
Example code includes:  
```{r}

# Import the data
pay <- readr::read_csv("./RInputFiles/fair_pay_data.csv")

# Get an overview of the data
summary(pay)

# Check average salary of new hires and non-new hires
pay %>% 
  group_by(new_hire) %>%
  summarize(avg_salary=mean(salary))


# Perform the correct statistical test
t.test(salary ~ new_hire, data = pay)
t.test(salary ~ new_hire, data = pay) %>%
  broom::tidy()


# Create a stacked bar chart
pay %>%
  ggplot(aes(x=new_hire, fill=job_level)) + 
  geom_bar(position="fill")

# Calculate the average salary for each group of interest
pay_grouped <- pay %>% 
  group_by(new_hire, job_level) %>% 
  summarize(avg_salary = mean(salary))
  
# Graph the results using facet_wrap()  
pay_grouped %>%
  ggplot(aes(x=new_hire, y=avg_salary)) + 
  geom_col() + 
  facet_wrap(~ job_level)


# Filter the data to include only hourly employees
pay_filter <- pay %>%
  filter(job_level == "Hourly")

# Test the difference in pay
t.test(salary ~ new_hire, data=pay_filter) %>%
  broom::tidy()


# Run the simple regression
model_simple <- lm(salary ~ new_hire, data = pay)

# Display the summary of model_simple
model_simple %>% 
  summary()

# Display a tidy summary
model_simple %>% 
  broom::tidy()


# Run the multiple regression
model_multiple <- lm(salary ~ new_hire + job_level, data = pay)

# Display the summary of model_multiple
model_multiple %>% 
  summary()

# Display a tidy summary
model_multiple %>% 
  broom::tidy()

```
  
  
  
***
  
Chapter 4 - Are performance ratings being given consistently?  
  
Joining HR data:  
  
* Employee data tend to be stored in different locations, requiring joins (merges) prior to running analyses  
	* dplyr::left_join(hr_data, bonus_pay_data, by = "employee_id")  
    * All employees in hr_data will be kept, even if there is no matching record in bonus_pay_data  
    * Employee ID (or similar) is by far the best way to join data - names tend to be non-unique and can differ in different systems  
  
Performance ratings and fairness:  
  
* Performance ratings are inherently subjective and thus prone to bias  
* Unconscious bias is based on the brain's heuristics, and may include preferences for members of various groups (biases, as reflected in hiring, promotion, etc.)  
  
Logistic regression is especially helpful for modeling binary response variables:  
  
* glm(high_performer ~ salary, data = hr, family = "binomial") %>% tidy()  
* glm(high_performer ~ salary + department, data = hr, family = "binomial") %>% tidy()  
  
Example code includes:  
```{r}

# Import the data
hr_data <- readr::read_csv("./RInputFiles/hr_data.csv")
performance_data <- readr::read_csv("./RInputFiles/performance_data.csv")

# Examine the datasets
summary(hr_data)
summary(performance_data)


# Join the two tables
joined_data <- left_join(hr_data, performance_data, by = "employee_id")

# Examine the result
summary(joined_data)

# Check whether the average performance rating differs by gender 
joined_data %>%
  group_by(gender) %>%
  summarize(avg_rating = mean(rating))


# Add the high_performer column
performance <- joined_data %>%  
  mutate(high_performer = ifelse(rating >= 4, 1, 0))

# Test whether one gender is more likely to be a high performer
chisq.test(performance$gender, performance$high_performer)   
 
# Do the same test, and tidy the output
chisq.test(performance$gender, performance$high_performer) %>% broom::tidy()


# Visualize the distribution of high_performer by gender
performance %>%
  ggplot(aes(x=gender, fill=factor(high_performer))) + 
  geom_bar(position="fill")

# Visualize the distribution of all ratings by gender
performance %>%
  ggplot(aes(x=gender, fill=factor(rating))) + 
  geom_bar(position="fill")

# Visualize the distribution of job_level by gender
performance %>%
  ggplot(aes(x = gender, fill = job_level)) +
  geom_bar(position = "fill")
 
# Test whether men and women have different job level distributions
chisq.test(performance$gender, performance$job_level) 


# Visualize the distribution of high_performer by gender, faceted by job level
performance %>%
  ggplot(aes(x = gender, fill = factor(high_performer))) +
  geom_bar(position = "fill") + 
  facet_wrap(~ job_level)


# Run a simple logistic regression
logistic_simple <- glm(high_performer ~ gender, family = "binomial", data = performance) 

# View the result with summary()
logistic_simple %>%
  summary()

# View a tidy version of the result
logistic_simple %>%
  broom::tidy()


# Run a multiple logistic regression
logistic_multiple <- glm(high_performer ~ gender + job_level, family = "binomial", data = performance)

# View the result with summary() or tidy()
logistic_multiple %>% broom::tidy()

```
  
  
  
***
  
Chapter 5 - Improving employee safety with data  
  
Employee safety - looking at accident rates and drivers:  
  
* Requires joining data on multiple variables  
	* joined_data <- left_join(hr_data, safety_data, by = c("year", "employee_id"))  
    * joined_data %>% filter(is.na(accident_time)) # use is.na() instead  
  
Focusing on the location of interest:  
  
* May want to run comparisons of the same location over time  
* May want to assess differences by locations to see if they may be explanatory variables  
  
Explaining the increase in accidents:  
  
* Can use multiple regression to help test for explanatory variables that impact the accident rate  
  
Wrap up:  
  
* Key tools from the Tidyverse (ggplot2, broom, dplyr, etc.) to assess HR data  
* Analytics usage within HR, including differences in HR and other data  
* Can apply additional data science techniques on HR data  
  
Example code includes:  
```{r}

# Import the data 
hr_data <- readr::read_csv("./RInputFiles/hr_data_2.csv")
accident_data <- readr::read_csv("./RInputFiles/accident_data.csv")

# Create hr_joined with left_join() and mutate()
hr_joined <- left_join(hr_data, accident_data, by=c("year", "employee_id")) %>% 
  mutate(had_accident=ifelse(is.na(accident_type), 0, 1))
  
hr_joined


# Find accident rate for each year
hr_joined %>% 
  group_by(year) %>% 
  summarize(accident_rate = mean(had_accident))

# Test difference in accident rate between years
chisq.test(hr_joined$year, hr_joined$had_accident)

# Which location had the highest acccident rate?
hr_joined %>%
  group_by(location) %>%
  summarize(accident_rate=mean(had_accident)) %>%
  arrange(-accident_rate)


# Compare annual accident rates by location
accident_rates <- hr_joined %>% 
  group_by(location, year) %>% 
  summarize(accident_rate = mean(had_accident))
  
accident_rates

# Graph it
accident_rates %>% 
  ggplot(aes(factor(year), accident_rate)) +
  geom_col() +
  facet_wrap(~location)


# Filter out the other locations
southfield <- hr_joined %>% 
  filter(location == "Southfield")

# Find the average overtime hours worked by year
southfield %>%
  group_by(year) %>% 
  summarize(average_overtime_hours = mean(overtime_hours))

# Test difference in Southfield's overtime hours between years
t.test(overtime_hours ~ year, data=southfield) 


# Import the survey data
survey_data <- readr::read_csv("./RInputFiles/survey_data_2.csv")

# Create the safety dataset
safety <- left_join(hr_joined, survey_data, by=c("employee_id", "year")) %>%
  mutate(disengaged=ifelse(engagement <= 2, 1, 0), year=factor(year))


# Visualize the difference in % disengaged by year in Southfield
safety %>% 
    filter(location=="Southfield") %>%
    ggplot(aes(x = year, fill = factor(disengaged))) +
    geom_bar(position = "fill")
 
# Test whether one year had significantly more disengaged employees
southSafety <- safety %>% 
    filter(location=="Southfield")
chisq.test(southSafety$disengaged, southSafety$year)


# Filter out Southfield
other_locs <- safety %>% 
  filter(location != "Southfield")

# Test whether one year had significantly more overtime hours worked
t.test(overtime_hours ~ year, data = other_locs) 

# Test whether one year had significantly more disengaged employees
chisq.test(other_locs$year, other_locs$disengaged)


# Use multiple regression to test the impact of year and disengaged on accident rate in Southfield
regression <- glm(had_accident ~ year + disengaged, family = "binomial", data = southSafety)

# Examine the output
regression %>% broom::tidy()

```
  
  
  
***
  
###_Supervised Learning in R: Case Studies_  
  
Chapter 1 - Cars Data  
  
Making predictions using machine learning:  
  
* Course focuses on applied skills from predictive learning, using regression and classification as well as EDA  
	* Regression tends to be for predicting continuous, numeric variables  
    * Classification tends to be for predicting categorical variables  
* Case studies include 1) fuel efficiency, 2) Stack Overflow developer survey, 3) voter turnout, and 4) ages of nuns  
* The fuel efficiency data is stored in cars2018 and is based on data from the US Department of Energy  
	* Variables names with spaces can be handled by surrounding them with backticks  
    * Tidyverse includes tibble, readr, ggplot2, dplyr, tidyr, purrr, etc. - can be loaded as a package using library(tidyverse)  
  
Getting started with caret:  
  
* The caret package is useful for predictive modeling - full process including the test/train split for the raw dataset  
	* in_train <- createDataPartition(cars_vars$Aspiration, p = 0.8, list = FALSE)  # will stratify on 'Aspiration' variable  
    * training <- cars_vars[in_train,]  
    * testing <- cars_vars[-in_train,]  
* Can then train the model using only the training dataset  
	* fit_lm <- train(log(MPG) ~ ., method = "lm", data=training, trControl=trainControl(method = "none"))  
    * Can then use the yardstick package to assess the quality of the model  
  
Sampling data:  
  
* Bootstrap resampling means sampling with replacement, and then fitting on the resampled dataset (run multiple times)  
	* cars_rf_bt <- train(log(MPG) ~ ., method = "rf", data = training, trControl = trainControl(method = "boot"))  # default 25 resamples  
    * Can both visualize the models and assess the model statistically  
  
Example code includes:  
```{r cache=TRUE}

cars2018 <- readr::read_csv("./RInputFiles/cars2018.csv")
str(cars2018, give.attr = FALSE)
summary(cars2018)

# Print the cars2018 object
cars2018

# Plot the histogram
ggplot(cars2018, aes(x = MPG)) +
    geom_histogram(bins = 25) +
    labs(y = "Number of cars",
         x = "Fuel efficiency (mpg)")


# Deselect the 2 columns to create cars_vars
cars_vars <- cars2018 %>%
    select(-Model, -`Model Index`)

# Fit a linear model
fit_all <- lm(MPG ~ ., data = cars_vars)

# Print the summary of the model
summary(fit_all)


# Load caret
library(caret)

# Split the data into training and test sets
set.seed(1234)
in_train <- createDataPartition(cars_vars$Transmission, p = 0.8, list = FALSE)
training <- cars_vars[in_train, ]
testing <- cars_vars[-in_train, ]

# Train a linear regression model
fit_lm <- train(log(MPG) ~ ., method = "lm", data = training,
                trControl = trainControl(method = "none"))

# Print the model object
fit_lm


# Train a random forest model
fit_rf <- train(log(MPG) ~ ., method = "rf", data = training,
                trControl = trainControl(method = "none"))

# Print the model object
fit_rf


# Create the new columns
results <- training %>%
    mutate(`Linear regression` = predict(fit_lm, training),
           `Random forest` = predict(fit_rf, training))

# Evaluate the performance
yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)


# Create the new columns
results <- testing %>%
    mutate(`Linear regression` = predict(fit_lm, testing),
           `Random forest` = predict(fit_rf, testing))

# Evaluate the performance
yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)


# Fit the models with bootstrap resampling
cars_lm_bt <- train(log(MPG) ~ ., method = "lm", data = training,
                   trControl = trainControl(method = "boot"))
cars_rf_bt <- train(log(MPG) ~ ., method = "rf", data = training,
                   trControl = trainControl(method = "boot"))
                   
# Quick look at the models
cars_lm_bt
cars_rf_bt


results <- testing %>%
    mutate(`Linear regression` = predict(cars_lm_bt, testing),
           `Random forest` = predict(cars_rf_bt, testing))

yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)

results %>%
    gather(Method, Result, `Linear regression`:`Random forest`) %>%
    ggplot(aes(log(MPG), Result, color = Method)) +
    geom_point(size = 1.5, alpha = 0.5) +
    facet_wrap(~Method) +
    geom_abline(lty = 2, color = "gray50") +
    geom_smooth(method = "lm")

```
  
  
  
***
  
Chapter 2 - Stack Overflow Developer Data  
  
Essential copying and pasting from Stack Overflow (largest and most trusted developer community):  
  
* Annual survey of developer perspectives on Stack Overflow - can be used for predictive modeling  
* Data is made available publicly at insights.stackoverflow.com/survey  
* Key question is "what makes a developer more likely to work remotely" (size of company, geography of employee, etc.)  
	* Data are calss imbalanced, with many more Non-Remote employees than Remote employees  
    * Best first step is the simplest model - logit, without any tricks  
    * simple_glm <- stackoverflow %>% select(-Respondent) %>% glm(Remote ~ ., family = "binomial", + data = .)  # Remote ~ . Means "all variables" while data=. Means from the piped dataset  
  
Dealing with imbalanced data:  
  
* Class imbalance is a common problem that can negatively impact model performance  
	* This dataset has 10x the number of non-remote, which can influence models to just start predicting non-remote in all cases  
* One approach to class imbalance is upsampling, basically running resampling with replacement on the small class until it is the same size as the large class  
	* Simple to implement, but with the risk of over-fitting  
    * up_train <- upSample(x = select(training, -Remote), y = training$Remote, yname = "Remote") %>% as_tibble()  
    * stack_glm <- train(Remote ~ ., method = "glm", family = "binomial", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
  
Predicting remote status:  
  
* Classification models can include logistic regression and random forests  
	* stack_glm <- train(Remote ~ ., method = "glm", family = "binomial", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
    * stack_rf <- train(Remote ~ ., method = "rf", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
* Classification models can be evaluated using the confusion matrix  
	* confusionMatrix(predict(stack_glm, testing), testing$Remote)  
    * yardstick::accuracy(testing_results, truth = Remote, estimate = `Logistic regression`)  
    * yardstick::ppv(testing_results, truth = Remote, estimate = `Logistic regression`)  
    * yardstick::npv(testing_results, truth = Remote, estimate = `Logistic regression`)  
  
Example code includes:  
```{r cache=TRUE}

stackoverflow <- readr::read_csv("./RInputFiles/stackoverflow.csv")
stackoverflow$Remote <- factor(stackoverflow$Remote, levels=c("Not remote", "Remote"))
str(stackoverflow, give.attr = FALSE)


# Print stackoverflow
stackoverflow

# First count for Remote
stackoverflow %>% 
    count(Remote, sort = TRUE)

# then count for Country
stackoverflow %>% 
    count(Country, sort = TRUE)


ggplot(stackoverflow, aes(x=Remote, y=YearsCodedJob)) +
    geom_boxplot() +
    labs(x = NULL,
         y = "Years of professional coding experience") 


# Build a simple logistic regression model
simple_glm <- stackoverflow %>%
        select(-Respondent) %>%
        glm(Remote ~ .,
            family = "binomial",
            data = .)

# Print the summary of the model
summary(simple_glm)


stack_select <- stackoverflow %>%
    select(-Respondent)

# Split the data into training and testing sets
set.seed(1234)
in_train <- caret::createDataPartition(stack_select$Remote, p=0.8, list = FALSE)
training <- stack_select[in_train,]
testing <- stack_select[-in_train,]


up_train <- caret::upSample(x = select(training, -Remote), y = training$Remote, yname = "Remote") %>%
    as_tibble()

up_train %>%
    count(Remote)


# Sub-sample to 5% of original
inUse <- sample(1:nrow(training), round(0.05*nrow(training)), replace=FALSE)
useTrain <- training[sort(inUse), ]

# Build a logistic regression model
stack_glm <- caret::train(Remote ~ ., method="glm", family="binomial", data = training, 
                          trControl = trainControl(method = "boot", sampling = "up")
                          )

# Print the model object 
stack_glm


# Build a random forest model
stack_rf <- caret::train(Remote ~ ., method="rf", data = useTrain, 
                         trControl = trainControl(method = "boot", sampling="up")
                         )

# Print the model object
stack_rf


# Confusion matrix for logistic regression model
caret::confusionMatrix(predict(stack_glm, testing), testing$Remote)

# Confusion matrix for random forest model
caret::confusionMatrix(predict(stack_rf, testing), testing$Remote)


# Predict values
testing_results <- testing %>%
    mutate(`Logistic regression` = predict(stack_glm, testing), `Random forest` = predict(stack_rf, testing))

## Calculate accuracy
yardstick::accuracy(testing_results, truth = Remote, estimate = `Logistic regression`)
yardstick::accuracy(testing_results, truth = Remote, estimate = `Random forest`)

## Calculate positive predict value
yardstick::ppv(testing_results, truth = Remote, estimate = `Logistic regression`)
yardstick::ppv(testing_results, truth = Remote, estimate = `Random forest`)

```
  
  
  
***
  
Chapter 3 - Voting  
  
Predicting voter turnout from survey data:  
  
* Survey data available from https://www.voterstudygroup.org/publications/2016-elections/data  
	* Opinions about political and economic topics  
    * Includes whether the voter turned out (voted), based on self-reporting, in the 2016 election  
    * Data are coded as integers, requiring a data dictionary to map the questions and responses to what they mean  
  
Vote 2016:  
  
* Exploratory data analysis will help with learning about the underlying dataset  
	* There are differences on many of the individual dimensions between voters and non-voters  
    * A good first step can be to start with the very simplest model, Dependent ~ .  
  
Cross-validation is the process of sub-dividing the data into folds, with each fold used once as the validation set:  
  
* Allows for more accurate estimates of model performance on out-of-sample error  
* Each process of CV will work through the data k times (assuming there are k folds)  
	* Repeated CV is the process of running CV multiple times (this is particularly well suited to parallel processing)  
  
Comparing model performance:  
  
* Random forest models tend to be more powerful and capable of classifying the training data (and thus subject to risk of overfits and associated poor quality of test set predictions)  
  
Example code includes:  
```{r cache=TRUE}

voters <- readr::read_csv("./RInputFiles/voters.csv")
voters$turnout16_2016 <- factor(voters$turnout16_2016, levels=c("Did not vote", "Voted"))
str(voters, give.attr = FALSE)

# Print voters
voters

# How many people voted?
voters %>%
    count(turnout16_2016)


# How do the reponses on the survey vary with voting behavior?
voters %>%
    group_by(turnout16_2016) %>%
    summarize(`Elections don't matter` = mean(RIGGED_SYSTEM_1_2016 <= 2),
              `Economy is getting better` = mean(econtrend_2016 == 1),
              `Crime is very important` = mean(imiss_a_2016 == 2))


## Visualize difference by voter turnout
voters %>%
    ggplot(aes(econtrend_2016, ..density.., fill = turnout16_2016)) +
    geom_histogram(alpha = 0.5, position = "identity", binwidth = 1) +
    labs(title = "Overall, is the economy getting better or worse?")


# Remove the case_indetifier column
voters_select <- voters %>%
        select(-case_identifier)

# Build a simple logistic regression model
simple_glm <- glm(turnout16_2016 ~ .,  family = "binomial", 
                  data = voters_select)

# Print the summary                  
summary(simple_glm)


# Split data into training and testing sets
set.seed(1234)
in_train <- caret::createDataPartition(voters_select$turnout16_2016, p = 0.8, list = FALSE)
training <- voters_select[in_train, ]
testing <- voters_select[-in_train, ]


# Perform logistic regression with upsampling and no resampling
vote_glm_1 <- caret::train(turnout16_2016 ~ ., method = "glm", family = "binomial", data = training,
                           trControl = trainControl(method = "none", sampling = "up")
                           )

# Print vote_glm
vote_glm_1


useSmall <- sort(sample(1:nrow(training), round(0.1*nrow(training)), replace=FALSE))
trainSmall <- training[useSmall, ]

# Logistic regression
vote_glm <- caret::train(turnout16_2016 ~ ., method = "glm", family = "binomial", data = trainSmall,
                         trControl = trainControl(method = "repeatedcv", repeats = 2, sampling = "up")
                         )

# Print vote_glm
vote_glm


# Random forest
vote_rf <- caret::train(turnout16_2016 ~ ., method = "rf", data = trainSmall,
                        trControl = trainControl(method="repeatedcv", repeats=2, sampling = "up")
                        )

# Print vote_rf
vote_rf


# Confusion matrix for logistic regression model on training data
caret::confusionMatrix(predict(vote_glm, trainSmall), trainSmall$turnout16_2016)

# Confusion matrix for random forest model on training data
caret::confusionMatrix(predict(vote_rf, trainSmall), trainSmall$turnout16_2016)

# Confusion matrix for logistic regression model on testing data
caret::confusionMatrix(predict(vote_glm, testing), testing$turnout16_2016)

# Confusion matrix for random forest model on testing data
caret::confusionMatrix(predict(vote_rf, testing), testing$turnout16_2016)

```
  
  
  
***
  
Chapter 4 - Nuns  
  
Catholic sisters survey from 1967 - https://curate.nd.edu/show/0r967368551 with codebook at https://curate.nd.edu/downloads/0v838051f6x	 
	
* Responses from 130,000 sisters in ~400 congergations  
* There was significant change occuring during this time period, both in society at large and within the community of nuns  
* Age has been binned in groups of 10 years (has been recoded as a numeric at the top of the range, so 20 will mean 11-20 and 30 will mean 21-30 and the like)  
* Historical dataset, centered in the context of nuns in 1967  
* Good first step is to tidy the data, so that it is easier for exploratory data analysis  
	* sisters67 %>% select(-sister) %>% gather(key, value, -age)  
  
Exploratory data analysis with tidy data:  
  
* Easy to see levels of agreement (overall) using dplyr::count()  
* Agreement with specific questions by age  
	* tidy_sisters %>% filter(key %in% paste0("v", 153:170)) %>% group_by(key, value) %>% summarise(age = mean(age)) %>% ggplot(aes(value, age, color = key)) + geom_line(alpha = 0.5, size = 1.5) + geom_point(size = 2) + facet_wrap(~key)  
	* Can use the mix of responses to make estimates about the ages of the nuns  
* Data will be split in to training, validation, and test sets  
	* The validation set will be used for model selection  
  
Predicting age with supervised learning:  
  
* "rpart" - building a tree-based (CART) model  
* "xgbLinear" - extreme gradient boosting  
* "gbm" - gradient boosted ensembles  
* Validation datasets are useful for assessing hyper-parameters and model choices, leaving the test dataset pure for a final out-of-sample error estimate  
  
Wrap up:  
  
* Train-Validation-Test to select the best models, tune the parameters, and estimate the out-of-sample error rates  
* Dealing with class imbalances; improving performance with resamples (bootstraps, cross-validation, etc.)  
* Hyper-parameter tuning can be valuable, but the time investment in other areas can often generate a greater return  
* Gradient boosting and random forests tend to perform very well, but there is always value in trying out multiple models  
	* Start with EDA and begin with a very simple model  
  
Example code includes:  
```{r cache=TRUE}

sisters67 <- readr::read_csv("./RInputFiles/sisters.csv")
str(sisters67, give.attr = FALSE)


# View sisters67
glimpse(sisters67)

# Plot the histogram
ggplot(sisters67, aes(x = age)) +
    geom_histogram(binwidth = 10)


# Tidy the data set
tidy_sisters <- sisters67 %>%
    select(-sister) %>%
    gather(key, value, -age)

# Print the structure of tidy_sisters
glimpse(tidy_sisters)


# Overall agreement with all questions varied by age
tidy_sisters %>%
    group_by(age) %>%
    summarize(value = mean(value, na.rm = TRUE))

# Number of respondents agreed or disagreed overall
tidy_sisters %>%
    count(value)


# Visualize agreement with age
tidy_sisters %>%
    filter(key %in% paste0("v", 153:170)) %>%
    group_by(key, value) %>%
    summarize(age = mean(age, na.rm = TRUE)) %>%
    ggplot(aes(value, age, color = key)) +
    geom_line(show.legend = FALSE) +
    facet_wrap(~key, nrow = 3)


# Remove the sister column
sisters_select <- sisters67 %>% 
    select(-sister)

# Build a simple linear regression model
simple_lm <- lm(age ~ ., 
                data = sisters_select)

# Print the summary of the model
summary(simple_lm)


# Split the data into training and validation/test sets
set.seed(1234)
in_train <- caret::createDataPartition(sisters_select$age, p = 0.6, list = FALSE)
training <- sisters_select[in_train, ]
validation_test <- sisters_select[-in_train, ]

# Split the validation and test sets
set.seed(1234)
in_test <- caret::createDataPartition(validation_test$age, p = 0.5, list = FALSE)
testing <- validation_test[in_test, ]
validation <- validation_test[-in_test, ]


# Fit a CART model
sisters_cart <- caret::train(age ~ ., method = "rpart", data = training)

# Print the CART model
sisters_cart


inSmall <- sample(1:nrow(training), 500, replace=FALSE)
smallSisters <- training[sort(inSmall), ]

sisters_xgb <- caret::train(age ~ ., method = "xgbTree", data = smallSisters)
sisters_gbm <- caret::train(age ~ ., method = "gbm", data = smallSisters, verbose=FALSE)

# Make predictions on the three models
modeling_results <- validation %>%
    mutate(CART = predict(sisters_cart, validation),
           XGB = predict(sisters_xgb, validation),
           GBM = predict(sisters_gbm, validation))

# View the predictions
modeling_results %>% 
    select(CART, XGB, GBM)


# Compare performace
yardstick::metrics(modeling_results, truth = age, estimate = CART)
yardstick::metrics(modeling_results, truth = age, estimate = XGB)
yardstick::metrics(modeling_results, truth = age, estimate = GBM)


# Calculate RMSE
testing %>%
    mutate(prediction = predict(sisters_gbm, testing)) %>%
    yardstick::rmse(truth = age, estimate = prediction)

```
  
  
  
***
  
###_Business Process Analytics in R_  
  
Chapter 1 - Introduction to Process Analysis  
  
Introduction and overview:  
  
* Efficient processes are core to many businesses, and improved data makes further analysis possible  
* The "internet of things" has created significant amounts of event data - why, what, and who  
	* Why is the purpose  
    * What is the steps in the process  
    * Who is the person responsible for the activity (can be machines or IS or the like; referred to as "resources")  
* Process workflow is iterative across Extraction-Processing-Analysis  
  
Activities as cornerstones of processes:  
  
* Data from an online learning platform; activities are captured and can be used for further analysis  
* Activities describe the flow of the process, and are one of the most important components of the process  
	* bupaR::activities_labels() is like names() for activities data  
    * bupaR::activities() is like summary() for activities data  
* Each case is described by the sequence of activities, known as its "trace"  
	* bupaR::traces() will create a frequency table of the traces  
    * bupaR::trace_explorer() will visualize the cases  
  
Components of process data:  
  
* Cases are the objects flowing through the process, while activities are the actions performed on them  
	* An activity instance is the occurrence of an activity (which can be a series of events) - specific action, case, time, etc.  
    * The "lifecycle status" is an area like Scheduled, Started, Completed, and the like  
    * The "event log" is the journal of the events  
    * The "resources" are the actors in the process  
* Can create an event log using the eventlog() function  
	* event_data %>% eventlog(case_id = "patient", activity_id = "handling", activity_instance_id = "handling_id", timestamp = "time", lifecycle_id = "registration_type", resource = "employee")  
  
Example code includes:  
```{r}

# Load the processmapR package using library
library(processmapR)
library(bupaR)


handling <- c('Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'X-Ray', 'X-Ray', 'X-Ray', 'X-Ray', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'X-Ray', 'X-Ray', 'X-Ray', 'X-Ray', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out')
patient <- c('43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '156', '170', '172', '184', '278', '348', '420', '43', '156', '170', '172', '184', '278', '348', '420', '155', '221', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '156', '170', '172', '184', '278', '348', '420', '43', '156', '170', '172', '184', '278', '348', '420', '155', '221', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493')
employee <- c('r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r5', 'r5', 'r5', 'r5', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r5', 'r5', 'r5', 'r5', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7')
handling_id <- c('43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '543', '655', '656', '670', '672', '684', '721', '778', '848', '920', '955', '993', '1020', '1072', '1081', '1082', '1088', '1127', '1163', '1199', '1257', '1309', '1318', '1319', '1325', '1364', '1400', '1436', '1557', '1587', '1710', '1730', '1777', '1889', '1890', '1904', '1906', '1918', '1955', '2012', '2082', '2154', '2189', '2227', '2272', '2384', '2385', '2399', '2401', '2413', '2450', '2507', '2577', '2649', '2684', '2720', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '543', '655', '656', '670', '672', '684', '721', '778', '848', '920', '955', '993', '1020', '1072', '1081', '1082', '1088', '1127', '1163', '1199', '1257', '1309', '1318', '1319', '1325', '1364', '1400', '1436', '1557', '1587', '1710', '1730', '1777', '1889', '1890', '1904', '1906', '1918', '1955', '2012', '2082', '2154', '2189', '2227', '2272', '2384', '2385', '2399', '2401', '2413', '2450', '2507', '2577', '2649', '2684', '2720')
registration_type <- c('start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete')
rTime <- c('2017-02-19 04:38:51', '2017-06-03 10:05:28', '2017-06-03 10:05:28', '2017-06-17 15:10:30', '2017-06-17 23:00:33', '2017-06-27 07:48:22', '2017-08-03 17:05:27', '2017-09-26 20:22:49', '2017-11-24 08:28:44', '2018-02-08 03:39:21', '2018-03-14 21:04:28', '2018-04-29 04:55:10', '2017-02-19 07:28:53', '2017-06-04 06:27:00', '2017-06-03 13:23:14', '2017-06-17 16:31:58', '2017-06-18 18:29:13', '2017-06-28 00:14:50', '2017-08-04 07:22:06', '2017-09-27 22:57:03', '2017-11-24 10:33:00', '2018-02-08 17:33:12', '2018-03-15 15:12:41', '2018-04-30 19:40:22', '2017-02-20 19:59:18', '2017-06-04 15:18:50', '2017-06-18 22:51:07', '2017-06-21 02:43:27', '2017-07-01 23:55:10', '2017-09-28 22:58:23', '2017-11-25 12:06:18', '2018-02-12 09:01:38', '2017-02-21 06:49:49', '2017-06-04 23:23:28', '2017-06-19 06:44:30', '2017-06-21 11:16:30', '2017-07-02 11:16:08', '2017-09-29 07:28:10', '2017-11-25 21:54:56', '2018-02-12 19:43:42', '2017-06-05 00:12:24', '2017-08-05 08:25:17', '2018-03-17 10:30:24', '2018-05-02 07:32:45', '2017-02-21 14:50:43', '2017-06-05 14:03:19', '2017-06-05 10:26:16', '2017-06-19 22:46:10', '2017-06-22 04:39:35', '2017-07-03 01:28:49', '2017-08-05 22:06:23', '2017-09-29 19:13:51', '2017-11-26 06:52:23', '2018-02-17 02:44:58', '2018-03-18 00:20:51', '2018-05-02 18:14:11', '2017-02-24 14:58:43', '2017-06-05 15:58:53', '2017-06-05 15:58:53', '2017-06-20 03:48:37', '2017-06-22 08:40:55', '2017-07-03 03:39:51', '2017-08-08 23:17:45', '2017-09-29 21:16:01', '2017-11-27 04:56:53', '2018-02-20 09:49:29', '2018-03-18 08:12:07', '2018-05-03 00:11:10', '2017-02-19 07:28:53', '2017-06-03 14:19:00', '2017-06-03 13:23:14', '2017-06-17 16:31:58', '2017-06-18 01:07:42', '2017-06-27 12:22:51', '2017-08-03 19:25:12', '2017-09-26 22:17:18', '2017-11-24 10:33:00', '2018-02-08 06:01:38', '2018-03-15 00:34:01', '2018-04-29 07:39:14', '2017-02-19 21:58:08', '2017-06-04 14:23:26', '2017-06-04 06:27:00', '2017-06-18 04:14:55', '2017-06-19 00:40:19', '2017-06-28 12:48:20', '2017-08-04 21:09:17', '2017-09-28 12:00:12', '2017-11-25 00:44:30', '2018-02-09 07:05:52', '2018-03-16 04:08:03', '2018-05-01 10:37:51', '2017-02-21 03:12:26', '2017-06-04 19:35:51', '2017-06-19 03:01:11', '2017-06-21 08:02:20', '2017-07-02 07:43:48', '2017-09-29 04:58:49', '2017-11-25 18:30:43', '2018-02-12 13:57:13', '2017-02-21 09:57:05', '2017-06-05 02:46:59', '2017-06-19 11:40:53', '2017-06-21 16:09:26', '2017-07-02 16:03:16', '2017-09-29 12:44:39', '2017-11-26 02:40:30', '2018-02-12 23:53:46', '2017-06-05 04:39:38', '2017-08-05 13:56:39', '2018-03-17 14:09:40', '2018-05-02 12:24:41', '2017-02-21 17:57:58', '2017-06-05 15:58:53', '2017-06-05 14:03:19', '2017-06-20 01:44:29', '2017-06-22 08:40:55', '2017-07-03 03:39:51', '2017-08-05 23:53:27', '2017-09-29 21:16:01', '2017-11-26 09:44:37', '2018-02-17 06:17:57', '2018-03-18 03:22:17', '2018-05-02 21:17:12', '2017-02-24 16:03:49', '2017-06-05 17:22:16', '2017-06-05 17:15:30', '2017-06-20 05:36:40', '2017-06-22 10:59:58', '2017-07-03 05:00:48', '2017-08-09 00:13:39', '2017-09-29 23:42:48', '2017-11-27 06:53:23', '2018-02-20 12:04:00', '2018-03-18 10:48:34', '2018-05-03 02:11:42')
rOrder <- c(43, 155, 156, 170, 172, 184, 221, 278, 348, 420, 455, 493, 543, 655, 656, 670, 672, 684, 721, 778, 848, 920, 955, 993, 1020, 1072, 1081, 1082, 1088, 1127, 1163, 1199, 1257, 1309, 1318, 1319, 1325, 1364, 1400, 1436, 1557, 1587, 1710, 1730, 1777, 1889, 1890, 1904, 1906, 1918, 1955, 2012, 2082, 2154, 2189, 2227, 2272, 2384, 2385, 2399, 2401, 2413, 2450, 2507, 2577, 2649, 2684, 2720, 2764, 2876, 2877, 2891, 2893, 2905, 2942, 2999, 3069, 3141, 3176, 3214, 3264, 3376, 3377, 3391, 3393, 3405, 3442, 3499, 3569, 3641, 3676, 3714, 3741, 3793, 3802, 3803, 3809, 3848, 3884, 3920, 3978, 4030, 4039, 4040, 4046, 4085, 4121, 4157, 4278, 4308, 4431, 4451, 4498, 4610, 4611, 4625, 4627, 4639, 4676, 4733, 4803, 4875, 4910, 4948, 4993, 5105, 5106, 5120, 5122, 5134, 5171, 5228, 5298, 5370, 5405, 5441)

pFrame <- tibble(handling=factor(handling, levels=c('Blood test', 'Check-out', 'Discuss Results', 'MRI SCAN', 'Registration', 'Triage and Assessment', 'X-Ray')), 
                 patient=patient, 
                 employee=factor(employee, levels=c('r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7')), 
                 handling_id=handling_id, 
                 registration_type=factor(registration_type, levels=c("complete", "start")), 
                 time=as.POSIXct(rTime), 
                 .order=rOrder
                 )

patients <- eventlog(pFrame,
    case_id = "patient",
    activity_id = "handling",
    activity_instance_id = "handling_id",
    lifecycle_id = "registration_type",
    timestamp = "time",
    resource_id = "employee")


# The function slice can be used to take a slice of cases out of the eventdata. slice(1:10) will select the first ten cases in the event log, where first is defined by the current ordering of the data.

# How many patients are there?
n_cases(patients)

# Print the summary of the data
summary(patients)

# Show the journey of the first patient
slice(patients, 1)


# How many distinct activities are there?
n_activities(patients)

# What are the names of the activities?
activity_labels(patients)

# Create a list of activities
activities(patients)


# Have a look at the different traces
traces(patients)

# How many are there?
n_traces(patients)

# Visualize the traces using trace_explorer
trace_explorer(patients, coverage=1)

# Draw process map
process_map(patients)


claims <- tibble(id=c("claim1", "claim1", "claim2", "claim2", "claim2"), 
                 action=c(10002L, 10011L, 10015L, 10024L, 10024L), 
                 action_type=c("Check Contract", "Pay Back Decision", "Check Contract", "Pay Back Decision", "Pay Back Decision"), 
                 date=as.Date(c("2008-01-12", "2008-03-22", "2008-01-13", "2008-03-23", "2008-04-14")), 
                 originator=c("Assistant 1", "Manager 2", "Assistant 6", "Manager 2", "Manager 2"), 
                 status=as.factor(c("start", "start", "start", "start", "complete"))
                 )
claims


#create eventlog claims_log 
claims_log <- eventlog(claims,
    case_id = "id",
    activity_id = "action_type",
    activity_instance_id = "action",
    lifecycle_id = "status",
    timestamp = "date",
    resource_id = "originator")

# Print summary
summary(claims_log)

# Check activity labels
activity_labels(claims_log)

# Once you have an eventlog, you can access its complete metadata using the function mapping or the functions case_id, activity_id etc., to inspect individual identifiers.

```
  
  
  
***
  
Chapter 2 - Analysis Techniques  
  
Organizational analysis:  
  
* Processes are always dependent on resources, even if automated (machines and algorithms can be resources)  
	* Who executes the task, how specialized is the knowledge, etc.  
    * resource_labels(log_hospital)  # will pull out the resources  
    * resources(log_hospital)  # will pull out frequencies by resource  
* Can create a resource-activity matrix  
	* A person who performs only a few activities is considered to be specialized in that activity  
    * If only one person ever performs a specific activity, then there is a high risk of "brain drain"  
    * The plot() function, applied to an event_log, will create the resource-activity matrix  
    * resource_map(log_hospital)  # shows arrows between the work flows  
  
Structuredness:  
  
* Control-flow refers to the succession of activities  
	* Each unique flow is referred to as a trace  
    * Metrics include entry/exit points, length of cases, presence of activities, rework, etc.  
    * log_healthcare %>% start_activities("activity") %>% plot()  
    * log_healthcare %>% end_activities("activity") %>% plot()  
* Rework is when the same activity is done multiple times for the same case  
	* Repetitions are when the activity is repeated after some intervening steps  
    * Sel-loops are when the activity is repeated immediately after itself  
* The precedence matrix shows the relationships between the activities in a more structured manner  
	* eventlog %>% precedence_matrix(type = "absolute") %>% plot  # can be type="relative" also  
  
Performance analysis:  
  
* Visuals can include performance process maps and dotted charts; metrics can include throughput time, processing time, idle time  
	* eventlog %>% process_map(type = frequency())  # normal process map  
    * eventlog %>% process_map(type = performance())  # performance process map  
* The dotted chart shows the freqency of activities over time; basically, a form of scatter plot  
	* throughput_time is total time, processing_time is the sum of activity time, idle_time is the sume of when nothing is happening  
  
Linking perspectives:  
  
* Granularity can help give the statistics at the desired levels  
	* <process_metric>(level = "log", "trace", "case", "activity", "resource", "resource-activity")  
* Categorical data can be leveraged using the group_by() functionality - each group will then be calculated separately  
	* eventlog %>% group_by(priority) %>% number_of_repetitions(level = "resource") %>% plot()  
  
Example code includes:  
```{r}


data(sepsis, package="eventdataR")
str(sepsis)


# Print list of resources
resource_frequency(sepsis, level="resource")

# Number of resources per activity
resource_frequency(sepsis, level = "activity")

# Plot Number of executions per resource-activity (not working in R 3.5.3)
# resource_frequency(sepsis, level = "resource-activity") %>% plot


# Calculate resource involvement
resource_involvement(sepsis, level="resource")

# Show graphically 
sepsis %>% resource_involvement(level = "resource") %>% plot

# Compare with resource frequency
resource_frequency(sepsis, level="resource")


# Min, max and average number of repetitions
sepsis %>% number_of_repetitions(level = "log")

# Plot repetitions per activity
sepsis %>% number_of_repetitions(level = "activity") %>% plot

# Number of repetitions per resources
sepsis %>% number_of_repetitions(level = "resource")


eci <- c('21', '21', '21', '21', '21', '21', '21', '21', '21', '31', '31', '31', '31', '31', '31', '31', '31', '31', '31', '41', '41', '41', '41', '41', '41', '41', '51', '51', '51', '51', '51', '51', '51', '61', '61', '61', '61', '61', '61', '91', '91', '91', '91', '91', '91', '101', '101', '101', '101', '101', '101', '111', '111', '111', '111', '121', '121', '121', '121', '121', '121', '121', '121', '121', '131', '131', '131', '131', '131', '131', '131', '131', '161', '161', '171', '171', '171', '171', '181', '181', '181', '181', '181', '181', '201', '201', '201', '201', '201', '201', '201', '12', '12', '12', '12', '12', '22', '22', '22', '22', '22', '22', '32', '32', '32', '32', '32', '32', '42', '42', '42', '42', '52', '52', '52', '52', '52', '82', '82', '82', '82', '82', '92', '92', '92', '92', '92', '102', '102', '102', '102', '102', '112', '112', '122', '122', '21', '21', '21', '21', '21', '21', '21', '21', '21', '31', '31', '31', '31', '31', '31', '31', '31', '31', '31', '41', '41', '41', '41', '41', '41', '41', '51', '51', '51', '51', '51', '51', '51', '61', '61', '61', '61', '61', '61', '91', '91', '91', '91', '91', '91', '101', '101', '101', '101', '101', '101', '111', '111', '111', '111', '121', '121', '121', '121', '121', '121', '121', '121', '121', '131', '131', '131', '131', '131', '131', '131', '131', '161', '161', '171', '171', '171', '171', '181', '181', '181', '181', '181', '181', '201', '201', '201', '201', '201', '201', '201', '12', '12', '12', '12', '12', '22', '22', '22', '22', '22', '22', '32', '32', '32', '32', '32', '32', '42', '42', '42', '42', '52', '52', '52', '52', '52', '82', '82', '82', '82', '82', '92', '92', '92', '92', '92', '102', '102', '102', '102', '102', '112', '112', '122', '122')
ea1 <- c('prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'prepareBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'snack', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'eatingLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast')
ea2 <- c('eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'prepareBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'snack', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'eatingLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast')
eaii <- c('9', '10', '19', '23', '24', '26', '36', '40', '41', '51', '52', '58', '60', '62', '63', '67', '69', '72', '73', '86', '87', '89', '90', '104', '105', '107', '119', '120', '128', '132', '133', '138', '139', '149', '150', '156', '159', '160', '164', '174', '175', '192', '194', '195', '198', '205', '206', '208', '211', '213', '214', '229', '236', '237', '239', '245', '251', '252', '253', '255', '259', '260', '262', '264', '271', '276', '281', '287', '292', '293', '297', '299', '310', '312', '331', '332', '336', '347', '363', '364', '374', '376', '387', '389', '434', '435', '447', '448', '450', '453', '454', '462', '463', '471', '472', '475', '483', '484', '487', '491', '492', '496', '508', '509', '512', '517', '518', '522', '536', '540', '541', '543', '562', '563', '565', '566', '572', '584', '585', '589', '590', '598', '615', '616', '618', '619', '627', '639', '640', '642', '643', '653', '665', '666', '682', '683', '9', '10', '19', '23', '24', '26', '36', '40', '41', '51', '52', '58', '60', '62', '63', '67', '69', '72', '73', '86', '87', '89', '90', '104', '105', '107', '119', '120', '128', '132', '133', '138', '139', '149', '150', '156', '159', '160', '164', '174', '175', '192', '194', '195', '198', '205', '206', '208', '211', '213', '214', '229', '236', '237', '239', '245', '251', '252', '253', '255', '259', '260', '262', '264', '271', '276', '281', '287', '292', '293', '297', '299', '310', '312', '331', '332', '336', '347', '363', '364', '374', '376', '387', '389', '434', '435', '447', '448', '450', '453', '454', '462', '463', '471', '472', '475', '483', '484', '487', '491', '492', '496', '508', '509', '512', '517', '518', '522', '536', '540', '541', '543', '562', '563', '565', '566', '572', '584', '585', '589', '590', '598', '615', '616', '618', '619', '627', '639', '640', '642', '643', '653', '665', '666', '682', '683')
elci <- c('start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete')
ets1 <- c('2012-11-12 09:42:02', '2012-11-12 09:52:33', '2012-11-12 11:05:44', '2012-11-12 13:45:49', '2012-11-12 13:48:49', '2012-11-12 15:23:00', '2012-11-12 18:47:29', '2012-11-12 22:35:21', '2012-11-12 22:35:21', '2012-11-13 08:56:37', '2012-11-13 09:04:54', '2012-11-13 10:14:04', '2012-11-13 13:47:45', '2012-11-13 14:08:24', '2012-11-13 14:19:01', '2012-11-13 17:34:23', '2012-11-13 18:51:51', '2012-11-13 23:05:07', '2012-11-13 23:17:07', '2012-11-14 09:06:08', '2012-11-14 09:17:48', '2012-11-14 10:38:16', '2012-11-14 10:44:16', '2012-11-14 21:30:09', '2012-11-14 21:37:09', '2012-11-14 22:14:23', '2012-11-15 09:37:15', '2012-11-15 09:47:12', '2012-11-15 10:11:08', '2012-11-15 14:35:27', '2012-11-15 14:41:27', '2012-11-15 22:07:26', '2012-11-15 22:26:02', '2012-11-16 10:39:14', '2012-11-16 10:52:56', '2012-11-16 12:09:10', '2012-11-16 14:13:00', '2012-11-16 14:19:00', '2012-11-16 18:11:36', '2012-11-19 10:13:23', '2012-11-19 10:25:00', '2012-11-19 15:55:22', '2012-11-19 21:47:27', '2012-11-19 21:59:27', '2012-11-19 22:31:06', '2012-11-20 10:20:00', '2012-11-20 10:21:02', '2012-11-20 11:00:16', '2012-11-20 13:03:28', '2012-11-20 14:25:11', '2012-11-20 14:41:22', '2012-11-21 10:01:00', '2012-11-21 15:02:08', '2012-11-21 15:15:08', '2012-11-21 17:50:29', '2012-11-22 01:40:42', '2012-11-22 10:19:15', '2012-11-22 10:26:15', '2012-11-22 11:02:27', '2012-11-22 11:56:06', '2012-11-22 15:05:51', '2012-11-22 15:12:55', '2012-11-22 16:43:08', '2012-11-22 18:15:32', '2012-11-23 00:36:00', '2012-11-23 01:03:00', '2012-11-23 09:49:00', '2012-11-23 12:53:06', '2012-11-23 14:01:08', '2012-11-23 14:23:08', '2012-11-23 16:57:24', '2012-11-23 17:58:00', '2012-11-26 09:06:12', '2012-11-26 09:57:12', '2012-11-27 10:20:26', '2012-11-27 10:30:50')
ets2 <- c('2012-11-27 11:54:15', '2012-11-27 19:46:15', '2012-11-28 09:27:15', '2012-11-28 09:34:15', '2012-11-28 12:28:02', '2012-11-28 13:16:33', '2012-11-28 19:30:08', '2012-11-28 22:15:02', '2012-11-30 10:43:19', '2012-11-30 10:46:19', '2012-11-30 14:51:36', '2012-11-30 15:08:36', '2012-11-30 17:30:40', '2012-11-30 22:12:05', '2012-11-30 22:16:07', '2011-11-28 10:38:00', '2011-11-28 10:43:00', '2011-11-28 14:31:06', '2011-11-28 14:42:00', '2011-11-28 20:20:55', '2011-11-29 12:09:09', '2011-11-29 12:11:01', '2011-11-29 13:25:29', '2011-11-29 15:15:14', '2011-11-29 15:23:00', '2011-11-29 16:32:20', '2011-11-30 10:23:46', '2011-11-30 10:28:46', '2011-11-30 13:05:27', '2011-11-30 14:39:42', '2011-11-30 14:56:00', '2011-11-30 16:41:05', '2011-11-30 14:37:00', '2011-12-01 11:17:05', '2011-12-01 11:20:05', '2011-12-01 14:29:37', '2011-12-02 12:29:08', '2011-12-02 12:32:08', '2011-12-02 14:47:18', '2011-12-02 14:51:00', '2011-12-02 19:40:44', '2011-12-05 12:15:45', '2011-12-05 12:18:05', '2011-12-05 15:00:55', '2011-12-05 15:14:00', '2011-12-05 19:24:11', '2011-12-06 11:30:19', '2011-12-06 11:33:02', '2011-12-06 14:41:16', '2011-12-06 14:56:00', '2011-12-06 19:22:50', '2011-12-07 11:12:17', '2011-12-07 11:17:22', '2011-12-07 14:04:32', '2011-12-07 14:14:00', '2011-12-07 19:23:55', '2011-12-08 11:25:12', '2011-12-08 11:29:01', '2011-12-09 11:00:13', '2011-12-09 11:03:33', '2012-11-12 09:50:02', '2012-11-12 09:55:29', '2012-11-12 12:39:42', '2012-11-12 14:48:14', '2012-11-12 14:53:14', '2012-11-12 15:31:53', '2012-11-12 19:00:56', '2012-11-12 22:37:55', '2012-11-12 22:40:55', '2012-11-13 09:00:26', '2012-11-13 09:10:12', '2012-11-13 10:51:55', '2012-11-13 14:03:31', '2012-11-13 14:18:36', '2012-11-13 14:42:36', '2012-11-13 17:36:34', '2012-11-13 19:45:03', '2012-11-13 23:15:33', '2012-11-13 23:37:33', '2012-11-14 09:09:41', '2012-11-14 09:21:43', '2012-11-14 11:43:23', '2012-11-14 11:06:23', '2012-11-14 21:35:17', '2012-11-14 21:47:18', '2012-11-14 22:17:47', '2012-11-15 09:44:06', '2012-11-15 09:48:08', '2012-11-15 10:23:49', '2012-11-15 15:40:32', '2012-11-15 15:46:32', '2012-11-15 22:22:44', '2012-11-15 22:31:00', '2012-11-16 10:42:13') 
ets3 <- c('2012-11-16 10:52:58', '2012-11-16 12:09:57', '2012-11-16 14:58:55', '2012-11-16 14:55:55', '2012-11-16 18:14:49', '2012-11-19 10:17:12', '2012-11-19 10:33:59', '2012-11-19 16:07:49', '2012-11-19 21:59:01', '2012-11-19 22:24:58', '2012-11-19 22:31:59', '2012-11-20 10:21:02', '2012-11-20 10:37:51', '2012-11-20 11:14:44', '2012-11-20 13:28:35', '2012-11-20 14:40:16', '2012-11-20 15:10:16', '2012-11-21 10:06:50', '2012-11-21 15:14:47', '2012-11-21 15:30:55', '2012-11-21 17:55:48', '2012-11-22 01:45:42', '2012-11-22 10:25:45', '2012-11-22 10:59:45', '2012-11-22 11:10:30', '2012-11-22 12:09:07', '2012-11-22 15:12:19', '2012-11-22 15:26:18', '2012-11-22 16:51:54', '2012-11-22 18:17:25', '2012-11-23 00:41:13', '2012-11-23 10:28:57', '2012-11-23 10:01:57', '2012-11-23 12:57:33', '2012-11-23 14:20:47', '2012-11-23 14:38:47', '2012-11-23 16:57:43', '2012-11-23 18:06:38', '2012-11-26 10:37:28', '2012-11-26 10:05:28', '2012-11-27 10:30:43', '2012-11-27 10:44:43', '2012-11-27 11:54:59', '2012-11-27 19:46:56', '2012-11-28 09:33:52', '2012-11-28 09:44:52', '2012-11-28 12:57:42', '2012-11-28 13:38:45', '2012-11-28 19:45:20', '2012-11-28 22:18:43', '2012-11-30 11:45:40', '2012-11-30 11:51:40', '2012-11-30 15:05:54', '2012-11-30 15:20:00', '2012-11-30 17:42:59', '2012-11-30 22:15:48', '2012-11-30 22:39:48', '2011-11-28 10:42:55', '2011-11-28 10:49:00', '2011-11-28 14:41:54', '2011-11-28 15:04:00', '2011-11-28 20:20:59', '2011-11-29 12:10:37', '2011-11-29 12:19:00', '2011-11-29 13:25:32', '2011-11-29 15:22:57', '2011-11-29 15:49:00', '2011-11-29 16:32:23', '2011-11-30 10:27:58', '2011-11-30 10:38:58', '2011-11-30 13:05:31', '2011-11-30 14:55:24', '2011-11-30 15:11:00', '2011-11-30 16:41:09', '2011-11-30 15:08:00', '2011-12-01 11:19:43', '2011-12-01 11:29:43', '2011-12-01 14:36:38', '2011-12-02 12:31:10', '2011-12-02 12:37:10', '2011-12-02 14:50:19', '2011-12-02 15:24:00', '2011-12-02 19:40:50', '2011-12-05 12:17:58', '2011-12-05 12:26:02', '2011-12-05 15:13:55', '2011-12-05 15:42:00', '2011-12-05 19:24:16', '2011-12-06 11:32:49', '2011-12-06 11:38:51', '2011-12-06 14:55:18', '2011-12-06 15:18:18', '2011-12-06 19:22:55', '2011-12-07 11:17:14', '2011-12-07 11:22:35', '2011-12-07 14:13:34', '2011-12-07 14:41:00', '2011-12-07 20:38:18', '2011-12-08 11:28:24', '2011-12-08 11:35:55', '2011-12-09 11:03:09', '2011-12-09 11:09:08')
etsF <- c(ets1, ets2, ets3)

eatData <- tibble(case_id=eci, 
                  activity=factor(c(ea1, ea2)), 
                  activity_instance_id=eaii, 
                  lifecycle_id=factor(elci), 
                  resource=factor("UNDEFINED"), 
                  timestamp=as.POSIXct(etsF)
                  )

eat_patterns <- eventlog(eatData,
    case_id = "case_id",
    activity_id = "activity",
    activity_instance_id = "activity_instance_id",
    lifecycle_id = "lifecycle_id",
    timestamp = "timestamp",
    resource_id = "resource")


# Create performance map
eat_patterns %>% process_map(type = performance(FUN = median, units = "hours"))

# Inspect variation in activity durations graphically
eat_patterns %>% processing_time(level = "activity") %>% plot()

# Draw dotted chart
eat_patterns %>% dotted_chart(x = "relative_day", sort = "start_day", units = "secs")


# Time per activity
# daily_activities %>% processing_time(level = "activity") %>% plot

# Average duration of recordings
# daily_activities %>% throughput_time(level="log", units = "hours")

# Missing activities
# daily_activities %>% idle_time(level="log", units = "hours")


# Distribution throughput time
# vacancies %>% throughput_time(units="days")

# Distribution throughput time per department
# vacancies %>% group_by(vacancy_department) %>% throughput_time(units="days") %>% plot()

# Repetitions of activities
# vacancies %>% number_of_repetitions(level = "activity") %>% arrange(-relative)

```
  
  
  
***
  
Chapter 3 - Event Data Processing  
  
Filtering cases:  
  
* Sometimes there are too many cases, too many activities, missing data, and the like  
	* Can filter by either cases or events (time periods or specific activity types)  
    * Three levels of cases - performance, control-flow, and time frame  
* Look at long cases for what went wrong, and short cases for what to mimic  
	* filter_throughput_time(log, interval = c(5,10))  # absolute case length is 5-10 days  
    * filter_throughput_time(log, percentage = 0.5)  # shortest 50% of the cases  
    * filter_throughput_time(log, interval = c(5,10), units = "days", reverse =TRUE)  # cases that are NOT 5-10 days  
    * filter_throughput_time(log, interval = c(5,NA), units = "days") # cases longer than 5 days  
* Control-flow filters can be based on activity presence/absence, timing, and the like  
  
Filtering events - trim, frequency, label, general attribute:  
  
* Can trim to a time period based on start or end  
	* filter_time_period(log, interval = ymd(c("20180110","20180122")), filter_method = "trim")  # discards everything else  
* Can trim based on a specific start and end activities  
	* filter_trim(start_activities = "blues")  # traces that have no blues will be discarded  
    * filter_trim(start_activities = "blues", end_activities = "greens")  # traces that do not have blues followed by greens will be discarded  
    * Can set reverse=TRUE to get the opposites of these  
* Can filter by frequencies by either activity or resource  
	* filter_activity_frequency(log, interval = c(50,100))  
    * filter_activity_frequency(log, percentage = 0.8)  
    * filter_resource_frequency(log, interval = c(60,900))  
    * filter_resource_frequency(log, percentage = 0.6)  
* Can filter by labels  
	* filter_activity(log, activities = c("reds","oranges","purples")))  
    * dplyr::filter(log, cost > 1000, priority == "High", ...)  
  
Aggregating events - Is-A and Part-of:  
  
* The Is-A is when there are many subtypes of activity that are really all part of a main activity  
	* act_unite(log, "New name" = c("Old Variant 1","Old Variant 2","Old Variant 3"), ...)  # same number of activity instances, just fewer names  
* The Part-of is when there are clearly distinct activities that can also be considered components of a higher-level activity  
	* act_collapse(log, "Sub process" = c("Part 1","Part 2","Part 3"), ...)  # fewer number of activity instances, as they are collapsed to a single activity  
  
Enriching events - mutation (adding calculated variables):  
  
* The dplyr::mutate() can be used to directly add variables such as the cost  
	* log %>% group_by_case() %>% mutate(total_cost = sum(cost, na.rm = TRUE)  # group_by_case() is a function applied to event logs  
    * log %>% group_by_case() %>% mutate(total_cost = sum(cost, na.rm = TRUE) %>% mutate(impact = case_when(cost <= 1000 ~ "Low", cost <= 5000 ~ "Medium", TRUE ~ "High"))  
    * log %>% group_by_case() %>% mutate(refund_made = any(str_detect(activity, "Pay Claim")))  
* Metric functions can be used directly, with apped=TRUE, to both calculate the metric and add to the event log  
	* log %>% througput_time(level = "case", units = "days", append = TRUE) %>% mutate(on_time = processing_time_case <= 7)  
  
Example code includes:  
```{r eval=FALSE}

# Select top 20% of cases according to trace frequency
happy_path <- filter_trace_frequency(vacancies, percentage = 0.2)

# Visualize using process map
happy_path %>% process_map(type=requency(value = "absolute_case"))

# Compute throughput time
happy_path %>% throughput_time(units="days")


# Find no_declines
no_declines <- filter_activity_presence(vacancies, activities = "Decline Candidate", reverse=TRUE)

# What is the average number of  
first_hit <- filter_activity_presence(vacancies, activities = c("Send Offer", "Offer Accepted"), method="all")

# Create a performance map
first_hit %>% process_map(type=performance())

# Compute throughput time
first_hit %>% throughput_time()


# Create not_refused
not_refused <- vacancies %>% filter_precedence(antecedents = "Receive Response", consequents = "Review Non Acceptance", precedence_type = "directly_follows", filter_method = "none") 

# Select longest_cases
worst_cases <- not_refused %>% filter_throughput_time(interval=c(300, NA))

# Show the different traces
worst_cases %>% trace_explorer(coverage=1)


# Select activities
disapprovals <- vacancies %>% filter_activity(activities=c("Construct Offer", "Disapprove Offer", "Revise Offer","Disapprove Revision", "Restart Procedure"))

# Explore traces
disapprovals %>% trace_explorer(coverage=0.8)

# Performance map
disapprovals %>% process_map(type = performance(FUN = sum, units = "weeks"))


# Select cases
high_paid <- vacancies %>% filter(vacancy_department=="R&D", vacancy_salary_range==">100000")

# Most active resources
high_paid %>% resource_frequency(level="resource")

# Create a dotted chart
high_paid %>% dotted_chart(x="absolute", sort="start")

# Filtered dotted chart
library(lubridate)
high_paid %>% filter_time_period(interval = ymd(c("20180321","20180620")), filter_method = "trim") %>% dotted_chart(x="absolute", sort="start")


# Count activities and instances
n_activities(vacancies)
n_activity_instances(vacancies)

# Combine activities
united_vacancies <- vacancies %>% 
    act_unite("Disapprove Contract Offer" = c("Disapprove Offer","Disapprove Revision"),
              "Approve Contract Offer" = c("Approve Offer","Approve Revision"), 
              "Construct Contract Offer" = c("Construct Offer","Revise Offer")
              )
              
# Count activities and instances
n_activities(united_vacancies)
n_activity_instances(united_vacancies)


# Aggregate sub processes
aggregated_vacancies <- act_collapse(united_vacancies, 
                            "Interviews" = c("First Interview","Second Interview","Third Interview"),
                            "Prepare Recruitment" = c("Publish Position","File Applications","Check References"),
                            "Create Offer" = c("Construct Contract Offer", "Disapprove Contract Offer", "Approve Contract Offer")
                            )

# Calculated number of activities and activity instances
n_activities(aggregated_vacancies)
n_activity_instances(aggregated_vacancies)

# Create performance map
aggregated_vacancies %>% process_map(type=performance())


# Add total_cost
vacancies_cost <- vacancies %>% 
    group_by_case() %>% 
    mutate(total_cost = sum(activity_cost, na.rm = TRUE))

# Add cost_impact
vacancies_impact <- vacancies_cost %>%




# Compute throughput time per impact
vacancies_impact %>% group_by(cost_impact) %>% throughput_time(units = "weeks") %>% plot()


# Create cost_profile
vacancies_profile <- vacancies_impact %>%
    mutate(cost_profile = case_when(cost_impact == "High" & urgency < 7 ~ "Disproportionate",
                                    cost_impact == "Medium" & urgency < 5 ~ "Excessive",
                                    cost_impact == "Low" & urgency > 6 ~ "Lacking",
                                    TRUE ~ "Appropriate")) 

# Compare number of cases 
vacancies_profile %>% 
    group_by(cost_profile) %>%
    n_cases()
    
# Explore lacking traces
vacancies_profile %>%
  filter(cost_profile == "Lacking") %>%
  process_map()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Preparing the event data - example includes data from Sales, Purchasing, Manufacturing, Packaging & Delivery, Accounting:  
  
* While all departments need to work together, it is common for each department to have different data, business rules, relational data, etc.  
* Need to create event data first prior to running anything in the bupar package  
* Various field names (ends in _at or _by) may indicate the timing and resource levels  
* The tidyverse tools are helpful for creating the initial data  
  
Getting to know the process:  
  
* Identify data sources, transform so that each row is an event, harmonize them, create an eventlog  
* Start with high-level understanding of the process - summary(otc)  
	* activity_presence(otc) %>% plot()  
    * trace_length(otc) %>% plot()  
    * start_activities(otc, "activity") %>% plot()  
    * end_activities(otc, "activity") %>% plot()  
  
Roles and rules:  
  
* Parallel activities can be run in any order, which can cause an explosion in the number of traces - collapsing can help with abstraction  
* Research questions may be related to performance, compliance, etc.  
* The "4-eye" pricniple says that certain activities should not be performed by the same person  
  
Fast production, fast delivery:  
  
* Dotted charts can show the progression of the cases - request for quotation may be declined, or the offer may only be sent (no response)  
* May want to look at the performance by stages (sub-groups of activities), for more fair comparisons  
  
Course recap:  
  
* Process maps  
* Process analytics  
* Data preprocessing  
* Analysis and use cases  
  
Example code includes:  
```{r cache=TRUE}

quotations <- readRDS("./RInputFiles/otc_quotations.RDS")

# Inspect quotations
str(quotations)

# Create offer_history
offer_history <- quotations %>%
    gather(key, value, -quotation_id) %>%
    separate(key, into = c("activity", "info"))

# Recode the key variable
offer_history <- offer_history %>%
    mutate(info = fct_recode(info,  "timestamp" = 'at',  "resource" = 'by'))

# Spread the info variable
offer_history <- offer_history %>%
    spread(info, value)


validations <- readRDS("./RInputFiles/otc_validations.RDS")

# Inspect validations
str(validations)

# Create validate_history
validate_history <- validations %>%
    mutate(
        activity = "Validate",
        action = paste(quotation_id, "validate",  sep = "-"))

# Gather the timestamp columns
validate_history <- validate_history  %>%
    gather(lifecycle, timestamp, started, completed)


# Recode the lifecycle column of validate_history
validate_history <- validate_history %>%
    mutate(lifecycle = fct_recode(lifecycle,
                "start" = "started",
                "complete" = "completed"))


# Add lifecycle and action column to offer_history
offer_history <- offer_history %>%
    mutate(
        lifecycle = "complete",
        action = paste(quotation_id, 1:n(), sep = "-"))

# Create sales_history
sales_history <- bind_rows(validate_history, offer_history)


sales_history <- readRDS("./RInputFiles/otc_sales_history.RDS")
order_history <- readRDS("./RInputFiles/otc_order_history.RDS")
# sales_quotations <- readRDS("./RInputFiles/otc_sales_quotation.RDS")

str(sales_history)
str(order_history)
# str(sales_quotations)

order_history <- order_history %>% 
    rename(timestamp=time, lifecycle=status) %>%
    select(-activity_cost) %>%
    mutate(activity=as.character(activity), 
           resource=as.character(activity), 
           lifecycle=as.character(lifecycle)
           )
sales_history <- sales_history %>%
    mutate(timestamp=lubridate::as_datetime(timestamp))

# sales_history <- sales_history %>% left_join(sales_quotations)
otc <- bind_rows(sales_history, order_history)


# Create the eventlog object 
otc <- otc %>%
    mutate(case_id = paste(quotation_id, sales_order_id, sep = "-")) %>%
    eventlog(
        case_id = "case_id",
        activity_id = "activity",
        activity_instance_id = "action",
        timestamp = "timestamp",
        resource_id = "resource",
        lifecycle_id = "lifecycle"
        )

# Create trace coverage graph
trace_coverage(otc, level="trace") %>% plot()

# Explore traces
otc %>%
    trace_explorer(coverage = 0.25)


# Collapse activities
otc_high_level <- act_collapse(otc, "Delivery" = c(
  "Handover To Deliverer",
  "Order Delivered",
  "Present For Collection",
  "Order Fetched")
  )

# Draw a process map
process_map(otc_high_level)

# Redraw the trace coverage graph
otc_high_level %>% trace_coverage(level="trace") %>% plot()

# Compute activity wise processing time
otc_high_level %>% processing_time(level="activity", units="days")

# Plot a resource activity matrix of otc (does not work in R 3.5.3)
# otc %>% resource_frequency(level = "resource-activity") %>% plot()


# Create otc_selection
otc_selection <- otc %>% filter_activity(activities = c("Send Quotation","Send Invoice"))

# Explore traces
otc %>% trace_explorer(coverage=1)

# Draw a resource map
otc_selection %>% resource_map()


# Create otc_returned
otc_returned <- otc %>% filter_activity_presence("Return Goods")

# Compute percentage of returned orders
n_cases(otc_returned)/n_cases(otc)

# Trim cases and visualize
otc_returned %>% filter_trim(start_activities="Return Goods") %>% process_map()


# Time from order to delivery
# otc %>% filter_trim(start_activities="Receive Sales Order", end_activities="Order Delivered") %>% 
#     processing_time(units="days")


# Plot processing time by type
# otc %>%
#     group_by(type) %>%
#     throughput_time() %>%
#     plot()

```
  
  
  
***
  
###_Network Science in R - A Tidy Approach_  
  
Chapter 1 - Hubs of the Network  
  
Network science - include social networks, neural networks, etc.:  
  
* Nodes and edges (connections between nodes, aka "ties") make up a network  
	* In a directed network, ties have a direction (for example, followers and follwing)  
    * In an undirected network, ties do not have a direction (for example, mutual friendship)  
    * In a weighted network, the ties have an associated weight (such as bandwidth, duration of friendship, etc.)  
* Chapter will focus on the terrorism network associated with the Madrid train bombing of 2004  
	* Ties include friendhsip, training camps, previous attacks, and other terrorists  
* The network is reflected in tidy fashion, using one data frame for nodes and another for ties  
    * g <- igraph::graph_from_data_frame(d = ties, directed = FALSE, vertices = nodes)  
    * V(g); vcount(g)  
    * E(g); ecount(g)  
* And, then working with attributes of the network  
    * g$name <- "Madrid network"; g$name  
    * V(g)$id <- 1:vcount(g)  
    * E(g)$weight  
  
Visualizing networks:  
  
* The ggraph package can help with visualizing networks  
	* ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = weight)) + geom_node_point()  
    * Much like the language of ggplot2  
  
Centrality measures:  
  
* Objective is to find the most important nodes - connections among members of the networks  
* Network science is a spinoff of data science, with the goal of measuring networks  
* The agree of "degree" measures the number of ties (edges) that a node has  
	* degree(g) # gives the number of edges per node  
    * strength(g) # sumes the weights of the edges per node  
  
Example code includes:  
```{r eval=FALSE}

# read the nodes file into the variable nodes
nodes <- readr::read_csv("./RInputFiles/nodes.csv")
nodes

# read the ties file into the variable ties
ties <- readr::read_csv("./RInputFiles/ties.csv")
ties


library(igraph)
library(ggraph)


# make the network from the data frame ties and print it
g <- graph_from_data_frame(ties, directed = FALSE, vertices = nodes)
g

# explore the set of nodes
V(g)

# print the number of nodes
vcount(g)

# explore the set of ties
E(g)

# print the number of ties
ecount(g)


# give the name "Madrid network" to the network and print the network `name` attribute
g$name <- "Madrid network"
g$name

# add node attribute id and print the node `id` attribute
V(g)$id <- 1:vcount(g)
V(g)$id

# print the tie `weight` attribute
E(g)$weight

# print the network and spot the attributes
g


# visualize the network with layout Kamada-Kawai
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# add an id label to nodes
ggraph(g, layout = "with_kk") +
  geom_edge_link(aes(alpha = weight)) +
  geom_node_point()  + 
  geom_node_text(aes(label = id), repel=TRUE)


# visualize the network with circular layout. Set tie transparency proportional to its weight
ggraph(g, layout = "in_circle") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# visualize the network with grid layout. Set tie transparency proportional to its weight
ggraph(g, layout = "grid") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# compute the degrees of the nodes
dgr <- degree(g)

# add the degrees to the data frame object
nodes <- mutate(nodes, degree = dgr)

# add the degrees to the network object
V(g)$degree <- dgr

# arrange the terrorists in decreasing order of degree
arrange(nodes, -degree)


# compute node strengths
stg <- strength(g)

# add strength to the data frame object using mutate
nodes <- mutate(nodes, strength = stg)

# add the variable stg to the network object as strength
V(g)$strength <- stg

# arrange terrorists in decreasing order of strength and then in decreasing order of degree
arrange(nodes, -degree)
arrange(nodes, -strength)

```
  
  
  
***
  
Chapter 2 - Weakness and strength  
  
Tie betweenness:  
  
* Betweeness is the number of shortest paths that go through a specific tie (edge) - these removals would be the most disruptive  
* In a weighted network, the shortest path is defined as the lowest sum of weights, rather than the fewest edges  
	* Often need to inverse the weights prior to running, since a "high" weight usually means a close connection and thus an easy path  
    * dist_weight = 1 / E(g)$weight  
    * edge_betweenness(g, weights = dist_weight)  
	
Visualizing centrality measures:  
  
* Visualizing betweenness can be done within the igraph package  
    * ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = betweenness)) + geom_node_point()  
    * ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = weight)) + geom_node_point(aes(size = degree))  
  
The strength of weak ties:  
  
* "The strength of weak ties" is a research paper written about network strengths  
	* Argument is that the "weak ties" in a network are often the most important - relationships between diverse communities, leading to diverse ideas  
    * The "strong ties" are the relationships between people who are frequently together - can lead to group-think and stasis  
    * Noted that the Madrid group (and similar) tended to be highly dispersed and thus having many weak ties  
    * ties %>% group_by(weight) %>% summarise(n = n(), p = n / nrow(ties)) %>% arrange(-n)  
  
Example code includes:  
```{r eval=FALSE}

# save the inverse of tie weights as dist_weight
dist_weight <- 1 / E(g)$weight

# compute weighted tie betweenness
btw <- edge_betweenness(g, weights = dist_weight)

# mutate the data frame ties adding a variable betweenness using btw
ties <- mutate(ties, betweenness=btw)

# add the tie attribute betweenness to the network
E(g)$betweenness <- btw


# join ties with nodes
ties_joined <- ties %>% 
  left_join(nodes, c("from" = "id")) %>% 
  left_join(nodes, c("to" = "id")) 

# select only relevant variables and save to ties
ties_selected <- ties_joined %>% 
  select(from, to, name_from = name.x, name_to = name.y, betweenness)

# arrange named ties in decreasing order of betweenness
arrange(ties_selected, -betweenness)


# set (alpha) proportional to weight and node size proportional to degree
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha=weight)) + 
  geom_node_point(aes(size=degree))

# produce the same visualization but set node size proportional to strength
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point(aes(size = strength))


# visualize the network with tie transparency proportional to betweenness
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness)) + 
  geom_node_point()

# add node size proportional to degree
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness)) + 
  geom_node_point(aes(size = degree))


# find median betweenness
q = median(E(g)$betweenness)

# filter ties with betweenness larger than the median
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness, filter = (betweenness > q))) + 
  geom_node_point() + 
  theme(legend.position="none")


# find number and percentage of weak ties
ties %>%
  group_by(weight) %>%
  summarise(number = n(), percentage=n()/nrow(.)) %>%
  arrange(-number)


# build vector weakness containing TRUE for weak ties
weakness <- ifelse(ties$weight == 1, TRUE, FALSE)

# check that weakness contains the correct number of weak ties
sum(weakness)


# visualize the network by coloring the weak and strong ties
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(color = weakness)) + 
  geom_node_point()


# visualize the network with only weak ties using the filter aesthetic
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(filter=weakness), alpha = 0.5) + 
  geom_node_point()

```
  
  
  
***
  
Chapter 3 - Connection patterns  
  
Connection patterns:  
  
* The adjacency matrix can be calculated using as_adjacency_matrix(g)  
	* For each match of row/column, there will be a 1 for adjacency and a 0 for non-adjacency  
    * Alternately, can have the weight of the tie as the entry for each row/column (with 0 as before meaning non-adjacency)  
    * A = as_adjacency_matrix(g, attr = "weight")  
    * diag(A)  
* Can use the adjacency matrix to assess similarity of nodes in the matrix  
	* The Pearson similarity measures the correlation between the columns in the matrix  
  
Pearson correlation coefficient:  
  
* Can visualize the correlations using scatterplots  
* Can compute the correlations analytically as well  
	* cor(nodes$degree, nodes$strength)  
  
Most similar and most dissimilar terrorists:  
  
* Can use named graphs with weighted ties for a graphical representation of nodes and paths  
* Can use the adjacency matrix to reprsent the ties in a manner simplified for algebra  
* Can use the data frame format (one for nodes, and one for ties) for use with dplur and ggplot2  
	* as_data_frame(g, what = "both")  
* Can easily switch back and forth between the representations of the network  
	* as_adjacency_matrix(g)  
    * graph_from_adjacency_matrix(A)  
    * as_data_frame(g, what = "both")  
    * graph_from_data_frame(df$ties, vertices = df$nodes)  
    * as_data_frame(graph_from_adjacency_matrix(A), what = "both")  
    * as_adjacency_matrix(graph_from_data_frame(df$ties, vertices = df$nodes))  
  
Example code includes:  
```{r eval=FALSE}

# mutate ties data frame by swapping variables from and to 
ties_mutated <- mutate(ties, temp = to, to = from, from = temp) %>% select(-temp)

# append ties_mutated data frame to ties data frame
ties <- rbind(ties, ties_mutated)

# use a scatter plot to visualize node connection patterns in ties setting color aesthetic to weight
ggplot(ties, aes(x = from, y = to, color = factor(weight))) +
  geom_point() +
  labs(color = "weight")


# get the weighted adjacency matrix
A <- as_adjacency_matrix(g, attr = "weight", sparse = FALSE, names = FALSE)

# print the first row and first column of A
A[1, ]
A[, 1]

# print submatrix of the first 6 rows and columns
A[1:6, 1:6]


# obtain a vector of node strengths
rowSums(A)

# build a Boolean (0/1) matrix from the weighted matrix A
B <- ifelse(A > 0, 1, 0)

# obtain a vector of node degrees using the Boolean matrix
rowSums(B)


# compute the Pearson correlation on columns of A
S <- cor(A)

# set the diagonal of S to 0
diag(S) = 0

# print a summary of the similarities in matrix S
summary(c(S))

# plot a histogram of similarities in matrix S
hist(c(S), xlab = "Similarity", main = "Histogram of similarity")


# Scatter plot of degree and strength with regression line
ggplot(nodes, aes(x = degree, y = strength)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Pearson correlation coefficient 
cor(nodes$degree, nodes$strength)


# build weighted similarity network and save to h
h <- graph_from_adjacency_matrix(S, mode = "undirected", weighted = TRUE)

# convert the similarity network h into a similarity data frame sim_df
sim_df <- as_data_frame(h, what = "edges")

# map the similarity data frame to a tibble and save it as sim_tib
sim_tib <- as_tibble(sim_df)

# print sim_tib
sim_tib


# left join similarity and nodes data frames and then select and rename relevant variables
sim2 <- sim_tib %>% 
  left_join(nodes, c("from" = "id")) %>% 
  left_join(nodes, c("to" = "id")) %>%
  select(from, to, name_from = name.x, name_to = name.y, similarity = weight, 
         degree_from = degree.x, degree_to = degree.y, strength_from = strength.x, strength_to = strength.y)
  
# print sim2
sim2


# arrange sim2 in decreasing order of similarity. 
sim2 %>% arrange(-similarity)

# filter sim2, allowing only pairs with a degree of least 10, arrange the result in decreasing order of similarity
sim2 %>%
  filter(degree_from >= 10, degree_to >= 10) %>%
  arrange(-similarity)

# Repeat the previous steps, but in increasing order of similarity
sim2 %>%
  filter(degree_from >= 10, degree_to >= 10) %>%
  arrange(similarity)


# filter the similarity data frame to similarities larger than or equal to 0.60
sim3 <- filter(sim2, similarity >= 0.6)

# build a similarity network called h2 from the filtered similarity data frame
h2 <- graph_from_data_frame(sim3, directed = FALSE)

# visualize the similarity network h2
ggraph(h2, layout = "with_kk") + 
  geom_edge_link(aes(alpha = similarity)) + 
  geom_node_point()

```
  
  
  
***
  
Chapter 4 - Similarity Clusters  
  
Hierarchical clustering - find clusters of similar people:  
  
* Basic idea is to define a measure of similarity, then match the most similar entities to groups, proceeding until there is a single cluster containing everyone  
* The dendrogram (tree diagram) is helpful for viewing this data  
* The similarity measure between individual nodes (person similarity) exists, and needs to be extended to groups  
	* Single-linkage - similarity is the maximum of the similarities of anyone in the groups  
    * Complete-linkage - similarity is the minimum of the similarities of anyone in the groups  
    * Average-linkage - similarity is the average of the simlarities of everyone in the groups  
* The clustering algorithm works as follows  
	* Evaluate simlarity for all node pairs  
    * Assign each node to its own group  
    * Find the pair of groups with the highest simlarity, and join them  
    * Calculate simlarity of this newly formed group to all previously existing entities (groups or individuals)  
    * Repeat until there is just a single cluster remaining  
* The R implementation is hclust()  
    * D <- 1-S  
    * d <- as.dist(D)  
    * cc <- hclust(d, method = "average")  
    * cls <- cutree(cc, k = 4)  
  
Interactive visualizations with visNetwork:  
  
* visNetwork is an interactive package for viewing networks  
	* Many different layouts are available, and you can interact with the nodes and the ties  
    * Can select nodes and see their neighborhoods (nodes within a certain distance)  
    * Can select nodes by name  
    * Can partition nodes in to groups and color, highlight, etc.  
  
Wrap up:  
  
* Analysis of networks with measures of centrality and similarity  
* Visualization of networks, including interactivity  
  
Example code includes:  
```{r eval=FALSE}

# compute a distance matrix
D <- 1 - S

# obtain a distance object 
d <- as.dist(D)

# run average-linkage clustering method and plot the dendrogram 
cc <- hclust(d, method = "average")
plot(cc)

# find the similarity of the first pair of nodes that have been merged 
S[40, 45]


# cut the dendrogram at 4 clusters
cls <- cutree(cc, k = 4)

# add cluster information to the nodes data frame
nodes <- mutate(nodes, cluster = cls)

# print the nodes data frame
nodes


# output the names of terrorists in the first cluster
filter(nodes, cluster == 1) %>% 
    select(name)

# for each cluster select the size of the cluster, the average node degree, and the average node strength and sorts by cluster size
group_by(nodes, cluster) %>%
  summarise(size = n(), 
            avg_degree = mean(degree),
            avg_strength = mean(strength)
            ) %>%
  arrange(-size)


# add cluster information to the network 
V(g)$cluster <- nodes$cluster

# visualize the original network with colored clusters
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) + 
  geom_node_point(aes(color = factor(cluster))) +
  labs(color = "cluster")

# facet the network with respect to cluster attribute
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) + 
  geom_node_point(aes(color = factor(cluster))) +
  facet_nodes(~cluster, scales="free")  +
  labs(color = "cluster")


# convert igraph to visNetwork
data <- visNetwork::toVisNetworkData(g)

# print head of nodes and ties
head(data$nodes)
head(data$edges)

# visualize the network
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300)


# use the circle layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk")

# use the circle layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_in_circle")

# use the grid layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_on_grid")


# highlight nearest nodes and ties of the selected node
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(highlightNearest = TRUE) 


# select nodes by id 
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(nodesIdSelection = TRUE)

# set color to cluster and generate network data
V(g)$color = V(g)$cluster
data <- visNetwork::toVisNetworkData(g)

# select by group (cluster)
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(selectedBy = "group")

```
  
  
  
***
  
###_Data Privacy and Anaonymization in R_  
  
Chapter 1 - Introduction to Data Privacy  
  
Intro to Anonymization - Part I:  
  
* Need to implement better data privacy techniques - e.g., census data, healthcare data, etc.  
* Need to have data such as individualized health, but not in a manner that identifies specific individuals  
* Topics covered in this course will include  
	* Remove identifiers, synthesize data  
    * Laplace mechnaism for removing names  
    * Differential privacy and post-processing  
    * Release of data using the above techniques  
* Data sets will include White House salaries and male infertility data  
	* One basic technique is removing identifiers, such as replacing names with numbers  
    * Another basic technique is to round continuous values (such as to the nearest 1000)  
  
Intro to Anonymization - Part II:  
  
* Additional approaches include generalization and top/bottom coding  
	* Generalization creates larger buckets of data  
    * Top/bottom is about setting outliers back to a pre-defined top and bottom of the range  
* Additional dplyr functions of interest  
	* count() is used to find the number of observations for each distinct group  
    * whitehouse %>% count(Status)  
    * whitehouse %>% count(Status, Title, sort = TRUE)  # sort=TRUE sorts by descending n  
    * summarize_at() lets you get summary statistics for a key variable  
    * whitehouse %>% summarise_at(vars(Salary), sum)  # vars() holds the bare variables, while sum is the requested function  
    * whitehouse %>% summarise_at(vars(Salary), funs(mean, sd))  # funs() holds the list of functions that you want to apply  
  
Data Synthesis:  
  
* Fake datasets created based on sampling from a probability distribution  
* Goal is a fake dataset (by definition anaonymized) that is statistically similar to the real dataset  
	* For 1/0 data, sampling from the binomial distribution can work well  
    * For bell-shaped data, the normal or log-normal can often work well (though there can be issues with bounding)  
    * Hard-bounding is setting values to a proper max/min, while another approach is to discard the record and sample again  
  
Example code includes:  
```{r}

load("./RInputFiles/dataPriv.RData")


# Preview data
whitehouse

# Set seed
set.seed(42)

# Replace names with random numbers from 1 to 1000
whitehouse_no_names <- whitehouse %>%
    mutate(Name = sample(1:1000, nrow(.), replace=FALSE))

whitehouse_no_names


# Rounding Salary to the nearest ten thousand
whitehouse_no_identifiers <- whitehouse_no_names %>%
    mutate(Salary = round(Salary, -4))

whitehouse_no_identifiers


# Convert the salaries into three categories
whitehouse.gen <- whitehouse %>%
    mutate(Salary = ifelse(Salary < 50000, 0, 
                           ifelse(Salary >= 50000 & Salary < 100000, 1, 2)))

whitehouse.gen


# Bottom Coding
whitehouse.bottom <- whitehouse %>%
    mutate(Salary = pmax(Salary, 45000))

# Filter Results
whitehouse.bottom %>%
    filter(Salary <= 45000)


# View fertility data
fertility

# Number of participants with Surgical_Intervention and Diagnosis
fertility %>%
    summarise_at(vars(Surgical_Intervention, Diagnosis), sum)

# Mean and Standard Deviation of Age
fertility %>%
    summarise_at(vars(Age), funs(mean, sd))

# Counts of the Groups in High_Fevers
fertility %>%
    count(High_Fevers)

# Counts of the Groups in Child_Disease
fertility %>%
    count(Child_Disease, Accident_Trauma)

# Find proportions
fertility %>%
    summarise_at(vars(Accident_Trauma, Surgical_Intervention), mean)


# Set seed
set.seed(42)

# Generate Synthetic data
accident <- rbinom(100, 1, prob=0.440)
surgical <- rbinom(100, 1, prob=0.510)


# Square root Transformation of Salary
whitehouse.salary <- whitehouse %>%
    mutate(Salary = sqrt(Salary))

# Calculate the mean and standard deviation
stats <- whitehouse.salary %>%
    summarize(mean(Salary), sd(Salary))

stats


# Generate Synthetic data
set.seed(42)
salary_transformed <- rnorm(nrow(whitehouse), mean=279, sd=71.8)

# Power transformation
salary_original <- salary_transformed ** 2

# Hard bound
salary <- ifelse(salary_original < 0, 0, salary_original)

```
  
  
  
***
  
Chapter 2 - Introduction to Differential Privacy  
  
Differential Privacy - quantification of privacy loss via a privacy budget:  
  
* The worst-case scenario is that no assumptions are made about data intruders  
	* If an individual is from a small group, their data may be 100% available by looking at statistics in aggregate and statistics for the group that excludes them (everyone else)  
* The privacy budget is defined using epsilon - smaller numbers mean that less information will be made available  
* The general concept is to look at a dataset that includes the segment the individual is in, and a dataset that includes all other segments  
	* The answer sent back to the query will have noise added to it depending on the privacy budget  
* Basically, the differential privacy algorithm finds the most "unique" person in the dataset, and then decides how much noise to add based on how identifiable they are by attribute  
  
Global Sensitivity - usual decision-making factor for differential privacy:  
  
* The global sensitivity of a query is the most a variable could change based on removing one individual  
	* By definition, count queries always have a global sensitivity of 1 (exclude 1 individual)  
    * Therefore, proportion queries always have a global sensitity of 1/n  
    * Mean queries always have a global sensitivity of (max - min) / n  
    * Variance queries always have a global sensitivity of (max - min)^2 / n  
* The global sensitivity and the epsilon work together to determine the amount of noise  
	* Measures like median are not very sensitive to outliers, and thus very little noise needs to be added  
    * Measures like maximum are very sensitive to outliers (e.g., Bill Gates income), and thus very little noise needs to be added  
  
Laplace Mechanism - adds noise based on the Laplace distribution with mean 0 and parameters global sensitivity and privacy budget:  
  
* fertility %>% summarise_at(vars(Child_Disease), sum)  
* library(smoothmest)  # has function rdoublex(draws, mean, shaping) - set draws=1, mean=true_mean, shaping=globalSensitivity / epsilon  
  
Example code includes:  
```{r}

# Number of observations
n <- nrow(fertility)

# Global sensitivity of counts
gs.count <- 1

# Global sensitivity of proportions
gs.prop <- 1/n


# Lower bound of Hours_Sitting
a <- 0

# Upper bound of Hours_Sitting
b <- 1

# Global sensitivity of mean for Hours_Sitting
gs.mean <- (b - a) / n

# Global sensitivity of proportions Hours_Sitting
gs.var <- (b - a)**2 / n


# How many participants had a Surgical_Intervention?
fertility %>%
   summarise_at(vars(Surgical_Intervention), sum)

# Set the seed
set.seed(42)

# Apply the Laplace mechanism
eps <- 0.1
smoothmest::rdoublex(1, 51, 1/eps)


# Proportion of Accident_Trauma
stats <- fertility %>%
   summarise_at(vars(Accident_Trauma), mean)

stats

# Set the seed
set.seed(42)

# Apply the Laplace mechanism
eps <- 0.1
smoothmest::rdoublex(1, 0.440, (1/n)/eps)


# Mean and Variance of Hours Sitting
fertility %>%
    summarise_at(vars(Hours_Sitting), funs(mean, var))

# Setup
set.seed(42)
eps <- 0.1

# Laplace mechanism to mean
smoothmest::rdoublex(1, 0.41, gs.mean/eps)

# Laplace mechanism to variance
smoothmest::rdoublex(1, 0.03, gs.var/eps)

```
  
  
  
***
  
Chapter 3 - Differentially Private Properties  
  
Sequential Composition - method to require that someone cannot find the real answer by just sending multiple queries:  
  
* Idea is that the privacy budget is divided by the number of queries you plan to send  
* For example, if a query will be made for mean and another query will be made for maximum, then epsilon needs to be divided by two  
  
Parallel Composition - method to account for queries to different parts of the database (no adjustment to epsilon needed):  
  
* Deciding between sequential and parallel is whether queries could be answered using completely different (MECE) splits of the dataset  
  
Post-processing:  
  
* When new queries can be answered using data that has already been privatized, it can be synthesized to a noisy answer to this new query  
	* The privacy budget need not be adjusted in this case  
    * For example, if there are three groups, can just add noise to two of the groups and let the third group be total minus these two groups  
  
Impossible and inconsistent answers:  
  
* Bounding can be introduced, such as making all negative numbers zero or anything greater than the total to the total  
	* rdoublex(1, 12, gs.count / eps) %>% round() %>% max(0)  # lower bound is zero  
    * normalized <- (smoking/sum(smoking)) * (nrow(fertility))  # upper bound is the size of the dataset  
  
Example code includes:  
```{r}

# Set Value of Epsilon
eps <- 0.1 / 2

# Number of observations
n <- nrow(fertility)

# Lower bound of Age
a <- 0

# Upper bound of Age
b <- 1

# GS of counts for Diagnosis
gs.count <- 1

# GS of mean for Age
gs.mean <- (b-a)/n


# Number of Participants with abnormal diagnosis
stats1 <- fertility %>% 
    summarize_at(vars(Diagnosis), sum)

stats1

# Mean of age
stats2 <- fertility %>%
    summarize_at(vars(Age), mean)

stats2


# Set seed
set.seed(42)

# Laplace mechanism to the count of abnormal diagnosis
smoothmest::rdoublex(1, 12, gs.count/eps)

# Laplace mechanism to the mean of age
smoothmest::rdoublex(1, 0.67, gs.mean/eps)


# Set Value of Epsilon
eps <- 0.1

# Mean of Age per diagnosis level 
fertility %>%
  group_by(Diagnosis) %>%
  summarise_at(vars(Age), mean)


# Set the seed
set.seed(42)

# Laplace mechanism to the mean age of participants with an abnormal diagnoisis
smoothmest::rdoublex(1, 0.71, gs.mean/eps)

# Laplace mechanism to the mean age of participants with a normal diagnoisis
smoothmest::rdoublex(1, 0.66, gs.mean/eps)


# Set Value of Epsilon
eps <- 0.5/3

# GS of Counts
gs.count <- 1

# Number of participants in each of the four seasons
fertility %>%
    group_by(Diagnosis) %>%
    summarise_at(vars(Age), mean)

# Set the seed
set.seed(42)

# Laplace mechanism to the number of participants who were evaluated in the winter, spring, and summer
winter <- smoothmest::rdoublex(1, 28, gs.count / eps) %>%
    round()

spring <- smoothmest::rdoublex(1, 37, gs.count / eps) %>%
    round()

summer <- smoothmest::rdoublex(1, 4, gs.count / eps) %>%
    round()

# Post-process based on previous queries
fall <- nrow(fertility) - winter - spring - summer


# Set Value of Epsilon
eps <- 0.01

# GS of counts
gs.count <- 1

# Number of Participants with Child_Disease
fertility %>%
    summarise_at(vars(Child_Disease), sum)

# Apply the Laplace mechanism
set.seed(42)
lap_childhood <- smoothmest::rdoublex(1, 87, gs.count / eps) %>%
    round()

# Total number of observations in fertility
max_value <- nrow(fertility)

# Bound the value such that the noisy answer does not exceed the total number of observations
ifelse(lap_childhood > max_value, max_value, lap_childhood)


# Set the seed
set.seed(42)

# Apply the Laplace mechanism
fever1 <- smoothmest::rdoublex(1, 9, gs.count/eps) %>%
    max(0)
fever2 <- smoothmest::rdoublex(1, 63, gs.count/eps) %>%
    max(0)
fever3 <- smoothmest::rdoublex(1, 28, gs.count/eps) %>%
    max(0)

fever <- c(fever1, fever2, fever3)

# Normalize noise 
fever_normalized <- (fever/sum(fever)) * (nrow(fertility))

# Round the values
round(fever_normalized)

```
  
  
  
***
  
Chapter 4 - Differentially Private Data Synthesis  
  
Laplace Sanitizer - basic way to generate "noisy" categorical data:  
  
* Takes advantage of parallel - if the data can be binned or placed in a contingency table, assumes no more need to divide the privacy budget  
	* Since the data is queries as a histogram, it can be considered disjoint (non-overlapping) and thus parallel composition  
* Can generate data using rep() for a single vector  
  
Parametric Approaches:  
  
* Sampling from a binomial distribution (where appropriate), with a known proportion that has been modified by Laplace differential privacy guarantee  
* Sampling from a normal or log-normal distribution (where appropriate), with a known mean and variance that has been modified by Laplace differential privacy guarantee  
  
Wrap up:  
  
* Basics of anonymyzing data, such as removing names  
* Basics of modifying data such as generalizing to categorical data  
* Basics of generating synthetic data using rbinom() and rnorm()  
* Basics of privacy budgets, global sensitivities, and the Laplace mechanism  
* Basics of differential privacy, such as sequential (split epsilon) or parallel (including through binning or continegnecy tables)  
* Basics of the Laplace sanitizer for both categorical data (rbinom) and continuous data (rnorm)  
* Next steps include managing data gaps, incorrect statistics distributions with hard bounding, etc.  
	* Local differential privacy (Apple) and probabilistic differential privacy (US census)  
    * Techniques specific to GPS data or PCA  
  
Example code includes:  
```{r}

# Set Value of Epsilon
eps <- 0.1

# GS of Counts
gs.count <- 1

# Number of participants in each season
fertility %>%
    count(Season)


# Set the seed
set.seed(42)

# Apply the Laplace mechanism 
winter <- smoothmest::rdoublex(1, 28, gs.count/eps) %>% max(0)
spring <- smoothmest::rdoublex(1, 37, gs.count/eps) %>% max(0)
summer <- smoothmest::rdoublex(1, 4, gs.count/eps) %>% max(0)
fall <- smoothmest::rdoublex(1, 31, gs.count/eps) %>% max(0)


# Store noisy results
seasons <- c(winter = winter, spring = spring, summer = summer, fall = fall)

# Normalizing seasons
seasons_normalized <- (seasons/sum(seasons)) * nrow(fertility)

# Round the values
round(seasons_normalized)

# Generate synthetic data for winter
rep(-1, 29)

# Generate synthetic data for spring
rep(-0.33, 38)

# Generate synthetic data for summer
rep(0.33, 0)

# Generate synthetic data for fall
rep(1, 33)


# Calculate proportions
fertility %>%
    summarise_at(vars(Accident_Trauma, Surgical_Intervention), mean)

# Number of Observations
n <- nrow(fertility)

# Set Value of Epsilon
eps <- 0.1

# GS of Proportion
gs.prop <- (1/n)


# Apply the Laplace mechanism
set.seed(42)
smoothmest::rdoublex(1, 0.44, gs.prop/eps)
smoothmest::rdoublex(1, 0.51, gs.prop/eps)

# Generate Synthetic data
set.seed(42)
accident <- rbinom(n, 1, 0.46)
surgical <- rbinom(n, 1, 0.54)


# Set Value of Epsilon
eps <- 0.1 / 2

# Number of observations
n <- nrow(fertility)

# Upper and lower bounds of age
a <- 0
b <- 1

# GS of mean and variance for age
gs.mean <- (b-a) / n
gs.var <- (b-a)**2 / n


# Mean and Variance of Age
fertility %>%
    summarise_at(vars(Age), funs(mean, var))

# Apply the Laplace mechanism
set.seed(42)
smoothmest::rdoublex(1, 0.67, gs.mean/eps)
smoothmest::rdoublex(1, 0.01, gs.var/eps)


# Generate Synthetic data
set.seed(42)
age <- rnorm(n, mean=0.71, sd=sqrt(0.07))

# Hard Bounding the data
age[age < 0] <- 0
age[age > 1] <- 1

```
  
  
  
*** 
  
###_Marketing Analytics in R: Statistical Modeling_  
  
Chapter 1 - Modeling Customer Lifetime Value with Linear Regression  
  
Introduction - Verena from INWT Statistics (consultancy in marketing analytics):  
  
* Customer Lifetime Value (CLV) is the expected value of forecasted customer value to the company  
	* CLV is based on margin, and needs to use current information to predict future margins  
    * Customers predicted to have higher CLV can then be targeted  
* Can inspect the data without seeing attributes using str(clvData1, give.attr = FALSE)  
* Can derive correlations using corrplot  
	* library(corrplot)  
    * clvData1 %>% select(nOrders, nItems, ... ,margin, futureMargin) %>% cor() %>% corrplot()  
  
Simple linear regression - one predictor variable to predict one response variable:  
  
* Can run linear regressions using basic stats modules  
	* simpleLM <- lm(futureMargin ~ margin, data = clvData1)  
    * summary(simpleLM)  
* Can plot previous margin vs. current margin, including a linear regression (smooth)  
	* ggplot(clvData1, aes(margin, futureMargin)) + geom_point() + geom_smooth(method = lm, se = FALSE) + xlab("Margin year 1") + ylab("Margin year 2")  
* Several conditions must apply for linear regression to be the best method  
	* Linear relationship between x and y  
    * No measurement error in x (weak exogeneity)  
    * Independence of errors  
    * Expectation of errors is 0  
    * Constant variance of prediction errors (homoscedasticity)  
    * Normality of errors  
  
Multiple linear regression:  
  
* Omitted variable bias is when a variable not in the regression is correlated with both the predictor and the response variables  
	* Simpson's Paradox is an example - upward sloping becomes downward sloping after properly splitting on the extra variable  
* Multicollinearity is a threat to a linear regression - leads to unstable regression coefficients, with associated under-reporting of standard errors  
	* rms::vif(myLMModel)  # above 5 is concerning, above 10 almost always needs to be addressed  
  
Model validation, fit, and prediction:  
  
* The R-squared is the proportion of variance in the depedent variable that is explained by the regression  
* Can look at the p-value of the F-test to assess the overall statistical significance of the model  
* There is a risk of over-fitting, when the model is overly complex and learns artifacts of the training data rather than genuine patterns  
	* Can use stats::AIC() or MASS::stepAIC(), with the goal being to minimize AIC (needs to be models of the same data)  
    * AIC(multipleLM2)  
* Can predict outputs automatically, such as with  
	* predMargin <- predict(multipleLM2, newdata = clvData2)  
  
Example code includes:  
```{r}

salesData <- readr::read_csv("./RInputFiles/salesData.csv")

# Structure of dataset
str(salesData, give.attr = FALSE)

# Visualization of correlations
salesData %>% select_if(is.numeric) %>%
  select(-id) %>%
  cor() %>%
  corrplot::corrplot()

# Frequent stores
ggplot(salesData) +
    geom_boxplot(aes(x = mostFreqStore, y = salesThisMon))

# Preferred brand
ggplot(salesData) +
    geom_boxplot(aes(x = preferredBrand, y = salesThisMon))


# Model specification using lm
salesSimpleModel <- lm(salesThisMon ~ salesLast3Mon, data = salesData)

# Looking at model summary
summary(salesSimpleModel)


# Estimating the full model
salesModel1 <- lm(salesThisMon ~ . -id, data = salesData)

# Checking variance inflation factors
car::vif(salesModel1)

# Estimating new model by removing information on brand
salesModel2 <- lm(salesThisMon ~ . -id -preferredBrand -nBrands, data = salesData)

# Checking variance inflation factors
car::vif(salesModel2)


salesData2_4 <- readr::read_csv("./RInputFiles/salesDataMon2To4.csv")

# getting an overview of new data
summary(salesData2_4)

# predicting sales
predSales5 <- predict(salesModel2, newdata = salesData2_4)

# calculating mean of future sales
mean(predSales5)

```
  
  
  
***
  
Chapter 2 - Logistic Regression for Churn Prevention  
  
Churn prevention in online marketing:  
  
* Objective is to predict the likelihood of a customer repeating their business, assessed using logistic regression  
	* Model the log-odds (defined as log (P(Y=1) / P(Y=0))) as a linear function of the inputs  
    * Convert the log-odds to odds (defined as P(Y=1) / P(Y=0)) by exponentiation  
    * Convert the odds to a probability of churning, using odds / (1 + odds)  
* Can begin with basic data exploration  
	* ggplot(churnData, aes(x = returnCustomer)) + geom_histogram(stat = "count")  
  
Modeling and model selection:  
  
* The logit model can be run using the GLM provided in R  
	* logitModelFull <- glm(returnCustomer ~ title + newsletter + websiteDesign + ..., family = binomial, churnData)  
* Interpreting the coefficients is not easy - they are related to the log-odds  
	* Can exponentiate the coefficients to get their impact on the odds  
    * Can then interpret that greater than 1 means "more likely, all else equal"  
* Can use MASS::stepAIC() to help refine the modeling  
	* library(MASS)  
    * logitModelNew <- stepAIC(logitModelFull, trace = 0)  
    * summary(logitModelNew)  
    * Produces a model with fewer variables and a lower AIC  
  
In-sample model fit and thresholding:  
  
* There are three types of pseudo-R-squared statistics available for the results of logistical regression  
	* McFadden: R-squared = 1 - L(null) / L(full)  
    * Cox-Snell: R-squared = 1 - (L(null) / L(full)) ** (2/n)  
    * Nagelkerke: R-squared = [1 - (L(null) / L(full)) ** (2/n)] / [1 - L(null) ** (2/n)]  
    * Generally, anything above 0.2 is reasonably good  
    * descr::LogRegR2(logitModelNew)  
    * library(SDMTools)  
    * churnData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)  # get the prediction probabilities  
    * data %>% select(returnCustomer, predNew) %>% tail()  
    * confMatrixNew <- confusion.matrix(churnData$returnCustomer, churnData$predNew, threshold = 0.5)  # this is the version from SDMTools  
* Can give different weights to the different errors (false negatives, false positives, etc.)  
	* Can instead look at a payoff, defined based on scalars for the various quadrants  
  
Out-of-sample validation and cross validation:  
  
* Begin by randomly splitting data in to training (roughly two-thirds) and holding back the remainder for validation (roughly one-third)  
	* set.seed(534381)  
    * churnData$isTrain <- rbinom(nrow(churnData), 1, 0.66)  
    * train <- subset(churnData, churnData$isTrain == 1)  
    * test <- subset(churnData, churnData$isTrain == 0)  
    * test$predNew <- predict(logitTrainNew, type = "response", newdata = test)  # make predictions only on the test dataset  
* Cross-validation is an even more powerful tool for assessing out-of-sample error  
	* Split the data in to k subsets, and run the model k times with k-1 training data and the last subset used as the validation data  
    * Acc03 <- function(r, pi = 0) {  
    *   cm <- confusion.matrix(r, pi, threshold = 0.3)  
    *   acc <- sum(diag(cm)) / sum(cm) return(acc)  
    * }  
    * set.seed(534381)  
    * boot::cv.glm(churnData, logitModelNew, cost = Acc03, K = 6)$delta  
* Can continually tweak the model to see if transforms, variable additions, etc., might tend to improve the out-of-sample error rate  
  
Example code includes:  
```{r}

defaultData <- readr::read_delim("./RInputFiles/defaultData.csv", delim=";")

# Summary of data
summary(defaultData)

# Look at data structure
str(defaultData, give.attr=FALSE)

# Analyze the balancedness of dependent variable
ggplot(defaultData, aes(x = PaymentDefault)) +
  geom_histogram(stat = "count") 


# Build logistic regression model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                family = "binomial", data = defaultData)

# Take a look at the model
summary(logitModelFull)

# Take a look at the odds
coefsexp <- coef(logitModelFull) %>% exp() %>% round(2)
coefsexp


# The old (full) model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                 family = binomial, defaultData)

#Build the new model
logitModelNew <- MASS::stepAIC(logitModelFull, trace=0) 

#Look at the model
summary(logitModelNew) 

# Save the formula of the new model (it will be needed for the out-of-sample part) 
formulaLogit <- as.formula(summary(logitModelNew)$call)
formulaLogit


# Make predictions using the full Model
defaultData$predFull <- predict(logitModelFull, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelFull <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                                  defaultData$predFull, 
                                                  threshold = 0.5
                                                  )
confMatrixModelFull

# Calculate the accuracy for the full Model
accuracyFull <- sum(diag(confMatrixModelFull)) / sum(confMatrixModelFull)
accuracyFull


# Calculate the accuracy for 'logitModelNew'
# Make prediction
defaultData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelNew <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                                 defaultData$predNew, 
                                                 threshold = 0.5
                                                 )
confMatrixModelNew

# Calculate the accuracy...
accuracyNew <- sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew)
accuracyNew

# and compare it to the full model's accuracy
accuracyFull
accuracyNew


# Prepare data frame with threshold values and empty payoff column
payoffMatrix <- data.frame(threshold = seq(from = 0.1, to = 0.5, by = 0.1), payoff = NA) 
payoffMatrix
 
for(i in 1:length(payoffMatrix$threshold)) {
  # Calculate confusion matrix with varying threshold
  confMatrix <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                           defaultData$predNew, 
                                           threshold = payoffMatrix$threshold[i]
                                           )
  # Calculate payoff and save it to the corresponding row
  payoffMatrix$payoff[i] <- confMatrix[1, 1]*250 + confMatrix[1, 2]*(-1000)
}
payoffMatrix


# Split data in train and test set
set.seed(534381) 
defaultData$isTrain <- rbinom(nrow(defaultData), 1, 0.66)
train <- subset(defaultData, isTrain == 1)
test <- subset(defaultData, isTrain  == 0)

logitTrainNew <- glm(formulaLogit, family = binomial, data = train) # Modeling
test$predNew <- predict(logitTrainNew, type = "response", newdata = test) # Predictions

# Out-of-sample confusion matrix and accuracy
confMatrixModelNew <- SDMTools::confusion.matrix(test$PaymentDefault, test$predNew, threshold = 0.3) 
sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew) # Compare this value to the in-sample accuracy


# Accuracy function
costAcc <- function(r, pi = 0) {
  cm <- SDMTools::confusion.matrix(r, pi, threshold = 0.3)
  acc <- sum(diag(cm)) / sum(cm)
  return(acc)
}

# Cross validated accuracy for logitModelNew
set.seed(534381)
boot::cv.glm(defaultData, logitModelNew, cost = costAcc, K = 6)$delta[1]

```
  
  
  
***
  
Chapter 3 - Modeling Time to Reorder with Survival Analysis  
  
Survival Analysis Introduction:  
  
* Often have "censored" data, meaning that the customer journeys are not yet complete  
	* Random Type I Right censoring is the most common - a point can only be observed if it has occurred before time X, and it is otherwise unknowable (but known that they have not yet churned)  
    * Can plot histograms of whether someone has churned depending on the length of time  
    * plotTenure <- dataSurv %>% mutate(churn = churn %>% factor(labels = c("No", "Yes"))) %>%  
    *   ggplot() + geom_histogram(aes(x = tenure, fill = factor(churn))) + facet_grid( ~ churn) +           
    *   theme(legend.position = "none")  
* Survival analysis attempts to estimate when something will happen (churn, second order, renewal, etc.)  
  
Survival curve analysis by Kaplan-Meier:  
  
* Begin by creating a new object containing the survival attribute  
	* cbind(dataSurv %>% select(tenure, churn), surv = Surv(dataSurv$tenure, dataSurv$churn)) %>% head(10)  
* The survival function is the probability of "no event" in cumulative by time t  
	* The hazard function is the cumulative probability of "event" by time t  
    * The "hazard rate" is the probability of the event happening in a small time, provided that it has not yet happened  
* The Kaplan-Meier analysis can be used to estimate survival  
	* fitKM <- survival::survfit(Surv(dataSurv$tenure, dataSurv$churn) ~ 1, type = "kaplan-meier")  
    * print(fitKM)  # gives a few rough summary statistics  
    * plot(fitKM) # survival curve with confidence interval  
    * fitKMstr <- survfit(Surv(tenure, churn) ~ Partner, data = dataSurv)  # add covariates, such as ~ Partner rather than ~1 as in the baseline  
  
Cox PH model with constant covariates:  
  
* Model definition: cannot parse to ISO - see Excel notes  
	* Predictors are lineary and multiplicatively related to the hazard function, lambda  
    * Relative hazard function needs to remain constant over time  
* Fitting a survival model in R  
	* library(rms)  
    * units(dataSurv$tenure) <- "Month"  
    * dd <- datadist(dataSurv)  
    * options(datadist = "dd")  
    * fitCPH1 <- cph(Surv(tenure, churn) ~ gender + SeniorCitizen + Partner + Dependents + StreamMov + PaperlessBilling + PayMeth + MonthlyCharges, data = dataSurv, x = TRUE, y = TRUE, surv = TRUE, time.inc = 1)  
    * Coefficient interpretation is relatively similar to logistic regression - exp(fitCPH1$coefficients) - can simplify the coefficients be making them multiplicative (1.00 is no impact)  
    * survplot(fitCPH1, MonthlyCharges, label.curves = list(keys = 1:5))  # plots the survival probabilities based on varying 1 variable, assuming other variables constant  
    * survplot(fitCPH1, Partner)  # covariate with partner, plotted  
    * plot(summary(fitCPH1), log = TRUE)  # visualizing the hazard ratios  
  
Checking model assumptions and making predictions:  
  
* Can again use the Cox PH function  
	* testCPH1 <- cox.zph(fitCPH1)  
    * print(testCPH1)  # if p < 0.05, can reject the assumption that the predictor meets the proportional hazard assumption  
    * plot(testCPH1, var = "Partner=Yes")  
    * plot(testCPH1, var = "MonthlyCharges")  
    * This test is conservative and sensitive to the number of observations  
* If the PH (proportional hazard) assumptions are violated, can correct for this using  
	* fitCPH2 <- cph(Surv(tenure, churn) ~ MonthlyCharges + SeniorCitizen + Partner + Dependents + StreamMov + Contract, stratum = "gender = Male", data = dataSurv, x = TRUE, y = TRUE, surv = TRUE)  
    * rms::validate(fitCPH1, method = "crossvalidation", B = 10, pr = FALSE)  # pr=FALSE means only print at the end; R2 is the R-squared corrected by cross-validation  
* Can then assess probabilities for the event to occur  
	* oneNewData <- data.frame(gender = "Female", SeniorCitizen = "Yes", Partner = "No", Dependents = "Yes", StreamMov = "Yes", PaperlessBilling = "Yes", PayMeth = "BankTrans(auto)", MonthlyCharges = 37.12)  
    * str(survest(fitCPH1, newdata = oneNewData, times = 3))  
    * plot(survfit(fitCPH1, newdata = oneNewData))  
    * print(survfit(fitCPH1, newdata = oneNewData))  
  
Example code includes:  
```{r}

survData <- readr::read_delim("./RInputFiles/survivalDataExercise.csv", delim=",")


dataNextOrder <- survData %>%
    select(daysSinceFirstPurch, boughtAgain)

# Look at the head of the data
head(dataNextOrder)

# Plot a histogram
ggplot(dataNextOrder) +
  geom_histogram(aes(x = daysSinceFirstPurch, fill = factor(boughtAgain))) +
  facet_grid( ~ boughtAgain) + # Separate plots for boughtAgain = 1 vs. 0
  theme(legend.position = "none") # Don't show legend


# Create survival object
survObj <- survival::Surv(dataNextOrder$daysSinceFirstPurch, dataNextOrder$boughtAgain)

# Look at structure
str(survObj)


# Compute and print fit
fitKMSimple <- survival::survfit(survObj ~ 1)
print(fitKMSimple)

# Plot fit
plot(fitKMSimple, conf.int = FALSE, xlab = "Time since first purchase", 
     ylab = "Survival function", main = "Survival function"
     )


dataNextOrder <- survData %>%
    select(daysSinceFirstPurch, boughtAgain, voucher)

# Compute fit with categorical covariate
fitKMCov <- survival::survfit(survObj ~ voucher, data = dataNextOrder)

# Plot fit with covariate and add labels
plot(fitKMCov, lty = 2:3, xlab = "Time since first purchase", 
     ylab = "Survival function", main = "Survival function"
     )
legend(90, .9, c("No", "Yes"), lty = 2:3)


dataNextOrder <- survData

# Determine distributions of predictor variables
dd <- rms::datadist(dataNextOrder)
options(datadist = "dd")

# Compute Cox PH Model and print results
fitCPH <- rms::cph(survival::Surv(daysSinceFirstPurch, boughtAgain) ~ 
                       shoppingCartValue + voucher + returned + gender, data = dataNextOrder, 
                   x = TRUE, y = TRUE, surv = TRUE
                   )
print(fitCPH)

# Interpret coefficients
exp(fitCPH$coefficients)

# Plot result summary
plot(summary(fitCPH), log = TRUE)


# Check proportional hazard assumption and print result
testCPH <- survival::cox.zph(fitCPH)
print(testCPH)

# Plot time-dependent beta
plot(testCPH, var = "gender=male")

# Validate model
rms::validate(fitCPH, method = "crossvalidation", B = 10, dxy = TRUE, pr = FALSE)


# Create data with new customer
newCustomer <- data.frame(daysSinceFirstPurch = 21, shoppingCartValue = 99.9, gender = "female", 
                          voucher = 1, returned = 0, stringsAsFactors = FALSE
                          )

# Make predictions
pred <- survival::survfit(fitCPH, newdata = newCustomer)
print(pred)
plot(pred)

# Correct the customer's gender
newCustomer2 <- newCustomer
newCustomer2$gender <- "male"

# Redo prediction
pred2 <- survival::survfit(fitCPH, newdata = newCustomer2)
print(pred2)

```
  
  
  
***
  
Chapter 4 - Reducing Dimensionality with Principal Component Analysis  
  
PCA for CRM Data - address mutlicollinearity and data volume issues in the raw CRM data:  
  
* PCA reduces a large number of correlated variables to a smaller number of uncorrelated (orthogonal) variables  
* PCA can also help with creating an index, such as using the first component of the PCA  
* All variables must be either continuous or binary prior to running the PCA analysis  
	* dataCustomers %>% cor() %>% corrplot()  # plot the initial correlations  
  
PCA Computation:  
  
* Need to manage for variance, otherwise high-variance variables will be over-represented in the PCA  
	* lapply(dataCustomers, var)  
    * dataCustomers <- dataCustomers %>% scale() %>% as.data.frame()  
    * pcaCust <- prcomp(dataCustomers)  
    * pcaCust$sdev %>% round(2)  # standard deviations by component  
    * pcaCust$sdev ^ 2 %>% round(2)  # variances, also known as eigenvalues, by component give a good sense for relative importance (relative ratio is percent of variance explained)  
    * round(pcaCust$rotation[, 1:6], 2)  # correlations between original variables and principal components (can use these to give descriptive names to components)  
* Values of the observations are the weightings for the PC to make up the underlying data  
	* sum(dataCustomers[1,] * pcaCust$rotation[,1])  # Value on 1st component for 1st customer  
    * pcaCust$x[1:5, 1:6]  # first 5 customers and first 6 component loadings (weightings)  
  
PCA Model Specification:  
  
* Need to decide on how many components to keep - balance size of data vs. reconstruction of original data  
	* Can set a minimum requirement for percentage of variance explained (such as 70%)  
    * summary(pcaCust)  # will show cumulatives also  
    * Can use the Kaiser-Guttman criteria, which keeps only components with an eigenvalue of 1 (since 1 is the average)  
    * Can also draw a scree plot to see the variances (eigenvalues) in descending order - look for an elbow  
    * screeplot(pcaCust, type = "lines")  
    * Generally, use a few different techniques, and pick a number that is "in the range"  
* The biplot can help to show how the data map on to the principal components  
	* biplot(pcaCust, choices = 1:2, cex = 0.7)  # will show PC1 and PC2, with arrows for the various features and how they map on them  
  
Principal components in a regression analysis:  
  
* PCA can help to solve the multi-collinearity problem in a regression  
	* dataCustComponents <- cbind(dataCustomers[, "customerSatis"], pcaCust$x[, 1:6]) %>% as.data.frame  
    * mod2 <- lm(customerSatis ~ ., dataCustComponents)  
    * vif(mod2)  # by construction, these will all be 1, since the principal components are orthogonal  
* Factor analysis is another dimension-reduction technique, sometimes confused with PCA  
	* Factor analysis theorizes that latent constructs (e.g., intelligence) which cannot be directly measured are influencing the observed variables  
    * Factor analysis is often used in questionnaires - factor analysis can investigate where multiple questions really just measure one thing  
    * In contrast, with PCA, the features are actually being combined to model the data  
  
Wrap up:  
  
* Logistic regression for churn  
* Survival analysis to prevent churn  
* Principal component analysis (PCA) to reduce multicollinearity  
  
Example code includes:  
```{r}

load("./RInputFiles/newsData.RData")

rawData <- newsData
newsData <- newsData[, c('n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords', 'is_weekend', 'kw_avg_min', 'kw_avg_avg', 'kw_avg_max', 'average_token_length', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'avg_positive_polarity', 'avg_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity')]


# Overview of data structure:
str(newsData, give.attr = FALSE)

# Correlation structure:
newsData %>% cor() %>% corrplot::corrplot()


# Standardize data
newsData <- newsData %>% scale() %>% as.data.frame()

# Compute PCA
pcaNews <- newsData %>% prcomp()

# Eigenvalues
pcaNews$sdev**2


# Screeplot:
screeplot(pcaNews, type = "lines")

# Cumulative explained variance:
summary(pcaNews)

# Kaiser-Guttmann (number of components with eigenvalue larger than 1):
sum(pcaNews$sdev > 1)


# Print loadings of the first six components
pcaNews$rotation[, 1:6] %>% round(2)

pcaNews %>% biplot(choices=1:2, cex = 0.5)


# Predict log shares with all original variables
logShares <- rawData %>%
    select(shares) %>%
    mutate(logShares=log(1+shares)) %>%
    pull(logShares) %>%
    scale()

newsData <- newsData %>%
    cbind(logShares)

mod1 <- lm(logShares ~ ., data = newsData)

# Create dataframe with log shares and first 6 components
dataNewsComponents <- cbind(logShares = newsData[, "logShares"], pcaNews$x[, 1:6]) %>%
  as.data.frame()

# Predict log shares with first six components
mod2 <- lm(logShares ~ ., data = dataNewsComponents)

# Print adjusted R squared for both models
summary(mod1)$adj.r.squared
summary(mod2)$adj.r.squared

```
  
  
  
***
  
###_Interactive Maps with leaflet in R_  
  
Chapter 1 - Setting Up Interactive Web Maps  
  
Introduction to leaflet - open-source JavaScript library that makes interactive, mobile-friendly maps:  
  
* Objective for this course is to build up to an interactive map of 4-year colleges, including incorporation type (public, private, etc.)  
	* Additionally, labels that occur when hovering  
* Leaflet builds maps using tiles, which join many smaller maps together  
	* library(leaflet)  
    * leaflet() %>% addTiles()  # zooming and scrolling lead to new tiles being shown  
* In Chapter 1, will use multiple tile types to create maps of the DataCamp HQ in Belgium and Boston  
	* leaflet() %>% addProviderTiles("CartoDB") %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  
  
Map tiles - over 100 pre-canned maps that are available as bases:  
  
* Selecting a base map - consider the intended purpose of the map, and ensure that the maps selected meet that purpose  
	* Instructor has a preference for gray-scale maps (for ease of seeing other data)  
* The base maps are stored as "providers" - most are available for immediate use, but a few require registration  
	* names(providers)  # get all the available providers  
    * names(providers)[str_detect(names(providers), "OpenStreetMap")]  # all from OpenStreetMap  
    * leaflet() %>% # addTiles() addProviderTiles("OpenStreetMap.BlackAndWhite")  # replace the default with the BW OpenStreetMap  
  
Setting the default map view:  
  
* Can load the map centered on a specific point and with a requested zoom level - coomon to use ggmap::geocode()  
	* ggmap::geocode("350 5th Ave, New York, NY 10118")   # will return the lat-lon where possible (uses google API unless source="dsk" is chosen)  
* Can use either setView() or fitBounds()  
	* leaflet() %>% addTiles() %>% setView(lng = -73.98575, lat = 40.74856, zoom = 13)  # setView picks a lat/lon and zoom  
    * leaflet() %>% addTiles() %>% fitBounds( lng1 = -73.910, lat1 = 40.773, lng2 = -74.060, lat2 = 40.723)  # fitBounds defines a rectangle  
* Can limit user controls such as panning and zooming  
	* leaflet(options = leafletOptions(dragging = FALSE, minZoom = 14, maxZoom = 18)) %>% addProviderTiles("CartoDB") %>% setView(lng = -73.98575, lat = 40.74856, zoom = 18)  
    * dragging=FALSE removes the ability to pan  
    * maxZoom and minZoom limit the options for zooming  
    * leaflet() %>% addTiles() %>% setView(lng = -73.98575, lat = 40.74856, zoom = 18) %>% setMaxBounds(lng1 = -73.98575, lat1 = 40.74856, lng2 = -73.98575, lat2 = 40.74856)  
    * setMaxBounds() limits the user to the boundaries that you pre-specify  
* For more information, can go to  
	* http://leafletjs.com/reference-1.3.0.html  
    * https://rstudio.github.io/leaflet/  
  
Plotting DataCamp HQ:  
  
* Location markers are a common addition, managed using addMarkers()  
	* leaflet() %>% addTiles() %>% addMarkers(lng = -73.98575, lat = 40.74856)  
    * If single vectors are passed to lng and lat, then a single blue pin will be placed and the map will be centered/zoomed there  
    * dc_hq <- tibble( hq = c("DataCamp - NYC", "DataCamp - Belgium"), lon = c(-73.98575, 4.717863), lat = c(40.74856, 50.881363))  
    * leaflet() %>% addTiles() %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat)  
    * When the tibble is passed, then the map will be zoomed/centered such that all the pins can be displayed
dc_hq %>% leaflet() %>% addTiles() %>% addMarkers()   
    * The functions will seek a lat and lon column from the piped in data (dc_hq in this case), and pass along a note that they were used  
* Pop-ups are a common way to provide additional information about a marker  
	* leaflet() %>% addTiles() %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  # markers, with popup enabled on clicking  
    * leaflet() %>% addTiles() %>% addPopups(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  # popups instead of markers  
* Leaflets can be stored as objects (similar to ggplot2), with additions and prints and whatnot called later  
  
Example code includes:  
```{r}

# Load the leaflet library
library(leaflet)

# Create a leaflet map with default map tile using addTiles()
leaflet() %>%
    addTiles()


# Print the providers list included in the leaflet library
providers

# Print only the names of the map tiles in the providers list 
names(providers)

# Use str_detect() to determine if the name of each provider tile contains the string "CartoDB"
str_detect(names(providers), "CartoDB")

# Use str_detect() to print only the provider tile names that include the string "CartoDB"
names(providers)[str_detect(names(providers), "CartoDB")]


# Change addTiles() to addProviderTiles() and set the provider argument to "CartoDB"
leaflet() %>% 
    addProviderTiles("CartoDB")

# Create a leaflet map that uses the Esri provider tile 
leaflet() %>% 
    addProviderTiles("Esri")

# Create a leaflet map that uses the CartoDB.PositronNoLabels provider tile
leaflet() %>% 
    addProviderTiles("CartoDB.PositronNoLabels")


# Map with CartoDB tile centered on DataCamp's NYC office with zoom of 6
leaflet()  %>% 
    addProviderTiles("CartoDB")  %>% 
    setView(lng = -73.98575, lat = 40.74856, zoom = 6)


dc_hq <- tibble::tibble(hq=c("NYC", "Belgium"), lon=c(-73.98575, 4.71786), lat=c(40.7486, 50.8814))
dc_hq

# Map with CartoDB.PositronNoLabels tile centered on DataCamp's Belgium office with zoom of 4
leaflet() %>% 
    addProviderTiles("CartoDB.PositronNoLabels") %>% 
    setView(lng = dc_hq$lon[2], lat = dc_hq$lat[2], zoom = 4)


leaflet(options = leafletOptions(
                    # Set minZoom and dragging 
                    minZoom = 12, dragging = TRUE))  %>% 
  addProviderTiles("CartoDB")  %>% 
  # Set default zoom level 
  setView(lng = dc_hq$lon[2], lat = dc_hq$lat[2], zoom = 14) %>% 
  # Set max bounds of map 
  setMaxBounds(lng1 = dc_hq$lon[2] + 0.05, 
               lat1 = dc_hq$lat[2] + .05, 
               lng2 = dc_hq$lon[2] - 0.05, 
               lat2 = dc_hq$lat[2] - .05) 


# Plot DataCamp's NYC HQ
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = dc_hq$lon[1], lat = dc_hq$lat[1])

# Plot DataCamp's NYC HQ with zoom of 12    
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = -73.98575, lat = 40.74856)  %>% 
    setView(lng = -73.98575, lat = 40.74856, zoom = 12)    

# Plot both DataCamp's NYC and Belgium locations
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = dc_hq$lon, lat = dc_hq$lat)


# Store leaflet hq map in an object called map
map <- leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    # add hq column of dc_hq as popups
    addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, 
               popup = dc_hq$hq
               )

# Center the view of map on the Belgium HQ with a zoom of 5  
map_zoom <- map %>% 
      setView(lat = 50.881363, lng = 4.717863, zoom = 5)

# Print map_zoom
map_zoom

```
  
  
  
***
  
Chapter 2 - Plotting points  
  
Introduction to IPEDS Data:  
  
* Can clear the boundaries of a map, while keeping everything else (data and the like) constant  
	* m %>% clearBounds()  # kills the bounds layers  
    * m %>% clearBounds() %>% clearMarkers()  # kills the markers layers  
* The IPEDS data is the Integrated Post-Secondary Education dataset - this course uses a subset consisting of 4-year colleges  
	* Goal is to create a subset of the IPEDS data consisting of the ~300 colleges in California  
    * Can then plot and color-code the California colleges  
  
Mapping California colleges:  
  
* Clustered markers are poorly shown by pins due to obscuring  
* A nice alternative is to use circle markers, which have much less tendency for overlaps  
	* maine_colleges_map %>% clearMarkers() %>% addCircleMarkers(data = maine, radius = 3)  
    * maine_colleges_map %>% addCircleMarkers( data = maine_colleges, radius = 4, color = "red", popup = ~name)  # custom color and radius while maintaining popups  
  
Labels and pop-ups:  
  
* Can use piping as well as the tilde, which allows for referring to key variables in the piped in data  
	* ipeds %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers( lng = ~lng, lat = ~lat, popup = ~name, color = "#FF0000")  
    * Colors can be specified using hexadecimal, as shown in the example above - can find these using google and color sliders  
* Can build better popups using pipes and tildes  
	* addCircleMarkers(popup = ~paste0(name, "-", sector_label)  
    * addCircleMarkers(popup = ~paste0("<b>",name,"</b>","<br/>",sector_label))  # enhanced with html tags  
* Labels provide similar information as pop-ups, but require only a hover rather than a click  
	* ipeds %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(label = ~name, radius = 2)  
  
Color coding colleges:  
  
* Can include differential colors depending on a variables that has been piped in using colorFactor()  
	* OR <- ipeds %>% filter(state == "OR")  
    * pal <- colorFactor(palette = c("red", "blue", "#9b4a11"), levels = c("Public", "Private", "For-Profit"))  # create the color palette for future use  
    * oregon_colleges <- OR %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(radius = 2, color = ~pal(sector_label), label = ~name)  # apply as pal()  
    * oregon_colleges %>% addLegend(position = "bottomright", pal = pal, values = c("Public", "Private", "For-Profit"))  # add to legend  
* Can instead color based on a numeric value using colorNumeric()  
	* admit <- admit %>% filter(!is.na(rate), rate < 50, rate > 0)  # filer for rates that exist and are between 0 and 50  
    * pal <- colorNumeric(palette = "Reds", domain = c(1:50), reverse = TRUE)  # reverse=TRUE flips the gradient so that lower admit rates are darker red  
    * admit_map <- admit %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(radius = 4, color = ~pal(rate), label = ~name) %>% addLegend(title = "Admit Rate", pal = pal, values = c(1:50), position = "bottomright")  
* Can use RColorBrewer for default color palettes  
	* library(RColorBrewer)  
    * display.brewer.all()  
  
Example code includes:  
```{r}

# Remove markers, reset bounds, and store the updated map in the m object
map <- map %>%
    clearMarkers() %>% 
    clearBounds()

# Print the cleared map
map


ipedsRaw <- readr::read_csv("./RInputFiles/ipeds.csv")


# Remove colleges with missing sector information
ipeds <- 
    ipedsRaw %>% 
    tidyr::drop_na()

# Count the number of four-year colleges in each state
ipeds %>% 
    group_by(state)  %>% 
    count()

# Create a list of US States in descending order by the number of colleges in each state
ipeds  %>% 
    group_by(state)  %>% 
    count()  %>% 
    arrange(desc(n))

# Create a dataframe called `ca` with data on only colleges in California
ca <- ipeds %>%
    filter(state == "CA")

map <- leaflet() %>% 
    addProviderTiles("CartoDB")

# Use `addMarkers` to plot all of the colleges in `ca` on the `m` leaflet map
map %>%
    addMarkers(lng = ca$lng, lat = ca$lat)


la_coords <- data.frame(lat = 34.05223, lon = -118.2437) 

# Center the map on LA 
map %>% 
    addMarkers(data = ca) %>% 
    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 12)

# Set the zoom level to 8 and store in the m object
map_zoom <-
    map %>%
    addMarkers(data = ca) %>%
    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 8)

map_zoom


# Clear the markers from the map 
map2 <- map %>% clearMarkers()

# Use addCircleMarkers() to plot each college as a circle
map2 %>%
    addCircleMarkers(lng = ca$lng, lat = ca$lat)

# Change the radius of each circle to be 2 pixels and the color to red
map2 %>% 
    addCircleMarkers(lng = ca$lng, lat = ca$lat, radius = 2, color = "red")


# Add circle markers with popups for college names
map %>%
    addCircleMarkers(data = ca, radius = 2, popup = ~name)

# Change circle color to #2cb42c and store map in map_color object
map_color <- map %>% 
    addCircleMarkers(data = ca, radius = 2, color = "#2cb42c", popup = ~name)

# Print map_color
map_color


# Clear the bounds and markers on the map object and store in map2
map2 <- map %>% 
    clearBounds() %>% 
    clearMarkers()

# Add circle markers with popups that display both the institution name and sector
map2 %>% 
    addCircleMarkers(data = ca, radius = 2, 
                     popup = ~paste0(name, "<br/>", sector_label)
                     )

# Make the institution name in each popup bold
map2 %>% 
    addCircleMarkers(data = ca, radius = 2, 
                     popup = ~paste0("<b>", name, "</b>", "<br/>", sector_label)
                     )


# Add circle markers with labels identifying the name of each college
map %>% 
    addCircleMarkers(data = ca, radius = 2, label = ~name)

# Use paste0 to add sector information to the label inside parentheses 
map %>% 
    addCircleMarkers(data = ca, radius = 2, label = ~paste0(name, " (", sector_label, ")"))


# Make a color palette called pal for the values of `sector_label` using `colorFactor()`  
# Colors should be: "red", "blue", and "#9b4a11" for "Public", "Private", and "For-Profit" colleges, respectively
pal <- colorFactor(palette = c("red", "blue", "#9b4a11"), 
                   levels = c("Public", "Private", "For-Profit")
                   )

# Add circle markers that color colleges using pal() and the values of sector_label
map2 <- map %>% 
        addCircleMarkers(data = ca, radius = 2, 
                         color = ~pal(sector_label), 
                         label = ~paste0(name, " (", sector_label, ")")
                         )

# Print map2
map2


# Add a legend that displays the colors used in pal
map2 %>% 
    addLegend(pal = pal, values = c("Public", "Private", "For-Profit"))

# Customize the legend
map2 %>% 
    addLegend(pal = pal, 
              values = c("Public", "Private", "For-Profit"),
              # opacity of .5, title of Sector, and position of topright
              opacity = 0.5, title = "Sector", position = "topright"
              )

```
  
  
  
***
  
Chapter 3 - Groups, Layers, Extras  
  
Leaflet Extras Package:  
  
* The leaflet.extras package provides some nice extensibility to the baseline leaflet package  
	* leaflet() %>% addTiles() %>% addSearchOSM()  # searching open-source-maps (magnifying glass icon with search box)  
    * leaflet() %>% addTiles() %>% addSearchOSM() %>% addReverseSearchOSM()  # can also use geocode to find a click, as requested by addReverseSearchOSM()  
    * leaflet() %>% addTiles() %>% addSearchOSM() %>% addReverseSearchOSM() %>% addResetMapButton()  # can click "reset" to return to the default view  
  
Overlay Groups - ability to control the segments that are displayed on the map:  
  
* One option is to segment the data in advance, then to add as layers using addCircleMarkers  
	* ca_public <- ipeds %>% filter(sector == "Public", state == "CA")  
    * m %>% addCircleMarkers( data = ca_public, group = "Public")  
* After creating multiple calls for addCircleMarkers(), each with group=, can then activate the grouping  
	* addLayersControl( overlayGroups = c("Public", "Private", "For-Profit"))  
* Since the layers are stacked, the order in which they are added matters (they layer/stack on top of each other)  
  
Base Groups - can provide multiple options for toggling (only one may be selected at a time):  
  
* Need to call addProviderTiles() once for each layer that is an option, then activate using addLayersControl()  
	* a <- leaflet() %>% addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>% addProviderTiles("Esri", group = "Esri")   
    * a %>% addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), position = "topleft")  
* Can be handy to try a few different base groups during exploratory analysis, to find the base that best matches the rest of the analysis  
* Basic four-step process for building up the base groups includes  
	* leaflet() %>% # initialize leaflet map  
    * addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>% addProviderTiles("Esri", group = "Esri") %>% # add basemaps with groups  
    * addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>% addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Private") %>% addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "For-Profit") %>% # add marker layer for each sector with corresponding group name  
    * addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), overlayGroups = c("Public", "Private", "For-Profit")) # add layer controls for base and overlay groups  
  
Pieces of Flair:  
  
* Can customize ths search function using leaflet.extra capability  
	* ca_public <- ipeds %>% filter(sector_label == "Public", state == "CA")  
    * ca_public %>% leaflet() %>% addProviderTiles("Esri") %>% addCircleMarkers(radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>% addSearchFeatures(targetGroups = 'Public', options = searchFeaturesOptions(zoom = 10))  # will filter the search on Public data, with a specified zoom  
* Can cluster the colleges to improve readability of the maps  
	* ipeds %>% leaflet() %>% addTiles() %>% addCircleMarkers(radius = 2, color = ~pal(sector_label), clusterOptions = markerClusterOptions())  # many colleges in one circle  
  
Example code includes:  
```{r}

library(leaflet.extras)
library(htmltools)

leaflet() %>%
  addTiles() %>% 
  addSearchOSM() %>% 
  addReverseSearchOSM() 


m2 <- ipeds %>% 
    leaflet() %>% 
    # use the CartoDB provider tile
    addProviderTiles("CartoDB") %>% 
    # center on the middle of the US with zoom of 3
    setView(lat = 39.8282, lng = -98.5795, zoom=3)

# Map all American colleges 
m2 %>% 
    addCircleMarkers() 


# Create data frame called public with only public colleges
public <- filter(ipeds, sector_label == "Public")  

# Create a leaflet map of public colleges called m3 
m3 <- leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Public"
                     )

m3


# Create data frame called private with only private colleges
private <- filter(ipeds, sector_label == "Private")  

# Add private colleges to `m3` as a new layer
m3 <- m3 %>% 
    addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Private"
                     ) %>% 
    addLayersControl(overlayGroups = c("Public", "Private"))

m3


# Create data frame called profit with only for-profit colleges
profit <- filter(ipeds, sector_label == "For-Profit")  

# Add for-profit colleges to `m3` as a new layer
m3 <- m3 %>% 
    addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label),   group = "For-Profit"
                     )  %>% 
    addLayersControl(overlayGroups = c("Public", "Private", "For-Profit"))  

# Center the map on the middle of the US with a zoom of 4
m4 <- m3 %>%
    setView(lat = 39.8282, lng = -98.5795, zoom = 4) 
        
m4


leaflet() %>% 
  # Add the OSM, CartoDB and Esri tiles
  addTiles(group = "OSM") %>% 
  addProviderTiles("CartoDB", group = "Carto") %>% 
  addProviderTiles("Esri", group = "Esri") %>% 
  # Use addLayersControl to allow users to toggle between basemaps
  addLayersControl(baseGroups = c("OSM", "Carto", "Esri"))


m4 <- leaflet() %>% 
    addTiles(group = "OSM") %>% 
    addProviderTiles("CartoDB", group = "Carto") %>% 
    addProviderTiles("Esri", group = "Esri") %>% 
    addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label),  group = "Public"
                     ) %>% 
    addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Private"
                     )  %>% 
    addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "For-Profit"
                     )  %>% 
    addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), 
                     overlayGroups = c("Public", "Private", "For-Profit")
                     ) %>% 
    setView(lat = 39.8282, lng = -98.5795, zoom = 4) 

m4


ipeds %>% 
    leaflet() %>% 
    addTiles() %>% 
    # Sanitize any html in our labels
    addCircleMarkers(radius = 2, label = ~htmlEscape(name), 
                     # Color code colleges by sector using the `pal` color palette 
                     color = ~pal(sector_label), 
                     # Cluster all colleges using `clusterOptions` 
                     clusterOptions = markerClusterOptions()
                     ) 

```
  
  
  
***
  
Chapter 4 - Plotting Polygons  
  
Spatial Data - ability to plot polygons rather than points:  
  
* Polygons have many points, and are stored in SPDF (Spatial Polygons Data Frame) with 5 slots  
	* data - one observation per polygon  
    * polygons - coordinates to plot each polygon  
    * plotOrder - order for plotting  
    * bbox - rectangle containing all the polygons  
    * proj4string - coordinate reference system (CRS)  
    * All accessed using the @ symbol  
* Can join from the data component of the SPDF, accessed using @  
	* hp@data <- shp@data %>% left_join(nc_income, by = c("GEOID10" = "zipcode"))  
    * shp@polygons[[1]] %>% leaflet() %>% addPolygons()  # can plot a single polygon  
  
Mapping Polygons - can pipe SPDF in to a series of leaflet calls:  
  
* The basic polygon plotting method using leaflet() may produce shape boundaries that are too thick  
	* shp %>% leaflet() %>% addTiles() %>% addPolygons()  
    * weight - thickness of lines  
    * color - color of lines  
    * label - information shown on hover  
    * highlight - options to highlight polygon on hover  
* The refined plotting approach adds customization for better readability  
	* shp %>% leaflet() %>% addTiles() %>% addPolygons(weight = 1, color = "grey", label = ~paste0("Total Income: " dollar(income)), highlight = highlightOptions(weight = 3, color = "red", bringToFront = TRUE))  
* Can color numeric data when plotting polygons  
	* colorNumeric - maps continuous data to interpolated palettes  
    * colorBin - colors based on cut function  
    * colorQuantile - colors based on quantile  
    * nc_pal <- colorNumeric(palette = "Blues", domain = high_inc@data$mean_income)  
    * nc_pal <- colorBin(palette = "YlGn", bins = 5, domain = high_inc@data$mean_income)  
    * nc_pal <- colorQuantile(palette = "YlGn", n = 4, domain = high_inc@data$mean_income)  
* Example of coloring using colorNumeric()  
	* nc_pal <- colorNumeric("Blues", domain = high_inc@data$mean_income)  
    * previewColors(pal = nc_pal, values = c(seq(100000, 600000, by = 100000)))  # explore sample values  
    * shp %>% leaflet() %>% # addTiles() %>% addPolygons(weight = 1, fillOpacity = 1, color = ~nc_pal(mean_income), label = ~paste0("Mean Income: ", dollar(mean_income)), highlight = highlightOptions(weight = 3, color = "red", bringToFront = TRUE))  
* Sometimes need to log-transform skewed data for better displays  
  
Putting Everything Together:  
  
* Leaflet and htmlwidgets for base maps and coloring  
* Base and overlay groups to enhance interactivity  
* Features available in the leaflet.extras function  
* Can piece together a full map that includes both polygons and circle markers  
	* leaflet() %>% addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>%           
	*   addProviderTiles("Esri", group = "Esri") %>%  
    *   addPolygons(data = shp, weight = 1, fillOpacity = .75, color = ~nc_pal(log(mean_income)), label = ~paste0("Mean Income: ", dollar(mean_income)), group = "Mean Income") %>%  
    *   addCircleMarkers(data = nc_public, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>%  
    *   addCircleMarkers(data = nc_private, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Private") %>%  
    *   addCircleMarkers(data = nc_profit, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "For-Profit") %>%  
    * addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), overlayGroups = c("Public", "Private", "For-Profit", "Mean Income"))  
* Can also save a map for future use  
	* m <- leaflet() %>% addTiles() %>% addMarkers( data = ipeds, clusterOptions = markerClusterOptions())%>% addPolygons(data = shp)  
    * library(htmlwidgets)  
    * saveWidget(m, file="myMap.html")  # saves the file as html  
  
Wrap up - additional resources:  
  
* RStudio's leaflet website: https://rstudio.github.io/leaflet/  
* Leaflet extras: https://github.com/bhaskarvk/leaflet.extras  
* JavaScript library: http://leafletjs.com/  
  
Example code includes:  
```{r}


load("./RInputFiles/nc_zips.Rda")
load("./RInputFiles/wealthiest_zips.Rda")
nc_income <- readr::read_csv("./RInputFiles/mean_income_by_zip_nc.csv")
str(nc_income, give.attr = FALSE)


# Print a summary of the `shp` data
summary(shp)

# Print the class of `shp`
class(shp)

# Print the slot names of `shp`
slotNames(shp)


# Glimpse the data slot of shp
glimpse(shp@data)

# Print the class of the data slot of shp
class(shp@data)

# Print GEOID10
shp@data$GEOID10
shp@data$GEOID10 <- as.integer(as.character(shp@data$GEOID10))
str(shp@data$GEOID10)


# Glimpse the nc_income data
glimpse(nc_income)

# Summarise the nc_income data
summary(nc_income)

# Left join nc_income onto shp@data and store in shp_nc_income
shp_nc_income <- shp@data %>% 
                left_join(nc_income, by = c("GEOID10" = "zipcode"))

# Print the number of missing values of each variable in shp_nc_income
shp_nc_income %>%
  summarise_all(funs(sum(is.na(.))))


shp <- merge(shp, shp_nc_income, by=c("GEOID10", "ALAND10"))


# map the polygons in shp
shp %>% 
    leaflet() %>% 
    addTiles() %>% 
    addPolygons()

# which zips were not in the income data?
shp_na <- shp[is.na(shp$mean_income),]

# map the polygons in shp_na
shp_na %>% 
    leaflet() %>% 
    addTiles() %>% 
    addPolygons()


# summarise the mean income variable
summary(shp$mean_income)

# subset shp to include only zip codes in the top quartile of mean income
high_inc <- shp[!is.na(shp$mean_income) & shp$mean_income > 55917,]

# map the boundaries of the zip codes in the top quartile of mean income
high_inc %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons()


dollar <- function (x, negative_parens=TRUE, prefix="$", suffix="") {
    # KLUGE to make this work . . . 
    needs_cents <- function(...) { FALSE }
    if (length(x) == 0) 
        return(character())
    x <- plyr::round_any(x, 0.01)
    if (needs_cents(x, largest_with_cents)) {
        nsmall <- 2L
    }
    else {
        x <- plyr::round_any(x, 1)
        nsmall <- 0L
    }
    negative <- !is.na(x) & x < 0
    if (negative_parens) {
        x <- abs(x)
    }
    amount <- format(abs(x), nsmall = nsmall, trim = TRUE, big.mark = ",", scientific = FALSE, digits = 1L)
    if (negative_parens) {
        paste0(ifelse(negative, "(", ""), prefix, amount, suffix, ifelse(negative, ")", ""))
    }
    else {
        paste0(prefix, ifelse(negative, "-", ""), amount, suffix)
    }
}


# create color palette with colorNumeric()
nc_pal <- colorNumeric("YlGn", domain = high_inc@data$mean_income)

high_inc %>%
  leaflet() %>%
  addTiles() %>%
  # set boundary thickness to 1 and color polygons blue
  addPolygons(weight = 1, color = ~nc_pal(mean_income),
              # add labels that display mean income
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              # highlight polygons on hover
              highlight = highlightOptions(weight = 5, color = "white",
              bringToFront = TRUE))


# Create a logged version of the nc_pal color palette
nc_pal <- colorNumeric("YlGn", domain = log(high_inc@data$mean_income))

# apply the nc_pal
high_inc %>%
  leaflet() %>%
  addProviderTiles("CartoDB") %>%
  addPolygons(weight = 1, color = ~nc_pal(log(mean_income)), fillOpacity = 1,
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlightOptions = highlightOptions(weight = 5, color = "white", bringToFront = TRUE))


# Print the slot names of `wealthy_zips`
slotNames(wealthy_zips)

# Print a summary of the `mean_income` variable
summary(wealthy_zips$mean_income)

# plot zip codes with mean incomes >= $200k
wealthy_zips %>% 
  leaflet() %>% 
  addProviderTiles("CartoDB") %>% 
  addPolygons(weight = 1, fillOpacity = .7, color = "Green",  group = "Wealthy Zipcodes", 
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlightOptions = highlightOptions(weight = 5, color = "white", bringToFront = TRUE))


# Add polygons using wealthy_zips
final_map <- m4 %>% 
   addPolygons(data = wealthy_zips, weight = 1, fillOpacity = .5, color = "Grey",  group = "Wealthy Zip Codes", 
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlight = highlightOptions(weight = 5, color = "white", bringToFront = TRUE)) %>% 
    # Update layer controls including "Wealthy Zip Codes"
    addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), 
                         overlayGroups = c("Public", "Private", "For-Profit", "Wealthy Zip Codes"))

# Print and explore your very last map of the course!
final_map

```
  
  
  
***
  
###_Support Vector Machines in R_  
  
Chapter 1 - Introduction  
  
Sugar content of soft drinks:  
  
* Course covers Support Vector Machines (SVM), including visualization, mechanics, situations where they work best, etc.  
	* Will stick with binary classification for this course  
* For a 1-dimensional dataset, the clusters can be separated by choosing a "separating boundary" (decision boundary)  
* Margins are the distances between the decision boundary and the closest point  
	* The best decision boundary is considered to be the decision boundary that maximizes the margin (more robust to noise)  
    * The SVM tries to find the decision boundary that maximizes the margin in n-dimensions  
  
Generating a linearly separable dataset  
  
* Can use runif to generate random data that is unifotm from 0 to 1  
	* n <- 200  
    * set.seed(42)  
    * df <- data.frame(x1 = runif(n), x2 = runif(n))  
* Can define the points with x1 < x2 as class A and the points with x1 > x2 as class B  
	* Can also create a margin by filtering out points where abs(x1-x2) is below a user-specified threshold  
  
Example code includes:  
```{r}

df <- data.frame(sample=1:25, 
                 sugar_content=c(10.9, 10.9, 10.6, 10, 8, 8.2, 8.6, 10.9, 10.7, 8, 7.7, 7.8, 8.4, 11.5, 11.2, 8.9, 8.7, 7.4, 10.9, 10, 11.4, 10.8, 8.5, 8.2, 10.6)
                 )
str(df)

#print variable names
names(df)

#build plot
plot_ <- ggplot(data = df, aes(x = sugar_content, y = c(0))) + 
    geom_point() + 
    geom_text(label = df$sugar_content, size = 2.5, vjust = 2, hjust = 0.5)

#display plot
plot_


#The maximal margin separator is at the midpoint of the two extreme points in each cluster.
mm_separator <- (8.9 + 10)/2


#create data frame
separator <- data.frame(sep = c(mm_separator))

#add ggplot layer 
plot_ <- plot_ + 
  geom_point(data = separator, x = separator$sep, y = c(0), color = "blue", size = 4)

#display plot
plot_


#set seed
set.seed(42)

#set number of data points. 
n <- 600

#Generate data frame with two uniformly distributed predictors lying between 0 and 1.
df <- data.frame(x1 = runif(n), x2 = runif(n))

#classify data points depending on location
df$y <- factor(ifelse(df$x2 - 1.4*df$x1 < 0, -1, 1), levels = c(-1, 1))


#set margin
delta <- 0.07

# retain only those points that lie outside the margin
df1 <- df[abs(1.4*df$x1 - df$x2) > delta, ]

#build plot
plot_margins <- ggplot(data = df1, aes(x = x1, y = x2, color = y)) + geom_point() + 
    scale_color_manual(values = c("red", "blue")) + 
    geom_abline(slope = 1.4, intercept = 0)+
    geom_abline(slope = 1.4, intercept = delta, linetype = "dashed") +
    geom_abline(slope = 1.4, intercept = -delta, linetype = "dashed")
 
#display plot 
plot_margins

```
  
  
  
***
  
Chapter 2 - Support Vector Classifiers - Linear Kernels  
  
Linear Support Vector Machines:  
  
* Can split the data from the previous chapter (perfectly separable) in to train/test on an 80-20 basis  
	* set.seed() = 1  
    * df[, "train"] <- ifelse(runif(nrow(df))<0.8,1,0)  
    * trainset <- df[df$train==1,]  
    * testset <- df[df$train==0,]  
    * trainColNum <- grep("train", names(trainset))  
    * trainset <- trainset[,-trainColNum]  
    * testset <- testset[,-trainColNum]  
* Decision boundaries have many shapes-types (called kernels) such as lines, polynomials, etc.  
* For this chapter, will use e1071::svm(), a function with many options  
	* formula, data, type ("C-classification" for classification), kernel ("linear" for this chapter), cost/gamma (tuning parameters, which will be left at the defaults for now), scale (boolean telling whether to scale the data in advance - FALSE makes for easier plotting, but typically would be set to TRUE in the real-world)  
* Example of running e1071::svm()  
	* library(e1071)  
    * svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear", scale = FALSE)  
    * svm_model  
    * svm_model$index  # indices of the support vectors  
    * svm_model$SV  # support vector coordinates  
    * svm_model$rho  # negative y-intercept of the decision boundary  
    * svm_model$coefs  # weighting coefficients of support vectors (magnitude is importance, side is which part of boundary)  
    * pred_train <- predict(svm_model,trainset)  
    * pred_test <- predict(svm_model,testset)  
  
Visualizing Linear SVM:  
  
* Can begin by plotting the training data, distinguished by color  
	* p <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + geom_point() + scale_color_manual(values = c("red","blue"))  
    * df_sv <- trainset[svm_model$index,]  
    * p <- p + geom_point(data = df_sv, aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.5)  
    * p  
* The support vectors tend to be close to the decision boundary - in fact, they are defined as points that "support" the boundary  
* Goal is to extract the slope and coefficients from the model (not stored in the model object)  
	* w <- t(svm_model$coefs) %*% svm_model$SV  
    * slope_1 <- -w[1]/w[2]  
    * intercept_1 <- svm_model$rho/w[2]  
    * p <- p + geom_abline(slope = slope_1, intercept = intercept_1)  
    * p <- p + geom_abline(slope = slope_1, intercept = intercept_1-1/w[2], linetype = "dashed") + geom_abline(slope = slope_1, intercept = intercept_1+1/w[2], linetype = "dashed")  
    * p  
* There are several properties observed in the plot  
	* The boundary is supported by the support vectors  
    * The boundary is "soft", which allows for uncertainty in location/shape of the boundary  
    * Can also use e1071::plot(x=myModel, data=myData) to plot the function  
  
Tuning Linear SVM:  
  
* Can tweak the cost parameter to change the size of the soft boundary for the SVM  
	* Higher costs lead to harder (smaller, narrower) decision boundaries, with fewer support vectors  
    * The implication is that raising the cost can be a good idea if the data are known to be linearly separable  
  
Multi-class problems:  
  
* SVM can manage classification problems with 3+ target types also - using the example iris data  
	* p <- ggplot(data = iris, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point()  
    * p  
* The SVM at core is a binary classifier, but can be used in a multi-class setting  
	* Have a model for each of the choose(m, 2) possible combinations, and use majority voting on the outputs (ties broken by random)  
    * This method is called the "one against one" classification, and it is automatically included in e1071  
    * svm_model<- svm(Species ~ ., data = trainset, type = "C-classification", kernel = "linear")  # all run automatically  
  
Example code includes:  
```{r}


dfOld <- df
delta <- 0.07
df <- df[abs(1.4*df$x1 - df$x2) > delta, ]


#split train and test data in an 80/20 proportion
df[, "train"] <- ifelse(runif(nrow(df))<0.8, 1, 0)

#assign training rows to data frame trainset
trainset <- df[df$train == 1, ]
#assign test rows to data frame testset
testset <- df[df$train == 0, ]

#find index of "train" column
trainColNum <- grep("train", names(df))

#remove "train" column from train and test dataset
trainset <- trainset[, -trainColNum]
testset <- testset[, -trainColNum]


library(e1071)

#build svm model, setting required parameters
svm_model<- svm(y ~ ., 
                data = trainset, 
                type = "C-classification", 
                kernel = "linear", 
                scale = FALSE)


#list components of model
names(svm_model)

#list values of the SV, index and rho
svm_model$SV
svm_model$index
svm_model$rho

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)


#build scatter plot of training dataset
scatter_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c("red", "blue"))
 
#add plot layer marking out the support vectors 
layered_plot <- 
    scatter_plot + geom_point(data = trainset[svm_model$index, ], aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.5)

#display plot
layered_plot


#calculate slope and intercept of decision boundary from weight vector and svm model
w <- c(x1=6.55241, x2=-4.73278)  # calculated manually outside of this module
slope_1 <- -w[1]/w[2]
intercept_1 <- svm_model$rho/w[2]

#build scatter plot of training dataset
scatter_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c("red", "blue"))
#add decision boundary
plot_decision <- scatter_plot + geom_abline(slope = slope_1, intercept = intercept_1) 
#add margin boundaries
plot_margins <- plot_decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = "dashed")+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = "dashed")
#display plot
plot_margins


#build svm model
svm_model<- 
    svm(y ~ ., data = trainset, type = "C-classification", 
        kernel = "linear", scale = FALSE)

#plot decision boundaries and support vectors
plot(x = svm_model, data = trainset)


#build svm model, cost = 1
svm_model_1 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 1,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_1

#build svm model, cost = 100
svm_model_100 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 100,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_100


# Create the base train_plot
train_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c("red", "blue"))
w_1 <- c(x1=6.55241, x2=-4.73278)  # calculated manually outside of this module
w_100 <- c(x1=18.3097, x2=-13.09972)  # calculated manually outside of this module
intercept_1 <- -0.005515526  # calculated outside of this module
intercept_100 <- 0.001852543  # calculated outside of this module
slope_1 <- -w_1[1]/w_1[2]
slope_100 <- -w_100[1]/w_100[2]


#add decision boundary and margins for cost = 1 to training data scatter plot
train_plot_with_margins <- train_plot + 
    geom_abline(slope = slope_1, intercept = intercept_1) +
    geom_abline(slope = slope_1, intercept = intercept_1 - 1/w_1[2], linetype = "dashed")+
    geom_abline(slope = slope_1, intercept = intercept_1 + 1/w_1[2], linetype = "dashed")

#display plot
train_plot_with_margins

#add decision boundary and margins for cost = 100 to training data scatter plot
train_plot_with_margins <- train_plot_with_margins + 
    geom_abline(slope = slope_100, intercept = intercept_100, color = "goldenrod") +
    geom_abline(slope = slope_100, intercept = intercept_100 - 1/w_100[2], linetype = "dashed", color = "goldenrod")+
    geom_abline(slope = slope_100, intercept = intercept_100 + 1/w_100[2], linetype = "dashed", color = "goldenrod")

#display plot 
train_plot_with_margins


svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear", scale = FALSE)

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot
plot(svm_model, trainset)


data(iris)
nTrials <- 100
accuracy <- numeric(nTrials)

#calculate accuracy for n distinct 80/20 train/test partitions
for (i in 1:nTrials){ 
    iris[, "train"] <- ifelse(runif(nrow(iris))<0.8, 1, 0)
    trainColNum <- grep("train", names(iris))
    trainset <- iris[iris$train == 1, -trainColNum]
    testset <- iris[iris$train == 0, -trainColNum]
    svm_model <- svm(Species~ ., data = trainset, 
                     type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$Species)
}

#mean accuracy and standard deviation
mean(accuracy) 
sd(accuracy)

```
  
  
  
***
  
Chapter 3 - Polynomial Kernels  
  
Generating radially separable datasets:  
  
* The goal is to generate 2D points (again uniformly distributed on x1 and x2 using runif)  
* Can then define a value for whether the points are within x of the center  
	* radius <- 0.7  
    * radius_squared <- radius^2  
    * df$y <- factor(ifelse(df$x1^2 + df$x2^2 < radius_squared, -1, 1), levels = c(-1,1))  
    * p <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + geom_point() + scale_color_manual(values = c("-1" = "red","1" = "blue"))   
    * p  
* Can add a circular boundary  
	* circle <- function(x1_center, x2_center, r, npoint = 100){ 
	*   #angular spacing of 2*pi/npoint between points  
	*   theta <- seq(0,2*pi,length.out = npoint)  
	*   x1_circ <- x1_center + r * cos(theta)  
	*   x2_circ <- x2_center + r * sin(theta)  
	*   return(data.frame(x1c = x1_circ, x2c = x2_circ))  
	*   }  
    * boundary <- circle(x1_center = 0, x2_center = 0, r = radius)  
    * p <- p + geom_path(data = boundary, aes(x = x1c, y = x2c), inherit.aes = FALSE)  
  
Linear SVM on radially separable datasets:  
  
* The linear SVM will perform poorly on the radially separable dataset  
	* svm_model<- svm(y ~ ., data=trainset, type="C-classification", kernel="linear")  
    * svm_model  
    * pred_test <- predict(svm_model,testset)  
    * plot(svm_model,trainset)  # all points are classified as 1  
  
Kernel trick - devise a mathematical transformation that makes the data linearly separable:  
  
* For a circles could map X1 = x1**2 and X2 = x2**2, where X1 + X2 = 0.49 (which is linearly separable)  
* The polynomial kernel has a degree (e.g., 1 for linear, 2 for quadratic, etc.) and tuning parameters gamma and coef0  
	* The kernel also uses u dot v where u and v are vectors belonging to the dataset  
    * (gamma * (u dot v) + coef0) ** degree  
* Applying the quadratic kernel to the circular data from above  
	* svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)  
    * plot(svm_model, trainset)  
  
Tuning SVM:  
  
* Set a search range for each parameter, typically as a sequence of variable (e.g., in multiples of 10)  
* For each combination of parameters, build an SVM and assess the out-of-sample accuracy - can become computationally intensive, though  
	* tune_out <- tune.svm(x = trainset[,-3], y = trainset[,3], type = "C-classification", kernel = "polynomial", degree = 2, cost = 10^(-1:2), gamma = c(0.1,1,10), coef0 = c(0.1,1,10))  
    * tune_out$best.parameters$cost  
    * tune_out$best.parameters$gamma  
    * tune_out$best.parameters$coef0  
    * svm_model <- svm(y~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2, cost = tune_out$best.parameters$cost, gamma = tune_out$best.parameters$gamma, coef0 = tune_out$best.parameters$coef0)  
  
Example code includes:  
```{r}

#set number of variables and seed
n <- 400
set.seed(1)

#Generate data frame with two uniformly distributed predictors, x1 and x2
df <- data.frame(x1 = runif(n, min = -1, max = 1), x2 = runif(n, min = -1, max = 1))

#We want a circular boundary. Set boundary radius 
radius <- 0.8
radius_squared <- radius^2

#create dependent categorical variable, y, with value -1 or 1 depending on whether point lies
#within or outside the circle.
df$y <- factor(ifelse(df$x1**2 + df$x2**2 < radius_squared, -1, 1), levels = c(-1, 1))


#build scatter plot, distinguish class by color
scatter_plot <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() +
    scale_color_manual(values = c("red", "blue"))

#display plot
scatter_plot


inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]


#default cost mode;
svm_model_1 <- svm(y ~ ., data = trainset, type = "C-classification", cost = 1, kernel = "linear")

#training accuracy
pred_train <- predict(svm_model_1, trainset)
mean(pred_train == trainset$y)

#test accuracy
pred_test <- predict(svm_model_1, testset)
mean(pred_test == testset$y)

#cost = 100 model
svm_model_100 <- svm(y ~ ., data = trainset, type = "C-classification", cost = 100, kernel = "linear")

#accuracy
pred_train <- predict(svm_model_100, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model_100, testset)
mean(pred_test == testset$y)


#print average accuracy and standard deviation
accuracy <- rep(NA, 100)
set.seed(2)

#comment
for (i in 1:100){
    df[, "train"] <- ifelse(runif(nrow(df))<0.8, 1, 0)
    trainset <- df[df$train == 1, ]
    testset <- df[df$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


#transform data
df1 <- data.frame(x1sq = df$x1^2, x2sq = df$x2^2, y = df$y)

#plot data points in the transformed space
plot_transformed <- ggplot(data = df1, aes(x = x1sq, y = x2sq, color = y)) + 
    geom_point()+ guides(color = FALSE) + 
    scale_color_manual(values = c("red", "blue"))

#add decision boundary and visualize
plot_decision <- plot_transformed + geom_abline(slope = -1, intercept = 0.64)
plot_decision


# Still want to use the old (non-squared) data
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
df$train <- NULL
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]

svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)

#measure training and test accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot
plot(svm_model, trainset)


#tune model
tune_out <- 
    tune.svm(x = trainset[, -3], y = trainset[, 3], 
             type = "C-classification", 
             kernel = "polynomial", degree = 2, cost = 10^(-1:2), 
             gamma = c(0.1, 1, 10), coef0 = c(0.1, 1, 10))

#list optimal values
tune_out$best.parameters$cost
tune_out$best.parameters$gamma
tune_out$best.parameters$coef0


#Build tuned model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", 
                 kernel = "polynomial", degree = 2, 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma, 
                 coef0 = tune_out$best.parameters$coef0)

#Calculate training and test accuracies
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model
plot(svm_model, trainset)

```
  
  
  
***
  
Chapter 4 - Radial Basis Kernel Functions  
  
Generating complex datasets:  
  
* The RBF kernel is highly flexible, can fit complex boundaries, and is common in the real-world  
* Can generate complex data by using different distributions for x and y  
	* n <- 600  
    * set.seed(42)  
    * df <- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), x2 = runif(n, min = -1, max = 1))  
* The decision boundary can then be two circles that just barely touch at the origin  
	* radius <- 0.7  
    * radius_squared <- radius^2  
    * center_1 <- c(-0.7,0)  
    * center_2 <- c(0.7,0)  
    * df$y <- factor(ifelse( (df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 < radius_squared| (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 < radius_squared, -1,1), levels = c(-1,1))  
    * p <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + geom_point() + guides(color = FALSE) + scale_color_manual(values = c("red","blue"))  
    * p  
* Can then build linear, polynomial, and RBF kernels to model the data  
  
Motivating the RBF kernel:  
  
* Neither the linear kernel nor the polynomial kernel will work well for the dataset as described  
* Can use the heuristic that points near each other probably belong to the same class (similar to kNN)  
	* The kernel should have a maximum at (a, b), and should decay as you move away from (a, b)  
    * The rate of decay, all else equal should be the same in all directions, with a tunable gamma  
    * As good fortune has it, the exponential exp(-gamma * r) has all of these properties  
    * rbf <- function(r, gamma) exp(-gamma*r)  
    * ggplot(data.frame(r = c(-0, 10)), aes(r)) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.2), aes(color = "0.2")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.4), aes(color = "0.4")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.6), aes(color = "0.6")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.8), aes(color = "0.8")) +  
    *   stat_function(fun = rbf, args = list(gamma = 1), aes(color = "1")) +  
    *   stat_function(fun = rbf, args = list(gamma = 2), aes(color = "2")) +  
    *   scale_color_manual("gamma", values = c("red","orange","yellow", "green","blue","violet")) +  
    *   ggtitle("Radial basis function (gamma=0.2 to 2)")  
  
The RBF kernel simulates some of the principles of kNN using exponential decay:  
  
* The RBF kernel can be built using pre-set R commands  
	* svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "radial")  
* The predicted decision boundary will no longer be linear, and can be refined through tuning  
	* tune_out <- tune.svm(x = trainset[,-3], y = trainset[,3], gamma = 5*10^(-2:2), cost = c(0.01,0.1,1,10,100), type = "C-classification", kernel = "radial")  
    * tune_out$best.parameters$cost  
    * tune_out$best.parameters$gamma  
    * svm_model <- svm(y~ ., data=trainset, type="C-classification", kernel="radial", cost=tune_out$best.parameters$cost, gamma=tune_out$best.parameters$gamma)  
  
Example code includes:  
```{r}

#number of data points
n <- 1000

#set seed
set.seed(1)

#create dataframe
df <- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), x2 = runif(n, min = -1, max = 1))


#set radius and centers
radius <- 0.8
center_1 <- c(-0.8, 0)
center_2 <- c(0.8, 0)
radius_squared <- radius^2

#create binary classification variable
df$y <- factor(ifelse((df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 < radius_squared |
                      (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 < radius_squared, -1, 1),
                      levels = c(-1, 1))


#create scatter plot
scatter_plot<- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c("red", "blue"))
 
scatter_plot 


# Create 75/25 split for train/test
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]


#build model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")

#accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model against testset
plot(svm_model, testset)


#build model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)

#accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model
plot(svm_model, trainset)


#create vector to store accuracies and set random number seed
accuracy <- rep(NA, 100)
set.seed(2)


# Create a dummy frame dfDum for use in the for loop
dfDum <- df

#calculate accuracies for 100 training/test partitions
for (i in 1:100){
    dfDum[, "train"] <- ifelse(runif(nrow(dfDum))<0.8, 1, 0)
    trainset <- dfDum[dfDum$train == 1, ]
    testset <- dfDum[dfDum$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


#create vector to store accuracies and set random number seed
accuracy <- rep(NA, 100)
set.seed(2)

#calculate accuracies for 100 training/test partitions
for (i in 1:100){
    dfDum[, "train"] <- ifelse(runif(nrow(dfDum))<0.8, 1, 0)
    trainset <- dfDum[dfDum$train == 1, ]
    testset <- dfDum[dfDum$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "radial")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


# Re-create original 75/25 split for train/test
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]

#tune model
tune_out <- tune.svm(x = trainset[, -3], y = trainset[, 3], 
                     gamma = 5*10^(-2:2), 
                     cost = c(0.01, 0.1, 1, 10, 100), 
                     type = "C-classification", kernel = "radial")
tune_out

#build tuned model
svm_model <- svm(y~ ., data = trainset, type = "C-classification", kernel = "radial", 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma)

#calculate test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#Plot decision boundary against test data
plot(svm_model, testset)

```
  
  
  
***
  
###_Experimental Design in R_  
  
Chapter 1 - Introduction to Experimental Design  
  
Introduction to experimental design:  
  
* Experiments start with a question in mind, then finding and analyzing data  
* This course will use open data, meaning that we do not know the original experimental design  
* Key conditions of an experiment include randomization, replication, and blocking  
  
Hypothesis testing:  
  
* The null hypothesis changes depending on the question of interest - "no effect" (two-sided) or "no positive effect" (one-sided) or etc.  
* Power is the probability that the test correctly reject the null hypothesis when the alternative hypothesis is true (target >= 80%)  
* The effect size is the standardized measure of the difference that you are trying to detect  
* Sample size is generally chosen so that the effect size can be measured at the required power  
* Example of using the power package for calculating the metrics  
	* library(pwr)  
    * pwr.anova.test(k = 3, n = 20, f = 0.2, sig.level = 0.05, power = NULL)  # one must be entered as NULL (this will be calculated) ; k groups with n per group and f effect size  
  
Example code includes:  
```{r}

# load the ToothGrowth dataset
data("ToothGrowth")

#perform a two-sided t-test
t.test(x = ToothGrowth$len, alternative = "two.sided", mu = 18)

#perform a t-test
ToothGrowth_ttest <- t.test(len ~ supp, data = ToothGrowth)

#tidy the t-test model object
broom::tidy(ToothGrowth_ttest)


#group by supp, dose, then examine how many observations in ToothGrowth there are by those groups
ToothGrowth %>% 
    group_by(supp, dose) %>% 
    summarize(n=n())

#create a boxplot with geom_boxplot()
ggplot(ToothGrowth, aes(x=as.factor(dose), y=len)) + 
    geom_boxplot()

#create the ToothGrowth_aov model object
ToothGrowth_aov <- aov(len ~ dose + supp, data = ToothGrowth)

#examine the model object with summary()
summary(ToothGrowth_aov)


#less than
t.test(x = ToothGrowth$len, alternative = "less", mu = 18)

#greater than
t.test(x = ToothGrowth$len, alternative = "greater", mu = 18)


#calculate power
pwr::pwr.t.test(n = 100, d = 0.35, sig.level = 0.10, type = "two.sample", 
                alternative = "two.sided", power = NULL
                )

#calculate sample size
pwr::pwr.t.test(n = NULL, d = 0.25, sig.level = 0.05, 
                type = "one.sample", alternative = "greater", power = 0.8
                )

```
  
  
  
***
  
Chapter 2 - Basic Experiments  
  
Single and Multiple Factor Experiments:  
  
* The ANOVA (Analysis of Variance) test allows for comparing means across 3-groups; is at least one mean different  
	* model_1 <- lm(y ~ x, data = dataset)  # first option is lm followed by aov  
    * anova(model_1)  # first option is lm followed by anova  
    * aov(y ~ x, data = dataset)  # second option is a straight call to aov  
* The multiple factor experiment includes additional potential explanatory variables  
	* model2 <- lm(y ~ x + r + s + t)  
    * anova(model2)  
* The Lending Club data is 890k x 75, and contains data from a lending company  
  
Model Validation:  
  
* EDA is an important step prior to modeling the data  
* Boxplots can be a helpful way to explore the data  
	* ggplot(data = lendingclub, aes(x = verification_status, y = funded_amnt)) + geom_boxplot()  
* ANOVA and other linear models generally assume that the residuals are normally distributed  
  
A/B Testing:  
  
* A/B tests are a type of controlled experiment with only two variants of something  
* Power and sample size are crucial to A/B testing, allowing for an understanding of the required size for a desired power and expected effect size  
  
Example code includes:  
```{r}

lendingclub <- readr::read_csv("./RInputFiles/lendclub.csv")


#examine the variables with glimpse()
glimpse(lendingclub)

#find median loan_amt, mean int_rate, and mean annual_inc with summarise()
lendingclub %>% summarise(median(loan_amnt), mean(int_rate), mean(annual_inc))

# use ggplot2 to build a bar chart of purpose
ggplot(data=lendingclub, aes(x = purpose)) + geom_bar()

#use recode() to create the new purpose_recode variable.
lendingclub$purpose_recode <- lendingclub$purpose %>% recode( 
        "credit_card" = "debt_related",
        "debt_consolidation" = "debt_related", 
        "medical" = "debt_related",
        "car" = "big_purchase", 
        "major_purchase" = "big_purchase", 
        "vacation" = "big_purchase",
        "moving" = "life_change", 
        "small_business" = "life_change", 
        "wedding" = "life_change",
        "house" = "home_related", 
        "home_improvement" = "home_related"
        )


#build a linear regression model, stored as purpose_recode_model
purpose_recode_model <- lm(funded_amnt ~ purpose_recode, data = lendingclub)

#look at results of purpose_recode_model
summary(purpose_recode_model)

#get anova results and save as purpose_recode_anova
purpose_recode_anova <- anova(purpose_recode_model)

# look at the class of purpose_recode_anova
class(purpose_recode_anova)


#Use aov() to build purpose_recode_aov
purpose_recode_aov <- aov(funded_amnt ~ purpose_recode, data = lendingclub)

#Conduct Tukey's HSD test to create tukey_output
tukey_output <- TukeyHSD(purpose_recode_aov)

#tidy tukey_output to make sense of the results
broom::tidy(tukey_output)


#Use aov() to build purpose_emp_aov
purpose_emp_aov <- aov(funded_amnt ~ purpose_recode + emp_length, data=lendingclub)

#print purpose_emp_aov to the console
purpose_emp_aov

#call summary() to see the p-values
summary(purpose_emp_aov)


#examine the summary of int_rate
summary(lendingclub$int_rate)

#examine int_rate by grade
lendingclub %>% 
    group_by(grade) %>% 
    summarise(mean = mean(int_rate), var = var(int_rate), median = median(int_rate))

#make a boxplot of int_rate by grade
ggplot(lendingclub, aes(x = grade, y = int_rate)) + geom_boxplot()

#use aov() to create grade_aov plus call summary() to print results
grade_aov <- aov(int_rate ~ grade, data = lendingclub)
summary(grade_aov)


#for a 2x2 grid of plots:
par(mfrow=c(2, 2))

#plot grade_aov
plot(grade_aov)

#back to defaults
par(mfrow=c(1, 1))

#Bartlett's test for homogeneity of variance
bartlett.test(int_rate ~ grade, data=lendingclub)


#use the correct function from pwr to find the sample size
pwr::pwr.t.test(n=NULL, d=0.2, sig.level=0.05, 
                type="two.sample", alternative="two.sided", power=0.8
                )


lc_A <- c(11976148, 1203719, 54998739, 5801830, 31587242, 7711391, 54494666, 57663583, 8967787, 21760921, 44765721, 8596988, 5794746, 59501253, 10578432, 36058744, 11727607, 357888, 51936863, 1178593, 57315811, 5705168, 46024211, 12947039, 57345207, 55299831, 28763037, 49763149, 20077511, 60216198, 12295190, 1570287, 61408414, 59121340, 32349527, 5773180, 26899704, 55412161, 2217935, 16462713, 9196065, 27802028, 40949245, 56007625, 56935379, 62187473, 20178048, 604912, 58533358, 652594, 44066849, 38942161, 6414816, 65617953, 51816492, 43489983, 6794967, 42345315, 59532019, 13107597, 63249029, 7371829, 12335467, 8560739, 7337238, 887484, 23493355, 41031080, 60537197, 12816159, 38446687, 51026618, 6374688, 18685270, 296645, 44439325, 4915968, 63449566, 25256236, 63407874, 36753301, 20728660, 7937228, 13058684, 636359, 50527238, 40450502, 1018943, 12438198, 3065732, 1510626, 5764344, 37840363, 27460227, 39751366, 5028066, 43956700, 56109033, 1412622, 44289534, 41770436, 49956562, 44409121, 47168726, 60953428, 52189251, 64281487, 51928150, 1002880, 4537354, 12605849, 477843, 6808167, 38629237, 33311208, 36109419, 58593881, 40362979, 440300, 9848361, 30656060, 15691500, 4375269, 15360849, 7077904, 66076532, 33350264, 4175651, 44006939, 21130605, 54098234, 53192890, 7371114, 12967808, 58061230, 34803392, 5544911, 28843825, 63244663, 38504887, 68565204, 1211255, 63427670, 56472411, 10548622, 43957279, 59313014, 5768723, 66210490, 25507112, 55472659, 61339767, 65684813, 45544639, 43710238, 46833245, 13028661, 13167268, 3064642, 62072249, 27631726, 65825964, 15540990, 64320858, 8605358, 17795606, 9894584, 543619, 2380700, 20959552, 57743104, 63917130, 38480348, 61393540, 19916851)
lc_A <- c(lc_A, 12528162, 7264617, 61480809, 36411752, 20139228, 21290880, 390228, 45584424, 17755019, 23413261, 15490914, 1254285, 875004, 24274579, 51006600, 11458143, 5125832, 37802077, 57327243, 41059894, 64978360, 58683523, 4290736, 40919379, 65029207, 7096004, 42285591, 7388784, 65914238, 46833088, 21221678, 62855006, 10557733, 44915714, 23083224, 67289213, 9746670, 349608, 66610322, 1595886, 3635144, 38419356, 9715410, 9726377, 621152, 23213635, 18685424, 65782663, 57304429, 20770003, 8865120, 58664359, 1454540, 42404539, 60952405, 61339308, 7367648, 11215938, 41207320, 23553299, 1681376, 7617266, 30485630, 10604792, 46044414, 63094909, 59189668, 10106916, 52058386, 17763104, 6396213, 8981232, 48070364, 10615808, 11956507, 38444903, 60216940, 58310439, 10099562, 7504691, 17533228, 62236540, 38626163, 55657128, 7728107, 42415348, 42454693, 4777573, 23834164, 25157042, 1339435, 50587486, 55998961, 32950014, 28422748, 492346, 50607472, 11335041, 4254623, 65058537, 5375256, 5646680, 44430975, 4054992, 55253292, 68375791, 16822421, 64978226, 59859214, 65424555, 10112206, 6908772, 67879649, 4794842, 31227479, 17423361, 64049774, 58624386, 14829134, 50233873, 44389635, 29684724, 452267, 43044890, 55942742, 19516366, 34443897, 57135665, 34392172, 17352839, 12896521, 40451807, 43255228, 40372428, 8568706, 68364520, 3486848, 40991148, 19196658, 8658538, 65885614, 38352455, 65674149, 1029473, 39290483, 47420355, 65364529, 32318884, 13115811, 48484348, 65975356, 56129109, 3378980, 31026386, 55231010, 41113253, 1480114, 51406116, 2445051, 8627441, 60942818, 55453270, 58573102, 25767158, 9655554, 49783137, 42273770, 32038806, 681948, 65059359, 48546050, 20169281, 68546780, 7065575, 46387142, 66180493, 58430918, 1390497, 41950574, 39888056, 11774847, 55308824, 51969105, 7936525, 5960208, 7700566, 14529825, 14688918, 43024566, 21110140, 55797803, 31236439, 6817136, 1467168, 36028128, 60781310, 66595886, 57548184, 3194733, 8589175, 1546517, 17654773, 40572454, 63284984, 5780985, 39660177, 64050493, 55081623, 51346675, 1235123, 65633931, 66390924, 17413278, 57950994, 55911330, 11814853, 31357211, 56038385, 40038565, 64400706, 35034758, 60296238, 6527713, 5685238, 1062701, 63406447, 64008930, 63476297, 5114652, 20060374, 10085133, 61328568, 9435001, 56057656, 49934674, 39661404, 19616499, 34342717, 46653815, 45614269, 59290211, 31296803, 50605437, 46928301, 58562582, 63879452, 65733359, 51086476, 40601201, 9845217, 29213549, 41227222, 7337659, 46517072, 38610653, 9694813, 21350102, 46716202, 50535150, 39729407, 22263578, 25987787, 64913590, 19636684, 59311687, 4295372, 571012, 20588847, 63424767, 1099384, 3810242, 5604591, 39760687, 43739869, 56019939, 51526987, 45494853, 4302122, 21009984, 66210827, 67255219, 46613149, 63345017, 43570211, 62002161, 2214708, 4234697, 51055338, 19647002, 28593783, 6804647, 40542044, 42263319, 4784593, 19636686, 44015285, 55697847, 5814660, 15409525, 2307393, 54404433, 15490230, 62245810, 64969544, 48120716, 41040511, 51176224, 6376426, 60386775, 826517, 27601385, 8185587, 28564285, 68613325, 58623041, 60941473, 1635691, 7729270, 46417835, 57285778, 55960993, 66510262, 60285691, 61902329, 68565071)


lc_B <- c(62012715, 49974687, 27570947, 63417796, 61449107, 12906517, 57074291, 21021086, 404854, 15139172, 46774978, 50486061, 4305577, 65783354, 48544529, 31667129, 36980133, 19117791, 3845908, 846821, 40381968, 64018601, 57184860, 49963980, 44142706, 6327771, 20811335, 67336862, 3628833, 31247310, 4764984, 1619549, 56492219, 67959628, 61672211, 1472227, 55268407, 13497237, 57538143, 43096178, 35723158, 226780, 2307012, 1210773, 50273799, 28903599, 50839792, 44916418, 9714937, 51876659, 3919804, 12968154, 54978278, 6938022, 53854432, 63350177, 39692948, 67216234, 22253060, 59099446, 46135199, 11717805, 48596572, 8475061, 61462130, 21480483, 2014943, 41430440, 43196143, 243173, 61543762, 66562164, 67878273, 41100627, 11915326, 28753020, 12617369, 59090559, 55583726, 31256585, 544537, 61430245, 1681767, 7670078, 38506546, 36500594, 31367711, 46694948, 2080069, 38457330, 54524836, 27651989, 63358477, 62002922, 8995111, 45694307, 61470409, 17933815, 27370082, 66612753, 1536521, 54948920, 57548472, 876991, 40127147, 57365210, 1904740, 3195692, 743529, 67408356, 8766184, 23643466, 51336378, 13397002, 3700020, 49935259, 38455198, 63506356, 11386690, 32479126, 6300017, 67427011, 63344398, 51366616, 727247, 59291548, 21551336, 8776003, 16111335, 1051513, 61973285, 60764833, 59190150, 25406927, 10138072, 61361677, 32279884, 63337618, 49933340, 30565592, 3217416, 61883095, 63436296, 58290318, 29884855, 50353289, 14699170, 67625637, 6815821, 2286867, 6274586, 17853756, 55948157, 6995898, 44126015, 66643915, 41338910, 8626219, 67858810, 38597465, 45884338, 565018, 46436141, 15259622, 6594706, 39479497, 5535388, 5855546, 48734782, 2896555, 67296211, 713979, 33110251, 8987918, 1224687, 5637315, 484473, 9814600, 29694710, 60902260, 25897153, 40705483, 1439301, 3055155, 26319992, 6245002, 66441896, 46427698, 36330836, 8915199, 46205024, 62459417, 3497439, 54888931, 30475522, 38998249, 12636103, 60536957)
lc_B <- c(lc_B, 27521279, 2365984, 361549, 43430210, 35843833, 9768308, 12705933, 59179388, 60830121, 67929084, 36138408, 854552, 8865548, 13096420, 23836169, 61502149, 1621627, 11426617, 48274995, 41123011, 7296181, 29635336, 30565882, 8145149, 46116481, 21119590, 43894290, 65866235, 44143687, 873468, 12419378, 26378681, 55140334, 56964922, 61682200, 14338072, 65047247, 57267246, 59581503, 41093708, 48524124, 513842, 1685090, 42723216, 60647576, 55341080, 9735578, 41110083, 30255415, 56010965, 63214550, 67828966, 671468, 38540004, 65107371, 18645038, 26017706, 660734, 573283, 9454644, 64017354, 617449, 7645594, 43286428, 55941273, 8636865, 31226902, 46194753, 6160505, 1412225, 65741544, 24084859, 58532795, 41880754, 45515321, 60585561, 65272380, 7937327, 1489732, 17553239, 7638498, 1473206, 38162164, 3355990, 15610681, 57025137, 6254978, 38162571, 52768311, 5938741, 58101279, 18895673, 30175739, 38222417, 55909312, 65663878, 6607837, 24725076, 61722475, 11895058, 28182084, 185962, 55259655, 16241080, 66602227, 5781939, 60801476, 6996130, 12346893, 65672013, 19076244, 1475379, 9056893, 59492895, 56864322, 60942704, 44015940, 62225220, 39739191, 66435524, 44199929, 59471139, 38547168, 6205030, 38615829, 6698930, 66514563, 1623685, 60545969, 46703319, 39739315, 12636426, 65364691, 16403147, 9204637, 19306532, 66270322, 65653692, 22313524, 59082682, 19796545, 10766253, 50436003, 49363132, 27600713, 44865530, 57763719, 47857115, 48535477, 65986020, 58603818, 42934257, 1167844, 66390187, 58281312, 63888770, 48596526, 67385135, 24775459, 55090096, 12347068, 37317537, 64007908, 1683908, 11976597, 41019342, 6855113, 7964638, 65701227, 44037648, 23133074, 9787718, 61389384, 38418035, 33130454, 13038119, 14639242, 38505864, 65725266, 62904623, 68513661, 36039498, 6538734, 51857455, 59139740, 64341225, 21430833, 55455899, 17795459, 65128493, 46428798, 43216120, 59199242, 50364311, 41079485, 27711293, 63218354, 65492649, 50819365, 40737432, 377507, 65736437, 61488876, 44886450, 31467727, 46651816, 11914779, 65352381, 24726593, 52989922, 43105128, 34322310, 8669148, 12795739, 38485516, 39559934, 4280915, 63437401, 7103037, 44946049, 15400322, 28583975, 59592185, 877645, 56019484, 3372858, 60556772, 19846532, 11658194, 6894823, 61414862, 52708301, 48806212, 12204849, 60863986, 3919883, 37661631, 47210580, 14689912, 23393084, 60961679, 6170889, 55191727, 14690280, 42415518, 65855022, 62156039, 38536464, 44603544, 63527328, 48182146, 25867085, 61952845, 4744682, 20110370, 65854766, 57722242, 11438361, 34111919, 53262232, 12247443, 64210396, 37630339, 41237564, 46722148, 65791211, 16882760, 7719304, 37622016, 3220774, 51906280, 12446784, 50064210, 57733299, 63437152, 38445791, 3730324, 56052115, 57354312, 58010576, 626701, 7224706, 64079786, 62167132, 8396526, 7625377, 12707224, 35084508, 56022111, 52027979, 43215589, 50425264, 59253209, 28312549, 67376619, 30795837, 43869662, 20849433, 55351366, 39549686, 22972745, 1025579)


# The specific member IDs in lc_A and lc_B are not in dataset lendingclub
lendingclub_ab <- lendingclub %>%
    mutate(Group=ifelse(member_id %in% lc_A, "A", ifelse(member_id %in% lc_B, "B", "C")))


# ggplot(lendingclub_ab, aes(x=Group, y=loan_amnt)) + geom_boxplot()

#conduct a two-sided t-test
# t.test(loan_amnt ~ Group, data=lendingclub_ab)


#build lendingclub_multi
# lendingclub_multi <-lm(loan_amnt ~ Group + grade + verification_status, data=lendingclub_ab)

#examine lendingclub_multi results
# broom::tidy(lendingclub_multi)

```
  
  
  
***
  
Chapter 3 - Randomized Complete (and Balanced Incomplete) Block Designs  
  
Intro to NHANES Dataset and Sampling:  
  
* NHANES is the National Health and Nutrition Examination Study, run once every 2 years in the US since the late 1990s (was run on different frequency since the 1960s)  
* NHANES individuals are sampled from a scheme to match the US demographics - upsampling of elderly and minorities for sufficient sample size for statistical conclusions  
* Two key types of sampling  
	* Probability sampling - probability is used to select the sample (will be covered in this course)  
    * Non-probability sampling - voluntary (whoever responds), convenience (whoever the researcher can find)  
* Many types of random sampling can be run in R  
	* Simple Random Sampling - sample()  
    * Stratified Sampling - dataset %>% group_by(strata_variable) %>% sample_n()  # sample a specified number of people inside each segment  
    * Cluster Sampling - cluster(dataset, cluster_var_name, number_to_select, method = "option")  # select everyone in each randomly select cluster  
    * Systematic Sampling - every 5th or 10th or etc. person (implemented by custom functions)  
    * Multi-Stage Sampling - combinations of 2+ of the above approaches in a sensible and structured manner  
  
Randomized Complete Block Designs (RCBD):  
  
* RCBD is run when there is a potential nuisance factor in the data that might otherwise impact the results and conclusions  
	* Randomized - treatment is assigned randomly inside each block  
    * Complete - each treatment is used the same number of times inside each block  
    * Block - experimental groups are blocked to be similar (differences within the group are expected to be lesser than differences across the groups)  
    * Design - the experiment  
    * "Block what you can, randomize what you cannot"  
* The library(agricolae) allows for drawing some of the experimental designs such as an RCBD  
	* library(agricolae)  
    * trt <- letters[1:4]  
    * rep <- 4  
    * design.rcbd <- design.rcbd(trt, r = rep, seed = 42, serie = 0)  # serie has to do with tagging of number blocks  
    * design.rcbd$sketch  
  
Balanced Incomplete Block Designs (BIBD):  
  
* Incomplete blocaks are when you cannot fully fit a treatment inside a block  
	* Balanced - each pair of treatments occur together in a block an equal number of times  
    * Incomplete - not every treatment will appear in every block  
    * Block - experimental groups are blocked to be similar (differences within the group are expected to be lesser than differences across the groups  
    * Design - the experiment  
* Suppose that t is the number of treatments, k is the number of treatments per block, and r is the number of replications  
	* lambda = r * (k - 1) / (t - 1)  
    * If lambda is a whole number, then a BIBD is possible; otherwise, it is not  
  
Example code includes:  
```{r}

nhanes_demo <- readr::read_csv("./RInputFiles/nhanes_demo.csv")
nhanes_medical <- readr::read_csv("./RInputFiles/nhanes_medicalconditions.csv")
nhanes_bodymeasures <- readr::read_csv("./RInputFiles/nhanes_bodymeasures.csv")
dummy_nhanes_final <- readr::read_csv("./RInputFiles/nhanes_final.csv")

#merge the 3 datasets you just created to create nhanes_combined
nhanes_combined <- list(nhanes_demo, nhanes_medical, nhanes_bodymeasures) %>%
  Reduce(function(df1, df2) inner_join(df1, df2, by="seqn"), .)


#fill in the dplyr code
nhanes_combined %>% group_by(mcq365d) %>% summarise(mean = mean(bmxwt, na.rm = TRUE))

#fill in the ggplot2 code
nhanes_combined %>% filter(ridageyr > 16) %>% 
  ggplot(aes(x=as.factor(mcq365d), y=bmxwt)) +
  geom_boxplot()


#filter out anyone less than 16
nhanes_filter <- nhanes_combined %>% filter(ridageyr > 16)

#use simputation & impute bmxwt to fill in missing values
nhanes_final <- simputation::impute_median(nhanes_filter, bmxwt ~ riagendr)

#recode mcq365d with ifelse() & examine with table()
nhanes_final$mcq365d <- ifelse(nhanes_final$mcq365d==9, 2, nhanes_final$mcq365d)
table(nhanes_final$mcq365d)


#use sample() to create nhanes_srs
nhanes_srs <- nhanes_final[sample(nrow(nhanes_final), 2500), ]

#create nhanes_stratified with group_by() and sample_n()
nhanes_stratified <- nhanes_final %>%
  group_by(riagendr) %>%
  sample_n(2000)
table(nhanes_stratified$riagendr)

#load sampling package and create nhanes_cluster with cluster()
nhanes_cluster <- sampling::cluster(nhanes_final, "indhhin2", 6, method = "srswor")


#use str() to view design.rcbd's criteria
str(agricolae::design.rcbd)

#build trt and rep
trt <- LETTERS[1:5]
rep <- 4

#Use trt and rep to build my.design.rcbd and view the sketch part of the object
my_design_rcbd <- agricolae::design.rcbd(trt, r=rep, seed = 42, serie=0)
my_design_rcbd$sketch


#make nhanes_final$riagendr a factor variable
nhanes_final$riagendr <- factor(nhanes_final$riagendr)

#use aov() to create nhanes_rcbd
nhanes_rcbd <- aov(bmxwt ~ mcq365d + riagendr, data=nhanes_final)

#check the results of nhanes_rcbd with summary()
summary(nhanes_rcbd)

#print the difference in weights by mcq365d and riagendr
nhanes_final %>% group_by(mcq365d, riagendr) %>% summarise(mean_wt = mean(bmxwt))


#set up the 2x2 plotting grid and then plot nhanes_rcbd
par(mfrow=c(2, 2))
plot(nhanes_rcbd)
par(mfrow=c(1, 1))

#run the code to view the interaction plots
with(nhanes_final, interaction.plot(mcq365d, riagendr, bmxwt))

#run the code to view the interaction plots
with(nhanes_final, interaction.plot(riagendr, mcq365d, bmxwt))


#create my_design_bibd_1
# my_design_bibd_1 <- design.bib(LETTERS[1:3], k = 4, r = 16, serie = 0, seed = 42)  # will throw an error

#create my_design_bibd_2
# my_design_bibd_2 <- design.bib(letters[1:2], k = 3, r = 5, serie = 0, seed = 42)  # will throw warning

#create my_design_bibd_3
my_design_bibd_3 <- agricolae::design.bib(letters[1:4], k = 4, r = 6, serie = 0, seed = 42)
my_design_bibd_3$sketch


lambda <- function(t, k, r){
  return((r*(k-1)) / (t-1))
}

#calculate lambda
lambda(4, 3, 3)


#build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"))
color <- as.factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))

#create cat_model & then wrong_cat_model and examine them with summary()
cat_model <- aov(creatinine ~ food + color, data=cat_experiment)
summary(cat_model)


#calculate lambda
lambda(3, 3, 2)

#create weightlift_model & examine results (variable does not exist in dataset)
# weightlift_model <- aov(bmxarmc ~ weightlift_treat + ridreth1, data=nhanes_final)
# summary(weightlift_model)

```
  
  
  
***
  
Chapter 4 - Latin Squares, Graeco-Latin Squares, Factorial Experiments  
  
Latin Squares have two blocking factors, assumed not to interact with each other or the treatment, and each with the same number of levels:  
  
* Latin squares can be analyzed just like an RCBD  
* In a Latin square, both the rows and the columns are the blocking factors  
* Can use nyc_scores dataset containing reading, writing, and math scores from all accredited high schools  
	* Goal is to assess the impact of a (fabricated) tutoring program on the scores by school  
  
Graeco-Latin Squares builds on Latin squares by adding an additional blocking factor:  
  
* Three blocking factors, all with the same number of levels, and assumed not to interact with each other or the treatment  
	* Greek letters added next to the Latin letters indicate the third blocking factors (can use Latin and numbers instead)  
    * All of the combinations occur only once (each letter once per row/column, and each number once per letter)  
  
Factorial Experiments - designs in which 2+ variables are crossed in an experiment, with each combination considered a factor:  
  
* Example of testing all combinations of high/low water and high/low light - each combination is tested, with TukeyHSD() applied after  
* This course will focus on 2^k factor experiments, meaning that each level has only a High/Low (or similar) possibility  
  
Next steps:  
  
* Many other types of factorial designs - do not all need to be 2**k, with many factor levels  
	* Might consider a fractional factorial design to minimize the analytical burden  
* Design should be a valued and integrated part of the process  
* There will always be some unmeasured confounders, but good design can help to reduce that noise  
  
Example code includes:  
```{r}

nyc_scores <- readr::read_csv("./RInputFiles/nyc_scores.csv")
glimpse(nyc_scores)


tEL <- c('PhD', 'BA', 'BA', 'MA', 'MA', 'PhD', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'College Student', 'Grad Student', 'MA', 'MA', 'MA', 'BA', 'MA', 'BA', 'MA', 'College Student', 'PhD', 'MA', 'MA', 'BA', 'MA', 'College Student', 'BA', 'PhD', 'Grad Student', 'MA', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'MA', 'Grad Student', 'BA', 'BA', 'College Student', 'Grad Student', 'College Student', 'BA', 'BA', 'PhD', 'BA', 'Grad Student', 'Grad Student', 'College Student', 'College Student', 'BA', 'PhD', 'College Student', 'PhD', 'PhD', 'PhD', 'College Student', 'Grad Student', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'MA', 'MA', 'College Student', 'Grad Student', 'MA', 'PhD', 'MA', 'College Student', 'MA', 'PhD', 'MA', 'College Student', 'College Student', 'Grad Student', 'PhD', 'MA', 'MA', 'Grad Student', 'MA', 'MA', 'Grad Student', 'PhD', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'PhD', 'BA', 'MA', 'Grad Student', 'BA', 'College Student', 'MA', 'College Student', 'Grad Student', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'Grad Student', 'College Student', 'College Student', 'College Student', 'MA', 'BA', 'Grad Student', 'Grad Student', 'MA', 'College Student', 'BA', 'Grad Student', 'MA', 'Grad Student', 'PhD', 'MA', 'MA', 'College Student', 'MA', 'College Student', 'PhD', 'College Student', 'MA', 'MA', 'MA', 'MA', 'College Student', 'MA', 'BA', 'MA', 'Grad Student', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'MA', 'College Student', 'MA', 'MA', 'BA', 'MA', 'College Student', 'Grad Student', 'College Student', 'MA', 'BA', 'MA', 'BA', 'College Student', 'Grad Student', 'Grad Student', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'BA', 'MA', 'BA', 'College Student', 'MA', 'BA', 'MA', 'Grad Student', 'MA', 'PhD', 'MA', 'BA', 'Grad Student', 'MA', 'BA', 'BA', 'MA', 'BA', 'College Student', 'BA', 'MA', 'MA', 'BA', 'MA', 'College Student', 'BA', 'Grad Student', 'MA', 'BA', 'MA', 'MA', 'MA', 'BA', 'College Student', 'College Student')
tEL <- c(tEL, 'BA', 'Grad Student', 'BA', 'BA', 'MA', 'Grad Student', 'BA', 'MA', 'BA', 'PhD', 'MA', 'MA', 'MA', 'BA', 'College Student', 'PhD', 'BA', 'Grad Student', 'BA', 'College Student', 'BA', 'MA', 'College Student', 'MA', 'College Student', 'Grad Student', 'College Student', 'MA', 'PhD', 'BA', 'PhD', 'Grad Student', 'BA', 'BA', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'MA', 'BA', 'College Student', 'BA', 'MA', 'College Student', 'MA', 'College Student', 'BA', 'MA', 'BA', 'BA', 'MA', 'PhD', 'BA', 'MA', 'Grad Student', 'College Student', 'MA', 'College Student', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'Grad Student', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'Grad Student', 'MA', 'BA', 'College Student', 'MA', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'College Student', 'College Student', 'College Student', 'College Student', 'College Student', 'PhD', 'MA', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'MA', 'MA', 'PhD', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'MA', 'PhD', 'BA', 'BA', 'Grad Student', 'Grad Student', 'PhD', 'BA', 'BA', 'Grad Student', 'College Student', 'BA', 'College Student', 'MA', 'MA', 'MA', 'Grad Student', 'BA', 'BA', 'MA', 'Grad Student', 'PhD', 'BA', 'Grad Student', 'Grad Student', 'Grad Student', 'BA', 'MA', 'BA', 'College Student', 'College Student', 'Grad Student', 'MA', 'Grad Student', 'Grad Student', 'BA', 'BA', 'MA', 'College Student', 'BA', 'Grad Student', 'Grad Student', 'College Student', 'Grad Student', 'College Student', 'PhD', 'BA', 'MA', 'MA', 'BA', 'College Student', 'College Student', 'PhD', 'MA', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'PhD', 'MA', 'MA', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'College Student', 'Grad Student', 'BA', 'MA', 'MA', 'Grad Student', 'BA', 'Grad Student', 'Grad Student', 'MA', 'PhD', 'Grad Student', 'Grad Student', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'MA', 'BA', 'MA', 'College Student', 'MA', 'PhD', 'BA', 'MA', 'College Student', 'PhD', 'PhD', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'MA', 'BA', 'College Student', 'BA', 'BA', 'MA', 'MA', 'College Student', 'College Student', 'Grad Student', 'College Student', 'MA', 'MA', 'MA', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'BA', 'Grad Student', 'BA', 'MA', 'College Student', 'MA')


nyc_scores <- nyc_scores %>%
    mutate(Teacher_Education_Level=tEL)
glimpse(nyc_scores)


#mean, var, and median of Math score
nyc_scores %>%
    group_by(Borough) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))

#mean, var, and median of Math score
nyc_scores %>%
    group_by(Teacher_Education_Level) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))

#mean, var, and median of Math score
nyc_scores %>%
    group_by(Borough, Teacher_Education_Level) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))


# If we want to use SAT scores as our outcome, we need to examine their missingness
# First, look at the pattern of missingness using md.pattern() from the mice package
# There are 60 scores missing in each of the scores
# There are many R packages which help with more advanced forms of imputation, such as MICE, Amelia, mi, and more
# We will use the simputation andimpute_median() as we did previously

#examine missingness with md.pattern()
mice::md.pattern(nyc_scores)

#impute the Math, Writing, and Reading scores by Borough
nyc_scores_2 <- simputation::impute_median(nyc_scores, Average_Score_SAT_Math ~ Borough)

#convert Math score to numeric
nyc_scores_2$Average_Score_SAT_Math <- as.numeric(nyc_scores_2$Average_Score_SAT_Math)

#examine scores by Borough in both datasets, before and after imputation
nyc_scores %>% 
  group_by(Borough) %>% 
  summarise(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE))
nyc_scores_2 %>% 
  group_by(Borough) %>% 
  summarise(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE))


#design a LS with 5 treatments A:E then look at the sketch
my_design_lsd <- agricolae::design.lsd(LETTERS[1:5], serie=0, seed=42)
my_design_lsd$sketch


# To execute a Latin Square design on this data, suppose we want to know the effect of of our tutoring program, which includes one-on-one tutoring, two small groups, and an in and after school SAT prep class
# A new dataset nyc_scores_ls is available that represents this experiment. Feel free to explore the dataset in the console.

# We'll block by Borough and Teacher_Education_Level to reduce their known variance on the score outcome
# Borough is a good blocking factor because schools in America are funded partly based on taxes paid in each city, so it will likely make a difference on quality of education

lsID <- c('11X290', '10X342', '09X260', '09X412', '12X479', '14K478', '32K554', '14K685', '22K405', '17K382', '05M692', '02M427', '02M308', '03M402', '02M282', '30Q501', '26Q495', '24Q455', '29Q326', '25Q670', '31R450', '31R445', '31R080', '31R460', '31R455')
lsTP <- c('One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)')

nyc_scores_ls <- nyc_scores_2 %>%
    filter(School_ID %in% lsID) %>%
    mutate(Tutoring_Program=lsTP)


#build nyc_scores_ls_lm
nyc_scores_ls_lm <- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level,
                       data=nyc_scores_ls
                       )

#tidy the results with broom
nyc_scores_ls_lm %>% broom::tidy()

#examine the results with anova
nyc_scores_ls_lm %>% anova()


#create a boxplot of Math scores by Borough, with a title and x/y axis labels
ggplot(nyc_scores, aes(x=Borough, y=Average_Score_SAT_Math)) + 
  geom_boxplot() + 
  ggtitle("Average SAT Math Scores by Borough, NYC") + 
  xlab("Borough (NYC)") + 
  ylab("Average SAT Math Scores (2014-15)")


#create trt1 and trt2
trt1 <- LETTERS[1:5]
trt2 <- 1:5

#create my_graeco_design
my_graeco_design <- agricolae::design.graeco(trt1, trt2, serie=0, seed=42)

#examine the parameters and sketch
my_graeco_design$parameters
my_graeco_design$sketch


glsID <- c('09X241', '10X565', '09X260', '07X259', '11X455', '18K563', '23K697', '32K403', '22K425', '16K688', '02M135', '06M348', '02M419', '02M489', '04M495', '30Q502', '24Q530', '30Q555', '24Q560', '27Q650', '31R440', '31R064', '31R450', '31R445', '31R460')
glsTP <- c('SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)')
glsHT <- c('Small Group', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Individual')


nyc_scores_gls <- nyc_scores_2 %>%
    filter(School_ID %in% glsID) %>%
    mutate(Tutoring_Program=glsTP, Homework_Type=glsHT)


#build nyc_scores_gls_lm
nyc_scores_gls_lm <- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level + Homework_Type, data=nyc_scores_gls)

#tidy the results with broom
nyc_scores_gls_lm %>% broom::tidy()

#examine the results with anova
nyc_scores_gls_lm %>% anova()


pctTHL <- c(1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2)
pctBHL <- c(2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1)
tP <- c('Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No')

nyc_scores <- nyc_scores %>%
    select(-Teacher_Education_Level) %>%
    mutate(Percent_Tested_HL=factor(pctTHL), Percent_Black_HL=factor(pctBHL), Tutoring_Program=factor(tP))


#build the boxplots for all 3 factor variables: tutoring program, pct black, pct tested
ggplot(nyc_scores, aes(x=Tutoring_Program, y=Average_Score_SAT_Math)) + 
    geom_boxplot()

ggplot(nyc_scores, aes(x=Percent_Black_HL, y=Average_Score_SAT_Math)) + 
    geom_boxplot()

ggplot(nyc_scores, aes(x=Percent_Tested_HL, y=Average_Score_SAT_Math)) + 
    geom_boxplot()


#create nyc_scores_factorial and examine the results
nyc_scores_factorial <- aov(Average_Score_SAT_Math ~ Percent_Tested_HL * Percent_Black_HL * Tutoring_Program, data=nyc_scores)
broom::tidy(nyc_scores_factorial)


#use shapiro.test() to test the outcome
shapiro.test(nyc_scores$Average_Score_SAT_Math)

#plot nyc_scores_factorial to examine residuals
par(mfrow = c(2, 2))
plot(nyc_scores_factorial)
par(mfrow = c(1, 1))

```
  
  
  
***
  
###_Structural Equation Modeling with lavaan in R_  
  
Chapter 1 - One-Factor Models  
  
Model Specification - Structural Equation Models (SEM) - explore relationships between variables:  
  
* Can confirm the structure of a developed model also  
* Two variable types - manifest (directly measured) which are represented by squares, and latent (abstract underlying phenomenon) represented as circles  
	* The manifest variables are assumed to be driven by the latent variables (such as intelligence)  
* Can set up an analysis in R using lavaan based on 1939 intelligence data  
	* library(lavaan)  
    * data(HolzingerSwineford1939)  
    * example model <- 'latent_variable =~ manifest_variable1 + manifest_variable2 + ...'  # latent_variable can have any name not in dataset, =~ is direction of prediction  
    * visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'  # x1, x2, x3, x7, x8, x9 are visual components inside the 1939 dataset  
  
Model Analysis:  
  
* Degrees of freedom are based on df = Possible Values - Estimated Values  
	* Possible Values = Manifest Variables * (Manifest Variables + 1) / 2  
    * Models need to have at least 3 manifest variables and df > 0  
    * Can use scaling and constraints to control degrees of freedom - managed inside lavaan but can modify defaults  
* Can run the models using lavaan in R  
	* visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'  
    * visual.fit <- cfa(model = visual.model, data = HolzingerSwineford1939)  # include the previously defined model and the data frame  
    * summary(visual.fit)  # basic information about the model  
    * The loadings (weightings) for each of the manifest variables will be shown, typically with the first coefficient set to 1 as per the scaling  
    * The variance estimates are also provided for each of the variables - should be positive, but can be negative (needs to be troubleshot)  
  
Model Assessment:  
  
* Standardized loadings measure the strength of the relationships between the manifest variables and the latent variables  
	* Can be measured based on the estimates, relative to the variable that was set as 1.00  
    * Can instead use the standardized solution based on the z-scores  
    * summary(visual.fit, standardized = TRUE)  # to get the standardized solution (Std.all column, with close to 1 being best; Std.lv being scaled like the loading variable)  
* The model fit measures how well the data fit the specified model  
	* Goodness of fit indices like the Comparative Fit Index or the Tucker Lewis Index - goal is closer to 1 and 0.9+  
    * Badness of fit indices like RMSE Approximation or Standardized Root Mean Square Residual (SRMR) - goal is lower and 0.1-  
    * summary(visual.fit, standardized = TRUE, fit.measures = TRUE)  # will show most common fit meaasures  
  
Example code includes:  
```{r}

#Load the lavaan library
library(lavaan)

#Look at the dataset
data(HolzingerSwineford1939, package="lavaan")
head(HolzingerSwineford1939[ , 7:15])

#Define your model specification
text.model <- "textspeed =~ x4 + x5 + x6 + x7 + x8 + x9"

#Analyze the model with cfa()
text.fit <- lavaan::cfa(model=text.model, data=HolzingerSwineford1939)

#Summarize the model
summary(text.fit)
summary(text.fit, standardized=TRUE)
summary(text.fit, fit.measures=TRUE)


#Look at the dataset
data(PoliticalDemocracy, package="lavaan")
head(PoliticalDemocracy)

#Define your model specification
politics.model <- "poldemo60 =~ y1 + y2 + y3 + y4"

#Analyze the model with cfa()
politics.fit <- lavaan::cfa(model = politics.model, data = PoliticalDemocracy)

#Summarize the model
summary(politics.fit, standardized=TRUE, fit.measures=TRUE)

```
  
  
  
***
  
Chapter 2 - Multi-Factor Models  
  
Multifactor Specification - exploring multiple latent relationships, and their relationships to each other:  
  
* Combining manifest variables that represent different latent variables often results in a model with poor fit  
* Can instead convert each of the manifest variables to the appropriate latent variable, for example  
	* visual.model <- 'visual =~ x1 + x2 + x3'  
    * visual.fit <- cfa(model = visual.model, data = HolzingerSwineford1939)   
    * speed.model <- 'speed =~ x7 + x8 + x9'  
    * speed.fit <- cfa(model = speed.model, data = HolzingerSwineford1939)  
* However, having too many models can lead to having zero degrees of freedom; constraints (such as same loading for x2/x3) are used to address this  
	* visual.model <- 'visual =~ x1 + a*x2 + a*x3'  # The a means that x2 and x3 will be set equal to each other, while a number rather than a would use that exact number  
* One larger model can sometimes better capture all the relationships  
	* twofactor.model <- 'visual =~ x1 + x2 + x3 \n speed =~ x7 + x8 + x9'  # adding them all at the same time (must have new line for new model)  
    * twofactor.fit <- cfa(model = twofactor.model, data = HolzingerSwineford1939)  
    * summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)  
  
Model Structure:  
  
* The two-factor model assumes there is a covariant relationship between the latent variables - basically, one latent variable can predict another  
* Can see the correlation between the standardized variables using the summary() function - technically shows R-squared  
	* =~ creates latent variables  
    * ~~ creates covariances between variables  
    * ~ creates direct prediction between variables  
    * if there is a newline followed by 'speed ~~ 0*visual' then speed will be assumed NOT to vary at all with visual  
    * if there is a newline followed by 'speed ~ visual' then there is assumed to be a direct relationship between these variables  
  
Modification Indices:  
  
* If a model has a poor fit, can examine the standardized solutions - desire is to see loading greater than 0.3  
* Model problems can often be identified by variances that are very high relative to the raw data  
* Modification indices can help show the improvement in the model when an additional index is added  
	* modificationindices(twofactor.fit, sort = TRUE)  
    * Output will be lhs op rhs (left-hand side, operator, right-hand-side) followed by mi (modification index, a form of chi-squared)  
    * Parameters should be added one at a time, and only if they "make theoretical sense"  
    * Take the desired path(s) and add them as new lines in the model  
  
Model Comparison:  
  
* Can create and save two models, then analyze both using the same cfa(), then use anova() to compare the models  
	* anova(twofactor.fit, twofactor.fit1)  
    * This is only useful for nested models that otherwise share the same variables  
* Can also compare the fit indices using more detailed criteria  
	* fitmeasures(twofactor.fit)  
    * AIC (lower is better, including more negative better than less negative)  
    * ECVI is the likelihood of replicating the model with the same sample size and population (lower is better)  
    * fitmeasures(twofactor.fit1, c("aic", "ecvi"))  
  
Example code includes:  
```{r}

#Create your text model specification
text.model <- 'text =~ x4 + x5 + x6'

#Analyze the model
text.fit <- cfa(model=text.model, data=HolzingerSwineford1939)

#Summarize the model
summary(text.fit, standardized = TRUE, fit.measures = TRUE)


#Update the model specification by setting two paths to the label a
text.model <- 'text =~ x4 + a*x5 + a*x6'

#Analyze the model
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)

#Summarize the model
summary(text.fit, standardized = TRUE, fit.measures = TRUE)


#Create a two-factor model of text and speed variables
twofactor.model <- 'text =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'

#Previous one-factor model output
summary(text.fit, standardized = TRUE, fit.measures = TRUE)

#Two-factor model specification
twofactor.model <- 'text =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'

#Use cfa() to analyze the model
twofactor.fit <- cfa(model=twofactor.model, data=HolzingerSwineford1939)

#Use summary() to view the fitted model
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)


#Load the library and data
data(epi, package="psych")

#Specify a three-factor model with one correlation set to zero
epi.model <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
extraversion ~~ 0*neuroticism'

#Run the model
epi.fit <- cfa(model = epi.model, data = epi)

#Examine the output 
summary(epi.fit, standardized = TRUE, fit.measures = TRUE)


#Specify a three-factor model where lying is predicted by neuroticism
epi.model <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
lying ~ neuroticism'


#Run the model
epi.fit <- cfa(model = epi.model, data = epi)

#Examine the output 
summary(epi.fit, standardized = TRUE, fit.measures = TRUE)

#Calculate the variance of V1
var(epi$V1, na.rm=TRUE)

#Examine the modification indices
modificationindices(epi.fit, sort=TRUE)


#Edit the model specification
epi.model1 <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
neuroticism =~ V3'

#Reanalyze the model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Summarize the updated model
summary(epi.fit1, standardized = TRUE, fit.measures = TRUE)


#Analyze the original model
epi.fit <- cfa(model = epi.model, data = epi)

#Analyze the updated model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Compare those models
anova(epi.fit, epi.fit1)


#Analyze the original model
epi.fit <- cfa(model = epi.model, data = epi)

#Find the fit indices for the original model
fitmeasures(epi.fit)[c("aic", "ecvi")]

#Analyze the updated model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Find the fit indices for the updated model
fitmeasures(epi.fit1)[c("aic", "ecvi")]

```
  
  
  
***
  
Chapter 3 - Troubleshooting Model Errors and Diagrams  
  
Heywood Cases on the Latent Variable:  
  
* Heywood cases (defined by Heywood in 1931) are cases where correlations (greater than 1) or variances (negative) are out of bounds  
* The lavaan package will throw a warning that the matrix of latent variables is "not positive definite"  
	* Usually occurs because one of the latent variables is really a combination of the others  
    * Can then identify the highly correlated variables, and collapse them in to a single equation (fewer factors or the like)  
  
Heywood Cases on the Manifest Variable (negative error variances):  
  
* Generally occur dur to a mis-specified (under-specified) model, small sample sizes, manifest variables on vastly different scales, etc.  
* The lavaan package will throw a warning that "model has not converged"  
	* summary(negative.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)  # rsquare can help to identify the issue; variance in each manifest variable should be (0, 1)  
* Can just freeze the variance of one of the wonky variables to its variance in the raw data  
	* negative.model <- 'latent1 =~ V1 + V2 + V3\nlatent2 =~ V4 + V5 + V6\nV2 ~~ 18.83833*V2'   # 18.84 is var(V2)  
  
Create Diagrams with semPaths():  
  
* The semPlot library allows for diagramming the fit models  
	* library(semPlot)  
    * twofactor.model <- 'text =~ x4 + x5 + x6\nspeed =~ x7 + x8 + x9'  
    * twofactor.fit <- cfa(model = twofactor.model, data = HolzingerSwineford1939)  
    * semPaths(object = twofactor.fit)  
* The double-headed arrows on the manifest variables are variances, and the double-headed arrows on the latent variables are covariances  
* There are many options for semPaths, and allow a few will be covered here  
	* semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1)  # std is standardized while par is parameters; edge.label.cex is the font size for the edges  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "circle")  # "tree" is the default for layouts  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "tree", rotation = 2)  # rotation can only be used for trees; 2 means left/right  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "tree", rotation = 2, what = "std", edge.color = "purple")  # what colors arrows by strength  
  
Example code includes:  
```{r}

badlatentdata <- readr::read_csv("./RInputFiles/badlatentdata.csv")
badvardata <- readr::read_csv("./RInputFiles/badvardata.csv")

adoptsurvey <- badlatentdata %>%
    select(-X1) %>%
    rename(pictures=V1, background=V2, loveskids=V3, energy=V4, wagstail=V5, playful=V6)

#Look at the data
str(adoptsurvey, give.attr=FALSE)
head(adoptsurvey)

#Build the model
adopt.model <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model = adopt.model, data = adoptsurvey)
lavInspect(adopt.fit, "cov.lv")
summary(adopt.fit, standardized=TRUE, fit.measures=TRUE)


#Edit the original model 
adopt.model <- 'goodstory =~ pictures + background + loveskids + energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model = adopt.model, data = adoptsurvey)

#Look for Heywood cases
summary(adopt.fit, standardized = TRUE, fit.measures = TRUE)



adoptsurvey <- badvardata %>%
    select(-X1) %>%
    rename(pictures=V1, background=V2, loveskids=V3, energy=V4, wagstail=V5, playful=V6)
str(adoptsurvey, give.attr=FALSE)
summary(adoptsurvey)


#Build the model
adopt.model <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model=adopt.model, data=adoptsurvey)

#Summarize the model to view the negative variances
summary(adopt.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#View the variance of the problem manifest variable
var(adoptsurvey$wagstail)


#Update the model using 5 decimal places
adopt.model2 <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful
wagstail~~23.07446*wagstail'

#Analyze and summarize the updated model
adopt.fit2 <- cfa(model = adopt.model2, data = adoptsurvey)
summary(adopt.fit2, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)


#Create a default picture
semPlot::semPaths(adopt.fit)

#Update the default picture
semPlot::semPaths(object = adopt.fit, layout="tree", rotation=2)

#Update the default picture
semPlot::semPaths(object = adopt.fit, layout = "tree", rotation = 2, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "blue"
                  )

```
  
  
  
***
  
Chapter 4 - Full Example and Extension  
  
Model WAIS-III IQ Scale:  
  
* WAIS-III is a four-factor model of intelligence, including verbal, working memory, perceptual organization, and processing speed  
	* Idea is that Verbal IQ drives verbal and working memory, Performance IQ drives perceptual and processing, and Verbal/Performance drive each other  
    * 4 latent variables, measured by 12 manifest variables, with 2 additional latent variables at a higher layer that drive the initial 4 latent variables  
  
Update WAIS-III Model:  
  
* Once the model is stable, can look for additional areas to further improve the model  
* Variables that are poor on loadings and are also high in variance should be further explored  
* Can also use modification indices to better understand and model the data  
	* modificationindices(wais.fit, sort = TRUE)  
  
Hierarchical Model of IQ:  
  
* One overall IQ that is the latent variable for all of the other latent variable  
	* wais.model3 <- 'verbalcomp =~ vocab + simil + inform + compreh  
    * workingmemory =~ arith + digspan + lnseq  
    * perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch  
    * simil ~~ inform  
    * general =~ verbalcomp + workingmemory + perceptorg'  # the general is a new latent variable, built from other latent variables  
* The updated model will often have the same fit indices (simply shifting parameters from covariances to loadings)  
  
Wrap Up:  
  
* Learned model syntax for lavaan (=~ for latent, ~~ for covariance/correlation, and ~ for prediction)  
* Learned to add constraints and troubleshoot Heywood cases  
* Learned one-factor, multi-factor, and hierarchical models  
  
Example code includes:  
```{r}

IQdata <- readr::read_csv("./RInputFiles/IQdata.csv")
glimpse(IQdata)
IQdata <- IQdata %>%
    select(-X1)
glimpse(IQdata)


#Build a four-factor model
wais.model <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason
processing =~ digsym + symbolsearch'

#Analyze the model
wais.fit <- cfa(model=wais.model, data=IQdata)

#Summarize the model
summary(wais.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)


#Edit the original model
wais.model <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch'

#Analyze the model
wais.fit <- cfa(model=wais.model, data=IQdata)

#Summarize the model
summary(wais.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#Update the default picture
semPlot::semPaths(object = wais.fit, layout = "tree", rotation = 1, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "black"
                  )

#Examine modification indices 
modificationindices(wais.fit, sort = TRUE)


#Update the three-factor model
wais.model2 <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch
simil ~~ inform'

#Analyze the three-factor model where data is IQdata
wais.fit2 <- cfa(model=wais.model2, data=IQdata)

#Summarize the three-factor model 
summary(wais.fit2, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#Compare the models
anova(wais.fit, wais.fit2)


#View the fit indices for the original model
fitmeasures(wais.fit, c("aic", "ecvi"))

#View the fit indices for the updated model
fitmeasures(wais.fit2, c("aic", "ecvi"))


#Update the three-factor model to a hierarchical model
wais.model3 <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch
simil ~~ inform
general =~ verbalcomp + workingmemory + perceptorg'

#Analyze the hierarchical model where data is IQdata
wais.fit3 <- cfa(model = wais.model3, data = IQdata)

#Examine the fit indices for the old model
fitmeasures(wais.fit2, c("rmsea", "srmr"))

#Examine the fit indices for the new model
fitmeasures(wais.fit3, c("rmsea", "srmr"))


#Update the default picture
semPlot::semPaths(object = wais.fit3, layout = "tree", rotation = 1, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "navy"
                  )

```
  
  
  
***
  
###_Working with Data in the Tidyverse_  
  
Chapter 1 - Explore Data  
  
Import data:  
  
* Begging steps of the pipeline include importing, tidying, and transforming (wrangling)  
* Focus of this course will be recatngular data including both columns (variables) and rows (observations)  
	* bakers  # 10x6 tibble  
    * tibbles are a special type of data frame - both store rectangular data in R  
* Can read the data using readr::read_csv()  
	* ?read_csv  
    * bakers <- read_csv("bakers.csv")  
    * bakers  # same 10x6 tibble  
  
Know data:  
  
* The bakeoff data includes three types of challenges - Signature, Technical, Showstopper  
* Tibble printing by default will cut off columns and just show the variables - glimpse from dplyr can help with visualizing  
	* glimpse(bakers_mini)  
    * library(skimr)  
    * skim(bakers_mini)  # skim provides statistics for every column depending on the variable types  
  
Count data - broken video that provides some code snippets:  
  
* bakers %>% distinct(series)  
* bakers %>% count(series)  
* bakers %>% group_by(series) %>% summarize(n = n())  
* bakers %>% count(aired_us, series)  
* bakers %>% count(aired_us, series) %>% mutate(prop_bakers = n/sum(n))  
* bakers %>% group_by(aired_us, series) %>% summarize(n = n()) %>% mutate(prop_bakers = n/sum(n))  
* bakers %>% count(aired_us, series) %>% count(aired_us)  
  
Example code includes:  
```{r}

# Read in "bakeoff.csv" as bakeoff
bakeoff <- readr::read_csv("./RInputFiles/bakeoff.csv")

# Print bakeoff
bakeoff


# Data set above is already OK - UNKNOWN are NA in CSV
# Filter rows where showstopper is UNKNOWN 
bakeoff %>% 
    filter(showstopper == "UNKNOWN")

# Edit to add list of missing values
bakeoff <- read_csv("./RInputFiles/bakeoff.csv", na = c("", "NA", "UNKNOWN"))

# Filter rows where showstopper is NA 
bakeoff %>% 
    filter(is.na(showstopper))


# Edit to filter, group by, and skim
bakeoff %>% 
  filter(!is.na(us_season)) %>% 
  group_by(us_season) %>%
  skimr::skim()
  

bakeoff %>% 
  distinct(result)

# Count rows by distinct results
bakeoff %>% 
  count(result)

# Count whether or not star baker
bakeoff %>% 
  count(result=="SB")


# Count the number of rows by series and episode
bakeoff %>%
  count(series, episode)

# Add second count by series
bakeoff %>% 
  count(series, episode) %>%
  count(series)


# Count the number of rows by series and baker
bakers_by_series <- 
  bakeoff %>%
  count(series, baker)

# Print to view
bakers_by_series

# Count again by series
bakers_by_series %>%
  count(series)

# Count again by baker
bakers_by_series %>%
  count(baker, sort=TRUE)


ggplot(bakeoff, aes(x=episode)) + 
    geom_bar() + 
    facet_wrap(~series)

```
  
  
  
***
  
Chapter 2 - Tame Data  
  
Cast column types:  
  
* Type-casting can be an important step in taming data  
* The readr package has options for col_type within the read_csv() function  
	* By default, all of the column types are guessed from the first 1,000 rows  
    * bakers_raw %>% dplyr::slice(1:4)  # look at the first 4 rows  
* Can convert a character to a number using parse_number()  
	* parse_number("36 years")  # will become 36  
    * bakers_tame <- read_csv(file = "bakers.csv", col_types = cols(age = col_number()) )  # col_number() will wrangle the age column to a numeric  
* Can also use the parse_date capability to manage datetime inputs  
	* parse_date("14 August 2012", format = "%d %B %Y")  
    * bakers <- read_csv("bakers.csv", col_types = cols( last_date_uk = col_date(format = "%d %B %Y") ))  # col_date() will wrangle last_date_uk to a datetime  
* There is always both a parse_* and a col_* for any given data type; can practive with parse_* then use col_* in the read-in  
  
Recode values:  
  
* The recode() function in dplyr can be used to recode values in the data  
	* young_bakers %>% mutate(stu_label = recode(student, `0` = "other", .default = "student"))  # 0 will become other, anything else will become student  
    * young_bakers %>% mutate(stu_label = recode(student, `0` = NA_character_, .default = "student"))  # create NA for a specific string  
    * young_bakers %>% mutate(student = na_if(student, 0))  # na_if will convert to NA if the condition(s) is met  
  
Select variables:  
  
* Can select just a subset of the variables using select  
* The select() function is powerful when you only need to work with a subset of the data  
	* young_bakers2 %>% select(baker, series_winner)  # keep these variables  
    * young_bakers2 %>% select(-technical_winner)  # drop these variables (signalled by the minus sign)  
* Can use helper functions inside the select() call  
	* young_bakers2 %>% select(baker, starts_with("series"))  
    * young_bakers2 %>% select(ends_with("winner"), baker)  
    * young_bakers2 %>% select(contains("bake"))  
* The filter() function works on rows rather than columns  
	* young_bakers2 %>% filter(series_winner == 1 | series_runner_up == 1)  
  
Tame variable names:  
  
* Can rename variables while selecting  
	* young_bakers3 %>% select(baker, tech_1 = tre1)  
    * young_bakers3 %>% select(baker, tech_ = tre1:tre3)  
    * young_bakers3 %>% select(baker, tech_ = starts_with("tr"), result_ = starts_with("rs"))  
* Within the rename call, it is not possible to use the helper functions  
	* young_bakers3 %>% rename(tech_1 = t_first, result_1 = r_first)  # new = old  
    * young_bakers3 %>% select(everything(), tech_ = starts_with("tr"), result_ = starts_with("rs"))  # everything first keeps all the column orders the same  
* Can also use the janitor package to help with cleaning variables  
	* young_bakers3 %>% janitor::clean_names()  
    * Converts to snake case (lower case with underscores)  
  
Example code includes:  
```{r}

# NOTE THAT THIS WILL THROW WARNINGS
# Try to cast technical as a number
desserts <- readr::read_csv("./RInputFiles/desserts.csv",
                      col_types = cols(
                        technical = col_number())
                     )

# View parsing problems
readr::problems(desserts)

# NOTE THAT THIS WILL FIX THE ERRORS
# Edit code to fix the parsing error 
desserts <- readr::read_csv("./RInputFiles/desserts.csv",
                      col_types = cols(
                        technical = col_number()),
                      na = c("", "NA", "N/A") 
                     )

# View parsing problems
readr::problems(desserts)


# Find format to parse uk_airdate 
readr::parse_date("17 August 2010", format = "%d %B %Y")

# Edit to cast uk_airdate
desserts <- readr::read_csv("./RInputFiles/desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       technical = col_number(),
                       uk_airdate = col_date("%d %B %Y")
                     ))

# Print by descending uk_airdate
desserts %>%
  arrange(desc(uk_airdate))


# Cast result a factor
desserts <- readr::read_csv("./RInputFiles/desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       technical = col_number(),
                       uk_airdate = col_date(format = "%d %B %Y"),
                       result = col_factor(levels=NULL)
                     ))
                    
# Glimpse to view
glimpse(desserts)


oldDesserts <- desserts
tempDesserts <- desserts %>%
    gather(key="type_ing", value="status", starts_with(c("showstopper")), starts_with(c("signature"))) %>%
    separate(type_ing, into=c("challenge", "ingredient"), sep="_") %>%
    spread(ingredient, status)
glimpse(tempDesserts)
desserts <- tempDesserts


# Count rows grouping by nut variable
desserts %>%
  count(nut, sort=TRUE)

# Recode filberts as hazelnuts
desserts <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut"))

# Count rows again 
desserts %>% 
    count(nut, sort = TRUE)

# Edit code to recode "no nut" as missing
desserts <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut", 
                           "no nut" = NA_character_))

# Count rows again 
desserts %>% 
    count(nut, sort = TRUE)


# Edit to recode tech_win as factor
desserts <- desserts %>% 
  mutate(tech_win = recode_factor(technical, `1` = 1,
                           .default = 0))

# Count to compare values                      
desserts %>% 
  count(technical == 1, tech_win)


ratings0 <- readr::read_csv("./RInputFiles/02.03_messy_ratings.csv")
str(ratings0, give.attr=FALSE)

ratings <- ratings0 %>%
    filter(series >= 3) %>%
    rename(day=day_of_week) %>%
    mutate(series=factor(series), 
           season_premiere=lubridate::mdy(season_premiere), 
           season_finale=lubridate::mdy(season_finale), 
           viewer_growth = (e10_viewers_7day - e1_viewers_7day)
           ) %>%
    select(-contains("uk_airdate"))


# Recode channel as dummy: bbc (1) or not (0)
ratings <- ratings %>% 
  mutate(bbc = recode_factor(channel, "Channel 4"=0, .default=1))

# Look at the variables to plot next
ratings %>% select(series, channel, bbc, viewer_growth)

# Make a filled bar chart
ggplot(ratings, aes(x = series, y = viewer_growth, fill = bbc)) +
  geom_col()


# Move channel to first column
ratings %>% 
  select(channel, everything())

# Edit to drop 7- and 28-day episode viewer data
ratings %>% 
  select(-ends_with("day"))

# Edit to move channel to first and drop episode viewer data
ratings %>% 
  select(-ends_with("day")) %>%
  select(channel, everything())


# Glimpse messy names
# glimpse(messy_ratings)

# Reformat to lower camelcase
# ratings <- messy_ratings %>%
#   clean_names(case="lower_camel")
    
# Glimpse cleaned names
# glimpse(ratings)

# Reformat to snake case
# ratings <- messy_ratings %>% 
#     clean_names("snake")

# Glimpse cleaned names
# glimpse(ratings)


# Select 7-day viewer data by series
viewers_7day <- ratings %>%
  select(series, contains("7day"))

# Glimpse
glimpse(viewers_7day)

# Adapt code to also rename 7-day viewer data
viewers_7day <- ratings %>% 
    select(series, viewers_7day_ = ends_with("7day"))

# Glimpse
glimpse(viewers_7day)


# Adapt code to drop 28-day columns; move 7-day to front
viewers_7day <- ratings %>% 
    select(viewers_7day_ = ends_with("7day"), everything(), -contains("28day"))

# Glimpse
glimpse(viewers_7day)


# Adapt code to keep original order
viewers_7day <- ratings %>% 
    select(everything(), -ends_with("28day"), viewers_7day_ = ends_with("7day"))

# Glimpse
glimpse(viewers_7day)

```
  
  
  
***
  
Chapter 3 - Tidy Your Data  
  
Introduction to Tidy Data:  
  
* Tidy data helps with producing good plots - allows for faceting and the like  
* Data can be tidy but not tame, and can be tame but not tidy  
	* In general, tidy data is long rather than wide  
    * As a result, tidy data tends to take up more space, but with the advantage of being easier to plot or analyze  
* Can automatically get counts summed to a specific level  
	* juniors_tidy %>% count(baker, wt = correct)  # variable wt will be the sum of correct  
    * ggplot(juniors_tidy, aes(baker, correct)) + geom_col()  # roughly the equivalent if plotting the data  
  
Gather:  
  
* Gathering is the process of converting data from wide to long  
	* gather(data, key, value, .)  
    * key is the new column containing the variable  
    * value is the new column contining the value  
    * The . are the columns to be gathered, with column name going to the key column and associated values going to the value column  
    * The key and value need to be quoted while the . can be passed bare (unquoted)  
  
Separate:  
  
* Sometimes, a column really contains two variable, for example when there is spice_trail or the like  
* The separate function requires at least three arguments  
	* data - the data frame  
    * col - the column that you want to separate (can be a bare variable name since it already exists in the data frame)  
    * into - quoted variables to be created, inside the c() function  
    * By default, the existing column col is replaced  
    * There is also an option for convert=TRUE where it will try to pick the best variable type (especially helpful when creating numbers)  
    * There is also the option for sep, where the defaults for separators can be over-ridden to better match the data  
  
Spread:  
  
* The spread function is designed to convert long data to wide data  
	* Spread can be considered a tool to tidy messy rows, where gather is a tool to tidy messy columns  
    * data - the data frame  
    * key - the key is the column that currently contains what should become the new columns  
    * value - value is the column that currently contains what should become the values in the new columns  
    * convert=TRUE will help with re-casting variable types (particularly helpful when numbers are being pulled out of a mixed character-number column (likely what drove the need to spread)  
  
Tidy multiple sets of data:  
  
* Sometimes, there are multiple data components to tidy, where the columns need to be fixed in several ways  
	* For example, score_1, guess_1, score_2, guess_2  
    * Ideal target would be to have trials (1, 2, 3) in one column, and with columns score and guess containing the variables  
* Example code for converting multiple columns simultaneously  
	* juniors_multi %>% gather(key = "key", value = "value", score_1:guess_3) %>% separate(key, into = c("var", "order"), convert = TRUE)  
    * juniors_multi %>% gather(key = "key", value = "value", score_1:guess_3) %>% separate(key, into = c("var", "order"), convert = TRUE) %>% spread(var, value)  
  
Example code includes:  
```{r}

ratings1 <- readr::read_csv("./RInputFiles/messy_ratings.csv")
oldRatings <- ratings
ratings <- ratings1
ratings1

# Plot of episode 1 viewers by series
ratings %>%
  ggplot(aes(x=series, y=e1)) + 
  geom_bar(stat="identity")
  
# Adapt code to plot episode 2 viewers by series
ggplot(ratings, aes(x = series, y = e2)) +
    geom_col() 


# Gather and count episodes
tidy_ratings <- ratings %>%
    gather(key = "episode", value = "viewers_7day", -series, 
           factor_key = TRUE, na.rm = TRUE) %>% 
    arrange(series, episode) %>% 
    mutate(episode_count = row_number())

# Plot viewers by episode and series
ggplot(tidy_ratings, aes(x = episode_count, y = viewers_7day, fill = as.factor(series))) +
    geom_col()


ratings2 <- readr::read_csv("./RInputFiles/messy_ratings2.csv")
ratings2$series <- as.factor(ratings2$series)
ratings2

# Gather 7-day viewers by episode (ratings2 already loaded)
week_ratings <- ratings2  %>% 
    select(series, ends_with("7day")) %>% 
    gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE, factor_key = TRUE)
    
# Plot 7-day viewers by episode and series
ggplot(week_ratings, aes(x = episode, y = viewers_7day, group = series)) +
    geom_line() +
    facet_wrap(~series)


# Edit to parse episode number
week_ratings <- ratings2 %>% 
    select(series, ends_with("7day")) %>% 
    gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE) %>% 
    separate(episode, into = "episode", extra = "drop") %>% 
    mutate(episode = parse_number(episode))
    
# Edit your code to color by series and add a theme
ggplot(week_ratings, aes(x = episode, y = viewers_7day, 
                         group = series, color = series)) +
    geom_line() +
    facet_wrap(~series) +
    guides(color = FALSE) +
    theme_minimal() 


week_ratings_dec <- week_ratings %>%
    mutate(viewers_7day=as.character(viewers_7day)) %>%
    separate(viewers_7day, into=c("viewers_millions", "viewers_decimal"), sep="\\.") %>%
    mutate(viewers_decimal=ifelse(is.na(viewers_decimal), ".", paste0(".", viewers_decimal))) %>%
    dplyr::arrange(series, episode)

# Unite series and episode
ratings3 <- week_ratings_dec %>% 
    unite("viewers_7day", viewers_millions, viewers_decimal)

# Print to view
ratings3


# Adapt to change the separator
ratings3 <- week_ratings_dec  %>% 
    unite(viewers_7day, viewers_millions, viewers_decimal, sep="")

# Print to view
ratings3


# Adapt to cast viewers as a number
ratings3 <- week_ratings_dec  %>% 
    unite(viewers_7day, viewers_millions, viewers_decimal, sep="") %>%
    mutate(viewers_7day = parse_number(viewers_7day))

# Print to view
ratings3


# Create tidy data with 7- and 28-day viewers
tidy_ratings_all <- ratings2 %>%
    gather(episode, viewers, ends_with("day"), na.rm = TRUE) %>% 
    separate(episode, into = c("episode", "days")) %>%  
    mutate(episode = parse_number(episode),
           days = parse_number(days)) 

# Adapt to spread counted values
tidy_ratings_all %>% 
    count(series, days, wt = viewers) %>%
    spread(key=days, value=n, sep="_")

# Fill in blanks to get premiere/finale data
tidy_ratings <- ratings %>%
    gather(episode, viewers, -series, na.rm = TRUE) %>%
    mutate(episode = parse_number(episode)) %>% 
    group_by(series) %>% 
    filter(episode == 1 | episode == max(episode)) %>% 
    ungroup()


# Recode first/last episodes
first_last <- tidy_ratings %>% 
  mutate(episode = recode(episode, `1` = "first", .default = "last")) 

# Fill in to make slope chart
ggplot(first_last, aes(x = episode, y = viewers, color = as.factor(series))) +
  geom_point() +
  geom_line(aes(group = series))

# Switch the variables mapping x-axis and color
ggplot(first_last, aes(x = series, y = viewers, color = episode)) +
  geom_point() + # keep
  geom_line(aes(group = series)) + # keep
  coord_flip() # keep

# Calculate relative increase in viewers
bump_by_series <- first_last %>% 
  spread(episode, viewers) %>%   
  mutate(bump = (last - first) / first)
  
# Fill in to make bar chart of bumps by series
ggplot(bump_by_series, aes(x = series, y = bump)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) # converts to %

```
  
  
  
***
  
Chapter 4 - Transform Your Data  
  
Complex recoding with case_when:  
  
* The case_when function allow for vectoizing multiple if-else-then statements  
	* The LHS must give (or be) a boolean  
    * The default value for else is NA  
* Example using ages of the baker data  
	* bakers %>% mutate(gen = if_else(between(birth_year, 1981, 1996), "millenial", "not millenial"))  # simple if statement (boundaries of between are inclusive)  
    * bakers %>% mutate(gen = case_when( between(birth_year, 1965, 1980) ~ "gen_x", between(birth_year, 1981, 1996) ~ "millenial" ))  # logical ~ result  
    * bakers %>% mutate(gen = case_when( between(birth_year, 1928, 1945) ~ "silent", between(birth_year, 1946, 1964) ~ "boomer", between(birth_year, 1965, 1980) ~ "gen_x", between(birth_year, 1981, 1996) ~ "millenial", TRUE ~ "gen_z" ))  
    * bakers %>% count(gen, sort = TRUE) %>% mutate(prop = n / sum(n))  
  
Factors:  
  
* The forcats package is made specifically for working with factors - all functions start with fct_  
* Converting to factors helps ensure the proper ordering of the data  
	* ggplot(bakers, aes(x = fct_rev(fct_infreq(gen)))) + geom_bar()  # reverse by infrequency (build from small to large)  # on-the-fly conversions inside ggplot  
    * bakers <- bakers %>% mutate(gen = fct_relevel(gen, "silent", "boomer", "gen_x", "millenial", "gen_z"))  # conversions of the raw dataset  
    * bakers %>% dplyr::pull(gen) %>% levels()  # check that this worked  
    * ggplot(bakers, aes(x = gen)) + geom_bar()  # will now be plotted in the desired order  
* Need to be careful of the proper treatment of factors  
	* ggplot(bakers, aes(x = gen, fill = series_winner)) + geom_bar()  # FAIL  
    * bakers <- bakers %>% mutate(series_winner = as.factor(series_winner))  
    * ggplot(bakers, aes(x = gen, fill = series_winner)) + geom_bar()  # WORKS  
    * ggplot(bakers, aes(x = gen, fill = as.factor(series_winner))) + geom_bar()  # ALSO WORKS  
  
Dates:  
  
* Can use lubridate for convenience functions such as ymd() or dmy(), with the output being ISO (YYYY-MM-DD)  
	* Can also include a vector of suspected dates  
    * dmy("17 August 2010")  # will work  
    * hosts <- tibble::tribble( ~host, ~bday, ~premiere, "Mary", "24 March 1935", "August 17th, 2010", "Paul", "1 March 1966", "August 17th, 2010")  
    * hosts <- hosts %>% mutate(bday = dmy(bday), premiere = mdy(premiere))  
* There are three aspects of timespans  
	* interval - time span bound by two real dates  
    * duration - exact number of seconds in an interval  
    * period - change in clock time of an interval  
    * hosts <- hosts %>% mutate(age_int = interval(bday, premiere))  # new variable age_int will be of type interval  
    * hosts %>% mutate(years_decimal = age_int / years(1), years_whole = age_int %/% years(1))  # years(1) is one year, so this is fractional and whole (floored) years  
  
Strings:  
  
* The separate function splits one column in to 2+ columns (for example "age, job" could become "age" and "job")  
	* series5 <- series5 %>% separate(about, into = c("age", "occupation"), sep = ", ")  
    * series5 <- series5 %>% separate(about, into = c("age", "occupation"), sep = ", ") %>% mutate(age = parse_number(age))  # numeric age. Dropping years  
* The stringr package makes working with strings in R easier (typically used within a mutate) - all functions start with str_  
	* series5 <- series5 %>% mutate(baker = str_to_upper(baker), showstopper = str_to_lower(showstopper))  
    * series5 %>% mutate(pie = str_detect(showstopper, "pie"))  # returns a boolean  
    * series5 %>% mutate(showstopper = str_replace(showstopper, "pie", "tart"))  # find and replace for strings  
    * series5 %>% mutate(showstopper = str_remove(showstopper, "pie"))  # remove "pie", though there may be trailing whitespace  
    * series5 %>% mutate(showstopper = str_remove(showstopper, "pie"), showstopper = str_trim(showstopper))  # trim whitespace at the beginning or end  
  
Final thoughts:  
  
* R using the tidyverse for analysis and presentation  
* Reading data using readr and analyzing using dplyr and ggplot2  
* Taming variable types, names, and values  
* Transforming data using stringr and lubridate  
* The "here" package can make working with file paths much easier  
  
Example code includes:  
```{r}

baker_results <- readr::read_csv("./RInputFiles/baker_results.csv")
messy_baker_results <- readr::read_csv("./RInputFiles/messy_baker_results.csv")
bakers <- baker_results
glimpse(bakers)


# Create skill variable with 3 levels
bakers <- bakers %>% 
  mutate(skill = case_when(
    star_baker > technical_winner ~ "super_star",
    star_baker < technical_winner ~ "high_tech",
    TRUE ~ "well_rounded"
  ))
  
# Filter zeroes to examine skill variable
bakers %>% 
  filter(star_baker==0 & technical_winner==0) %>% 
  count(skill)


# Add pipe to drop skill = NA
bakers_skill <- bakers %>% 
  mutate(skill = case_when(
    star_baker > technical_winner ~ "super_star",
    star_baker < technical_winner ~ "high_tech",
    star_baker == 0 & technical_winner == 0 ~ NA_character_,
    star_baker == technical_winner  ~ "well_rounded"
  )) %>% 
  drop_na(skill)
  
# Count bakers by skill
bakers_skill %>%
  count(skill)


# Cast skill as a factor
bakers <- bakers %>% 
  mutate(skill = as.factor(skill))

# Examine levels
bakers %>%
  pull(skill) %>%
  levels()


baker_dates <- bakers %>%
    select(series, baker, contains("date")) %>%
    mutate(last_date_appeared_us=as.character(last_date_us), 
           first_date_appeared_us=as.character(first_date_us)
           ) %>%
    rename(first_date_appeared_uk=first_date_appeared, last_date_appeared_uk=last_date_appeared) %>%
    select(-last_date_us, -first_date_us)
glimpse(baker_dates)


# Add a line to extract labeled month
baker_dates <- baker_dates %>% 
  mutate(last_date_appeared_us=lubridate::ymd(last_date_appeared_us), 
         last_month_us=lubridate::month(last_date_appeared_us, label=TRUE)
         )
         
ggplot(baker_dates, aes(x=last_month_us)) + geom_bar()


baker_time <- baker_dates %>%
    mutate(first_date_appeared_us=lubridate::ymd(first_date_appeared_us)) %>%
    select(-last_month_us)
glimpse(baker_time)

           
# Add a line to create whole months on air variable
baker_time <- baker_time  %>% 
  mutate(time_on_air = lubridate::interval(first_date_appeared_uk, last_date_appeared_uk),
         weeks_on_air = time_on_air / lubridate::weeks(1), 
         months_on_air = time_on_air %/% months(1)
         )

# Count rows
messy_baker_results %>% 
  count(position_reached)


# Add another mutate to replace "THIRD PLACE" with "RUNNER UP"and count
messy_baker_results <- messy_baker_results %>% 
  mutate(position_reached = str_to_upper(position_reached),
         position_reached = str_replace(position_reached, "-", " "), 
         position_reached = str_replace(position_reached, "THIRD PLACE", "RUNNER UP"))

# Count rows
messy_baker_results %>% 
  count(position_reached)


# Add a line to create new variable called student
bakers <- bakers %>% 
    mutate(occupation = str_to_lower(occupation), 
           student=str_detect(occupation, "student")
           )

# Find all students and examine occupations
bakers %>% 
  filter(student) %>%
  select(baker, occupation, student)

```
  
  
  
***
  
###_Modeling Data in the Tidyverse_  
  
Chapter 1 - Introduction to Modeling  
  
Background on modeling for explanation:  
  
* Generally, the model has y as a function of x plus epsilon, where y is the outcome of interest and x is a set of explanatory variables and epsilon is irreducible error  
	* The x can be either explanatory or predictive - depends on the purpose of the analysis  
* Example of explanation - can differences in teacher evaluation scores be explained by teacher attributes  
	* library(dplyr)  
    * library(moderndive)  
    * glimpse(evals)  # evals data is available in the moderndivw package (From the moderndive package for ModernDive.com:)  
    * ggplot(evals, aes(x = score)) + geom_histogram(binwidth = 0.25) + labs(x = "teaching score", y = "count")  # EDA on scores using histogram  
    * evals %>% summarize(mean_score = mean(score), median_score = median(score), sd_score = sd(score))  # summary statistics using dplyr::summarize  
  
Background on modeling for prediction:  
  
* House sales in King County USA in 2014-2015 (from Kaggle) based on features such as size, bedrooms, etc.  
	* glimpse(house_prices)  
    * ggplot(house_prices, aes(x = price)) + geom_histogram() + labs(x = "house price", y = "count")  
    * house_prices <- house_prices %>% mutate(log10_price = log10(price))  
    * house_prices %>% select(price, log10_price)  
    * ggplot(house_prices, aes(x = log10_price)) + geom_histogram() + labs(x = "log10 house price", y = "count")  # after transformation  
  
Modeling problem for explanation:  
  
* Typically, both the function that relates x and y and the function that generates the errors is unknwon  
	* Goal is to create a model that can generate y-hat by separating signal from noise  
* Can start by considering linear models, assessed as a starting point by examining a scatter plot  
	* ggplot(evals, aes(x = age, y = score)) + geom_point() + labs(x = "age", y = "score", title = "Teaching score over age")  
    * ggplot(evals, aes(x = age, y = score)) + geom_jitter() + labs(x = "age", y = "score", title = "Teaching score over age (jittered)")  
* Can further explore the data by looking at correlations among some or all of the potential explanatory variables  
	* evals %>% summarize(correlation = cor(score, age))  
  
Modeling problem for prediction:  
  
* For explanation, we care about the form of the function  
* For prediction, we care mainly that the function makes good predictions (even if it may not be easy to explain)  
	* house_prices %>% select(log10_price, condition) %>% glimpse()  # condition is a categorical variable saved as a factor  
    * ggplot(house_prices, aes(x = condition, y = log10_price)) + geom_boxplot() + labs(x = "house condition", y = "log10 price", title = "log10 house price over condition")  
* Means tend to be at the center of the linear modeling process  
	* house_prices %>% group_by(condition) %>% summarize(mean = mean(log10_price), sd = sd(log10_price), n = n())  
  
Example code includes:  
```{r}

data(evals, package="moderndive")
glimpse(evals)


# Plot the histogram
ggplot(evals, aes(x = age)) +
  geom_histogram(binwidth = 5) +
  labs(x = "age", y = "count")

# Compute summary stats
evals %>%
  summarize(mean_age = mean(age),
            median_age = median(age),
            sd_age = sd(age))


data(house_prices, package="moderndive")
glimpse(house_prices)


# Plot the histogram
ggplot(house_prices, aes(x = sqft_living)) +
  geom_histogram() +
  labs(x="Size (sq.feet)", y="count")

# Add log10_sqft_living
house_prices_2 <- house_prices %>%
  mutate(log10_sqft_living = log10(sqft_living))

# Plot the histogram  
ggplot(house_prices_2, aes(x = log10_sqft_living)) +
  geom_histogram() +
  labs(x = "log10 size", y = "count")


# Plot the histogram
ggplot(evals, aes(x=bty_avg)) +
  geom_histogram(binwidth=0.5) +
  labs(x = "Beauty score", y = "count")

# Scatterplot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "teaching score")

# Jitter plot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "beauty score", y = "teaching score")


# Compute correlation
evals %>%
  summarize(correlation = cor(score, bty_avg))


house_prices <- house_prices %>%
    mutate(log10_price=log10(price))

# View the structure of log10_price and waterfront
house_prices %>%
  select(log10_price, waterfront) %>%
  glimpse()

# Plot 
ggplot(house_prices, aes(x = waterfront, y = log10_price)) +
  geom_boxplot() +
  labs(x = "waterfront", y = "log10 price")


# Calculate stats
house_prices %>%
  group_by(waterfront) %>%
  summarize(mean_log10_price = mean(log10_price), n = n())
  
# Prediction of price for houses with view
10^(6.12)

# Prediction of price for houses without view
10^(5.66)

```
  
  
  
***
  
Chapter 2 - Modeling with Regression  
  
Explaining teaching score with age:  
  
* Can overlay a regression line to the scatter plot for a bivariate relationship  
	* ggplot(evals, aes(x = age, y = score)) + geom_point() + labs(x = "age", y = "score", title = "Teaching score over age") + geom_smooth(method = "lm", se = FALSE)  
* In simple linear regression, the assumption is that f(x) is B0 + B1*x  
    * The fitted model f-hat does not have an error term, since it is just the model prediction for a given value of x  
    * model_score_1 <- lm(score ~ age, data = evals)  
    * moderndive::get_regression_table(model_score_1)  
  
Predicting teaching score using age:  
  
* Can make predictions based on the existing regression line - f-hat can be used for both explanatory and predictive purposes  
* The residuals are the errors (predictive vs. actual values), and correspond to the epsilon of the general modeling framework  
    * On average, for linear regression, the residuals should average out to zero  
    * get_regression_points(model_score_1)  # gives y, x, y-hat, and residuals  
  
Explaining teaching score with gender:  
  
* Can extend the models to include categorical data, such as gender  
	* ggplot(evals, aes(x = score)) + geom_histogram(binwidth = 0.25) + facet_wrap(~gender) + labs(x = "score", y = "count")  
    * model_score_3 <- lm(score ~ gender, data = evals)  # will just give an overall mean and a change in mean vs. the first-level factor  
* Can also look at multi-level factors, such as rank (teacher type)  
	* evals %>% group_by(rank) %>% summarize(n = n())  
  
Predicting teaching score with gender:  
  
* Can use group means as part of the predictive approach - if only factor are used in the regression, there will be the same prediction for everyone who is in the same class(es)  
	* model_score_3_points <- get_regression_points(model_score_3)  
  
Example code includes:  
```{r}

# Plot 
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "score") +
  geom_smooth(method = "lm", se = FALSE)

# Fit model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Output content
model_score_2

# Output regression table
moderndive::get_regression_table(model_score_2)

# Use fitted intercept and slope to get a prediction
y_hat <- 3.88 + 0.067 * 5
y_hat

# Compute residual y - y_hat
4.7 - y_hat


# Get regression table
moderndive::get_regression_table(model_score_2, digits = 5)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2) %>% 
  mutate(score_hat_2 = 3.88 + 0.0666 * bty_avg)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2) %>% 
  mutate(residual_2 = score - score_hat)


ggplot(evals, aes(x=rank, y=score)) +
  geom_boxplot() +
  labs(x = "rank", y = "score")

evals %>%
  group_by(rank) %>%
  summarize(n = n(), mean_score = mean(score), sd_score = sd(score))


# Fit regression model
model_score_4 <- lm(score ~ rank, data = evals)

# Get regression table
moderndive::get_regression_table(model_score_4, digits = 5)

# teaching mean
teaching_mean <- 4.28

# tenure track mean
tenure_track_mean <- 4.28-0.13 

# tenure mean
tenure_mean <- 4.28-0.145


# Calculate predictions and residuals
model_score_4_points <- moderndive::get_regression_points(model_score_4)
model_score_4_points

# Plot residuals
ggplot(model_score_4_points, aes(x=residual)) +
  geom_histogram() +
  labs(x = "residuals", title = "Residuals from score ~ rank model")

```
  
  
  
***
  
Chapter 3 - Modeling with Multiple Regression  
  
Explaining house price with year and size:  
  
* Can incorporate 2+ explanatory / predictive variable using multiple regression  
	* house_prices %>% select(price, sqft_living, condition, waterfront) %>% glimpse()  
    * The log-10 transformation is helpful for this specific dataset (assume the code below for future examples in this course)  
    * house_prices <- house_prices %>% mutate( log10_price = log10(price), log10_sqft_living = log10(sqft_living) )  
* Exploring the relationship between mutliple variables - EDA and regression  
	* Can create a 3D plot with associated regression plane using plotly  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * get_regression_table(model_price_1, digits = 5))  
  
Predicting house price using year and size:  
  
* Can get the fitted values and exponentiate as needed, assessing the overall fit or lack thereof (sum-squared residuals) of the model  
	* get_regression_points(model_price_1, digits = 5)  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(squared_residuals))  # SSR  
  
Explaining house price with size and condition:  
  
* The EDA from previous chapters is repeated  
	* house_prices <- house_prices %>% mutate( log10_price = log10(price), log10_sqft_living = log10(sqft_living) )  
    * house_prices %>% group_by(condition) %>% summarize(mean = mean(log10_price), sd = sd(log10_price), n = n())  
* The parallel slopes model is lines where the slopes are the same but they have a different intercept (likely, coefficients of a categorical variable)  
	* model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_table(model_price_3, digits = 5)  
  
Predicting house price using size and condition:  
  
* Objective is to predict on new data (as opposed to checking our predictions on data where we already had the answer)  
	* model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_table(model_price_3)  
* Automating the housing price prediction process  
	* new_houses <- read_csv("new_houses.csv")  
    * new_houses  
    * get_regression_points(model_price_3, newdata = new_houses)  # moderndata form of predict() function  
    * get_regression_points(model_price_3, newdata = new_houses) %>% mutate(price_hat = 10^log10_price_hat)  
  
Example code includes:  
```{r}

# Create scatterplot with regression line
ggplot(house_prices, aes(x=bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)

# Remove outlier
house_prices_transform <- house_prices %>%
    filter(bedrooms < 33) %>%
    mutate(log10_sqft_living=log10(sqft_living))

# Create scatterplot with regression line
ggplot(house_prices_transform, aes(x = bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)


# Fit model
model_price_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data = house_prices_transform)

# Get regression table
moderndive::get_regression_table(model_price_2)

# Make prediction in log10 dollars
2.69 + 0.941 * log10(1000) - 0.033 * 3

# Make prediction dollars
10**(2.69 + 0.941 * log10(1000) - 0.033 * 3)

# Automate prediction and residual computation
moderndive::get_regression_points(model_price_2) %>%
    mutate(squared_residuals = residual**2) %>%
    summarize(sum_squared_residuals = sum(squared_residuals))


# Fit model
model_price_4 <- lm(log10_price ~ log10_sqft_living + waterfront, data = house_prices_transform)

# Get regression table
moderndive::get_regression_table(model_price_4)

# Prediction for House A
10**(2.96 + 0.825*2.9 + 0.322)

# Prediction for House B
10**(2.96 + 0.825*3.1 + 0)


# View the "new" houses
new_houses_2 <- tibble(log10_sqft_living=c(2.9, 3.1), waterfront=c(TRUE, FALSE))
new_houses_2

# Get predictions price_hat in dollars on "new" houses
moderndive::get_regression_points(model_price_4, newdata = new_houses_2) %>% 
  mutate(price_hat = 10**log10_price_hat)

```
  
  
  
***
  
Chapter 4 - Model Selection and Assessment  
  
Model selection and assessment:  
  
* Can use multiple models for the same data and compare  
	* model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
    * get_regression_points(model_price_3) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
  
Assessing model fit with R-squared:  
  
* The R-squared is a reasonable measure of model fit - R-squared = 1 - Var(Residuals) / Var(Y)  
	* Larger R-squared is suggestive of better fit, with values (typically) constrained between 0 and 1  
    * R-squared is the proportion of variation in the outcome model that can be explained using the model  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * get_regression_points(model_price_1) %>% summarize(r_squared = 1 - var(residual) / var(log10_price))  
    * model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_points(model_price_3) %>% summarize(r_squared = 1 - var(residual) / var(log10_price))  
  
Assessing predictions with RMSE:  
  
* RMSE (Root Mean Squared Error) is a slight variation on RSS  
	* Where RSS is the sum-squared of the residuals, RMSE is the square root of the average of the residuals-squared  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)   
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals))  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals)) %>% mutate(rmse = sqrt(mse))  
* Cannot calculate RMSE on new data - predictions means that we do not know the actual values  
	* get_regression_points(model_price_3, newdata = new_houses) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals)) %>% mutate(rmse = sqrt(mse))  
    * The above code will crash out, since the residuals do not exist  
  
Validation set prediction framework:  
  
* Use two different datasets for modeling; a training set used for modeling, and a test set used for assessing likely out-of-sample errors  
	* house_prices_shuffled <- house_prices %>% sample_frac(size = 1, replace = FALSE)  # Randomly shuffle order of rows  
    * train <- house_prices_shuffled %>% slice(1:10000)  
    * test <- house_prices_shuffled %>% slice(10001:21613)  
    * train_model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = train)  
* After having trained the model on the train data, can assess the fit using the test data  
	* get_regression_points(train_model_price_1, newdata = test)  
    * get_regression_points(train_model_price_1, newdata = test) %>% mutate(sq_residuals = residual^2) %>% summarize(rmse = sqrt(mean(sq_residuals)))  
  
Next steps:  
  
* Tidyverse ties together many of the packages that help with data wrangling and analysis  
* Can extend regressions to areas like polynomials and trees  
* "ModernDive" is a textbook on the tidyverse tools  
  
Example code includes:  
```{r}

# Model 2
model_price_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data = house_prices_transform)

# Calculate squared residuals
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals=residual**2) %>% 
    summarize(sum_sq_residuals=sum(sq_residuals))

# Model 4
model_price_4 <- lm(log10_price ~ log10_sqft_living + waterfront, data = house_prices_transform)

# Calculate squared residuals
moderndive::get_regression_points(model_price_4) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(sum_sq_residuals=sum(sq_residuals))


# Get fitted/values & residuals, compute R^2 using residuals
moderndive::get_regression_points(model_price_2) %>% 
    summarize(r_squared = 1 - var(residual) / var(log10_price))

# Get fitted/values & residuals, compute R^2 using residuals
moderndive::get_regression_points(model_price_4) %>% 
    summarize(r_squared = 1 - var(residual) / var(log10_price))


# Get all residuals, square them, take the mean and square root
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals)) %>% 
    mutate(rmse = sqrt(mse))

# MSE and RMSE for model_price_2
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))

# MSE and RMSE for model_price_4
moderndive::get_regression_points(model_price_4) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))


# Set random number generator seed value for reproducibility
set.seed(76)

# Randomly reorder the rows
house_prices_shuffled <- house_prices_transform %>% 
    sample_frac(size = 1, replace = FALSE)

# Train/test split
train <- house_prices_shuffled %>% 
    slice(1:10000)
test <- house_prices_shuffled %>% 
    slice(10001:nrow(.))

# Fit model to training set
train_model_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data=train)


# Compute RMSE (train)
moderndive::get_regression_points(train_model_2) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(rmse = sqrt(mean(sq_residuals)))

# Compute RMSE (test)
moderndive::get_regression_points(train_model_2, newdata = test) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(rmse = sqrt(mean(sq_residuals)))

```
  
  
  
***
  
###_Analyzing Survey Data in R_  
  
Chapter 1 - Introduction to Survey Data  

What are survey weights?  
  
* Survey weights sometimes appear inside a dataset, to reflect potential over/under sampling  
	* Survey weights result from a complex survey design - number of points in the population represented by each entry in the sampling frame  
    * For example, average income would be the sum-product of weights and incomes divided by the sum of weights  
  
Specifying elements of the design in R:  
  
* Simple random sampling is when every member of the population is known and had an equal chance of being selected  
	* library(survey)  
    * srs_design <- svydesign(data = paSample, weights = ~wts, fpc=~N, id=~1)  # the ~ means that these are column names  
* Stratified sampling is when a simple random sample is taken from each of the strata (sub-units)  
	* For example, taking 100 people from every county in a state, so that county-level averages can be gathered  
    * stratified_design <- svydesign(data = paSample, id = ~1, weights = ~wts, strata = ~county, fpc = ~N)  
* Cluster sampling is when the population are grouped in to clusters, with a simple random sample of clusters selected, and with simple random samples taken within each selected cluster  
	* cluster_design <- svydesign(data = paSample, id = ~county + personid, fpc = ~N1 + N2, weights = ~wts) 
  
Visualizing impact of survey weights:  
  
* NHANES data - assessment of health of persons in the US, derived by a health check in a mobile doctor's office  
	* Stage 0 - stratified by geography and proportion minority  
    * Stage 1 - within strata, counties randomly selected (selection likelihood proportional to population)  
    * Stage 2 - within counties, city blocks randomly selected (selection likelihood proportional to population)  
    * Stage 3 - within city blocks, households randomly selected (based on demographics)  
    * Stage 4 - within households, people randomly selected  
* NHANES data are availabl through a package in R  
	* library(NHANES)  
    * dim(NHANESraw)  
    * summarize(NHANESraw, N_hat = sum(WTMEC2YR))  # sums to double the US population, due to having 4 years of data when desiring only 2 years of data  
    * NHANESraw <- mutate(NHANESraw, WTMEC4YR = WTMEC2YR/2)  # fix the double population issue  
    * NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR)  # id is the cluster (first-level), nest=TRUE is due to id being nested within strata (???)  
    * distinct(NHANESraw, SDMVPSU)  # only takes 3 values, since only 1-3 counties are selected  
  
Example code includes:  
```{r}

colTypes <- "FINLWT21 numeric _ FINCBTAX integer _ BLS_URBN integer _ POPSIZE integer _ EDUC_REF character _ EDUCA2 character _ AGE_REF integer _ AGE2 character _ SEX_REF integer _ SEX2 integer _ REF_RACE integer _ RACE2 integer _ HISP_REF integer _ HISP2 integer _ FAM_TYPE integer _ MARITAL1 integer _ REGION integer _ SMSASTAT integer _ HIGH_EDU character _ EHOUSNGC numeric _ TOTEXPCQ numeric _ FOODCQ numeric _ TRANSCQ numeric _ HEALTHCQ numeric _ ENTERTCQ numeric _ EDUCACQ integer _ TOBACCCQ numeric _ STUDFINX character _ IRAX character _ CUTENURE integer _ FAM_SIZE integer _ VEHQ integer _ ROOMSQ character _ INC_HRS1 character _ INC_HRS2 character _ EARNCOMP integer _ NO_EARNR integer _ OCCUCOD1 character _ OCCUCOD2 character _ STATE character _ DIVISION integer _ TOTXEST integer _ CREDFINX character _ CREDITB integer _ CREDITX character _ BUILDING character _ ST_HOUS integer _ INT_PHON character _ INT_HOME character _ "

ce <- readr::read_csv("./RInputFiles/ce.csv")
glimpse(ce)
ceColTypes <- ""
for (x in names(ce)) { ceColTypes <- paste0(ceColTypes, x, " ", class(ce[, x, drop=TRUE]), " _ ") }
all.equal(colTypes, ceColTypes)

# Construct a histogram of the weights
ggplot(data = ce, mapping = aes(x = FINLWT21)) +
    geom_histogram()

# In the next few exercises we will practice specifying sampling designs using different samples from the api dataset, located in the survey package
# The api dataset contains the Academic Performance Index and demographic information for schools in California
# The apisrs dataset is a simple random sample of schools from the api dataset
# Notice that pw contains the survey weights and fpc contains the total number of schools in the population

data(api, package="survey")
library(survey)

# Look at the apisrs dataset
glimpse(apisrs)

# Specify a simple random sampling for apisrs
apisrs_design <- svydesign(data = apisrs, weights = ~pw, fpc = ~fpc, id = ~1)

# Print a summary of the design
summary(apisrs_design)


# Now let's practice specifying a stratified sampling design, using the dataset apistrat
# The schools are stratified based on the school type stype where E = Elementary, M = Middle, and H = High School
# For each school type, a simple random sample of schools was taken

# Glimpse the data
glimpse(apistrat)

# Summarize strata sample sizes
apistrat %>%
  count(stype)

# Specify the design
strat_design <- svydesign(data = apistrat, weights = ~pw, fpc = ~fpc, id = ~1, strata = ~stype)

# Look at the summary information for the stratified design
summary(strat_design)


# Now let's practice specifying a cluster sampling design, using the dataset apiclus2
# The schools were clustered based on school districts, dnum
# Within a sampled school district, 5 schools were randomly selected for the sample
# The schools are denoted by snum
# The number of districts is given by fpc1 and the number of schools in the sampled districts is given by fpc2

# Glimpse the data
glimpse(apiclus2)

# Specify the design
apiclus_design <- svydesign(id = ~dnum + snum, data = apiclus2, weights = ~pw, fpc = ~fpc1 + fpc2)

#Look at the summary information stored for both designs
summary(apiclus_design)


# Construct histogram of pw
ggplot(data = apisrs, mapping = aes(x = pw)) + 
    geom_histogram()

# Construct histogram of pw
ggplot(data = apistrat, mapping = aes(x = pw)) + 
    geom_histogram()

# Construct histogram of pw
ggplot(data = apiclus2, mapping = aes(x = pw)) + 
    geom_histogram()



NHANESraw <- read.csv("./RInputFiles/NHANESraw.txt")
NHANESraw <- NHANESraw %>%
    mutate(WTMEC4YR=WTMEC2YR / 2)
names(NHANESraw)[1] <- "SurveyYr"
glimpse(NHANESraw)

#Create table of average survey weights by race
tab_weights <- NHANESraw %>%
  group_by(Race1) %>%
  summarize(avg_wt = mean(WTMEC4YR))

#Print the table
tab_weights


# The two important design variables in NHANESraw are SDMVSTRA, which contains the strata assignment for each unit, and SDMVPSU, which contains the cluster id within a given stratum
# Specify the NHANES design
NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, 
                           nest = TRUE, weights = ~WTMEC4YR
                           )

# Print summary of design
summary(NHANES_design)

# Number of clusters
NHANESraw %>%
  summarize(n_clusters = n_distinct(SDMVSTRA, SDMVPSU))

# Sample sizes in clusters
NHANESraw %>%
  count(SDMVSTRA, SDMVPSU) 

```
  
  
  
***
  
Chapter 2 - Exploring categorical data  
  
Visualizing categorical variables:  
  
* Can estimate distributions of race, including both the weighted and unweighted distributions  
	* tab_unw <- NHANESraw %>% group_by(Race1) %>% summarize(Freq = n()) %>% mutate(Prop = Freq/sum(Freq)) %>% arrange(desc(Prop))  
    * ggplot(data = tab_unw, mapping = aes(x = Race1, y = Prop)) + geom_col() + coord_flip() + scale_x_discrete(limits = tab_unw$Race1) # Labels layer omitted  
* Can convert back to the weighted frequencies  
	* tab_w <- svytable(~Race1, design = NHANES_design) %>% as.data.frame() %>% mutate(Prop = Freq/sum(Freq)) %>% arrange(desc(Prop))  
    * ggplot(data = tab_w, mapping = aes(x = Race1, y = Prop)) + geom_col() + coord_flip() + scale_x_discrete(limits = tab_w$Race1) # Labels layer omitted  
  
Exploring two categorical variables:  
  
* Can look at diabetes withing the NHANES data, using the syvtable() function  
	* svytable(~Diabetes, design = NHANES_design)  
    * tab_w <- svytable(~Race1 + Diabetes, design = NHANES_design)  # Race and Diabetes  
    * tab_w <- as.data.frame(tab_w)  # converts contingency table to frame  
    * ggplot(data = tab_w, mapping = aes(x = Race1, fill = Diabetes, y = Freq)) + geom_col() + coord_flip()  
    * ggplot(data = tab_w, mapping = aes(x = Race1, y = Freq, fill = Diabetes)) + geom_col(position = "fill") + coord_flip()  # stacked bars to 100%  
  
Inference for categorical variables:  
  
* Formal statistical tests for associations among categorical variables using chi-squared tests for association  
	* svychisq(~Race1 + Diabetes, design = NHANES_design, statistic = "Chisq")  
  
Example code includes:  
```{r}

# Specify the survey design
NHANESraw <- mutate(NHANESraw, WTMEC4YR = .5 * WTMEC2YR)
NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR)

# Determine the levels of Depressed
levels(NHANESraw$Depressed)

# Construct a frequency table of Depressed
tab_w <- svytable(~Depressed, design = NHANES_design)

# Determine class of tab_w
class(tab_w)

# Display tab_w
tab_w


# Add proportions to table
tab_w <- tab_w %>%
  as.data.frame() %>%
  mutate(Prop = Freq/sum(Freq))

# Create a barplot
ggplot(data = tab_w, mapping = aes(x = Depressed, y = Prop)) + 
  geom_col()


# Construct and print a frequency table
tab_D <- svytable(~Depressed, design = NHANES_design)
tab_D

# Construct and print a frequency table
tab_H <- svytable(~HealthGen, design = NHANES_design)
tab_H

# Construct and print a frequency table
tab_DH <- svytable(~Depressed + HealthGen, design = NHANES_design)
tab_DH


# Add conditional proportions to tab_DH
tab_DH_cond <- tab_DH %>%
    as.data.frame() %>%
    group_by(HealthGen) %>%
    mutate(n_HealthGen = sum(Freq), Prop_Depressed = Freq/n_HealthGen) %>%
    ungroup()

# Print tab_DH_cond
tab_DH_cond

# Create a segmented bar graph of the conditional proportions in tab_DH_cond
ggplot(data = tab_DH_cond, mapping = aes(x = HealthGen, y = Prop_Depressed, fill = Depressed)) + 
  geom_col() + 
  coord_flip() 


# We can also estimate counts with svytotal(). The syntax is given by:
# svytotal(x = ~interaction(Var1, Var2), design = design, na.rm = TRUE)
# For each combination of the two variables, we get an estimate of the total and the standard error


# Estimate the totals for combos of Depressed and HealthGen
tab_totals <- svytotal(x = ~interaction(Depressed, HealthGen), design = NHANES_design, na.rm = TRUE)

# Print table of totals
tab_totals

# Estimate the means for combos of Depressed and HealthGen
tab_means <- svymean(x = ~interaction(Depressed, HealthGen), design = NHANES_design, na.rm = TRUE)

# Print table of means
tab_means


# Run a chi square test between Depressed and HealthGen
svychisq(~Depressed + HealthGen, design = NHANES_design, statistic = "Chisq")

# Construct a contingency table
tab <- svytable(~Education + HomeOwn, design=NHANES_design)

# Add conditional proportion of levels of HomeOwn for each educational level
tab_df <- as.data.frame(tab) %>%
  group_by(Education) %>%
  mutate(n_Education = sum(Freq), Prop_HomeOwn = Freq/n_Education) %>%
  ungroup()

# Create a segmented bar graph
ggplot(data = tab_df, mapping = aes(x=Education, y=Prop_HomeOwn, fill=HomeOwn)) + 
  geom_col() + 
  coord_flip()

# Run a chi square test
svychisq(~Education + HomeOwn, 
    design = NHANES_design, 
    statistic = "Chisq")

```
  
  
  
***
  
Chapter 3 - Exploring quantitative data  
  
Summarizing quantitative data:  
  
* Can look at the physician health bad variable and summarize  
	* NHANESraw %>% filter(Age >= 12) %>% select(DaysPhysHlthBad)  # just the data  
    * svymean(x = ~DaysPhysHlthBad, design = NHANES_design, na.rm = TRUE)  # means of number of days feeling in bad heatlh  
    * svyquantile(x = ~DaysPhysHlthBad, design = NHANES_design, na.rm = TRUE, quantiles = 0.5)  # get the median (quantile 0.5) of the data  
* Can grab summaries by group using svyby with a function FUN provided  
	* svyby(formula = ~DaysPhysHlthBad, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, row.names = FALSE)  
    * svyby(formula = ~Age, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE)  
  
Visualizing quantitative data:  
  
* Can create bar graphs of the means  
	* out <- svyby(formula = ~DaysPhysHlthBad, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE)  
    * ggplot(data = out, mapping = aes(x = SmokeNow, y = DaysPhysHlthBad)) + geom_col() + labs(y = "Monthly Average Number\n of Bad Health Days", x = "Smoker?")  
    * out <- mutate(out, lower = DaysPhysHlthBad - se, upper = DaysPhysHlthBad + se)  
* Create histograms of the data  
	* ggplot(data = out, mapping = aes(x = SmokeNow, y = DaysPhysHlthBad, ymin = lower, ymax = upper)) + geom_col(fill = "lightblue") + geom_errorbar(width = .5) + labs(y = "Monthly Average Number\n of Bad Health Days", x = "Smoker?")  
    * ggplot(data = NHANESraw, mapping = aes(x = DaysPhysHlthBad, weight = WTMEC4YR)) + geom_histogram(binwidth = 1, color = "white") + labs(x = "Number of Bad Health Days in a Month")  
* Create density plots of the data  
	* NHANESraw %>% filter(!is.na(DaysPhysHlthBad)) %>% mutate(WTMEC4YR_std = WTMEC4YR/sum(WTMEC4YR)) %>%  
    *     ggplot(mapping = aes(x = DaysPhysHlthBad, weight = WTMEC4YR_std)) + geom_density(bw = .6, fill = "lightblue") + labs(x = "Number of Bad Health Days in a Month")  
  
Inference for quantitative data:  
  
* May want to compare means across two groups in the data using a weighted 2-sample t-test  
	* The test statistic is the difference in means divided by the SE (standard error)  
    * svyttest(formula = DaysPhysHlthBad ~ SmokeNow, design = NHANES_design)  
  
Example code includes:  
```{r}

# Compute the survey-weighted mean
svymean(x = ~SleepHrsNight, design = NHANES_design, na.rm = TRUE)

# Compute the survey-weighted mean by Gender
svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, 
      FUN = svymean, na.rm = TRUE, keep.names = FALSE
      )

# Compute the survey-weighted quantiles
svyquantile(x = ~SleepHrsNight, design = NHANES_design, na.rm = TRUE, 
            quantiles = c(0.01, 0.25, 0.5, 0.75, .99)
            )

# Compute the survey-weighted quantiles by Gender
svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, FUN = svyquantile, 
      na.rm = TRUE, quantiles = c(0.5), keep.rows = FALSE, keep.var = FALSE
      )

# Compute the survey-weighted mean by Gender
out <- svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, 
             FUN = svymean, na.rm = TRUE, keep.names = FALSE
             )
             
# Construct a bar plot of average sleep by gender
ggplot(data = out, mapping = aes(x=as.factor(Gender), y=SleepHrsNight)) + 
    geom_col() + 
    labs(y="Average Nightly Sleep")

# Add lower and upper columns to out
out_col <- mutate(out, lower = SleepHrsNight - 2*se, upper = SleepHrsNight + 2*se)

# Construct a bar plot of average sleep by gender with error bars
ggplot(data = out_col, mapping = aes(x = Gender, y = SleepHrsNight, ymin = lower, ymax = upper)) + 
    geom_col(fill = "gold") + 
    labs(y = "Average Nightly Sleep") + 
    geom_errorbar(width = 0.7)  


# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 1, color = "white") + 
    labs(x = "Hours of Sleep")

# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 0.5, color = "white") + 
    labs(x = "Hours of Sleep")

# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 2, color = "white") + 
    labs(x = "Hours of Sleep")


# Density plot of sleep faceted by gender
NHANESraw %>% 
    filter(!is.na(SleepHrsNight), !is.na(Gender)) %>%
    group_by(Gender) %>%
    mutate(WTMEC4YR_std = WTMEC4YR/sum(WTMEC4YR)) %>%
    ggplot(mapping = aes(x = SleepHrsNight, weight = WTMEC4YR_std)) + 
        geom_density(bw = 0.6,  fill = "gold") +
        labs(x = "Hours of Sleep") + 
        facet_wrap(~Gender, labeller = "label_both")


# Run a survey-weighted t-test
svyttest(formula = SleepHrsNight ~ Gender, design = NHANES_design)

# Find means of total cholesterol by whether or not active 
out <- svyby(formula = ~TotChol, by = ~PhysActive, design = NHANES_design, 
             FUN = svymean, na.rm = TRUE, keep.names = FALSE
             )

# Construct a bar plot of means of total cholesterol by whether or not active 
ggplot(data = out, mapping = aes(x=PhysActive, y=TotChol)) + 
    geom_col()

# Run t test for difference in means of total cholesterol by whether or not active
svyttest(formula = TotChol ~ PhysActive, design = NHANES_design)

```
  
  
  
***
  
Chapter 4 - Modeling quantitative data  
  
Visualization with scatter plots:  
  
* Can look at head circumference compared to age (only captured for babies) using a scatterplot  
	* babies <- filter(NHANESraw, AgeMonths <= 6) %>% select(AgeMonths, HeadCirc)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc)) + geom_point()  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc)) + geom_jitter(width = 0.3, height = 0)  # width jitter but no height jitter  
* Can use weighting to extrapolate the scatter plot to the entire population  
	* ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, size = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(size = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, size = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0, alpha = 0.3) + guides(size = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, color = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(color = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE)  
  
Visualizing trends:  
  
* Survey-weighted lines of best fit can be added using the geom_smooth() in ggplot2, with the weight= provided as an aestehtic to the geom_smooth()  
	* ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE) + geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR))  
* Can also graph the best fit trendlines split by a categorical variable  
	* babies <- filter(NHANESraw, AgeMonths <= 6) %>% select(AgeMonths, HeadCirc, WTMEC4YR, Gender)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR, color = Gender)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE) + geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR))  
  
Modeling survey data:  
  
* Can use the regression equations directly to predict values for a new data point  
	* mod <- svyglm(HeadCirc ~ AgeMonths, design = NHANES_design)  
    * summary(mod)  
* The standard errors are an assessment of the likely errors between the estimated regression line and the true regression line  
  
More complex modeling:  
  
* Can extend the simple regression to a multiple regression in a parallel slopes model  
	* mod <- svyglm(HeadCirc ~ AgeMonths + Gender, design = NHANES_design)  
* Can also extend the simple regression to a multiple regression with different slopes  
  
Wrap up:  
  
* Packages survey, dplyr, and ggplot2  
* Survey fundamentals - clusters, strata, weights, svydesign(), etc.  
* Categorical data, svytable(), svychisq()  
* Quantiative data, svymean(), svytotal(), svyby(), svyquantile(), svyttest()  
* Modeling trends, svyglm()  
  
Example code includes:  
```{r}

# Create dataset with only 20 year olds
NHANES20 <- filter(NHANESraw, Age == 20)

# Construct scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight)) + 
    geom_point(alpha = 0.3) + 
    guides(size = FALSE)

# Construct bubble plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, size=WTMEC4YR)) + 
    geom_point(alpha = 0.3) + 
    guides(size = FALSE)

# Construct a scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, color=WTMEC4YR)) + 
    geom_point() + 
    guides(color = FALSE)

# Construct a scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, alpha=WTMEC4YR)) + 
    geom_point() + 
    guides(alpha = FALSE)


# Add gender to plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, size=WTMEC4YR, color=Gender)) + 
    geom_point(alpha=0.3) + 
    guides(size = FALSE)

# Add gender to plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, alpha=WTMEC4YR, color=Gender)) + 
    geom_point() + 
    guides(alpha = FALSE)


# Bubble plot with linear of best fit
ggplot(data = NHANESraw, mapping = aes(x = Height, y = Weight, size=WTMEC4YR)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight=WTMEC4YR))

# Add quadratic curve and cubic curve
ggplot(data = NHANESraw, mapping = aes(x = Height, y = Weight, size = WTMEC4YR)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR)) +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), formula = y ~ poly(x, 2), color = "orange") +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), formula = y ~ poly(x, 3), color = "red")


# Add survey-weighted trend lines to bubble plot
ggplot(data = NHANES20, mapping = aes(x = Height, y = Weight, size = WTMEC4YR, color = Gender)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2)

# Add non-survey-weighted trend lines
ggplot(data = NHANES20, mapping = aes(x = Height, y = Weight, size = WTMEC4YR, color = Gender)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight=WTMEC4YR))


# Subset survey design object to only include 20 year olds
NHANES20_design <- subset(NHANES_design, Age == 20)

# Build a linear regression model
mod <- svyglm(Weight ~ Height, design = NHANES20_design)

# Print summary of the model
summary(mod)


# Build a linear regression model same slope
mod1 <- svyglm(Weight ~ Height + Gender, design = NHANES20_design)

# Print summary of the same slope model
summary(mod1)

# Build a linear regression model different slopes
mod2 <- svyglm(Weight ~ Height*Gender, design = NHANES20_design)

# Print summary of the different slopes model
summary(mod2)


# Plot BPDiaAve and BPSysAve by Diabetes and include trend lines
drop_na(NHANESraw, Diabetes) %>% 
    ggplot(mapping = aes(x=BPDiaAve, y=BPSysAve, size=WTMEC4YR, color=Diabetes)) + 
    geom_point(alpha = 0.2) +  
    guides(size = FALSE) + 
    geom_smooth(method="lm", se = FALSE, mapping = aes(weight=WTMEC4YR))

# Build simple linear regression model
mod1 <- svyglm(BPSysAve ~ BPDiaAve, design = NHANES_design)

# Build model with different slopes
mod2 <- svyglm(BPSysAve ~ BPDiaAve*Diabetes, design = NHANES_design)

# Summarize models
summary(mod1)
summary(mod2)

```
  
  
  
***
  
###_Inference for Catgeorical Data_  
  
Chapter 1 - Inference for a Single Parameter  
  
General Social Survey:  
  
* Categorical data are where the data are categories rather than numbers, which is prevalent in the General Social Survey (GSS)  
	* Several thousand people are surveyed, with a goal of drawing inferences about the population from the sample  
    * Can grab the "gss" dataframe from the tidyverse package  
* Can generate an approximate error by using mean +/- 2*SE  
* The bootstrap can be a valuable way to assess the standard errors - calculate the sample statistic within each replicate, and calculate its distribution  
	* library(infer)  
    * boot <- gss2016 %>% specify(response=happy, success="HAPPY") %>% generate(reps=500, type="bootstrap") %>% calculate(stat="prop")  
  
CI interpretations:  
  
* In classicial statistical inference, there is assumed to be a fix but unknown population parameter that is being estimated by way of sampling  
* A 95% CI means that 95% of the intervals formed from random samples would include the true population parameter  
  
Approximation shortcut:  
  
* Standard errors tend to increase when the sample size is small or the probability is close to 50%  
* The normal distribution (bell curve) can be a useful approximation for a large sample size - the normal becomes the sampling distribution  
	* SE = sqrt( p * (1-p) / n )  
    * n * p and n * (1-p) should both be greater than or equal to 10  
  
Example code includes:  
```{r}

load("./RInputFiles/gss.RData")
glimpse(gss)


# Subset data from 2016
gss2016 <- gss %>%
  filter(year == 2016)

gss2016 %>% count(consci)
gss2016 <- gss2016 %>%
    mutate(old_consci=consci, 
           consci=fct_other(fct_recode(old_consci, "High"="A GREAT DEAL"), keep="High", other_level="Low")
           )
gss2016 %>% count(consci)

# Plot distribution of consci
ggplot(gss2016, aes(x = consci)) +
  geom_bar()

# Compute proportion of high conf
p_hat <- gss2016 %>%
  summarize(p = mean(consci == "High", na.rm = TRUE)) %>%
  pull()


# Load the infer package
library(infer)

# Create single bootstrap data set
b1 <- gss2016 %>%
    specify(response = consci, success = "High") %>%
    generate(reps = 1, type = "bootstrap")

# Plot distribution of consci
ggplot(b1, aes(x = consci)) +
  geom_bar()

# Compute proportion with high conf
b1 %>%
  summarize(p = mean(consci == "High")) %>%
  pull()


# Create bootstrap distribution for proportion that favor
boot_dist <- gss2016 %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500) %>%
  calculate(stat = "prop", success = "High", na.rm = TRUE)

# Plot distribution
ggplot(boot_dist, aes(x=stat)) +
  geom_density()

# Compute estimate of SE
SE <- boot_dist %>%
  summarize(se = sd(stat)) %>%
  pull()

# Create CI
c(p_hat - 2*SE, p_hat + 2*SE)


# Two new smaller data sets have been created for you from gss2016: gss2016_small, which contains 50 observations, and gss2016_smaller which contains just 10 observations

id50 <- c(6, 98, 2673, 1435, 1535, 525, 2784, 1765, 163, 1859, 2497, 1780, 184, 575, 2781, 2310, 1677, 2478, 1226, 2350, 1139, 1635, 1350, 1809, 1842, 1501, 1502, 2610, 2456, 49, 56, 2167, 2401, 2002, 2343, 2012, 860, 2557, 1147, 1119, 2449, 695, 1511, 666, 1595, 1094, 2643, 769, 1263, 2426)
id10 <- c(1609, 1342, 2066, 2710, 1809, 503, 1889, 486, 1469, 6)

gss2016_small <- gss2016 %>%
    filter(id %in% id50)
gss2016_smaller <- gss2016 %>%
    filter(id %in% id10)

# Create bootstrap distribution for proportion
boot_dist_small <- gss2016_small %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "prop")

# Compute estimate of SE
SE_small_n <- boot_dist_small %>%
  summarize(se = sd(stat)) %>%
  pull()

# Create bootstrap distribution for proportion
boot_dist_smaller <- gss2016_smaller %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "prop")

# Compute estimate of SE
SE_smaller_n <- boot_dist_smaller %>%
  summarize(se = sd(stat)) %>%
  pull()

c(SE_small_n, SE_smaller_n)


# Create bootstrap distribution for proportion that have hardy any
boot_dist <- gss2016 %>%
  specify(response=consci,  success = "Low") %>%
  generate(reps=500, type="bootstrap") %>%
  calculate(stat = "prop", na.rm = TRUE)

# Compute estimate of SE
SE_low_p <- boot_dist %>%
    summarize(se = sd(stat)) %>%
    pull()


# Compute p-hat and n
p_hat <- gss2016_small %>% 
    summarize(p = mean(consci == "High", na.rm=TRUE)) %>%
    pull()
n <- nrow(gss2016_small)

# Check conditions
p_hat * n >= 10
(1 - p_hat) * n >= 10

# Calculate SE
SE_approx <- sqrt(p_hat * (1 - p_hat) / n)

# Form 95% CI
c(p_hat - 2 * SE_approx, p_hat + 2 * SE_approx)

```
  
  
  
***
  
Chapter 2 - Proportions (Testing and Power)  
  
Hypothesis test for a proportion:  
  
* The hypothesis test for a proportion looks at what sort of p-hat would be observed if p held a specific value  
	* The hypothesize() function prior to generate() sets out the hypothesis in question  
* Suppose that analysis is being run on whether people favor capital punishment  
	* null <- gss2016 %>% specify(response=cappun, success="FAVOR") %>% hypothesize(null="point", p=0.5) %>% generate(reps=500, type="simulate") %>% calculate(stat="prop")  
    * null %>% summarize(mean(stat > p_hat)) %>% pull() * 2  # The times 2 is for a two-sided test  
  
Intervals for differences:  
  
* Can also look at differences in proportions, for example men vs. women belief in afterlife  
* Can generate null data by rewording the null hypothesis to "there is no association between belief in the afterlife and gender" - enables test by permutation  
	* gss2016 %>% specify(response=postlife, explanatory=sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=1, type="permute")  
    * gss2016 %>% specify(postlife ~ sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=1, type="permute")  # can use formula notation; same command as above, but simplified  
    * null <- gss2016 %>% specify(postlife ~ sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=500, type="permute")  %>% calculate(stat="diff in props", order=c("FEMALE", "MALE")) # Full command  
    * null %>% summarize(mean(stat > d_hat)) %>% pull() * 2  
  
Statistical errors:  
  
* Type I errors - probability of rejecting a true null hypothesis - will happen with probability alpha  
* Type II errors - probability of not rejecting a false null hypothesis  - will happen with probability beta, meaning the test has power 1-beta  
  
Example code includes:  
```{r}

# Construct plot
ggplot(gss2016, aes(x = postlife)) + 
    geom_bar()

# Compute and save proportion that believe
p_hat <- gss2016 %>%
    summarize(mean(postlife == "YES", na.rm = TRUE)) %>%
    pull()

# Generate one data set under H0
sim1 <- gss2016 %>%
    specify(response = postlife, success = "YES") %>%
    hypothesize(null = "point", p = 0.75) %>%
    generate(reps = 1, type = "simulate")

# Construct plot
ggplot(sim1, aes(x=postlife)) +
    geom_bar()

# Compute proportion that believe
sim1 %>%
    summarize(mean(postlife == "YES")) %>%
    pull()


# Generate null distribution
null <- gss2016 %>%
    specify(response = postlife, success = "YES") %>%
    hypothesize(null = "point", p = .75) %>%
    generate(reps = 100, type = "simulate") %>%
    calculate(stat = "prop")

# Visualize null distribution
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = p_hat, color = "red")

# Compute the two-tailed p-value
null %>%
    summarize(mean(stat > p_hat)) %>%
    pull() * 2


# Plot distribution
ggplot(gss2016, aes(x = sex, fill = cappun)) +
    geom_bar(position = "fill")
  
# Compute two proportions
p_hats <- gss2016 %>%
    group_by(sex) %>%
    summarize(mean(cappun == "FAVOR", na.rm = TRUE)) %>%
    pull()

# Compute difference in proportions
d_hat <- diff(p_hats)


# Create null distribution
null <- gss2016 %>%
    specify(cappun ~ sex, success = "FAVOR") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))
  
# Visualize null
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, col = "red")
  
# Compute two-tailed p-value
null %>%
    summarize(mean(stat < d_hat)) %>%
    pull() * 2


# Create the bootstrap distribution
boot <- gss2016 %>%
    specify(cappun ~ sex, success="FAVOR") %>%
    generate(reps=500, type="bootstrap") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))
  
# Compute the standard error
SE <- boot %>%
    summarize(sd(stat)) %>%
    pull()
  
# Form the CI (lower, upper)
c( d_hat - 2*SE, d_hat + 2*SE )


gssmod <- gss2016 %>%
    mutate(coinflip=sample(c("heads", "tails"), size=nrow(.), replace=TRUE))
table(gssmod$coinflip)

# Find difference in props
p_hats <- gssmod %>%
    group_by(coinflip) %>%
    summarize(mean(cappun == "FAVOR", na.rm = TRUE)) %>%
    pull()

# Compute difference in proportions
d_hat <- diff(p_hats)

# Form null distribution
null <- gssmod %>%
    specify(cappun ~ coinflip, success = "FAVOR") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "diff in props", order = c("heads", "tails"))

ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red")


# Set alpha
alpha <- 0.05

# Find cutoffs
upper <- null %>%
    summarize(quantile(stat, probs = c(1-alpha/2))) %>%
    pull()
lower <- null %>%
    summarize(quantile(stat, probs = alpha/2)) %>%
    pull()
  
# Visualize cutoffs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red") +
    geom_vline(xintercept = lower, color = "blue") +
    geom_vline(xintercept = upper, color = "blue")

# check if inside cutoffs
d_hat %>%
    between(lower, upper)

```
  
  
  
***
  
Chapter 3 - Comparing Many Parameters (Independence)  
  
Contingency tables:  
  
* Can look at bivariate relationships, such as political party affiliation vs. opinions on military spending  
	* The broom package can help in movements to/from contingency tables, by keeping things cleaner  
    * tab <- gss2016 %>% select(natarms, party) %>% table()  
    * tab %>% broom::tidy() %>% uncount(Freq)  
  
Chi-squared test statistic:  
  
* Can use Chi-squared to look at dependence of variables  
* Can create a contingency table O of the observations and a contingency table E of the expected distribution if there is pure independence  
	* Can then look at (O-E)**2 / E, and sum up to get the overall Chi-squared distribution  
    * Hypothesis tests can then assess how extreme a given Chi-squared may be  
  
Alternative method - chi-squared test statistic:  
  
* The Chi-squared statistic is derived from the Chi-squared distribution, which is specified solely by the number of degrees of freedom  
	* The degrees of freedom are (nRows - 1) * (nCols - 1)  
    * pchisq(chi_obs_spac, df=4)  # gives the likelihood of actual being less than, can use 1-pchisq() for the amount that is greater (the p-value of interest  
* Generally, need to have 5+ counts per cell, and to only use chi-squared for df=2+ (for df=1, can just compare proportions using the normal distribution)  
  
Intervals for chi-squared:  
  
* Can remove the hypothesize() call and use bootstrap() instead, but there is no real meaning to a Chi-squared in the absence of a null hypothesis  
* It is very unlikely that you would ever see a confidence interval attached to a Chi-squared interval  
  
Example code includes:  
```{r}

# Exclude "other" party
gss_party <- gss2016 %>%
    mutate(party=fct_collapse(partyid, 
                              "D"=c("STRONG DEMOCRAT", "NOT STR DEMOCRAT"), 
                              "R"=c("NOT STR REPUBLICAN", "STRONG REPUBLICAN"),
                              "I"=c("IND,NEAR DEM", "INDEPENDENT", "IND,NEAR REP"),
                              "O"="OTHER PARTY"
                              )
           ) %>%
    filter(!is.na(party), party != "O") %>%
    droplevels()

# Bar plot of proportions
gss_party %>%
    ggplot(aes(x = party, fill = natspac)) +
    geom_bar(position = "fill")
  
# Bar plot of counts
gss_party %>%
    ggplot(aes(x=party, fill = natspac)) +
    geom_bar()


# Create table of natspac and party
O <- gss_party %>%
    select(natspac, party) %>%
    table()

# Convert table back to tidy df
O %>%
    broom::tidy() %>%
    uncount(n)


# Create one permuted data set
perm_1 <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1, type = "permute")
  
# Visualize permuted data
ggplot(perm_1, aes(x = party, fill = natarms)) +
    geom_bar()

# Make contingency table
tab <- perm_1 %>%
    ungroup() %>%
    select(natarms, party) %>%
    table()
  
# Compute chi-squared stat
(chi_obs_arms <- chisq.test(tab)$statistic)

(chi_obs_spac <- chisq.test(gss_party$natspac, gss_party$party)$statistic)

# Create null
null <- gss_party %>%
    specify(natspac ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 100, type = "permute") %>%
    calculate(stat = "Chisq")

# Visualize H_0 and obs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_spac, color = "red")

# Create null
null <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 100, type = "permute") %>%
    calculate(stat = "Chisq")

# Visualize H_0 and obs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_arms, color = "red")


# create bar plot
gss2016 %>%
    ggplot(aes(x = region, fill = happy)) +
    geom_bar(position = "fill") +
    coord_flip()

# create table
tab <- gss2016 %>%
    select(happy, region) %>%
    table()
  
# compute observed statistic
(chi_obs_stat <- chisq.test(tab)$statistic)


# generate null distribution
null <- gss2016 %>%
    mutate(happy=fct_other(happy, keep=c("VERY HAPPY"))) %>%
    specify(happy ~ region, success = "VERY HAPPY") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "Chisq")

# plot null(s)
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_stat) +
    stat_function(fun = dchisq, args = list(df = (9-1)*(2-1)), color = "blue")

# permutation p-value
null %>% 
    summarize(mean(stat > chi_obs_stat)) %>% 
    pull()

# approximation p-value
1 - pchisq(chi_obs_stat, df = (9-1)*(2-1))

```
  
  
  
***
  
Chapter 4 - Comparing Many Parameters (Goodness of Fit)  
  
Case Study: Election Fraud:  
  
* Election fraud has many meanings; this course will focus on altering vote totals  
* Benford's Law applies when looking at broad collections of data, and considering only the first digit  
	* The law proposed that 30.1% of the first digits should be 1, with decreases as the numbers increase  
    * The basic idea is that the 1's always happen first (get to the 100s before any other x00s)  
* Can look at the 2009 Iranian election, and assess in comparison to Benford's Law  
  
Goodness of Fit:  
  
* Desire to assess whether the voter data is well aligned with Benford's law - Chi-squared is a good statistic for this  
	* chisq.test(myTab, p=myProbNull)  
* Can simulate the null hypothesis, for example by using  
	* gss2016 %>% specify(response=party) %>% hypothesize(null="point", p=p_uniform) %>% generate(reps=1, type="simulate")  
  
Now to the US:  
  
* Comparison to the US election in Iowa in 2016  
* Can look at county-level data  
  
Wrap-Up:  
  
* Could have rejected the null hypothesis even when it is true - typically 5%  
* More fundamental errors could be at play, such as assuming the first digit should follow Benford's Law  
	* Population of world cities tend to fit Benford's Law criteria (uniform distribution, consistency of logs, etc.)  
* Techniques for carrying out inference on categorical data - confidence intervals, hypothesis tests, Chi-squared tests for independence, goodness of fit of distributions  
* All tests follow specify-hypohteize-generate-calculate  
  
Example code includes:  
```{r}

iran <- readr::read_csv("./RInputFiles/iran.csv")
glimpse(iran)


# Compute candidate totals
totals <- iran %>%
  summarize(ahmadinejad = sum(ahmadinejad),
            rezai = sum(rezai),
            karrubi = sum(karrubi),
            mousavi = sum(mousavi))

# Plot totals
totals %>%
  gather(key = "candidate", value = "votes") %>%
  ggplot(aes(x = candidate, y = votes)) +
  geom_bar(stat = "identity")
  
# Cities won by #2
iran %>%
  group_by(province) %>%
  summarize(ahmadinejad = sum(ahmadinejad),
            mousavi = sum(mousavi)) %>%
  mutate(mousavi_win = mousavi > ahmadinejad) %>%
  filter(mousavi_win)


# Print get_first
get_first <- function(x) {
    substr(as.character(x), 1, 1) %>%
      as.numeric() %>%
      as.factor()
}

# Create first_digit
iran2 <- iran %>%
  mutate(first_digit = get_first(total_votes_cast))
  
# Construct barchart
iran2 %>%
  ggplot(aes(x=first_digit)) +
  geom_bar()


# Tabulate the counts of each digit
tab <- iran2 %>%
  select(first_digit) %>%
  table()

# Compute observed stat
p_benford <- c(0.301029995663981, 0.176091259055681, 0.1249387366083, 0.0969100130080564, 0.0791812460476248, 0.0669467896306132, 0.0579919469776867, 0.0511525224473813, 0.0457574905606751)
names(p_benford) <- 1:9
p_benford[9] <- 1 - sum(p_benford[-9])
sum(p_benford)
chi_obs_stat <- chisq.test(tab, p = p_benford)$stat

# Form null distribution
null <- iran2 %>%
  specify(response=first_digit) %>%
  hypothesize(null = "point", p = p_benford) %>%
  generate(reps=500, type = "simulate") %>%
  calculate(stat = "Chisq")


# plot both nulls
ggplot(null, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = chi_obs_stat) + 
  stat_function(fun = dchisq, args = list(df = 9-1), color = "blue")

# permutation p-value
null %>%
  summarize(mean(stat > chi_obs_stat)) %>%
  pull()

# approximation p-value
pchisq(chi_obs_stat, df=9-1, lower.tail=FALSE)


iowa <- readr::read_csv("./RInputFiles/iowa.csv")
glimpse(iowa)

# Get R+D county totals
iowa2 <- iowa %>%
  filter(candidate == "Hillary Clinton / Tim Kaine" | candidate == "Donald Trump / Mike Pence") %>%
  group_by(county) %>%
  summarize(dem_rep_votes = sum(votes, na.rm = TRUE)) 

# Add first_digit
iowa3 <- iowa2 %>%
  mutate(first_digit = get_first(dem_rep_votes))

# Construct bar plot
iowa3 %>%
  ggplot(aes(x=first_digit)) + 
  geom_bar()


# Tabulate the counts of each digit
tab <- iowa3 %>%
  select(first_digit) %>%
  table()

# Compute observed stat
chi_obs_stat <- chisq.test(tab, p = p_benford)$stat

# Form null distribution
null <- iowa3 %>%
  specify(response = first_digit) %>%
  hypothesize(null = "point", p = p_benford) %>%
  generate(reps = 500, type = "simulate") %>%
  calculate(stat = "Chisq")
  
# Visualize null
ggplot(null, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = chi_obs_stat)

```
  
  
  
***
  
###_Building Dashboards with flexdashboard_  
  
Chapter 1 - Dashboard Layouts  
  
Introduction:  
  
* Dashboards are a collection of components in a single display - graphs, text, tables, widgets, etc.  
* The flexdashboard is an R package that allows for using R Markdown to create a dashboard  
	* Can include all the power of R  
    * Can combine with Shiny for reactive elements  
* Course will include capabilities of flexdashboard, decision as to whether to incorporate Shiny, and potential extensions  
  
Anatomy of flexdashboard:  
  
* Within R Markdown, the header controls the type of document created during knitting  
	* output: flexdashboard::flex_dashboard (will create the flesdashboard output)  
* The flexdashboard is made up of charts, with each chart denoted by ### ChartName  
	* The succeeding lines can then be R code, similar to other R Markdown processes  
* By default, all of the charts will stack in a single column, though multiple columns can also be declared  
	* Columns are created using 14+ dashes, with everything underneath contained in that column  
    * Can give a specific name and specify options for each of the columns  
* Can start in Rstudio using File - New File - R Markdown - From Template - flexdashboard  
* Course data will include bicycle sharing data from San Francisco  
  
Layout basics:  
  
* Columns can be of variable width by using data-width= such that they add up to 1000  
* Can create by rows rather than columns using orientation:rows underneath flexdashboard::flex_dashboard:  
	* Can use data-height to vary the row heights  
    * Can use certical_layout: scroll as an option to allow for scrolling rather than forcing everything on to one page (this is considered poor dashboard design, though)  
  
Advanced layouts:  
  
* Options for extending the dashboard include tabsets  
	* Column {.tabset} - will apply the tabset to every chart in that column  
* Can also extend by using pages, where columns and rows are children of their respective pages  
	* Sixteen (16) or more equal signs under a Page title specify a call to the page  
    * Can add Page xxx {data-navmenu=yyy} to specify that the page xxx should belong to the navmenu yyy  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 2 - Data Visualization for Dashboards  
  
Graphs:  
  
* The easiest way to add a graph is to include it as part of a code snippet in flexdashboard  
* Graphs will flex and resize to stay in the container  
* Sometimes, may want to set the figure width and height to match the aspect ratio of the target device as closely as possible  
	* {r, fig.width=10, fig.height=5}  
    * Downside #1 - trial and error needed  
    * Downside #2 - need to adjust every time charts are added  
    * Downside #3 - graphs are no longer responsive to user inputs  
  
Web-Friendly Visualizations:  
  
* Web-friendly packages include plotly, highcharter, dygraphs, rbokeh, ggvis  
* The plotly calls are helpful since they are closely linked to ggplot2  
	* library(plotly)  
    * ggplotly(my_ggplot)  # my_ggplot is a ggplot2 object  
  
htmlwidgets:  
  
* htmlwidgets are a framework that connects R with Javascript (web-friendly and well-suited to dashboards)  
	* Learn more at: http://htmlwidgets.org  
* The leaflet package allows for adding interactive maps  
	* library(leaflet)  
    * leaflet() %>% addTiles() %>% addMarkers(lng = data_df$longitude, lat = data_df$latitude)  
    * leaflet(data_df) %>% addTiles() %>% addMarkers()  # leaflet called on a data frame  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 3 - Dashboard Components  
  
Highlighting Single Values:  
  
* Gauges can be helpful for values in a defined range, such as 0%-100%  
	* gauge(value = pct_subscriber_trips, min = 0, max = 100)  # basics for creating a gauge include the value, the min, and the max  
    * gauge(value = pct_subscriber_trips, min = 0, max = 100, sectors = gaugeSectors( success = c(90, 100), warning = c(70, 89), danger = c(0, 69) ), symbol = '%')  # additional features  
* Value boxes can be helpful for values that do not fall in a pre-defined range  
	* valueBox(prettyNum(num_trips, big.mark = ","), caption = "Total Daily Trips", icon = "fa-bicycle")  # font-awesome bicycles  
* Both gauges and value boxes can be linked  
	* valueBox(prettyNum(num_trips, big.mark = ','), caption = 'Total Daily Trips', icon = 'fa-bicycle', href = '#trip-raw-data')  # href makes the caption linked and clickable  
  
Dashboard Tables:  
  
* The kable function from knitr is one of the easiest ways to create a table - but, not very well tuned to html  
	* library(knitr)  
    * kable(my_data_df)  
* The DT package is better suited to making responsive tables  
	* library(DT)  
    * datatable(my_data_df)  
    * datatable(my_data_df, rownames = FALSE)  # eliminate row numbering  
    * datatable(my_data_df, rownames = FALSE, options = list(pageLength = 15))  # most options are set by way of a list; note the contrast to rownames  
    * datatable( my_data_df, rownames = FALSE, extensions = 'Buttons', options = list( dom = 'Bfrtip', buttons = c('copy', 'csv', 'excel', 'pdf', 'print') ) )  # buttons for extract  
  
Text for Dashboards:  
  
* Captions (notes) are a common way to add text to a chart, and are added using a greater than sign with text; there must be an empty line between the end of the chunk and the caption  
* Another way to provide more context is with the storyboard format  
	* Presents one chart at a time in a specified order, where the user controls the navigation speed between the charts  
    * Good format for content that runs in order  
    * Requires storyboard: true in the yaml header in the flexdashboard::flex_dashboard items  
    * Within the story, the ### signal the next page of the story, and should have descriptive text  
    * Can add commentary using the triple asterisk (***) which needs to be AFTER the R chunk and separated by at least one space  
* Can also mix in storyboard on some pages but not on others (requires leaving this out of the yaml header)  
	* Add {.storyboard} to the end of the page description  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 4 - Adding Interactivity with Shiny  
  
Incorporating Shiny into Dashboards:  
  
* Incorporating Shiny is optional but can mak the dashboards even more interactive  
	* Shiny is interactive and lightweight, though at the expense of greater complication and hosting challenges  
* Even after incorporating Shiny, the flexdashboard document is still an interactive R Markdown document  
* runtime:shiny in the yaml header will make the flexdashboard in to a Shiny App  
  
Reactive Dataframe Pattern:  
  
* Creating a narrow sidebar using  
	* Column {data-width=200 .sidebar}  
* Widgets can be use inside of an R chunk, like any other dashboard component  
	* sliderInput("duration_slider", label="Select maximum trip duration to display (in minutes): ", min=0, max=120, value=15, step=5, dragRange=TRUE)  
    * show_trips_df <- reactive({ trips_df %>% filter(duration_sec <= input$duration_slider * 60) })  
    * To call the reactice data frame later, use show_trips_df()  
* Output from the reactive needs to be encloses in the appropriate render*() function  
	* renderLeaflet({ show_trips_df() %>% . %>% leaflet() %>% . })  # no need for the output call like there would be in a typical Shiny document  
* Five key steps for the reactive data frame pattern  
	* Create a sidebar column  
    * Add user inputs to the sidebar - *Input() Shiny widgets  
    * Make a data frame that uses the inputs, called later using ()  
    * Replace the dataframe in the dashboard component code with the reactive version  
    * Wrap with the appropriate rendering function render*()  
  
Customized Inputs for Charts:  
  
* Can have a reactive component impact everything, as per the example worked through in the previous section  
* May also want to have sliders that only impact a single object  
	* Putting these together in the same loaction can cause headaches due to the need to work with layouts  
* Example code to implement includes  
	* fillCol(height=600, flex=c(NA, 1), inputPanel(sliderInput("my_input", .)), plotOutput("my_plot", height="100%"))  # flex is the flexible height for the components  
    * output$my_plot <- renderPlot({ . })  
* Can use a global shortcut  
	* Can put all of the charts that are driven by the same slider on to the same page  
    * Can put the sidebar as a class with its own page, followed by all the other pages, to have the same sidebar drive all of the pages  
  
Wrap-up:  
  
* Additional resources available through Rstudio and htmlwidgets.org (information about all html widgets available in R)  
	* The highcharter can be helpful - high quality charts with some interactivity  
* Can use shinydashboard for just Shiny if R Markdown and flexdasboard are not needed  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
###_Network Analysis in R: Case Studies_  
  
Chapter 1 - Exploring Graphs Through Time  
  
Exploring Data Set:  
  
* Daily snapshots of items purchased together (co-purchases) from Amazon in 2003  
	* There is the to and from that will make up the graph, plus associated metadata  
    * Desire to look only at a single data, and only the from-to data (assuming a directional graph)  
* Can look at a smaller subset of the graph, for example  
	* sg <- induced_subgraph(amzn_g, 1:500)  
    * sg <- delete.vertices(sg, degree(sg) == 0)  
    * plot(sg, vertex.label = NA, edge.arrow.width = 0, edge.arrow.size = 0, margin = 0, vertex.size = 2)  
* Can count the number of diad (2-connect) and triad (3-connect) in the data  
	* null, asymmetric, and mutual are the potential results for diads  
    * Three-digit codes are used to reflect the 16 potential triad states - #Bi/#Assym/#Uncon - plus a letter D, U, and C  
  
Exploring Temporal Structure:  
  
* Dataset has 4 days worth of data - can build from the earliest date to the latest date  
* Can create a list to hold the igraphs at each time period, loop over the times, and then plot them  
	* A handful of vertices may be important and interesting across time  
  
Example code includes:  
```{r}

library(igraph)
amzn_g <- read.graph("./RInputFiles/amzn_g.gml", format=c("gml"))
amzn_g


# Perform dyad census
dc <- dyad_census(amzn_g)

# Perform triad census
tc <- triad_census(amzn_g)

# Find the edge density
ed <- edge_density(amzn_g)

# Output values
print(dc)
print(tc)
print(ed)


# Calculate transitivity
transitivity(amzn_g)

# Calculate reciprocity
amzn_rp <- reciprocity(amzn_g)

# Simulate our outputs
nv <- gorder(amzn_g)
ed <- edge_density(amzn_g)
rep_sim <- rep(NA, 1000)

# Simulate 
for(i in 1:1000){
  rep_sim[i] <- reciprocity(erdos.renyi.game(nv, ed, "gnp", directed = TRUE))
}

# Compare
quantile(rep_sim, c(0.25, .5, 0.975))
print(amzn_rp)


# Get the distribution of in and out degrees
table(degree(amzn_g, mode = "in"))
table(degree(amzn_g, mode = "out"))

# Find important products based on the ratio of out to in and look for extremes
imp_prod <- V(amzn_g)[degree(amzn_g, mode = "out") > 3 & degree(amzn_g, mode = "in") < 3]

## Output the vertices
print(imp_prod)


ipFrom <- c(1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 32129, 32129, 32129, 32129, 32129, 32129, 32129, 38131, 38131, 38131, 38131, 38131, 38131, 45282, 45282, 45282, 45282, 52831, 52831, 52831, 52831, 52831, 52831, 52831, 52831, 53591, 53591, 53591, 53591, 53591, 53591, 53591, 53591, 56427, 56427, 56427, 56427, 59706, 59706, 59706, 59706, 59706, 59706, 59706, 59706, 62482, 62482, 62482, 62482, 62482, 62482, 67038, 67038, 67038, 67038, 71192, 71192, 71192, 71192, 71192, 77957, 77957, 77957, 77957, 77957, 77957, 103733, 103733, 103733, 103733, 103733, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 144749, 144749, 144749, 144749, 144749, 144749, 144749, 170830, 170830, 170830, 170830, 170830, 170830, 177282, 177282, 177282, 177282, 177282, 177282, 177432, 177432, 177432, 177432, 177432, 177432, 177432, 184526, 184526, 184526, 184526, 184526, 191825, 191825, 191825, 191825, 191825, 215668, 215668, 215668, 221085, 221085, 221085, 221085, 221085, 231604, 231604, 231604, 231604, 231604, 231604, 239014, 239014, 239014, 239014, 239014, 242693, 242693, 242693, 242693, 242693, 257621, 257621, 257621, 257621, 261587, 261587, 261587, 261587, 261587, 261587, 261657, 261657, 261657, 261657, 261657, 261657)
ipTo <- c(190, 1366, 2679, 4023, 1625, 1627, 7529, 1272, 1628, 1630, 1631, 11124, 15360, 20175, 10626, 20970, 10776, 11164, 11166, 5955, 8719, 11164, 23842, 23843, 24115, 15312, 23329, 32127, 80473, 44848, 44849, 44850, 38133, 31084, 33711, 10920, 20178, 20179, 87093, 2134, 2136, 4119, 9995, 36524, 64698, 64700, 52833, 120083, 120085, 120086, 36689, 12340, 113789, 32094, 51015, 1898, 10076, 15800, 61488, 63836, 63837, 63838, 8882, 59708, 59711, 26982, 59708, 69497, 69498, 69499, 69500, 23349, 62480, 58926, 58928, 64118, 52271, 71190, 71380, 75384, 9762, 57876, 43543, 43546, 98488, 77951, 77953, 116842, 103732, 103734, 103735, 103728, 124733, 117842, 117843, 117845, 117842, 117843, 117845, 117842, 117842, 117843, 117845, 59267, 89503, 89506, 156, 190, 105428, 184973, 195785, 195787, 132753, 132754, 132755, 52563, 132755, 132756, 132759, 132762, 126757, 132754, 132755, 132756, 189269, 265886, 43155, 80519, 159667, 82479, 152760, 136747, 65216, 114684, 114686, 114687, 117132, 132667, 81755, 109198, 109199, 109202, 144124, 75023, 216449, 139527, 149146, 152038, 177428, 177430, 177428, 177430, 56930, 61658, 207112, 250755, 250756, 56930, 141148, 191036, 147084, 245110, 175959, 177376, 177377, 88463, 103641, 115111, 165118, 228427, 43553, 76706, 78278, 131353, 75725, 119146, 12615, 15740, 229533, 151325, 237568, 239545, 239546, 239547, 110872, 215593, 60310, 60312, 133398, 44502, 261582, 261590, 261599, 271593, 261584, 261588, 261649, 261653, 261654, 261658, 261662, 105814)
ipGroupFrom <- factor(c('DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD'), levels=c("DVD", "Video"))
ipSRFrom <- c(30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16, 16, 16, 16, 16, 16, 37, 37, 37, 37, 37, 37, 26, 26, 26, 26, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 10, 10, 10, 10, 1, 1, 1, 1, 1, 1, 1, 1, 19, 19, 19, 19, 19, 19, 10, 10, 10, 10, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 27, 27, 27, 27, 27, 27, 27, 10, 10, 10, 10, 10, 10, 6, 6, 6, 6, 6, 6, 19, 19, 19, 19, 19, 19, 19, 25, 25, 25, 25, 25, 3, 3, 3, 3, 3, 8, 8, 8, 27, 27, 27, 27, 27, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 26, 26, 26, 26, 26, 15, 15, 15, 15, 8, 8, 8, 8, 8, 8, 26, 26, 26, 26, 26, 26)
ipSRTo <- c(5, 2, 18, 20, 12, 6, 8, 14, 16, 4, 18, 20, 3, 6, 14, 5, 3, 3, 4, 3, 13, 3, 5, 9, 18, 17, 8, 2, 8, 9, 16, 9, 24, 11, 25, 6, 9, 3, 21, 1, 5, 2, 24, 2, 6, 6, 8, 18, 7, 4, 20, 6, 22, 13, 10, 19, 4, 22, 7, 7, 9, 7, 11, 21, 12, 17, 21, 5, 7, 2, 1, 26, 6, 14, 2, 17, 4, 13, 12, 6, 8, 13, 4, 7, 1, 7, 9, 15, 19, 6, 20, 0, 19, 14, 18, 11, 14, 18, 11, 14, 14, 18, 11, 16, 1, 5, 3, 5, 6, 22, 5, 20, 10, 29, 9, 22, 9, 12, 10, 9, 12, 29, 9, 12, 13, 6, 23, 6, 18, 10, 18, 6, 9, 11, 8, 8, 19, 12, 10, 9, 8, 14, 1, 7, 10, 13, 18, 6, 6, 4, 6, 4, 4, 22, 5, 8, 4, 4, 13, 11, 3, 4, 21, 22, 8, 18, 1, 6, 5, 5, 4, 8, 6, 12, 6, 3, 13, 8, 10, 1, 1, 22, 12, 18, 19, 5, 18, 31, 8, 13, 10, 14, 25, 4, 19, 17, 5, 21, 3, 1, 19, 10)
ipTRFrom <- c(290, 290, 290, 290, 290, 290, 290, 290, 290, 290, 290, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 73, 73, 73, 73, 73, 73, 73, 294, 294, 294, 294, 294, 294, 43, 43, 43, 43, 5, 5, 5, 5, 5, 5, 5, 5, 13, 13, 13, 13, 13, 13, 13, 13, 28, 28, 28, 28, 1, 1, 1, 1, 1, 1, 1, 1, 110, 110, 110, 110, 110, 110, 7, 7, 7, 7, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 25, 25, 25, 25, 25, 25, 25, 2, 2, 2, 2, 2, 2, 12, 12, 12, 12, 12, 12, 111, 111, 111, 111, 111, 111, 111, 294, 294, 294, 294, 294, 0, 0, 0, 0, 0, 0, 0, 0, 243, 243, 243, 243, 243, 43, 43, 43, 43, 43, 43, 15, 15, 15, 15, 15, 483, 483, 483, 483, 483, 1, 1, 1, 1, 12, 12, 12, 12, 12, 12, 2, 2, 2, 2, 2, 2)
ipTRTo <- c(19, 2, 22, 105, 22, 1, 6, 55, 40, 21, 47, 13, 0, 42, 14, 51, 2, 4, 0, 2, 41, 4, 0, 19, 21, 63, 5, 0, 2, 4, 63, 63, 7, 1, 8, 11, 134, 134, 12, 5, 10, 3, 58, 1, 6, 2, 27, 39, 2, 18, 87, 12, 218, 2, 30, 17, 0, 41, 13, 9, 3, 2, 13, 8, 10, 1, 8, 1, 0, 7, 1, 167, 63, 28, 0, 6, 1, 10, 4, 0, 2, 0, 5, 2, 3, 2, 2, 12, 24, 45, 21, 0, 8, 2, 21, 20, 2, 21, 20, 2, 2, 21, 20, 14, 6, 6, 3, 19, 13, 88, 4, 9, 6, 0, 19, 54, 19, 6, 9, 1, 2, 0, 19, 6, 3, 13, 46, 29, 6, 1, 15, 1, 4, 18, 28, 5, 15, 21, 10, 12, 3, 5, 4, 3, 8, 5, 0, 0, 5, 0, 5, 0, 1, 221, 1, 13, 3, 1, 7, 40, 5, 0, 8, 37, 67, 48, 0, 6, 1, 25, 1, 69, 0, 55, 3, 0, 5, 5, 2, 13, 0, 44, 53, 9, 4, 5, 13, 212, 3, 3, 1, 3, 8, 0, 3, 12, 11, 10, 5, 0, 49, 42)
ipTitleFrom <- c(16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 13, 13, 13, 13, 13, 13, 13, 30, 30, 30, 30, 30, 30, 11, 11, 11, 11, 26, 26, 26, 26, 26, 26, 26, 26, 5, 5, 5, 5, 5, 5, 5, 5, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 14, 14, 14, 14, 14, 12, 12, 12, 12, 12, 12, 21, 21, 21, 21, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 27, 27, 27, 27, 27, 27, 27, 27, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 29, 29, 29, 29, 29, 29, 29, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 15, 15, 15, 15, 15, 15, 15, 30, 30, 30, 30, 30, 7, 7, 7, 7, 7, 6, 6, 6, 9, 9, 9, 9, 9, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 24, 24, 24, 24, 24, 19, 19, 19, 19, 3, 3, 3, 3, 3, 3, 28, 28, 28, 28, 28, 28)
ipNames <- c('Attraction', 'Barbara The Fair With The Silken Hair', 'Cannibal Apocalypse', "DJ Qbert's Wave Twisters", 'David and Lisa', 'Def Comedy Jam  Vol. 13', 'Detroit Lions 2001 NFL Team Video', 'Donnie McClurkin: Live in London and More', 'El Hombre Sin Sombra (Hollow Man)', 'Gladiator', 'Kindergarten Cop', "Kingsley's Meadow - Wise Guy", "Lady & The Tramp II - Scamp's Adventure", 'Lojong - Transforming the Mind (Boxed Set)', 'Menace II Society', 'Merlin', 'Modern Times', 'Murder by Numbers (Full Screen Edition)', 'Nancy Drew: A Haunting We Will Go', 'Princess Nine - Triple Play (Vol. 3)', 'Secret Agent AKA Danger Man  Set 2', 'Seguire Tus Pasos', 'Selena Remembered', 'Seven (New Line Platinum Series)', 'Sheba  Baby', 'Slaughter', 'The Complete Guide to Medicine Ball Training', 'The Gambler', 'The Getaway', 'The Sum of All Fears')
ip_df <- data.frame(X=1:202, 
                    from=ipFrom, 
                    to=ipTo, 
                    salesrank.from=ipSRFrom, 
                    salesrank.to=ipSRTo, 
                    totalreviews.from=ipTRFrom, 
                    totalreviews.to=ipTRTo, 
                    group.from=ipGroupFrom, 
                    title.from=factor(ipNames[ipTitleFrom], levels=ipNames)
                    )


# Create a new graph
ip_g <- graph_from_data_frame(ip_df %>% select(from, to), directed = TRUE)

# Add color to the edges based on sales rank, blue is higer to lower, red is lower to higher
E(ip_g)$rank_flag <- ifelse(ip_df$salesrank.from <= ip_df$salesrank.to, "blue", "red")

# Plot and add a legend
plot(ip_g, vertex.label = NA, edge.arrow.width = 1, edge.arrow.size = 0, 
    edge.width = 4, margin = 0, vertex.size = 4, 
    edge.color = E(ip_g)$rank_flag, vertex.color = "black" )
legend("bottomleft", legend = c("Lower to Higher Rank", "Higher to Lower Rank"), 
       fill = unique(E(ip_g)$rank_flag ), cex = .7)


# Get a count of out degrees for all vertices
# deg_ct <- lapply(time_graph, function(x){return(degree(x, mode = "out") )})

# Create a dataframe starting by adding the degree count
# deg_df <- data.frame(ct = unlist(deg_ct))

# Add a column with the vertex names 
# deg_df$vertex_name <- names(unlist(deg_ct))

# Add a time stamp 
# deg_df$date <- ymd(rep(d, unlist(lapply(time_graph, function(x){length(V(x))}))))

# See all the vertices that have more than three out degrees
# lapply(time_graph, function(x){return(V(x)[degree(x, mode = "out") > 3])})

# Create a dataframe to plot of three important vertices
# vert_df <- deg_df %>% filter(vertex_name %in% c(1629, 132757, 117841))

# Draw the plot to see how they change through time
# ggplot(vert_df, aes(x = date, y = ct, group = vertex_name, colour = vertex_name)) + geom_path()


# Calculate clustering and reciprocity metrics
# trans <- unlist(lapply(all_graphs, FUN=transitivity))
# rp <- unlist(lapply(all_graphs, FUN=reciprocity))

# Create daaframe for plotting
# met_df <- data.frame("metric" = c(trans, rp))

# Repeat the data
# met_df$date <- rep(ymd(d), 2)

# Sort and then Repeat the metric labels
# met_df$name <- sort(rep(c("clustering", "reciprocity"), 4))

# Plot
# ggplot(met_df, aes(x= date, y= metric, group = name, colour = name)) + geom_path()

```
  
  
  
***
  
Chapter 2 - Talk About R on Twitter  
  
Creating retweet graphs:  
  
* Data is several days of tweets from #rstats - want to use retweets (starts with RT) to form the network  
	* raw_tweets <- read.csv("datasets/rstatstweets.csv", stringsAsFactors = F)  
    * all_sn <- unique(raw_tweets$screen_name)  
    * rt_g <- graph.empty()  
    * rt_g <- rt_g + vertices(all_sn)  
    * for(i in 1:dim(raw_tweets)[1]){  
    *     rt_name <- find_rt(raw_tweets$tweet_text[i])  
    *     if(!is.null(rt_name)){  
    *         if(!rt_name %in% all_sn){   
    *             rt_g <- rt_g + vertices(rt_name)  
    *         }  
    *     rt_g <- rt_g + edges(c(raw_tweets$screen_name[i], rt_name))  
    *     }  
    * }  
    * sum(degree(rt_g) == 0)  
    * rt_g <- simplify(rt_g)  
    * rt_g <- delete.vertices(rt_g, degree(rt_g) == 0)  
  
Building mentions graphs:  
  
* Tweets that mention someone can be a reply or a callout  
* There is more complexity than with the retweets, since there is no common format to a mention such as "starts with RT"  
    * ment_g <- graph.empty()  
    * ment_g <- ment_g + vertices(all_sn)  
    * for(i in 1:dim(raw_tweets)[1]) {  
    *     ment_name <- mention_ext(raw_tweets$tweet_text[i])  
    *     if(length(ment_name) > 0 ) {  
    *         for(j in ment_name) {  
    *             if(!j %in% all_sn) {  
    *                 ment_g <- ment_g + vertices(j)  
    *             }  
    *         ment_g <- ment_g + edges(c(raw_tweets$screen_name[i], j))  
    *         }  
    *     }  
    * }  
* The mentions graph is significantly different, with many more small conversations shown by way of a sub-graph  
  
Finding communities:  
  
* Communities are natural way to think of graphs - people who talk much more to each other than to the full network  
	* ment_edg <- cluster_edge_betweenness(as.undirected(ment_g))  
    * ment_eigen <- cluster_leading_eigen(as.undirected(ment_g))  
    * ment_lp <- cluster_label_prop(as.undirected(ment_g))  
    * length(ment_edg)  
    * table(sizes(ment_edg))  
* Can compare similarities within community structures  
	* compare(ment_edg, ment_eigen, method = 'vi')  # "vi" is "variance information"  
    * compare(ment_eigen, ment_lp, method = 'vi')  
    * compare(ment_lp, ment_edg, method = 'vi')  
* Can also plot the community structures  
	* lrg_eigen <- as.numeric(names(ment_eigen[which(sizes(ment_eigen) >45)]))  
    * eigen_sg <- induced.subgraph(ment_g, V(ment_g)[ eigen %in% lrg_eigen])  
* plot(eigen_sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2,  
*      coords = layout_with_fr(ment_sg), margin = 0, vertex.size = 6,  
*      vertex.color = as.numeric(as.factor(V(eigen_sg)$eigen))  
*      )  
  
Example code includes:  
```{r cache=TRUE}

rt_g <- read.graph("./RInputFiles/rt_g.gml", format=c("gml"))
rt_g


# Calculate the number of nodes
gsize(rt_g)

# Calculate the number of edges
gorder(rt_g)

# Calculate the density
graph.density(rt_g)

# Create the plot
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.4, vertex.size = 3)


# Set the default color to black
V(rt_g)$color <- "black"

# Set the color of nodes that were retweeted just once to blue
V(rt_g)[degree(rt_g, mode = "in") == 1]$color <- "blue"

# Set the color of nodes that were retweeters just once to green 
V(rt_g)[degree(rt_g, mode = "out") == 1 ]$color <- "green"

# Plot the network
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, 
    edge.arrow.size = 0.25, 
    vertex.size = 4, vertex.color = V(rt_g)$color)


# Set the default color to black
V(rt_g)$color <- "black"

# Set the color of nodes that were retweeted just once to blue
V(rt_g)[degree(rt_g, mode = "in") == 1 & degree(rt_g, mode = "out") == 0]$color <- "blue"

# Set the color of nodes that were retweeters just once to green 
V(rt_g)[degree(rt_g, mode = "in") == 0 & degree(rt_g, mode = "out") == 1 ]$color <- "green"

# Plot the network
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, 
    edge.arrow.size = 0.25, 
    vertex.size = 4, vertex.color = V(rt_g)$color)


# Calculate betweenness
rt_btw <- igraph::betweenness(rt_g, directed = TRUE)

# Plot histogram
hist(rt_btw, breaks = 2000, xlim = c(0, 1000), main = "Betweenness")

# Calculate eigen centrality
rt_ec <- eigen_centrality(rt_g, directed = TRUE)

# Plot histogram
hist(rt_ec$vector, breaks = 100, xlim = c(0, .2), main = "Eigen Centrality")


# Get top 1% of vertices by eigen centrality
top_ec <- rt_ec$vector[rt_ec$vector > quantile(rt_ec$vector, .99)]

# Get top 1% of vertices by betweenness
top_btw <- rt_btw[rt_btw > quantile(rt_btw, .99)]

# Make a nice data frame to print, with three columns, Rank, Betweenness, and Eigencentrality
most_central <- as.data.frame(cbind(1:length(top_ec), names(sort(top_btw, decreasing = T)), 
                                    names(sort(top_ec, decreasing = T))
                                    )
                              )

# Set column names
colnames(most_central) <- c("Rank", "Betweenness", "Eigen Centrality")

# Print out the data frame
print(most_central)


# Transform rt_btw and add as centrality
V(rt_g)$cent <-  log(rt_btw+2)

# Visualize
plot(rt_g, vertex.label = NA, edge.arrow.width = .2,
     edge.arrow.size = 0.0,
     vertex.size = unlist(V(rt_g)$cent), vertex.color = "red")

# Create subgraph 
rt_sub <-induced_subgraph(rt_g, V(rt_g)[V(rt_g)$cent >= quantile(V(rt_g)$cent, 0.99 )])

# Plot subgraph
plot(rt_sub, vertex.label = NA, edge.arrow.width = .2,
     edge.arrow.size = 0.0,
     vertex.size = unlist(V(rt_sub)$cent), vertex.color = "red")


ment_g <- read.graph("./RInputFiles/ment_g.gml", format=c("gml"))
ment_g


rt_ratio <- degree(rt_g, mode="in") / (degree(rt_g, mode="out"))
ment_ratio <- degree(ment_g, mode="in") / (degree(ment_g, mode="out"))

# Create a dataframe to plot with ggplot
ratio_df <- data.frame(io_ratio = c(ment_ratio, rt_ratio))
ratio_df["graph_type"] <- c(rep("Mention", length(ment_ratio)), rep("Retweet", length(rt_ratio)) )
ratio_df_filtered <- ratio_df %>% filter(!is.infinite(io_ratio) & io_ratio > 0)

# Plot the graph
ggplot(ratio_df, aes(x = io_ratio , fill= graph_type, group = graph_type)) +
  geom_density(alpha = .5) +
  xlim(0, 10)
 
# Check the mean and median of each ratio
ratio_df %>% group_by(graph_type) %>% summarise(m_ratio = mean(io_ratio))
ratio_df %>% group_by(graph_type) %>% summarise(med = median(io_ratio))
ratio_df %>% filter(io_ratio != +Inf) %>% group_by(graph_type) %>% summarise(m_ratio = mean(io_ratio))
ratio_df %>% filter(io_ratio != +Inf) %>% group_by(graph_type) %>% summarise(med = median(io_ratio))


# Plot mention graph 
plot(ment_g, vertex.label = NA, edge.arrow.width = .8,
     edge.arrow.size = 0.2,
     margin = 0,
     vertex.size = 3)

# Find the assortivity of each graph
assortativity_degree(rt_g, directed = TRUE)
assortativity_degree(ment_g, directed = TRUE)

# Find the reciprocity of each graph
reciprocity(rt_g) 
reciprocity(ment_g)


# Get size 3 cliques
clq_list <- cliques(ment_g, min = 3, max = 3)

# Convert to a dataframe and filter down to just revodavid cliques
clq_df <- data.frame(matrix(names(unlist(clq_list)), nrow = length(clq_list), byrow = T))
rev_d <- clq_df %>% filter(X1 == "revodavid" | X2 == "revodavid" | X3 == "revodavid") %>% droplevels()

# Create empty graph and build it up
clq_g_empty <- graph.empty()
clq_g <- clq_g_empty + vertices(unique(unlist(rev_d)))
for(i in 1:dim(rev_d)[1]){
  clq_g <- clq_g + edges(rev_d[i, 1], rev_d[i, 2])
  clq_g <- clq_g + edges(rev_d[i, 2], rev_d[i, 3])
  clq_g <- clq_g + edges(rev_d[i, 1], rev_d[i, 3])}

# Trim graph and plot using `simplify()`
clq_g_trimmed <- as.undirected(simplify(clq_g))
plot(clq_g_trimmed)


# Find the communities
rt_fgc <-  cluster_fast_greedy(as.undirected(rt_g))
rt_info <- cluster_infomap(as.undirected(rt_g))
rt_clust <- cluster_louvain(as.undirected(rt_g))

# Compare all the communities
compare(rt_fgc, rt_clust, method = 'vi')
compare(rt_info, rt_clust, method = 'vi')
compare(rt_fgc, rt_info, method = 'vi')

# Test membership of the same users
fgc_test <- which(names(membership(rt_fgc)) %in% c("bass_analytics", "big_data_flow"))
membership(rt_fgc)[fgc_test]

info_test <- which(names(membership(rt_info)) %in% c("bass_analytics", "big_data_flow"))
membership(rt_info)[info_test]


# The crossing() function in igraph will return true if a particular edge crosses communities
# This is useful when we want to see certain vertices that are bridges between communities

# Assign cluster membership to each vertex in rt_g using membership()
V(rt_g)$clust <- membership(rt_clust)

# Assign crossing value to each edge
E(rt_g)$cross <- crossing(rt_clust, rt_g)

# Plot the whole graph (this is probably a mess)
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2, 
     coords = layout_with_fr(rt_g), margin = 0, vertex.size = 3, 
     vertex.color = V(rt_g)$clust, edge.color = E(rt_g)$cross+1)

# Create a subgraph with just a few communities greater than 50 but less than 90 in size
mid_comm <- as.numeric(names(sizes(rt_clust)[sizes(rt_clust) > 50 & sizes(rt_clust) < 90 ]))
rt_sg <- induced.subgraph(rt_g, V(rt_g)[ clust %in% mid_comm ])

# Plot the subgraph
plot(rt_sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2, 
     coords = layout_with_fr(rt_sg), margin = 0, vertex.size = 3, 
     vertex.color = V(rt_sg)$clust, edge.color = E(rt_sg)$cross+1)

```
  
  
  
***
  
Chapter 3 - Bike Sharing in Chicago  
  
Creating our graph from raw data:  
  
* Dataset based on Chicago Divvy bike sharing, from freely available data  
	* bike_dat <- read.csv("/Users/edmundhart/wkspace/courses-case-studies-network-r/datasets/bike2_test3.csv", stringsAsFactors = F)  
    * trip_df <- bike_dat %>% group_by(from_station_id, to_station_id) %>% summarise(weights = n())  
    * head(trip_df)  
    * trip_g <- graph_from_data_frame(trip_df[, 1:2])  
    * E(trip_g)$weight <- trip_df$weights  
    * gsize(trip_g)  
    * gorder(trip_g)  
* Can create a sub-graph and run some initial explorations on the network - will notice that there are many loops (trips where to/from is the same)  
	* sg <- induced_subgraph(trip_g, 1:12)  
    * plot(sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.6, margin = 0, vertex.size = 6, edge.width = log(E(sg)$weight+2))  
  
Compare Graph Distance vs. Geographic Distance:  
  
* Graphs do not always reflect the geography well; graph distance may or may not be related to geographic distance  
* Can get graph distances using built in functions  
	* farthest_vertices(trip_g_simp)  
    * get_diameter(trip_g_simp)  
* Can use geographic coding (lat/lon) to find the geographic distances  
	* library(geosphere)  
    * st_to <- bike_dat %>% filter(from_station_id == 336 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    * st_from <- bike_dat %>% filter(from_station_id == 340 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    * farthest_dist <- distm(st_from, st_to, fun = distHaversine)  
    * bike_dist <- function(station_1, station_2, divy_bike_df){  
    *     st1 <- divy_bike_df %>% filter(from_station_id == station_1 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    *     st2 <- divy_bike_df %>% filter(from_station_id == station_2 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    *     farthest_dist <- distm(st1, st2, fun = distHaversine)  
    *     return(farthest_dist)  
    * }
  
Connectivity:  
  
* Can be measured either for vertex or edges - how many need to be removed to create 2 distinct graphs  
	* rand_g <- erdos.renyi.game(10, .4, "gnp", directed = F)  
    * plot(rand_g)  
    * vertex_connectivity(rand_g)  
    * edge_connectivity(rand_g)  
    * min_cut(rand_g, value.only = F)  # more information about the connectivity  
* Can then run comparisons between random graphs and bike-sharing graphs  
	* nv <- gorder(trip_g_ud)  
    * ed <- edge_density(trip_g_ud)  
    * graph_vec <- rep(NA, 1000)  
    * for(i in 1:1000){ w1 <- erdos.renyi.game(nv, ed, "gnp", directed = T) ; graph_vec[i]<- edge_connectivity(w1) }  
  
Example code includes:  
```{r}

bike_dat <- readr::read_csv("./RInputFiles/divvy_bike_sample.csv")
glimpse(bike_dat)

# Create trip_df_subs
trip_df_subs <- bike_dat %>% 
  filter(usertype == "Subscriber") %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_subs <- graph_from_data_frame(trip_df_subs[, 1:2])

# Add edge weights
E(trip_g_subs)$weights <- trip_df_subs$weights / sum(trip_df_subs$weights)

# Now work the same code and filter it down to non-subs
trip_df_non_subs <- bike_dat %>% 
  filter(usertype == "Customer") %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_non_subs <- graph_from_data_frame(trip_df_non_subs[, 1:2])

# Add edge weights
E(trip_g_non_subs)$weights <- trip_df_non_subs$weights / sum(trip_df_non_subs$weights)

# Now let's compare these graphs
gsize(trip_g_subs)
gsize(trip_g_non_subs)


# Create the subgraphs
sg_sub <- induced_subgraph(trip_g_subs, 1:12)
sg_non_sub <- induced_subgraph(trip_g_non_subs, 1:12)

# Plot sg_sub
plot(sg_sub, vertex.size = 20, edge.arrow.width = .8, edge.arrow.size = 0.4, 
     margin = 0, edge.width = E(sg_sub)$weights*10000, main = "Subscribers")

# Plot sg_non_sub
plot(sg_non_sub, vertex.size = 20, edge.arrow.width = .8, edge.arrow.size = 0.4, 
     margin = 0, vertex.size = 10, edge.width = E(sg_non_sub)$weights*10000, 
     main = "Customers")


bike_dist <- function(station_1, station_2, divy_bike_df){ 
   st1 <- divy_bike_df %>% filter(from_station_id == station_1 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)
   st2 <- divy_bike_df %>% filter(from_station_id == station_2 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)
   farthest_dist <- geosphere::distm(st1, st2, fun = geosphere::distHaversine)
   return(farthest_dist)
}


# See the diameter of each graph
get_diameter(trip_g_subs)
get_diameter(trip_g_non_subs)

# Find the farthest vertices
farthest_vertices(trip_g_subs)
farthest_vertices(trip_g_non_subs)

# See how far apart each one is and compare the distances
bike_dist(200, 298, bike_dat)
bike_dist(116, 281, bike_dat)


# Create trip_df
trip_df <- bike_dat %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_df <- graph_from_data_frame(trip_df[, 1:2])

# Add edge weights
E(trip_g_df)$weights <- trip_df$weights / sum(trip_df$weights)


trip_g_simp <- simplify(trip_g_df, remove.multiple=FALSE)
trip_g_simp


# Find the degree distribution
trip_out <- degree(trip_g_simp, mode = "out")
trip_in <- degree(trip_g_simp, mode = "in")

# Create a data frame for easier filtering 
trip_deg <- data.frame(cbind(trip_out, trip_in))
trip_deg$station_id <- names(trip_out)
trip_deg_adj <- trip_deg %>% mutate(ratio = trip_out / trip_in)

# Filter out rarely traveled to stations
trip_deg_filter <- trip_deg_adj %>% filter(trip_out > 10) %>% filter(trip_in > 10) 

# Plot histogram
hist(trip_deg_filter$ratio)

# See which stations were the most skewed using which.min() and which.max()
trip_deg_filter %>% slice(which.min(ratio))
trip_deg_filter %>% slice(which.max(ratio))


# If the weights are the same across all stations, then an unweighted degree ratio would work
# But if we want to know how many bikes are actually flowing, we need to consider weights
# The weighted analog to degree distribution is strength
# We can calculate this with the strength() function, which presents a weighted degree distribution based on the weight attribute of a graph's edges

# Calculate the weighted in and out degrees
trip_out_w <- strength(trip_g_simp, mode = "out")
trip_in_w <- strength(trip_g_simp, mode = "in")

# Create a data frame for easier filtering 
trip_deg_w <- data.frame(cbind(trip_out_w, trip_in_w))
trip_deg_w$station_id <- names(trip_out_w)
trip_deg_w_adj <- trip_deg_w %>% mutate(ratio = trip_out_w / trip_in_w)

# Filter out rarely traveled to stations
trip_deg_w_filter <- trip_deg_w_adj %>% filter(trip_out_w > 10) %>% filter(trip_in_w > 10) 

# Plot histogram of ratio
hist(trip_deg_w_filter$ratio)

# See which stations were the most skewed using which.min() and which.max()
trip_deg_w_filter %>% slice(which.min(ratio))
trip_deg_w_filter %>% slice(which.max(ratio))


latlong <- data.frame(from_longitude=c(-87.656495, -87.660996, -87.6554864, -87.642746, -87.67328, -87.661535, -87.623727, -87.668745, -87.65103, -87.666507, -87.666611), 
                      from_latitude=c(41.858166, 41.869417, 41.8694821, 41.880422, 41.87501, 41.857556, 41.864059, 41.857901, 41.871737, 41.865234, 41.891072)
                      )

# Create a sub graph of the least traveled graph 275
g275 <- make_ego_graph(trip_g_simp,  1, nodes = "275", mode= "out")[[1]]

# Plot graph with geographic coordinates
plot(g275, layout = as.matrix(latlong), vertex.label.color = "blue", vertex.label.cex = .6,
     edge.color = 'black', vertex.size = 15, edge.arrow.size = .1,
     edge.width = E(g275)$weight, main = "Lat/Lon Layout")

# Plot graph without geographic coordinates
plot(g275, vertex.label.color = "blue", vertex.label.cex = .6,
     edge.color = 'black', vertex.size = 15, edge.arrow.size = .1,
     edge.width = E(g275)$weight,
     main = "Default Layout")


# Eigen centrality weighted
ec_weight <- eigen_centrality(trip_g_simp, directed = T, weights = NULL)

# Eigen centrality unweighted
ec_unweight <- eigen_centrality(trip_g_simp, directed = T, weights = NA)

# Closeness weighted
close_weight <- closeness(trip_g_simp, weights = NULL)

# Closeness unweighted
close_unweight <- closeness(trip_g_simp, weights = NA)

# Output nicely with cbind()
cbind(c(
  names(V(trip_g_simp))[which.min(ec_weight$vector)],
  names(V(trip_g_simp))[which.min(close_weight)],
  names(V(trip_g_simp))[which.min(ec_unweight$vector)],
  names(V(trip_g_simp))[which.min(close_unweight)]
  ), c("Weighted Eigen Centrality", "Weighted Closeness", "Unweighted Eigen Centrality", "Unweighted Closeness")
)


trip_g_ud <- as.undirected(trip_g_simp)
trip_g_ud


# Find the minimum number of cuts using min_cut()
ud_cut <- min_cut(trip_g_ud, value.only = FALSE)

# Print the vertex with the minimum number of cuts
print(ud_cut$partition1)

# Make an ego graph
g<- make_ego_graph(trip_g_ud, 1, nodes = "281")[[1]]
plot(g, edge.color = 'black', edge.arrow.size = .1)

# Print the value
print(ud_cut$value)

# Print cut object
print(ud_cut$cut)

far_stations <- c("231", "321")
close_stations <- c("231", "213")

# Compare the output of close and far vertices
stMincuts(trip_g_simp, far_stations[1], far_stations[2])$value
stMincuts(trip_g_simp, close_stations[1], close_stations[2])$value


# Find the actual value
clust_coef <- transitivity(trip_g_simp, type = "global")

# Get randomization parameters using gorder() and edge_density()
nv <- gorder(trip_g_simp)
ed <- edge_density(trip_g_simp)

# Create an empty vector to hold output of 300 simulations
graph_vec <- rep(NA, 300)

# Calculate clustering for random graphs
for(i in 1:300){
  graph_vec[i]<- transitivity(erdos.renyi.game(nv, ed, "gnp", directed = T), type = "global")
}

# Plot a histogram of the simulated values
hist(graph_vec, xlim = c(.35, .6), main = "Unweighted clustering randomization")

# Add a line with the true value
abline(v = clust_coef, col = "red")


# Find the mean local weighted clustering coeffecient
m_clust <- mean(transitivity(trip_g_simp, type = "weighted"))
nv <- gorder(trip_g_simp)
ed <- edge_density(trip_g_simp)
graph_vec <- rep(NA, 100)

for(i in 1:100){
  g_temp <- erdos.renyi.game(nv, ed, "gnp", directed = T)
  # Sample existing weights and add them to the random graph
  E(g_temp)$weight <- sample(x = E(trip_g_simp)$weights, size = gsize(g_temp), replace = TRUE)
  graph_vec[i]<- mean(transitivity(g_temp, type = "weighted"))
}

# Plot a histogram of the simulated values
hist(graph_vec, xlim = c(.35, .7), main = "Unweighted clustering randomization")

# Add a line with the true value
abline(v = m_clust ,col = "red")

```
  
  
  
***
  
Chapter 4 - Other Ways to Visualize Graph Data  
  
Other packages for plotting graphs:  
  
* Base plotting in igraph is good for quick visualizations, but other libraries can make great plots simply  
* Can use the ggplot syntax with ggnet, for example  
	* library(ggnetwork)  
    * library(igraph)  
    * library(GGally)  
    * library(intergraph)  
* The basic igraph plotting is as follows  
	* rand_g <- erdos.renyi.game(30, .15, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * plot(rand_g)  
* The basic ggnet plotting requires use of asNetwork on the core graph  
	* net <- asNetwork(rand_g)  
    * ggnet2(net)  
* The basic ggnetwork plotting is more similar to ggplot2  
	* gn <- ggnetwork(rand_g)  
    * g <- ggplot(gn, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges() + geom_nodes() + theme_blank()  
    * plot(g)  
* Where an igraph might require voluminous code for extensions, the ggnet or ggnetwork can be extended in a much simpler manner  
	* ggnet2(net, node.size = "cent", node.color = "comm", edge.size = .8, color.legend = "Community Membership", color.palette = "Spectral", edge.color = c("color", "gray88"), size.cut = T, size.legend = "Centrality")  
    * g <- ggplot(gn, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(aes(color = as.factor(comm))) + geom_nodes(aes(color = as.factor(comm), size = cent)) + theme_blank() + guides(color = guide_legend(title = "Community"), size = guide_legend(title = "Centrality"))  
    * plot(g)  
  
Interactive visualizations:  
  
* Can use R to create interactive javascript plots for outputs to html or similar  
* Examples can be built on a simple random graph  
	* library(ggiraph)  
    * library(htmlwidgets)  
    * library(networkD3)  
    * rand_g <- erdos.renyi.game(30, .12, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * V(rand_g)$cent <- betweenness(rand_g)  
* Can plot using ggplot2 and ggiraph  
	* g <- ggplot(ggnetwork(rand_g), aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(color = "black") + geom_nodes(aes(size = cent)) + theme_blank() + guides(size = guide_legend(title = "Centrality"))  
    * my_gg <- g + geom_point_interactive(aes(tooltip = round(cent, 2)), size = 2)  # Create ggiraph object  
    * ggiraph(code = print(my_gg))  # Display ggiraph object  
* Can also customize the ggiraph call using valid CSS, for example using  
	* hover_css = "cursor:pointer;fill:red;stroke:red;r:5pt"  
    * data_id = round(cent, 2)), size = 2)  
    * ggiraph(code = print(my_gg), hover_css = hover_css, tooltip_offx = 10, tooltip_offy = -10)  
* Can also plot with networkD3 which is easy to use but does not allow for much customization  
	* nd3 <- igraph_to_networkD3(rand_g)  
    * simpleNetwork(nd3$links)  
* Can add complexity to the D3 plot by specifying information about the nodes and edges to be plotted insinde forceNetwork()  
	* nd3$nodes$group = V(rand_g)$comm  
    * nd3$nodes$cent = V(rand_g)$cent  
    * forceNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', Target = 'target', NodeID = 'name', Group = 'group', Nodesize = 'cent', legend = T, fontSize = 20)  
  
Alternative visualizations:  
  
* Hairball plots are designed to maximize spacing between vertices, but as plots get large the information is hard if not impossible to interpret  
* One potential solution is the hive plot  
	* library(HiveR)  
    * rand_g <- erdos.renyi.game(18, .3, "gnp", directed = T)  
    * plot(rand_g, vertex.size = 7)  # standard igraph plot  
    * rand_g_df <- as.data.frame(get.edgelist(rand_g))  
    * rand_g_df$weight <- 1  
    * rand_hive <- edge2HPD(edge_df = rand_g_df)  
    * rand_hive$nodes$axis <- sort(rep(1:3, 6))  
    * rand_hive$nodes$radius <- as.double(rep(1:6, 3))  
    * plotHive(rand_hive, method="abs", bkgnd="white")  
* Can modify hive plots by way of either nodes or edges  
	* # Setting location of each node  
    * rand_hive$nodes$axis <- sort(rep(1:3, 6))  
    * rand_hive$nodes$radius <- as.double(rep(1:6, 3))  
    * # Add weights to each edge  
    * rand_hive$edges$weight <- as.double(rpois(length(rand_hive$edges$weight), 5))  
    * # Add color based on edge origination  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 1:6] <- 'red'  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 7:12] <- 'blue'  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 13:18] <- 'green'  
    * # Plot  
    * plotHive(rand_hive, method = "abs", bkgnd = "white")  
* Another alternative is the biofabric plot  
	* # Create random graph  
    * rand_g <- erdos.renyi.game(10, .3, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * # Add names to vertices  
    * V(rand_g)$name <- LETTERS[1:length(V(rand_g))]  
    * # Create biofabric plot  
    * biofbc <- bioFabric(rand_g)  
    * bioFabric_htmlwidget(biofbc)  
  
Example code includes:  
```{r cache=TRUE}

verts <- c(1185, 3246, 1684, 3634, 3870, 188, 2172, 3669, 2267, 1877, 3931, 1862, 2783, 2351, 423, 3692, 1010, 173, 1345, 3913, 3646, 2839, 2624, 4072, 2685, 2901, 2227, 2431, 1183, 602, 3937, 3688, 2823, 3250, 101, 1951, 3097, 884, 1299, 945, 583, 1691, 1687, 1504, 622, 566, 949, 1897, 1083, 3491, 187, 1799, 3249, 496, 2280, 840, 519, 3060, 4115, 1520, 2700, 385, 1558, 1113, 3303, 1818, 3283, 3291, 3218, 1781, 3055, 2547, 2874, 3, 1923, 890, 1536, 2477, 1422, 449, 984, 2697, 1686, 3181, 415, 1754, 3972, 3600, 3573, 706, 527, 2631, 1383, 2644, 1290, 756, 3147, 377, 4109, 2056, 2411, 1337, 1963, 3833, 1939, 4030, 4111, 2442, 1647, 590, 3749, 1208, 244, 3796, 2886, 570, 2199, 3818, 2342, 1618, 2591, 1279, 1230, 878, 1476, 3930, 616, 364, 567, 2753, 2470, 3554, 2683, 2938, 2077, 2629, 3273, 3131, 3900, 1749, 1240, 1629, 42, 731, 3350, 919, 950, 305, 976, 2906, 3363, 1974, 1539, 978, 441, 1546, 4110, 860, 1762, 864, 1989, 1401, 2572, 1482, 1406, 2110, 2926, 874, 1631, 1050, 2488, 726, 3408, 2946, 2636, 2437, 1468, 2089, 3447, 2292, 3308, 1231, 2788, 1043, 2339, 1893, 3935, 2220, 3589, 3544, 1077, 1263, 4114, 2434, 3679, 1831, 1596, 2585, 598, 2246, 936, 3770, 2355, 2017, 1576, 3445, 1425, 1128, 668, 674, 1884, 989, 845, 2634, 4068, 2736, 1374, 3922, 3202, 3583, 1102, 3746, 2838, 2674, 206, 3966, 1860, 2180, 2717, 3562, 2405, 1666, 2107, 228, 1014, 1543, 768, 3229, 594, 3117, 2121, 2568, 666, 2454, 1209, 2807, 1545, 3753, 3744, 2812, 995, 858, 2293, 1034, 2053, 3034, 650, 1562, 1821, 3351, 3572, 3402, 2600, 3663, 1991, 2222, 1296, 1338, 78, 1936, 3352, 25, 278, 632, 2962, 2826, 3734, 1792, 286, 2491, 2912, 4028, 1522, 863, 223, 1518, 249, 866, 210, 2567, 1140, 386, 276, 3368, 2885, 3122, 3754, 396, 379, 3051, 2996, 36, 2973, 4106, 2404, 1834, 3920, 32, 1724, 1876, 1484, 1769, 2715, 211, 1350, 3054, 3178, 904, 1346, 3256, 3243, 1124, 559, 2672, 394, 128, 3790, 133, 1283, 3468, 3934, 1085, 2794, 3157, 1190, 1864, 2638, 2426, 2435, 3696, 1567, 451, 1987, 850, 1836, 1397, 3710, 1465, 865, 2350, 515, 3645, 1940, 614, 2341, 3711, 2516, 3914, 1216, 3140, 541, 725, 3369, 1157, 1364, 2943, 3947, 67, 1525, 1812, 1582, 1285, 4117, 1705, 1999, 3608, 2899, 782, 1155, 3632, 2187, 2844, 1393, 2873, 2008, 3412, 692, 1053, 355, 785, 3643, 1105, 2706, 2927, 393, 893, 1007, 4021, 439, 3687, 3667, 510, 3365, 2141, 1469, 1671, 2623, 307, 1259, 2526, 1176, 3083, 798, 1845, 1023, 712, 3520, 1191, 1771, 104, 2025, 2382, 2204, 3784, 3292, 2313, 1119, 1433, 593, 3182, 3516, 2079, 1215, 3673, 3831, 2257, 399, 1793, 366, 3690, 1041, 2147, 2690, 609, 3184, 2603, 2793, 540, 1315, 2471, 1922, 3792, 882, 214, 867, 3261, 3816, 2737, 3990, 457, 3566, 1595, 1697, 605, 2138, 990, 841, 2524, 1033, 2958, 343, 2998, 1559, 2756, 2414, 1620, 2285, 2, 791, 2566, 783, 2961, 1120, 2500, 3390, 421, 464, 2463, 4056, 3029, 3525, 256, 1668, 2544, 316, 3598, 917, 180, 2485, 2848, 1280, 1326, 1039, 290, 1321, 644)
verts <- c(verts, 1937, 1820, 3733, 1232, 1677, 298, 3102, 1427, 2653, 619, 1639, 2774, 226, 2934, 1084, 1312, 1123, 135, 1865, 2440, 3245, 92, 3551, 1088, 3370, 2467, 1604, 2928, 142, 2648, 1250, 2970, 1918, 983, 2866, 328, 2976, 3653, 2692, 4099, 291, 3819, 2864, 1375, 1169, 732, 2031, 3166, 1888, 2092, 2372, 1887, 1816, 58, 170, 3306, 3903, 715, 2312, 2323, 1404, 3824, 1942, 3142, 1964, 3214, 2084, 1502, 3366, 2513, 1464, 66, 2007, 1735, 3109, 2876, 3021, 1301, 3089, 535, 996, 3916, 3451, 2057, 1858, 215, 3417, 424, 312, 3103, 1791, 1189, 3149, 113, 835, 2415, 794, 3636, 612, 2816, 514, 2889, 1162, 1313, 2210, 339, 3850, 3481, 2047, 2739, 3124, 2643, 3428, 155, 3161, 3027, 2711, 1317, 148, 1273, 956, 2969, 1265, 1063, 3899, 3945, 1597, 2543, 363, 767, 3322, 2618, 2850, 1454, 2066, 2778, 3534, 1339, 314, 2174, 2589, 297, 3932, 2132, 2612, 3180, 1649, 1966, 2552, 3581, 3148, 196, 1741, 1213, 2924, 3936, 406, 3631, 813, 259, 3230, 543, 2233, 599, 70, 1797, 3607, 975, 1448, 2022, 2777, 696, 1581, 1542, 2523, 2457, 2857, 3046, 3272, 1891, 3681, 586, 1644, 871, 137, 2176, 1849, 480, 972, 1996, 565, 330, 1466, 1217, 2888, 889, 80, 3487, 1143, 2157, 3594, 3747, 634, 1463, 2150, 1775, 2247, 2484, 1658, 1309, 24, 13, 3383, 367, 1423, 2439, 2522, 3637, 2064, 3639, 4046, 2078, 3676, 3506, 1413, 2964, 2192, 3130, 4078, 1069, 2720, 3344, 1090, 5, 3848, 501, 167, 3915, 3787, 4049, 3986, 233, 2343, 3196, 3918, 4063, 537, 242, 3809, 1648, 1662, 2986, 124, 685, 1726, 4087, 1932, 3999, 1910, 484, 489, 1382, 2289, 2189, 3067, 2722, 2262, 2702, 429, 839, 1109, 1361, 2123, 4058, 3959, 2735, 52, 2183, 2707, 1538, 678, 63, 943, 3047, 3108, 1806, 730, 1628, 2664, 1355, 345, 932, 1201, 861, 3861, 1214, 403, 156, 3429, 3210, 3355, 1583, 2479, 3508, 164, 2299, 3320, 2923, 2562, 460, 4013, 417, 1947, 1853, 2272, 1027, 1997, 3266, 2449, 250, 1486, 177, 1118, 3644, 14, 2538, 3836, 2368, 3349, 1879, 2310, 3413, 4032, 319, 3155, 2413, 3842, 3724, 1802, 3319, 2940, 31, 773, 426, 1067, 2374, 3240, 2335, 4010, 3398, 3096, 392, 245, 2898, 4026, 138, 2109, 1526, 2011, 881, 512, 372, 1650, 3373, 3659, 552, 2474, 1712, 3786, 2185, 43, 3406, 2890, 3504, 348, 2982, 2186, 481, 4018, 3048, 1360, 962, 838, 720, 1826, 4011, 2161, 1763, 2617, 2447, 65, 1227, 3938, 2569, 3662, 1746, 2742, 4020, 2148, 1643, 2450, 4093, 3905, 230, 3401, 168, 2779, 1847, 1006, 3074, 1894, 1702, 1229, 3704, 2586, 3595, 1163, 3661, 2230, 3236, 1111, 1770, 438, 2504, 2828, 651, 2456, 1900, 3050, 506, 1674, 3477, 2766, 76, 3606, 3630, 1237, 3617, 295, 3512, 1286, 3623, 3495, 964, 3407, 494, 3629, 140, 1178, 3045, 2041, 194, 3852, 3800, 1605, 1420, 1968, 442, 3570, 1796, 1729, 369, 2401, 1507, 2462, 145, 2580, 848, 4043, 3443, 2979, 22, 3727, 1316, 1437, 3450, 3590, 3465, 3188, 2373, 432, 3425, 3449, 1356, 273, 700, 1789, 1251, 1767, 3998, 2005, 1222, 2214, 340)

# Create subgraph of rt_g
rt_samp <- induced_subgraph(rt_g, verts)

# Convert from igraph using asNetwork()
net <- intergraph::asNetwork(rt_samp)

# Plot using igraph
plot(rt_samp, vertex.label = NA, edge.arrow.size = 0.2, edge.size = 0.5, 
     vertex.color = "black", vertex.size = 1
     )

# Plot using ggnet2
GGally::ggnet2(net, node.size = 1, node.color = "black", edge.size = .4)


# Raw plot of rt_samp using ggnetwork()
library(ggnetwork)
library(GGally)

ggplot(ggnetwork(rt_samp, arrow.gap = .01) , aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), color = "black") +
    geom_nodes(size = 4) 

# Prettier plot of rt_samp using ggnetwork()
ggplot(ggnetwork(rt_samp, arrow.gap = .01),aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), color = "black", curvature = .2) +
    geom_nodes(size = 4) + theme_blank()

# NEED TO FIX!
rt_keys <- sort(table(vertex_attr(rt_g)$clust), decreasing=TRUE)
# rt_drops <- names(rt_keys)[11:length(rt_keys)]
# vt_drops <- which(vertex_attr(rt_g)$clust %in% rt_drops)
# rt_use <- delete_vertices(rt_g, vt_drops)
rt_use <- induced_subgraph(rt_g, which(V(rt_g)$clust %in% names(rt_keys[1:10])))

# Convert to a network object
net <- intergraph::asNetwork(rt_use)
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral"
       )

# Now remove the centrality legend by setting size to false in the guide() function
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral"
       ) + 
    guides( size = FALSE)

# Add edge colors
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral", 
       edge.color = c("color", "gray88")) +
  guides( size = FALSE)


# NEED TO CREATE rt_g_smaller!
# Basic plot where we set parameters for the plots using geom_edegs() and geom_nodes()
# ggplot(ggnetwork(rt_g_smaller, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
#   geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), curvature = .2, color = "black") + 
#   geom_nodes(size = 4, aes(color = comm)) + 
#   theme_blank()

# Added guide legend, changed line colors, added size 
# ggplot(ggnetwork(rt_g_smaller, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
#   geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), curvature = .2, lwd = .3, aes(color=comm)) +
#   geom_nodes(aes(color = comm, size = cent)) + 
#   theme_blank() +  
#   guides(color = guide_legend(title = "Community"), size = guide_legend(title = "Centrality"))


# NEED TO FIX!
# Add betweenness centrality using betweenness()
V(trip_g_simp)$cent <- igraph::betweenness(trip_g_simp)

# Create a ggplot object with ggnetwork to render using ggiraph
g <- ggplot(ggnetwork(trip_g_simp, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(size = cent)) + 
    theme_blank() 
plot(g)

# Create ggiraph object and assign the tooltip to be interactive
my_gg <- g + ggiraph::geom_point_interactive(aes(tooltip = round(cent, 2), 
                                                 data_id = round(cent, 2)
                                                 ), size = 2
                                             ) 

# Define some hover css so the cursor turns red
hover_css = "cursor:pointer;fill:red;stroke:red;r:3pt"
# ggiraph::ggiraph(code = print(my_gg), hover_css = hover_css, tooltip_offx = 10, tooltip_offy = -10)


# Add community membership as a vertex attribute using the cluster_walktrap algorithm
V(rt_g)$comm <- membership(cluster_walktrap(rt_g))

# Create an induced_subgraph
rt_sub_g <- induced_subgraph(rt_g, which(V(rt_g)$comm %in% 10:13))

# Plot to see what it looks like without an interactive plot using ggnetwork
ggplot(ggnetwork(rt_sub_g, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(color = as.factor(comm))) + 
    theme_blank()  

# Convert to a networkD3 object
# nd3 <- igraph_to_networkD3(rt_sub_g)

# Assign grouping factor as community membership
# nd3$nodes$group = V(rt_sub_g)$comm

# Render your D3.js graph
# forceNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', 
#              Target = 'target', NodeID = 'name', Group = 'group', legend = T, fontSize = 20
#              )

# Convert  trip_df to hive object using edge2HPD()
# bike_hive <- edge2HPD(edge_df =  as.data.frame(trip_df))

# Assign to trip_df edgecolor using our custom function
# trip_df$edgecolor <- dist_gradient(trip_df$geodist)

# Calculate centrality with betweenness()
# bike_cent <- betweenness(trip_g)

# Add axis and radius based on longitude and radius
# bike_hive$nodes$radius<- ifelse(bike_cent > 0, bike_cent, runif(1000, 0, 3))

# Set axis as integers and axis colors to black
# bike_hive$nodes$axis <- as.integer(dist_stations$axis)
# bike_hive$axis.cols <- rep("black", 3)

# Set the edge colors to a heatmap based on trip_df$edgecolor
# bike_hive$edges$color <- trip_df$edgecolor
# plotHive(bike_hive, method = "norm", bkgnd = "white")


# Add community membership as a vertex attribute
V(rt_g)$comm <- membership(cluster_walktrap(rt_g))

# Create a subgraph
rt_sub_g <- induced_subgraph(rt_g, which(V(rt_g)$comm %in% 10:15))

# Plot to see what it looks like without an interactive plot
ggplot(ggnetwork(rt_sub_g, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(color = as.factor(comm)))+ theme_blank() +
    theme(legend.position = "none")

# Make a Biofabric plot htmlwidget
# rt_bf <- bioFabric(rt_sub_g)
# bioFabric_htmlwidget(rt_bf)


# Create a dataframe of start and end latitude and longitude and add weights
# ll_to_plot <- bike_dat %>% group_by(from_station_id, to_station_id, from_latitude, 
#                                     from_longitude, to_latitude, to_longitude, usertype
#                                     ) %>% 
#     summarise(weight = n())

# Create a base map with station points with ggmap()
# ggmap(chicago) + 
#     geom_segment(data = ll_to_plot, aes(x = from_longitude, y = from_latitude, 
#                                         xend = to_longitude, yend = to_latitude, 
#                                         colour = usertype, size = weight
#                                         ), alpha = .5
#                )

```
  
  
  
***
  
###_Fundamentals of Bayesian Analysis in R:_  
  
Chapter 1 - What is Bayesian Analysis?  
  
Introduction:  
  
* British team spearheaded by Turing found ways to decrypt German communications in 1941  
	* Key to Turing's success was the use of Bayesian methods, which wree not very widely used  
* Bayesian inference is "A method for figuring out unobservable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be"  
	* The unknown was the configuration of the wheels in the encryption machine - British already knew what a given wheel configuration would produce  
    * Turing worked backwards to figure out the probable configuration of the wheels from the messages that he had received  
* Bayesian analysis is flexible and can be problem-specific and customized to a specific dataset and analysis need  
  
Bayesian data analysis - named for Thomas Bayes from the early-mid 1700s:  
  
* Bayesian data analysis is about probabilistic inference for learning from data and drawing conclusions  
* For this specific course, probability will be a statement about the certainty (p=1 means certain yes, p=0 means certain no), though there are other definitions that can be used  
	* Probability need not be only about yes/no statements, and can be associated to distributions such as "amount of rainfall tomorrow"  
    * "The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has been learned from data."  
* Example of using a Bayesian approach to patients  
	* prop_model(data) is a function that has been created to plot out probabilities vs. p  
    * The data is a vector of successes and failures represented by 1s and 0s  
    * There is an unknown underlying proportion of success  
    * If data point is a success is only affected by this proportion  
    * Prior to seeing any data, any underlying proportion of success is equally likely  
    * The result is a probability distribution that represents what the model knows about the underlying proportion of success  
  
Samples and posterior samples:  
  
* Prior probability distribution is a distribution PRIOR to updating with some data; for example, equally probable that p falls anywhere between 0 and 1  
* Posterior probability distribution is a distribution AFTER updating with what is learned by seeing some data  
* Can be valuable to have a vector of potential outcomes, appropriately weighted by the likelihood of each of the outcomes  
  
Chapter wrap-up:  
  
* Can draw conclusions about the probabilities based on even a small sample of data observed  
* Next chapters will cover mechanics of Bayesian inference in more detail  
  
Example code includes:  
```{r}

prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000) {
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))

    post_curves <- map_dfr(data_indices, function(i) {
        value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", "Failure"))
        label <- paste0("n=", i)
        probability <- dbeta(proportion_success, prior_prop[1] + sum(data[seq_len(i)]), 
                             prior_prop[2] + sum(!data[seq_len(i)])
                             )
        probability <- probability / max(probability)
        data_frame(value, label, proportion_success, probability)
        }
    )
    post_curves$label <- fct_rev(factor(post_curves$label, levels =  paste0("n=", data_indices )))
    post_curves$value <- factor(post_curves$value, levels = c("Prior", "Success", "Failure"))
  
    p <- ggplot(post_curves, aes(x = proportion_success, y = label, height = probability, fill = value)) +
        ggridges::geom_density_ridges(stat="identity", color = "white", 
                                      alpha = 0.8, panel_scaling = TRUE, size = 1
                                      ) +
        scale_y_discrete("", expand = c(0.01, 0)) +
        scale_x_continuous("Underlying proportion of success") +
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", 
                          drop = FALSE, labels =  c("Prior   ", "Success   ", "Failure   ")
                          ) +
        #ggtitle(paste0("Binomial model - Data: ", sum(data),  " successes, " , sum(!data), " failures"))  +
        theme_light(base_size = 18) +
        theme(legend.position = "top")
    print(p)
  
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data)))
}


# Define data and run prop_model
data = c(1, 0, 0, 1)
prop_model(data)


# Define data and run prop_model
data = c(1, 0, 0, 1)
prop_model(data)


data = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
posterior <- prop_model(data)
head(posterior)
hist(posterior, breaks = 30, xlim = c(0, 1), col = "palegreen4")

# Get some more information about posterior
median(posterior)
quantile(posterior, c(0.05, 0.95))
sum(posterior > 0.07) / length(posterior)

```
  
  
  
***
  
Chapter 2 - How Does Bayesian Inference Work?  
  
Parts needed for Bayesian inference:  
  
* Bayesian inference requires priors (what is known before seeing data), generative model, and data  
	* The generative model is a formula or computer expression that can generate simulated data based on provided input parameters  
    * For example, could assume that there is a proportion of zombies cured and a number of zombies treated and then simulate data based on these  
  
Using a generative model:  
  
* The binomial distribution function can be very helpful as a generative model for summing probabilities of single 1/0 events  
* Typically in data analysis, we know the outcome and want to figure out the likely parameters in our generative model (that is the Bayesian inference)  
  
Repressing uncertainty with priors:  
  
* The prior reflects our certainty/uncertainty in the parameters prior to running the analysis  
	* Example could be a uniform distribution from (a, b)  
    * proportion_clicks <- runif(n = 6, min = 0.0, max = 1.0)  # sample 6 values that are between 0 and 1 with every number being equally likely  
    * n_clicks <- rbinom(n = 6, size = 100, proportion_clicks)  # rbinom will vectorize over proportion_clicks  
  
Bayesian models and conditioning:  
  
* The Bayesian model is based on the generative model and the prior  
	* prior <- data.frame(proportion_clicks, n_visitors)  # joint PDF over proportion_clicks and n_visitors  
* Can then condition on the observed data and assess the joint PDF for the implications on the distribution of the underlying proportion  
	* "Bayesian inference is conditioning on data, in order to learn about parameter values."  
  
Chapter wrap-up:  
  
* Used the binomial model as an assumed generative function and a uniform distribution as the prior probabilities  
* Calculated joint probability distributions and then found a probability distribution based on a known outcome (data)  
* The posterior can then be used as the prior for future analyses, and can be repeated indefinitely  
* Bayesian machinery from simple cases can be extended to more complex cases  
	* Just need a generative model and an assumption for the prior  
    * Need a computational model that can scale easily  
  
Example code includes:  
```{r}

# Generative zombie drug model
# Parameters
prop_success <- 0.42
n_zombies <- 100
# Simulating data
data <- c()
for(zombie in 1:n_zombies) {
  data[zombie] <- runif(1, min = 0, max = 1) < prop_success
}
data <- as.numeric(data)
data
data_counts <- sum(as.numeric(data))
data_counts


# Try out rbinom
rbinom(n = 1, size = 100, prob = 0.42)

# Try out rbinom
rbinom(n = 200, size = 100, prob = 0.42)


# Fill in the parameters
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = proportion_clicks)

# Visualize the results
hist(n_visitors)


# Update proportion_clicks
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n = n_samples, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)

# Visualize the results
hist(n_visitors)
hist(proportion_clicks)


# Create prior
prior <- data.frame(proportion_clicks, n_visitors)
head(prior)

# Create posterior
posterior <- prior[prior$n_visitors==13, ]
hist(posterior$proportion_clicks)


prior <- posterior
head(prior)
prior$n_visitors <- rbinom(nrow(prior), size=100, prob=prior$proportion_clicks)
hist(prior$n_visitors)
mean(prior$n_visitors >= 5)

```
  
  
  
***
  
Chapter 3 - Why Use Bayesian Data Analysis?  
  
Four good things with Bayes:  
  
* Many good tools exist for Bayesian analysis, and those will be covered in Chapter 5  
* The main reasons for Bayesian analysis are the flexibility and power  
	* You can include information sources such as expertise in addition to the data  
    * You can make any comparisons between groups or data sets  
    * You can use the results of Bayesian analysis for Decision Analysis  
    * You can change the underlying statistical model  
* Background information, common knowledge, or expertise can be incorporated in to the prior  
	* Can also exclude all such information by assuming a uniform distribution for the prior  
    * The beta distribution can be useful for a proportion, and is set based on alpha and beta  
  
Contrasts and comparisons:  
  
* The more data, the more likely that the posterior is informed by the data than by the prior  
* There is often a benefit to comparing groups - for example, two different treatments or two different campaigns  
	* Can compare the posterior distributions from the two groups, which is fairly simple since each distribution is contained in a vector  
    * posterior$prop_diff <- posterior$video_prop - posterior$text_prop  
  
Decision analysis:  
  
* Can calculate median, mean, credible interval, likelihood of more extreme than a certain parameter, etc.  
* The results of a Bayesian analysis can be used for Decision analysis  
* Can add dimensions such as cost and revenue, then compare profitability  
	* video_cost <- 0.25  
    * text_cost <- 0.05  
    * visitor_spend <- 2.53  
    * posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost  
    * posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost  
    * posterior$profit_diff <- posterior$video_profit - posterior$text_profit  
  
Change anything and everything:  
  
* There can be large uncertainty as to the outcomes, particularly if the data sizes are small and the key metrics are close to the critical parameter  
* Can switch out the generating functions and re-run the approach - example of a banner that is pay-per-day rather than pay-per-impression  
	* One option is to split each day in to 1440 minutes and assume a probability of success (1/0), which has drawbacks  
    * In the limiting case of smaller and smaller slices, the Poisson distribution is created - has only a single parameter, the expected value of successes  
    * n_clicks <- rpois(n = 100000, labmda = 20)  
  
Bayes is optimal, kind of . . .   
  
* Bayesian analysis can be useful in many ways as shown in this chapter  
* Bayesian analysis is (kind of) optimal, provided that the generative model is perfectly precise and accurate  
	* In the world defined by the model the Bayesian approach is optimal  
    * However, no statistical model can ever be optimal in the real world  
  
Example code includes:  
```{r}

# Draw from the beta distribution
beta_sample <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)

# Explore the results
hist(beta_sample)

# Draw from the beta distribution
beta_sample <- rbeta(n = 10000, shape1 = 100, shape2 = 100)

# Explore the results
hist(beta_sample)

# Draw from the beta distribution
beta_sample <- rbeta(n = 10000, shape1 = 100, shape2 = 20)

# Explore the results
hist(beta_sample)


n_draws <- 100000
n_ads_shown <- 100

# Update proportion_clicks
proportion_clicks <- rbeta(n_draws, shape1 = 5, shape2 = 95)
n_visitors <- rbinom(n_draws, size = n_ads_shown, prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 13, ]

# Plots the prior and the posterior in the same plot
par(mfcol = c(2, 1))
hist(prior$proportion_clicks, 
     xlim = c(0, 0.25))
hist(posterior$proportion_clicks, 
     xlim = c(0, 0.25))
# Reset mfcol below

# Define parameters
n_draws <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown, prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)

# Create posteriors
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6, ]

# Visualize posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))


posterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000], 
                        text_prop  = posterior_text$proportion_click[1:4000]
                        )
    
# Create prop_diff
posterior$prop_diff <- posterior$video_prop - posterior$text_prop

# Plot your new column
hist(posterior$prop_diff)

# Explore prop_diff
median(posterior$prop_diff)
mean(posterior$prop_diff > 0)


visitor_spend <- 2.53
video_cost <- 0.25
text_cost <- 0.05

posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost
head(posterior)
hist(posterior$video_profit)
hist(posterior$text_profit)


posterior$profit_diff <- posterior$video_profit - posterior$text_profit
head(posterior)
hist(posterior$profit_diff)
median(posterior$profit_diff)
mean(posterior$profit_diff < 0)


x <- rpois(n = 10000, lambda = 3)
hist(x)

x <- rpois(n = 10000, lambda = 11.5)
hist(x)

x <- rpois(n = 10000, lambda = 11.5)
mean(x >= 15)


n_draws <- 100000
n_ads_shown <- 100
mean_clicks <- runif(n_draws, min = 0, max = 80)
n_visitors <- rpois(n_draws, lambda=mean_clicks)
                     
prior <- data.frame(mean_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 19, ]

hist(prior$mean_clicks)
hist(posterior$mean_clicks)

# Reset to default
par(mfcol = c(1, 1))

```
  
  
  
***
  
Chapter 4 - Bayesian Inference with Bayes' Theorem  
  
Probability rules:  
  
* The computation method used so far does not scale well, but there are alternatives  
* Bayesian statistics is a hot research error, and there are many methods to get to the same results in a faster method  
* Probability is defined as a statement of certainty/uncertainty between 0 and 1 (may be a distribution or an allocation of probabilities over all possible values)  
* Conditional probabilities are often of interest - P(A | B) is the probability of A given that B has occurred  
	* Can also get a conditional probability distribution  
* Sometimes probabilities can be summed - when they are exclusive and the goal is to get 1 of them  
* Sometimes probabilities can be multiplied - when they are independent and the goal is to get all of them  
  
Calculating likelihoods:  
  
* Can simulate or calculate probabilities - simulate and count with a common generative model and small dataset, or calculate using an optimized formula  
* For common distributions, can use the density functions such as dbinom() to get the key probabilities in many cases  
	* dbinom(13, size = 100, prob = 0.1) + dbinom(14, size = 100, prob = 0.1)  # probability of getting 13 or 14 successes in 100 trials with success 0.1 per trial  
    * n_visitors = seq(0, 100, by = 1)  
    * probability <- dbinom(n_visitors, size = 100, prob = 0.1)  
* For continuous distributions such as the uniform, there is no specific probability of any given value  
	* Probability densities are returned instead, and can be viewed as the relative probabilities  
  
Bayesian calculation:  
  
* Conditioning on the observed data is at the core of Bayesian inference  
* Example of converting previous simulations to precise calculations  
	* n_ads_shown <- 100  
    * n_visitors <- seq(0, 100, by = 1)  # full potential range of visitors based on ads  
    * proportion_clicks <- seq(0, 1, by = 0.01)  # fine grid of values even if it does not fully sample the given space  
    * pars <- expand.grid(proportion_clicks = proportion_clicks, n_visitors = n_visitors)  # all possible combinations of n_visitors and proportion_clicks  
    * pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)  # prior is of uniform density  
    * pars$likelihood <- dbinom(pars$n_visitors, size = n_ads_shown, prob = pars$proportion_clicks)  # likelihood given the rows in pars  
    * pars$probability <- pars$likelihood * pars$prior  # unscaled posterior probability  
    * pars$probability <- pars$probability / sum(pars$probability)  # normalized posterior probability  
    * pars <- pars[pars$n_visitors == 13, ]  # filter on the 13 observed clicks  
    * pars$probability <- pars$probability / sum(pars$probability)  # normalize remaining probs to 1  
  
Bayes theorem:  
  
* An example of Bayes' theorem is provided above by the multiplication of the prior and the probability  
	* pars$probability <- pars$likelihood * pars$prior  
    * pars$probability <- pars$probability / sum(pars$probability)  
    * This is an example of p(Param | Data) = P(Data | Param) * P(Param Before Seeing Data) / sum-of-all-numerators  
* The grid approximation technique was used above - cannot ever get all parameters for a continuous distribution  
* Can use mathematical notations where = is a point parameter and ~ is follows a specific distribution  
  
Example code includes:  
```{r}

prob_to_draw_ace <- 4 / 52
prob_to_draw_four_aces <- (4 / 52) * (3 / 51) * (2 / 50) * (1 / 49)


n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n = 99999, 
    size = n_ads_shown, prob = proportion_clicks)
prob_13_visitors <- sum(n_visitors == 13) / length(n_visitors)
prob_13_visitors

prob_13_visitors <- dbinom(x=13, size=n_ads_shown, prob=proportion_clicks)
prob_13_visitors


n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- 0:n_ads_shown
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
plot(x=n_visitors, y=prob, type="h")

n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 13
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
plot(x=proportion_clicks, y=prob, type="h")


n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- seq(0, 100, by = 1)
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
pars_conditioned <- pars[pars$n_visitors==6, ]
pars_conditioned$probability <- pars_conditioned$probability / sum(pars_conditioned$probability)
plot(x=pars_conditioned$proportion_clicks, y=pars_conditioned$probability, type="h")


# Simplify slightly for a known result of 6
n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 6
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
plot(pars$proportion_clicks, pars$probability, type = "h")

```
  
  
  
***
  
Chapter 5 - More Parameters, Data, and Bayes  
  
Temperature in a normal lake:  
  
* Example of having some water data temperature for a given day  
	* temp <- c(19, 23, 20, 17, 23)  
    * The normal distribution could be a good candidate for the generative function in this case - parameters with mu and sigma  
    * rnorm(n = , mean = , sd = )  
    * dnorm(x = , mean = , sd = )  
    * like <- dnorm(x = temp, mean = 20, sd = 2)  # likelihood of our observed temperatures given a mean of 20 and a standard deviation of 2  
    * prod(like)  # joint likelihood of probabilities  
    * log(like)  # addresses the problem of likelihoods being so small that computer precision becomes an issue  
  
Bayesian model of water temperature:  
  
* Can define priors such as sigma ~Uniform(0, 10) and mean ~ N(18, 5)  
* Can then run an additional grid-approximation exercise  
	* temp <- c(19, 23, 20, 17, 23)  
    * mu <- seq(8, 30, by = 0.5)  
    * sigma <- seq(0.1, 10, by = 0.3)  
    * pars <- expand.grid(mu = mu, sigma = sigma))  
    * pars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)  
    * pars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)  
    * pars$prior <- pars$mu_prior * pars$sigma_prior  
    * for(i in 1:nrow(pars)) {  
    *     likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])  
    *     pars$likelihood[i] <- prod(likelihoods)  
    * }  
    * pars$probability <- pars$likelihood * pars$prior  
    * pars$probability <- pars$probability / sum(pars$probability)  
  
Beach party implications of water temperatures:  
  
* Suppose that there is aminimum water temperature for holding a beach party and that we want the probability of exceedence of this temperature  
* It is helpful to create a frame for further analysis; random sampling, weighted by probability, can help  
	* sample_indices <- sample( 1:nrow(pars), size = 10000, replace = TRUE, prob = pars$probability)  # draw some random samples proportional to each row's probability  
    * pars_sample <- pars[sample_indices, c("mu", "sigma")]  
    * hist(pars_sample$mu, 30)  
    * pred_temp <- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma)  
    * hist(pred_temp, 30)  
    * sum(pred_temp >= 18) / length(pred_temp )  
  
Practical tool (BEST):  
  
* Models are often available off-the-shelf which can save time; one example is BEST by John Kruschke  
	* BEST assumes that data come from a t-distribution (more or less a normal distribution with the degrees of freedom added)  
    * BEST estimates standard deviation, mean, and degrees of freedom  
    * BEST uses MCMC (Markov chain Monte Carlo)  
* Can fit the data directly using BEST  
	* library(BEST)  
    * iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)  
    * fit <- BESTmcmc(iq)  
    * fit  # note that nu is the degrees of freedom  
    * plot(fit)  
  
Wrap and up and next steps:  
  
* Bayesian inference as a technique for modeling uncertainty - data, generative model, and prior probability distributions  
* Can use sampling and grid approximation to calculate probabilities and distributions  
	* Under-the-hood, used MCMC as implemented by way of BEST  
* Additional areas for exploration include full application of Bayesian approaches to time series, deep learning, and the like  
* Can also add more advanced computational models  
  
Example code includes:  
```{r}

mu <- 3500
sigma <- 600

weight_distr <- rnorm(n = 100000, mean = mu, sd = sigma)
hist(weight_distr, xlim = c(0, 6000), col = "lightgreen")


mu <- 3500
sigma <- 600

weight <- seq(0, 6000, by=100)
likelihood <- dnorm(weight, mean=mu, sd=sigma)

plot(x=weight, y=likelihood, type="h")


# The IQ of a bunch of zombies
iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
# Defining the parameter grid
pars <- expand.grid(mu = seq(0, 150, length.out = 100), 
                    sigma = seq(0.1, 50, length.out = 100))
# Defining and calculating the prior density for each parameter combination
pars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 100)
pars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)
pars$prior <- pars$mu_prior * pars$sigma_prior
# Calculating the likelihood for each parameter combination
for(i in 1:nrow(pars)) {
  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])
  pars$likelihood[i] <- prod(likelihoods)
}
# Calculating the probability of each parameter combination
pars$probability <- pars$likelihood * pars$prior / sum(pars$likelihood * pars$prior)
lattice::levelplot(probability ~ mu * sigma, data = pars)


head(pars)
sample_indices <- sample( nrow(pars), size = 10000,
    replace = TRUE, prob = pars$probability)
head(sample_indices)
pars_sample <- pars[sample_indices, c("mu", "sigma")]
hist(pars_sample$mu)
quantile(pars_sample$mu, c(0.025, 0.5, 0.975))


head(pars_sample)
pred_iq <- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma)
hist(pred_iq)
mean(pred_iq >= 60)


# The IQ of zombies on a regular diet and a brain based diet.
iq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)
iq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
mean(iq_brains) - mean(iq_regular)

# Need to load http://www.sourceforge.net/projects/mcmc-jags/files for rjags (called by BEST)
# library(BEST)
# best_posterior <- BESTmcmc(iq_brains, iq_regular)
# plot(best_posterior)

```
  
  
  
***
  
###_Categorical Data in the Tidyverse_  
  
Chapter 1 - Introduction to Factor Variables  
  
Introduction to qualitative variables:  
  
* Identifying and inspecting categorical data, using the forcats package, effective visualization  
* Qualitative data in this course will include categorical data and ordianl data  
	* Each have a fixed and known set of possible values - ordinal adds that there is an ordering, though no specific meaning such that 1 vs 2 and 2 vs 3 may be different distances  
* Categorical data is generally best converted to factors iff there is a finite set of potential values  
* Can check for factors using is.factor()  
  
Understanding your qualitative variables:  
  
* Data is from the Kaggle 2017 data science survey  
* High-level summaries include category names and levels; converting to factors can be valuable  
	* multipleChoiceResponses %>% mutate_if(is.character, as.factor)  # nice function!  Run the mutate only if the first condition (is.character) holds  
    * nlevels(multipleChoiceResponses$LearningDataScienceTime)  # number of levels  
    * levels(multipleChoiceResponses$LearningDataScienceTime)  # level names  
    * multipleChoiceResponses %>% summarise_if(is.factor, nlevels)  # run summarize only for the factors  
  
Making better plots:  
  
* Can use forcats::fct_reorder() to reorder data for plotting - fct_reorder(factor, orderingCriteria)  
	* ggplot(WorkChallenges) + geom_point(aes(x = fct_reorder(question, perc_problem), y = perc_problem))  
* Can use forcats::fct_infreq() to order based on frequency and reverse the order using forcats::fct_rev() as needed  
	* ggplot(multiple_choice_responses) + geom_bar(aes(x = fct_infreq(CurrentJobTitleSelect))  
    * ggplot(multiple_choice_responses) + geom_bar(aes(x = fct_rev(fct_infreq(CurrentJobTitleSelect))))  
  
Example code includes:  
```{r}

multiple_choice_answers <- readr::read_csv("./RInputFiles/smc_with_js.csv")

# Print out the dataset
glimpse(multiple_choice_answers)

# Check if CurrentJobTitleSelect is a factor
is.factor(multiple_choice_answers$CurrentJobTitleSelect)


# mutate() and summarise() in dplyr both have variants where you can add the suffix if, all, or at to change the operation
# mutate_if() applies a function to all columns where the first argument is true
# mutate_all() applies a function to all columns
# mutate_at() affects columns selected with a character vector or select helpers (e.g. mutate_at(c("height", "weight"), log))

# Change all the character columns to factors
responses_as_factors <- multiple_choice_answers %>%
    mutate_if(is.character, as.factor)

# Make a two column dataset with variable names and number of levels
number_of_levels <- responses_as_factors %>%
    summarise_all(nlevels) %>%
    gather(variable, num_levels)


# dplyr has two other functions that can come in handy when exploring a dataset
# The first is top_n(x, var), which gets us the first x rows of a dataset based on the value of var
# The other is pull(), which allows us to extract a column and take out the name, leaving only the value(s) from the column

# Select the 4 rows with the highest number of levels
number_of_levels %>%
    top_n(4, num_levels)
    
# How many levels does CurrentJobTitleSelect have? 
number_of_levels %>%
    filter(variable=="CurrentJobTitleSelect") %>%
    pull(num_levels)

# Get the names of the levels of CurrentJobTitleSelect
responses_as_factors %>%
    pull(CurrentJobTitleSelect) %>%
    levels()


# Make a bar plot
ggplot(multiple_choice_answers, aes(x=FormalEducation)) + 
    geom_bar() + 
    coord_flip()

# Make a bar plot
ggplot(multiple_choice_answers, aes(x=fct_rev(fct_infreq(FormalEducation)))) + 
    geom_bar() + 
    coord_flip()


multiple_choice_answers %>%
  filter(!is.na(Age) & !is.na(FormalEducation)) %>%
  group_by(FormalEducation) %>%
  summarize(mean_age = mean(Age)) %>%
  ggplot(aes(x = fct_reorder(FormalEducation, mean_age), y = mean_age)) + 
    geom_point() + 
    coord_flip()

```
  
  
  
***
  
Chapter 2 - Manipulating Factor Variables  
  
Reordering factors:  
  
* Can order by frequency or by another variable for pure categorical variables  
* For ordinal variables, typically is best to order by the implied order inside the ordinal variable - can manually enter using fct_relevel()  
	* ggplot(aes(nlp_frequency, x = fct_relevel(response, "Rarely", "Sometimes", "Often", "Most of the time"))) + geom_bar()  
    * nlp_frequency %>% pull(response) %>% levels()  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time")) %>% pull(response) %>% levels()  # This moves Often and Most of the time to the front, leaving others alone  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time", after = 2)) %>% pull(response) %>% levels()  # move these to after 2  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time", after = Inf) %>% pull(response) %>% levels()  # move this to the end  
  
Renaming factor levels:  
  
* Can convert names for factor levels using forcats::fct_recode()  
	* levels(flying_etiquette$middle_arm_rest_three)  # get the initial levels  
    * ggplot(flying_etiquette, aes(x = fct_infreq(middle_arm_rest_three))) + geom_bar() + coord_flip() + labs(x = "Arm rest opinions")  # labels are very wordy, graph is compressed  
    * flying_etiquette %>% mutate(middle_arm_rest_three = fct_recode(middle_arm_rest_three,   
    *     "Other" = "Other (please specify)", "Everyone should share" = "The arm rests should be shared",  
    *     "Aisle and window people" = "The people in the aisle and window seats get both arm rests",   
    *     "Middle person" = "The person in the middle seat gets both arm rests",  
    *     "Fastest person" = "Whoever puts their arm on the arm rest first")  
    * ) %>%   
    * count(middle_arm_rest_three)  
  
Collapsing factor levels:  
  
* Can collapse factor levels using forcats::fct_collapse()  
	* flying_etiquette %>% mutate(height = fct_collapse(height, under_5_3 = c("Under 5 ft.", "5'0\"", "5'1\"", "5'2\""), over_6_1 = c("6'1\"", "6'2\"", "6'3\"", "6'4\"", "6'5\"", "6'6\" and above"))) %>% pull(height) %>% levels()  
* Can collapse factor levels to other using forcats::fct_other()  
	* flying_etiquette %>% mutate(new_height = fct_other(height, keep = c("6'4\"", "5'1\""))) %>% count(new_height)  # will make everything other than keep in to Other  
    * flying_etiquette %>% mutate(new_height = fct_other(height, drop = c("Under 5 ft.", "5'0\"", "5'1\"", "5'2\"", "5'3\""))) %>% pull(new_height) %>% levels()  # will move the drop items to Other  
    * flying_etiquette %>% mutate(new_height = fct_lump(height, prop = .08)) %>% count(new_height)  # anything less than a proportion of 0.08 will be moved to other  
    * flying_etiquette %>% mutate(new_height = fct_lump(height, n = 3)) %>% count(new_height)  # keep the top 3 categories  
  
Example code includes:  
```{r}

multiple_choice_responses <- multiple_choice_answers

# Print the levels of WorkInternalVsExternalTools
levels(multiple_choice_responses$WorkInternalVsExternalTools)

# Reorder the levels from internal to external 
mc_responses_reordered <- multiple_choice_responses %>%
    mutate(WorkInternalVsExternalTools = fct_relevel(WorkInternalVsExternalTools, 
                                                     c('Entirely internal', 'More internal than external',
                                                       'Approximately half internal and half external', 
                                                       'More external than internal', 'Entirely external',
                                                       'Do not know'
                                                       )
                                                     )
           )

# Make a bar plot of the responses
ggplot(mc_responses_reordered, aes(x=WorkInternalVsExternalTools)) + 
    geom_bar() + 
    coord_flip()


multiple_choice_responses %>%
    # Move "I did not complete any formal education past high school" and "Some college/university study without earning a bachelor's degree" to the front
    mutate(FormalEducation = fct_relevel(FormalEducation, c("I did not complete any formal education past high school", "Some college/university study without earning a bachelor's degree"))) %>%
    # Move "Doctoral degree" to be the sixth level
    mutate(FormalEducation = fct_relevel(FormalEducation, after=6, "Doctoral degree")) %>%
    # Move "I prefer not to answer" to be the last level.
    mutate(FormalEducation = fct_relevel(FormalEducation, after=Inf, "I prefer not to answer")) %>%
    # Examine the new level order
    pull(FormalEducation) %>%
    levels()


# make a bar plot of the frequency of FormalEducation
ggplot(multiple_choice_responses, aes(x=FormalEducation)) + 
    geom_bar()


multiple_choice_responses %>%
    # rename levels
    mutate(FormalEducation = fct_recode(FormalEducation, "High school" ="I did not complete any formal education past high school", "Some college" = "Some college/university study without earning a bachelor's degree")) %>%
    # make a bar plot of FormalEducation
    ggplot(aes(x=FormalEducation)) + 
    geom_bar()


multiple_choice_responses %>%
    # Create new variable, grouped_titles, by collapsing levels in CurrentJobTitleSelect
    mutate(grouped_titles = fct_collapse(CurrentJobTitleSelect, 
        "Computer Scientist" = c("Programmer", "Software Developer/Software Engineer"), 
        "Researcher" = "Scientist/Researcher", 
        "Data Analyst/Scientist/Engineer" = c("DBA/Database Engineer", "Data Scientist", 
                                              "Business Analyst", "Data Analyst", 
                                              "Data Miner", "Predictive Modeler"))) %>%
    # Turn every title that isn't now one of the grouped_titles into "Other"
    mutate(grouped_titles = fct_other(grouped_titles, 
                             keep = c("Computer Scientist", 
                                     "Researcher", 
                                     "Data Analyst/Scientist/Engineer"))) %>% 
    # Get a count of the grouped titles
    count(grouped_titles)


multiple_choice_responses %>%
  # remove NAs of MLMethodNextYearSelect
  filter(!is.na(MLMethodNextYearSelect)) %>%
  # create ml_method, which lumps all those with less than 5% of people into "Other"
  mutate(ml_method = fct_lump(MLMethodNextYearSelect, prop=0.05)) %>%
  # print the frequency of your new variable in descending order
  count(ml_method, sort=TRUE)


multiple_choice_responses %>%
  # remove NAs 
  filter(!is.na(MLMethodNextYearSelect)) %>%
  # create ml_method, retaining the 5 most common methods and renaming others "other method" 
  mutate(ml_method = fct_lump(MLMethodNextYearSelect, 5, other_level="other method")) %>%
  # print the frequency of your new variable in descending order
  count(ml_method, sort=TRUE)

```
  
  
  
***
  
Chapter 3 - Creating Factor Variables  
  
Examining common themed variables:  
  
* Tidy data has each row as an observation and each column as a variable (generally, moving from wide to long)  
	* multipleChoiceResponses %>% select(contains("WorkChallengeFrequency")) %>% gather(work_challenge, frequency)  
    * work_challenges <- multipleChoiceResponses %>% select(contains("WorkChallengeFrequency")) %>% gather(work_challenge, frequency) %>%  
    *     mutate(work_challenge = str_remove(work_challenge, "WorkChallengeFrequency"))  # will remove the string "WorkChallengeFrequency" from column work_challenge  
* Can also convert the variables to 0/1 and then use for statistical summaries  
	* work_challenges %>% filter(!is.na(frequency)) %>% mutate(frequency = if_else( frequency %in% c("Most of the time", "Often"), 1, 0) ) %>%  
    * group_by(work_challenge) %>% summarise(perc_problem = mean(frequency))  
  
Tricks of ggplot2:  
  
* Initial plots may not look so good, for example  
	* ggplot(job_titles_by_perc, aes(x = CurrentJobTitleSelect,, y = perc_w_title)) + geom_point()  
* Can angle the tick axes for better readability  
	* ggplot(job_titles_by_perc, aes(x = CurrentJobTitleSelect, y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
* Can reorder by oreder of popularity using fct_reorder  
	* ggplot(job_titles_by_perc, aes(x = fct_reorder(CurrentJobTitleSelect, perc_w_title), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
    * ggplot(job_titles_by_perc, aes(x = fct_rev(fct_reorder(CurrentJobTitleSelect, perc_w_title)), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
* Can add axis labels and titles using labs()  
	* g <- ggplot(job_titles_by_perc, aes(x = fct_rev(fct_reorder(CurrentJobTitleSelect, perc_w_title)), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
    * g <- g + labs(x = "Job Title", y = "Percent with title")  
* Can change the y-axis ticks to be explicit percentages  
	* g + scale_y_continuous(labels = scales::percent_format())  
  
Changing and creating variables with dplyr::case_when():  
  
* Suppose that there is a simple vector to be recoded using simple business rules  
	* x <- 1:20  
    * case_when(x %% 15 == 0 ~ "fizz buzz", x %% 3 == 0 ~ "fizz", x %% 5 == 0 ~ "buzz", TRUE ~ as.character(x) )  # TRUE is the all others  
* Conditions evaluate from first to last, and the first matching condition is acted on for that value  
	* moods %>% mutate(action = case_when( mood == "happy" & status == "know it" ~ "clap your hands", mood == "happy" & status == "do not know it" ~ "stomp your feet", mood == "sad" ~ "look at puppies", TRUE ~ "jump around")  
  
Example code includes:  
```{r}

learning_platform_usefulness <- multiple_choice_responses %>%
  # select columns with LearningPlatformUsefulness in title
  select(contains("LearningPlatformUsefulness")) %>%
  # change data from wide to long
  gather(learning_platform, usefulness) %>%
  # remove rows where usefulness is NA
  filter(!is.na(usefulness)) %>%
  # remove "LearningPlatformUsefulness" from each string in `learning_platform 
  mutate(learning_platform = str_remove(learning_platform, "LearningPlatformUsefulness"))


learning_platform_usefulness %>%
  # change dataset to one row per learning_platform usefulness pair with number of entries for each
  count(learning_platform, usefulness) %>%
  # use add_count to create column with total number of answers for that learning_platform
  add_count(learning_platform, wt=n, name="nn") %>%
  # create a line graph for each question with usefulness on x-axis and percentage of responses on y
  ggplot(aes(x = usefulness, y = n/nn, group = learning_platform)) + 
  geom_line() + 
  facet_wrap(~ learning_platform)


avg_usefulness <- learning_platform_usefulness %>%
    # If usefulness is "Not Useful", make 0, else 1 
    mutate(usefulness = ifelse(usefulness=="Not Useful", 0, 1)) %>%
    # Get the average usefulness by learning platform 
    group_by(learning_platform) %>%
    summarize(avg_usefulness = mean(usefulness))

# Make a scatter plot of average usefulness by learning platform 
ggplot(avg_usefulness, aes(x=learning_platform, y=avg_usefulness)) + 
    geom_point()

ggplot(avg_usefulness, aes(x = learning_platform, y = avg_usefulness)) + 
    geom_point() + 
    # rotate x-axis text by 90 degrees
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    # rename y and x axis labels
    labs(x="Learning Platform", y="Percent finding at least somewhat useful") + 
    # change y axis scale to percentage
    scale_y_continuous(labels = scales::percent)

ggplot(avg_usefulness, 
       aes(x = fct_rev(fct_reorder(learning_platform, avg_usefulness)), y = avg_usefulness)
       ) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    labs(x = "Learning Platform", y = "Percent finding at least somewhat useful") + 
    scale_y_continuous(labels = scales::percent)


# Check the min age
min(multiple_choice_responses$Age, na.rm=TRUE)
# Check the max age
max(multiple_choice_responses$Age, na.rm=TRUE)
sum(is.na(multiple_choice_responses$Age))


multiple_choice_responses %>%
    # Eliminate any ages below 10 and above 90
    filter(between(Age, 10, 90)) %>%
    # Create the generation variable based on age
    mutate(generation=case_when(
      between(Age, 10, 22) ~ "Gen Z", 
      between(Age, 23, 37) ~ "Gen Y", 
      between(Age, 38, 52) ~ "Gen X", 
      between(Age, 53, 71) ~ "Baby Boomer", 
      between(Age, 72, 90) ~ "Silent"
    )) %>%
    # Get a count of how many answers in each generation
    count(generation)


multiple_choice_responses %>%
    # Filter out people who selected Data Scientist as their Job Title
    filter(!is.na(CurrentJobTitleSelect) & CurrentJobTitleSelect != "Data Scientist")  %>%
    # Create a new variable, job_identity
    mutate(job_identity = case_when(
        CurrentJobTitleSelect == "Data Analyst" & DataScienceIdentitySelect == "Yes" ~ "DS analysts", 
        CurrentJobTitleSelect == "Data Analyst" & DataScienceIdentitySelect %in% c("No", "Sort of (Explain more)") ~ "NDS analyst", 
        CurrentJobTitleSelect != "Data Analyst" & DataScienceIdentitySelect == "Yes" ~ "DS non-analysts", 
        TRUE ~ "NDS non analysts")
        ) %>%
    mutate(JobSat=case_when(
        is.na(JobSatisfaction) ~ NA_integer_,
        JobSatisfaction == "I prefer not to share" | JobSatisfaction == "NA" ~ NA_integer_, 
        JobSatisfaction == "1 - Highly Dissatisfied" ~ 1L, 
        JobSatisfaction == "10 - Highly Satisfied" ~ 10L, 
        TRUE ~ as.integer(JobSatisfaction))) %>%
    # Get the average job satisfaction by job_identity, removing NAs
    group_by(job_identity) %>%
    summarize(avg_js = mean(JobSat, na.rm=TRUE))

```
  
  
  
***
  
Chapter 4 - Case Study on Flight Etiquette  
  
Case study introduction:  
  
* Recreation of 538 dataset on flying etiquette  
* Need to begin by converting variable types and tidying the data and selecting key columns  
	* wide_data %>% mutate_if(is.character, as.factor)  
    * wide_data %>% gather(column, value)  
    * wide_data %>% select(contains("favorite"))  
  
Data preparation and regex:  
  
* Names will need to be changed to something more succinct for plotting  
	* gathered_data %>% distinct(response_var)  
* Regular expressions can be used in any computing language to find instances of general patterns  
	* str_detect("happy", ".")  # the . Will match anything  
    * str_detect("happy", "h.")  # TRUE  
    * str_detect("happy", "y.")  # FALSE, since nothing follows the y  
    * str_remove(string, ".*the ")  # will remove everything up to and including the followed by a space  
  
Recreating the plot:  
  
* The labs() command allows for both a caption and a subtitle  
	* ggplot(mtcars, aes(disp, mpg)) + geom_point() + labs(x = "x axis label", y = "y axis label", title = "My title", subtitle = "and a subtitle", caption = "even a caption!")  
* The geom_text() layer allows for adding the specific numbers to the plot  
	* initial_plot + geom_text(aes(label = round(mean_mpg)))  
    * initial_plot + geom_text(aes(label = round(mean_mpg), y = mean_mpg + 2))  # fix the issue where the text is on top of the tip of the bar  
* Can use the theme layer to modify the non-data layers of the plot  
	* initial_plot + geom_text(aes(label = round(mean_mpg), y = mean_mpg + 2)) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())  # get rid of x and y ticks  
  
End of course recap:  
  
* Basic forcats functions  
* Tidyverse functions  
* ggplot2 tricks  
* Recreating the 538 plot for airplane rudeness behaviors  
  
Example code includes:  
```{r}

flying_etiquette <- read.csv("./RInputFiles/flying-etiquette.csv", stringsAsFactors = FALSE)
names(flying_etiquette) <- 
    stringr::str_replace_all(stringr::str_replace_all(names(flying_etiquette), "\\.", " "), "  ", " ")
names(flying_etiquette) <- stringr::str_trim(names(flying_etiquette))
names(flying_etiquette)[2:22] <- paste0(names(flying_etiquette)[2:22], "?")
names(flying_etiquette) <- stringr::str_replace_all(names(flying_etiquette), "itrude", "it rude")
glimpse(flying_etiquette)


gathered_data <- flying_etiquette %>%
    mutate_if(is.character, as.factor) %>%
    filter(`How often do you travel by plane?` != "Never") %>%
    # Select columns containing "rude"
    select(contains("rude")) %>%
    # Change format from wide to long
    gather(response_var, value)


rude_behaviors <- gathered_data %>%
    mutate(response_var = str_replace(response_var, '.*rude to ', '')) %>%
    mutate(response_var = str_replace(response_var, 'on a plane', '')) %>%
    mutate(rude = if_else(value %in% c("No, not rude at all", "No, not at all rude"), 0, 1)) %>%
    # Create perc_rude, the percent considering each behavior rude
    group_by(response_var) %>%
    summarize(perc_rude=mean(rude))

rude_behaviors


# Create an ordered by plot of behavior by percentage considering it rude
initial_plot <- ggplot(rude_behaviors, aes(x=fct_reorder(response_var, perc_rude), y=perc_rude)) +
geom_col()

# View your plot
initial_plot


titled_plot <- initial_plot + 
    # Add the title, subtitle, and caption
    labs(title = "Hell Is Other People In A Pressurized Metal Tube",
         subtitle = "Percentage of 874 air-passenger respondents who said action is very or somewhat rude",
         caption = "Source: SurveyMonkey Audience", 
         # Remove the x- and y-axis labels
         x="",
         y=""
         ) 

titled_plot


flipped_plot <- titled_plot + 
    # Flip the axes
    coord_flip() + 
    # Remove the x-axis ticks and labels
    theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank())

flipped_plot + 
    # Add labels above the bar with the perc value
    geom_text(aes(label = paste0(round(100*perc_rude), "%"), y = perc_rude + .03), 
              position = position_dodge(0.9), vjust = 1)

```
  
  
  
***
  
###_Bayesian Modeling with RJAGS_  
  
Chapter 1 - Introduction to Bayesian Modeling  
  
Prior model:  
  
* Goals include foundational Bayesian models such as Beta-Binomial, Normal-Normal, and Bayesian regression  
	* Define, compile, simulate using RJAGS  
    * Conduct Bayesian posterior inference using RJAGS  
* Example of election poll - there is some uncertainty around the polling figures that have been released  
	* An additional poll might tend to update the prior model that was built using the previous elections data  
    * Bayesian posterior models are a powerful means of combining priors and data  
* The prior model depends on some specific assumptions and notations  
	* Suppose that p is the percentage of people who support you - assumed to be a random variable between 0 and 1  
    * The prior model for p is p ~ Beta(45, 55)  
    * The beta model can be tuned from pessimism Beta(1, 5) to complete uncertainty Beta(1, 1)  
  
Data and likelihood:  
  
* Suppose that a candidate running for election does a small poll and finds 6 of 10 plan to vote for them  
	* Can integrate even a small data sample like this with assumptions and priors  
    * Assumptions might include that voters are independent and p is a global probability that any given voter supports you  
    * Then, X ~ Bin(n, p) which is to say that X can be defined as the number of n polled voters that support you (assuming per above global probability p)  
* There is a dependence between X, n, and p, thus the data can help to assess how likely each of the values of p may be given that you observed X in n  
	* The likelihood function is the likelihood of observing X given that p takes on a specific value  
  
Posterior model:  
  
* The prior and the likelihood can be integrated to form the posterior  
	* The prior is knowledge that exists before data and the likelihood is what exists based on the data  
    * Multiply prior and likelihood and then scale so probabilities add to 1  
* The RJAGS package combines R with JAGS (Just Another Gibbs Sampler) - requires downloading JAGS and then loading rjags  
* Can define the model within RJAGS, for example  
	* vote_model <- "model{  
    *     # Likelihood model for X
    *     X ~ dbin(p, n)  # order of n and p are reversed (known difference for RJAGS)
    *     # Prior model for p
    *     p ~ dbeta(a, b)
    * }"  
    * vote_jags_A <- jags.model(textConnection(vote_model), data = list(a = 45, b = 55, X = 6, n = 10), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))  
    * vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)  # variable.names is the variable of interest, which is p in this case  
    * plot(vote_sim, trace = FALSE)  
  
Example code includes:  
```{r eval=FALSE}

# Make sure you have installed JAGS-4.x.y.exe (for any x >=0, y>=0) from http://www.sourceforge.net/projects/mcmc-jags/files

# Sample 10000 draws from Beta(45,55) prior
prior_A <- rbeta(n = 10000, shape1 = 45, shape2 = 55)

# Store the results in a data frame
prior_sim <- data.frame(prior_A)

# Construct a density plot of the prior sample
ggplot(prior_sim, aes(x = prior_A)) + 
    geom_density()    


# Sample 10000 draws from the Beta(1,1) prior
prior_B <- rbeta(n = 10000, shape1 = 1, shape2 = 1)    

# Sample 10000 draws from the Beta(100,100) prior
prior_C <- rbeta(n = 10000, shape1 = 100, shape2 = 100)

# Combine the results in a single data frame
prior_sim <- data.frame(samples = c(prior_A, prior_B, prior_C),
        priors = rep(c("A","B","C"), each = 10000))

# Plot the 3 priors
ggplot(prior_sim, aes(x = samples, fill = priors)) + 
    geom_density(alpha = 0.5)


# Define a vector of 1000 p values    
p_grid <- seq(0, 1, length.out=1000)

# Simulate 1 poll result for each p in p_grid   
poll_result <- rbinom(1000, 10, prob=p_grid)

# Create likelihood_sim data frame
likelihood_sim <- data.frame(p_grid, poll_result)    

# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) + 
    ggridges::geom_density_ridges()


# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result, fill = poll_result==6)) + 
    ggridges::geom_density_ridges()

# Keep the polls with X = 6    
likelihood_sim_6 <- likelihood_sim %>%     
    filter(poll_result==6)    

# Construct a density plot of the remaining p_grid values
ggplot(likelihood_sim_6, aes(x = p_grid)) + 
    geom_density() + 
    lims(x = c(0,1))


# DEFINE the model
vote_model <- "model{
    # Likelihood model for X
    X ~ dbin(p, n)
    
    # Prior model for p
    p ~ dbeta(a, b)
}"

# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE)


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))

```
  
  
  
***
  
Chapter 2 - Bayesian Models and Markov Chains  
  
Normal-Normal Model:  
  
* Example of reaction times in a sleep deprivation study  
	* Y(i) ~ N(m, s**2) meaning change in reaction time for subject I, assumed to be normally distributed as N(m, s**2)  
    * Prior information is that normal reaction time is 250 and is expected to increase by 0-150 after sleep deprivation (scale of the prior)  
    * The prior for the mean might then be defined as 50 ms increase in reaction time with a standard deviation of 25  
    * The prior for the standard deviation might then be uniform on 0-200  
* Overall formulation of the sleep study model includes  
	* Y(i) ~ N(m, s**2)  
    * m ~ N(50, 25**2)  
    * s ~ Unif(0, 200)  
  
Simulating Normal-Normal in RJAGS:  
  
* Posterior insights are based on the product of the prior and the data, which can be simulated using RJAGS  
	* sleep_model <- "model{  
    *     # Likelihood model for Y[i]
    *     for(i in 1:length(Y)) {
    *         Y[i] ~ dnorm(m, s^(-2))  # requires precision which can be defined as the inverse of sigma-squared  
    *     }  
    *     # Prior models for m and s  
    *     m ~ dnorm(50, 25^(-2))  # requires precision which can be defined as the inverse of sigma-squared  
    *     s ~ dunif(0, 200)  
    * }"  
    * sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))  
    * sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)  
  
Markov chains:  
  
* The RJAGS approach is approximating parameters based on Monte Carlo Markov Chains (MCMC)  
	* The goal of RJAGS is to use Markov chains to estimate (approximate) posteriors that would otherwise be too complicated to model  
* Each iteration of a Markov chain depends on the previous iteration (the iterations are not entirely random or independent)  
	* Over time, the Markov chain explores the sample space, but the exploration is often over a smaller range for smaller intervals (there are auto-corelations)  
    * The overall distribution of the Markov chain mimics a random sampling drawn from the posterior distribution  
  
Markov chain diagnostics and reproducibility:  
  
* The trace plots indicate the longitudinal behavior of the chain while the density plots indicate the distribution of the chain  
* Questions about what makes for a good chain (convergence, trials needed, etc.)  
	* Stability is good - long-run trends should be stabilized  
    * Multiple parallel chains should return very similar results with similar features (should not be overly dependent on RNG/seed)  
    * summary(sleep_sim)  # provides key Markov chain diagnostics  
* Generally, problems with Markov chains can be addressed with longer chains (more iterations)  
* Helpful to set the seed and RNG in the model for reproducibility  
	* inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)  # example inside jags.model()  
  
Example code includes:  
```{r eval=FALSE}

# Take 10000 samples from the m prior
prior_m <- rnorm(10000, 50, 25)

# Take 10000 samples from the s prior    
prior_s <- runif(10000, 0, 200)

# Store samples in a data frame
samples <- data.frame(prior_m, prior_s)

# Density plots of the prior_m & prior_s samples    
ggplot(samples, aes(x = prior_m)) + 
    geom_density()
ggplot(samples, aes(x = prior_s)) + 
    geom_density()


# Check out the first 6 rows of sleep_study
head(sleep_study)

# Define diff_3
sleep_study <- sleep_study %>%
  mutate(diff_3=day_3-day_0)

# Histogram of diff_3    
ggplot(sleep_study, aes(x = diff_3)) + 
    geom_histogram(binwidth = 20, color = "white")

# Mean and standard deviation of diff_3    
sleep_study %>%
  summarize(mean(diff_3), sd(diff_3))


# DEFINE the model    
sleep_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m, s^(-2))
    }

    # Prior models for m and s
    m ~ dnorm(50, 25^(-2))
    s ~ dunif(0, 200)
}"    

# COMPILE the model
sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))    

# SIMULATE the posterior    
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# PLOT the posterior    
plot(sleep_sim, trace = FALSE)    


# Let m be the average change in reaction time after 3 days of sleep deprivation
# In a previous exercise, you obtained an approximate sample of 10,000 draws from the posterior model of m
# You stored the resulting mcmc.list object as sleep_sim which is loaded in your workspace:
# In fact, the sample of m values in sleep_sim is a dependent Markov chain, the distribution of which converges to the posterior
# You will examine the contents of sleep_sim and, to have finer control over your analysis, store the contents in a data frame

# Check out the head of sleep_sim
head(sleep_sim)

# Store the chains in a data frame
sleep_chains <- data.frame(sleep_sim[[1]], iter = 1:10000)

# Check out the head of sleep_chains
head(sleep_chains)


# NOTE: The 10,000 recorded Iterations start after a "burn-in" period in which samples are discarded
# Thus the Iterations count doesn't start at 1!

# Use plot() to construct trace plots of the m and s chains
plot(sleep_sim, density=FALSE)

# Use ggplot() to construct a trace plot of the m chain
ggplot(sleep_chains, aes(x = iter, y = m)) + 
    geom_line()

# Trace plot the first 100 iterations of the m chain
ggplot(dplyr::filter(sleep_chains, iter<=100), aes(x = iter, y = m)) + geom_line()

# Note that the longitudinal behavior of the chain appears quite random and that the trend remains relatively constant
# This is a good thing - it indicates that the Markov chain (likely) converges quickly to the posterior distribution of m


# Use plot() to construct density plots of the m and s chains
plot(sleep_sim, trace=FALSE)

# Use ggplot() to construct a density plot of the m chain
ggplot(sleep_chains, aes(x = m)) + 
    geom_density()

# Density plot of the first 100 values in the m chain
ggplot(dplyr::filter(sleep_chains, iter<=100), aes(x = m)) + 
    geom_density()


# COMPILE the model
sleep_jags_multi <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), n.chains=4)   

# SIMULATE the posterior    
sleep_sim_multi <- coda.samples(model = sleep_jags_multi, variable.names = c("m", "s"), n.iter = 1000)

# Check out the head of sleep_sim_multi
head(sleep_sim_multi)

# Construct trace plots of the m and s chains
plot(sleep_sim_multi, density=FALSE)


# The mean of the m Markov chain provides an estimate of the posterior mean of m
# The naive standard error provides a measure of the estimate's accuracy.

# Suppose your goal is to estimate the posterior mean of m within a standard error of 0.1 ms
# If the observed naive standard error exceeds this target, no problem!
# You can simply run a longer chain


# SIMULATE the posterior    
sleep_sim_1 <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 1000)

# Summarize the m and s chains of sleep_sim_1
summary(sleep_sim_1)

# RE-SIMULATE the posterior    
sleep_sim_2 <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# Summarize the m and s chains of sleep_sim_2
summary(sleep_sim_2)


# COMPILE the model
sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)) 

# SIMULATE the posterior    
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# Summarize the m and s chains of sleep_sim
summary(sleep_sim)

```
  
  
  
***
  
Chapter 3 - Bayesian Inference and Prediction
  
Simple Bayesian Regression Model:  
  
* The simple Bayesian regression lays the ground work for more complicated modeling built on it  
* Suppose that the goal is to model human weights, and that they are N(m, s**2)  
	* Y(i) ~ N(m(i), s**2)  # m(i) is an average weight that depends on height  
    * m(i) = a + b*X(i) where X(i) is the height of individual i  
* Can specify Bayesian model with priors  
	* a = intercept ~ N(0, 200**2)  
    * b = slope (expected to be positive) ~ N(1, 0.5**2)  
    * s = residual standard deviation ~ Unif(0, 20)  
  
Bayesian Regression in RJAGS:  
  
* The basic lm() regression will give the parameters based on linear regression  
* Can instead define the Bayesian linear regression for RJAGS  
	* Within RJAGS, [i] means that it varies for each subject, i  
    * Within RJAGS, <- means there is an exact mathematical formula that does not need to be estimated  
    * weight_model <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b * X[i]  
    *     }  
    *     # Prior models for a, b, s  
    *     a ~ dnorm(0, 200^(-2))  
    *     b ~ dnorm(1, 0.5^(-2))  
    *     s ~ dunif(0, 20)  
    * }"  
    * weight_jags <- jags.model(textConnection(weight_model), data = list(X = bdims$hgt, Y = bdims$wgt), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 2018))  
    * weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 10000)  
* Options for addressing the Markov chain instability include  
	* Standardize the height predictor  
    * Increase the chain length  
  
Posterior estimation and inference:  
  
* Bayesian regression using RJAGS provided estimates for the slope and intercept parameters  
* The posterior densities can be summarized for better communication - for example, point estimates based on means  
* Can also plot the lines corresponding to each pair of slope, intercepts - can further create "credible intervals"  
	* The 95% credible intervals are the middle 95% of the densities for each of the parameters (slope and intercept)  
* Can also assess frequencies of exceedence for parameters  
	* table(weight_chains$b > 1.1)  
  
Posterior prediction:  
  
* Based on simulations, the posterior mean trend was estimated  
	* Can use the final coefficients to make estimates about the population  
    * Could instead model the regression based on each set of coefficients included in the chain, including calculating the credible interval  
* Rather than using the regression to find means, the goal may be to predict an individual  
	* Plug in the data as per finding the mean  
    * The residual standard deviation (s) can then be used inside chain, using all of the a, b, s, data  
  
Example code includes:  
```{r eval=FALSE}

# Note the 3 parameters in the model of weight by height: intercept a, slope b, & standard deviation s
# In the first step of your Bayesian analysis, you will simulate the following prior models for these parameters: a ~ N(0, 200^2), b ~ N(1, 0.5^2), and s ~ Unif(0, 20)

# Take 10000 samples from the a, b, & s priors
prior_a <- rnorm(10000, 0, 200)
prior_b <- rnorm(10000, 1, 0.5)
prior_s <- runif(10000, 0, 20)

# Store samples in a data frame
samples <- data.frame(prior_a, prior_b, prior_s, set=1:10000)

# Construct density plots of the prior samples    
ggplot(samples, aes(x = prior_a)) + 
    geom_density()
ggplot(samples, aes(x = prior_b)) + 
    geom_density()
ggplot(samples, aes(x = prior_s)) + 
    geom_density()


# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE)) 

# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>% 
    mutate(height = rnorm(600, 170, 10)) %>% 
    mutate(weight = rnorm(600, prior_a + prior_b*height, prior_s))

# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE, size = 0.75) + 
    facet_wrap(~ set)


# The bdims data set from the openintro package is loaded in your workspace
# bdims contains physical measurements on a sample of 507 individuals, including their weight in kg (wgt) and height in cm (hgt)

# Construct a scatterplot of wgt vs hgt
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point()

# Add a model smooth
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
    
# Obtain the sample regression model
wt_model <- lm(wgt ~ hgt, data = bdims)

# Summarize the model
summary(wt_model)


# DEFINE the model    
weight_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m[i], s^(-2))
        m[i] <- a + b * X[i]
    }

    # Prior models for a, b, s
    a ~ dnorm(0, 200^(-2))
    b ~ dnorm(1, 0.5^(-2))
    s ~ dunif(0, 20)
}"

# COMPILE the model
weight_jags <- jags.model(textConnection(weight_model), data = list(X=bdims$hgt, Y=bdims$wgt), 
                  inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

# COMPILE the model
weight_jags <- jags.model(textConnection(weight_model), data = list(Y = bdims$wgt, X = bdims$hgt), 
                          inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

# SIMULATE the posterior    
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 1000)

# PLOT the posterior    
plot(weight_sim)


# A 100,000 iteration RJAGS simulation of the posterior, weight_sim_big, is in your workspace along with a data frame of the Markov chain output:
head(weight_chains, 2)

# The posterior means of the intercept & slope parameters, a & b, reflect the posterior mean trend in the relationship between weight & height
# In contrast, the full posteriors of a & b reflect the range of plausible parameters, thus posterior uncertainty in the trend
# You will examine the trend and uncertainty in this trend below
# The bdims data are in your workspace

# Summarize the posterior Markov chains
summary(weight_sim_big)

# Calculate the estimated posterior mean of b
mean(weight_chains$b)

# Plot the posterior mean regression model
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red")

# Visualize the range of 20 posterior regression models
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = weight_chains$a[1:20], slope = weight_chains$b[1:20], color = "gray", size = 0.25)


# Summarize the posterior Markov chains
summary(weight_sim_big)

# Calculate the 95% posterior credible interval for b
quantile(weight_chains$b, c(0.025, 0.975))

# Calculate the 90% posterior credible interval for b
quantile(weight_chains$b, c(0.05, 0.95))

# Mark the 90% credible interval 
ggplot(weight_chains, aes(x = b)) + 
    geom_density() + 
    geom_vline(xintercept = quantile(weight_chains$b, c(0.05, 0.95)), color = "red")


# Mark 1.1 on a posterior density plot for b
ggplot(weight_chains, aes(x=b)) + 
    geom_density() + 
    geom_vline(xintercept = 1.1, color = "red")

# Summarize the number of b chain values that exceed 1.1
table(weight_chains$b > 1.1)

# Calculate the proportion of b chain values that exceed 1.1 
mean(weight_chains$b > 1.1)


# Calculate the trend under each Markov chain parameter set
weight_chains <- weight_chains %>% 
    mutate(m_180 = a + b*180)

# Construct a posterior density plot of the trend
ggplot(weight_chains, aes(x = m_180)) + 
    geom_density() 

# Calculate the average trend
mean(weight_chains$m_180)

# Construct a posterior credible interval for the trend
quantile(weight_chains$m_180, c(0.025, 0.975))


# Simulate 1 prediction under the first parameter set
rnorm(1, mean=weight_chains$m_180[1], sd=weight_chains$s[1])

# Simulate 1 prediction under the second parameter set
rnorm(1, mean=weight_chains$m_180[2], sd=weight_chains$s[2])

# Simulate & store 1 prediction under each parameter set
weight_chains <- weight_chains  %>% 
    mutate(Y_180=rnorm(nrow(weight_chains), mean=m_180, sd=s))

# Print the first 6 parameter sets & predictions
head(weight_chains)


# Construct a density plot of the posterior predictions
ggplot(weight_chains, aes(x=Y_180)) + 
    geom_density() + 
    geom_vline(xintercept = quantile(weight_chains$Y_180, c(0.025, 0.975)), color = "red")

# Construct a posterior credible interval for the prediction
quantile(weight_chains$Y_180, c(0.025, 0.975))

# Visualize the credible on a scatterplot of the data
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red") + 
    geom_segment(x = 180, xend = 180, y = quantile(weight_chains$Y_180, c(0.025)), yend = quantile(weight_chains$Y_180, c(0.975)), color = "red")

```
  
  
  
***
  
Chapter 4 - Multivariate and Generalized Linear Models  
  
Bayesian regression with categorical predictor:  
  
* Can incorporate categorical predictors in to the Bayesian regressions  
* Example of usage of a rail-trail in MA  
	* Y[i] ~ N(m[i], s**2) where [i] is the day - assumed to have varying means but constant standard deviations  
    * X[i] is a 1/0 variable where 1 is for weekdays and 0 is for weekends  
    * m[i] = a + b*X[i], meaning that a is the typical weekend volume and a+b is the typical weekday volume  
    * The prior will be a ~ N(400, 100**2) and b ~ N(0, 200**2) and s ~ Unif(0, 200)  
* Can then define the model using RJAGS  
	* rail_model_1 <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b[X[i]]  
    *     }  
    *     # Prior models for a, b, s  
    *     a ~ dnorm(400, 100^(-2))  
    *     s ~ dunif(0, 200)  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 200^(-2))  
    * }"  
    * Note that b[1] <- 0 is because we want m[i] = a for the reference level; b[2], the second level, is what we want to model  
  
Multivariate Bayesian regression:  
  
* Bayesian models can be generalized to multivariate models, for example  
	* Y[i] ~ N(m[i], s**2) where [i] is the day - assumed to have varying means but constant standard deviations  
    * X[i] is a 1/0 variable where 1 is for weekdays and 0 is for weekends  
    * Z[i] is the high temperatue on day [i] in degrees F  
    * m[i] = a + b*X[i] + c*Z[i]  
    * a ~ N(0, 200**2), b ~ N(0, 200**2), c ~ N(0, 20**2), s ~ Unif(0, 200)  
* Can then define and simulate this model using RJAGS  
	* rail_model_2 <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b[X[i]] + c * Z[i]  
    *     }  
    *     # Prior models for a, b, c, s  
    *     a ~ dnorm(0, 200^(-2))  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 200^(-2))  
    *     c ~ dnorm(0, 20^(-2))  
    *     s ~ dunif(0, 200)  
    * }"  
  
Bayesian Poisson regression:  
  
* Can generalize regression techniques to non-normalized settings, such as Poisson regressions  
* Bicycle riders per day might better be modeled as a Poisson - should be discrete and non-negative, for example  
	* Y ~ Pois(lambda[i])  
    * log(lambda[i]) = a + b * X[i] + c * Z[i]  
    * a ~ N(0, 200**2)  
    * b ~ N(0, 2**2)  
    * c ~ N(0, 2**2)  
* Can then define the model within RJAGS  
	* poisson_model <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dpois(l[i])  
    *         log(l[i]) <- a + b[X[i]] + c*Z[i]  
    *     }  
    *     # Prior models for a, b, c  
    *     a ~ dnorm(0, 200^(-2))  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 2^(-2))  
    *     c ~ dnorm(0, 2^(-2))  
    * }"  
* Caveat for the Poisson is the mean and variance should be roughly equal; might accept some imperfections of dispersions  
  
Wrap up:  
  
* Bayesian modeling has grown in popularity along with computing resources  
* RJAGS allows for defining, compiling, and simulating Bayesian models  
* Intutive posterior inference, including credible intervals  
* Generalizing from normal models to Poisson models  
  
Example code includes:  
```{r eval=FALSE}

# Confirm that weekday is a factor variable
is.factor(RailTrail$weekday)

# Construct a density plot of volume by weekday
ggplot(RailTrail, aes(x = volume, fill = weekday)) + 
    geom_density(alpha = 0.5)

# Calculate the mean volume on weekdays vs weekends
RailTrail %>%
  group_by(weekday) %>%
  summarize(mean(volume))


# DEFINE the model    
rail_model_1 <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
      Y[i] ~ dnorm(m[i], s^(-2))
      m[i] <- a + b[X[i]]
    }
    
    # Prior models for a, b, s
    a ~ dnorm(400, 100^(-2))
    b[1] <- 0
    b[2] ~ dnorm(0, 200^(-2))
    s ~ dunif(0, 200)
}"

# COMPILE the model
rail_jags_1 <- jags.model(textConnection(rail_model_1), 
    data = list(Y=RailTrail$volume, X=RailTrail$weekday),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
    )  

# COMPILE the model
rail_jags_1 <- jags.model(textConnection(rail_model_1), data = list(Y = RailTrail$volume, X = RailTrail$weekday), 
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10))

# SIMULATE the posterior    
rail_sim_1 <- coda.samples(model = rail_jags_1, variable.names = c("a", "b", "s"), n.iter = 10000)

# Store the chains in a data frame
rail_chains_1 <- data.frame(rail_sim_1[[1]])

# PLOT the posterior    
plot(rail_sim_1)


# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_1$'b.2.' < 0)

# Construct a chain of values for the typical weekday volume
rail_chains_1 <- rail_chains_1 %>% 
    mutate(weekday_mean = a + b.2.)

# Construct a density plot of the weekday chain
ggplot(rail_chains_1, aes(x=weekday_mean)) +
  geom_density()

# 95% credible interval for typical weekday volume
quantile(rail_chains_1$weekday_mean, c(0.025, 0.975))


# Construct a plot of volume by hightemp & weekday
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point()

# Construct a sample model
rail_lm <- lm(volume ~ weekday + hightemp, data=RailTrail)

# Summarize the model
summary(rail_lm)

# Superimpose sample estimates of the model lines
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = coef(rail_lm)["(Intercept)"], slope = coef(rail_lm)["hightemp"], color = "red") +
    geom_abline(intercept = sum(coef(rail_lm)[c("(Intercept)", "weekdayTRUE")]), slope = coef(rail_lm)["hightemp"], color = "turquoise3")


# DEFINE the model    
rail_model_2 <- "model{
  # Likelihood model for Y[i]
  for(i in 1:length(Y)){
    Y[i] ~ dnorm(m[i], s^(-2))
    m[i] <- a + b[X[i]] + c * Z[i]
  }
    
  # Prior models for a, b, c, s
  a ~ dnorm(0, 200^(-2))
  b[1] <- 0
  b[2] ~ dnorm(0, 200^(-2))
  c ~ dnorm(0, 20^(-2))
  s ~ dunif(0, 200)
}"

# COMPILE the model
rail_jags_2 <- jags.model(textConnection(rail_model_2), 
                          data = list(Y=RailTrail$volume, X=RailTrail$weekday, Z=RailTrail$hightemp),
                          inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
                          )

# SIMULATE the posterior    
rail_sim_2 <- coda.samples(model = rail_jags_2, variable.names = c("a", "b", "c", "s"), n.iter = 10000)

# Store the chains in a data frame
rail_chains_2 <- data.frame(rail_sim_2[[1]])

# PLOT the posterior    
plot(rail_sim_2)


# Summarize the posterior Markov chains
summary(rail_sim_2)

# Plot the posterior mean regression models
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = mean(rail_chains_2[, "a"]), slope = mean(rail_chains_2[, "c"]), color = "red") + 
    geom_abline(intercept = mean(rail_chains_2[, "a"]) + mean(rail_chains_2[, "b.2."]), slope = mean(rail_chains_2[, "c"]), color = "turquoise3")
  
# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_2$'b.2.' < 0)


# DEFINE the model    
poisson_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dpois(l[i])
        log(l[i]) <- a + b[X[i]] + c * Z[i]
    }

    # Prior models for a, b, c
    a ~ dnorm(0, 200^(-2))
    b[1] <- 0
    b[2] ~ dnorm(0, 2^(-2))
    c ~ dnorm(0, 2^(-2))
}" 

# COMPILE the model
poisson_jags <- jags.model(textConnection(poisson_model), 
                           data = list(Y=RailTrail$volume, X=RailTrail$weekday, Z=RailTrail$hightemp),
                           inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
                           )

# SIMULATE the posterior    
poisson_sim <- coda.samples(model = poisson_jags, variable.names = c("a", "b", "c"), n.iter = 10000)

# Store the chains in a data frame
poisson_chains <- data.frame(poisson_sim[[1]])

# PLOT the posterior    
plot(poisson_sim)


# Summarize the posterior Markov chains
summary(poisson_sim)

# Plot the posterior mean regression models
ggplot(RailTrail, aes(x = hightemp, y = volume, color = weekday)) + 
    geom_point() + 
    stat_function(fun = function(x){exp(5.01352 + 0.01426 * x)}, color = "red") + 
    stat_function(fun = function(x){exp(5.01352 - 0.12800 + 0.01426 * x)}, color = "turquoise3")


# Calculate the typical volume on 80 degree weekends & 80 degree weekdays
poisson_chains <- poisson_chains %>% 
    mutate(l_weekend=exp(a + c*80)) %>% 
    mutate(l_weekday=exp(a + b.2. + c*80))

# Construct a 95% CI for typical volume on 80 degree weekend
quantile(poisson_chains$l_weekend, c(0.025, 0.975))

# Construct a 95% CI for typical volume on 80 degree weekday
quantile(poisson_chains$l_weekday, c(0.025, 0.975))


# Simulate weekend & weekday predictions under each parameter set
poisson_chains <- poisson_chains %>% 
    mutate(Y_weekend=rpois(nrow(poisson_chains), l_weekend)) %>% 
    mutate(Y_weekday=rpois(nrow(poisson_chains), l_weekday))
    
# Print the first 6 sets of parameter values & predictions
head(poisson_chains)

# Construct a density plot of the posterior weekday predictions
ggplot(poisson_chains, aes(x=Y_weekday)) +
  geom_density()

# Posterior probability that weekday volume is less 400
mean(poisson_chains$Y_weekday < 400)

```
  
  
  
***
  
###_Parallel Programming in R_  
  
Chapter 1 - Can I run my application in parallel?  
  
Partitioning problems in to independent pieces:  
  
* Course contents include  
	* Methods of parallel programming and R packages for support  
    * The parallel package in R  
    * Packages foreach and future.apply  
    * Random numbers and reproducibility  
* Programs can be partitioned either by tasks (e.g., birth model, death model, migration model) or by data (chunks of data passed to a routine, such as rowSums to a matrix)  
	* Many independent tasks are referred to as "embarassingly parallel", and this is common to statistical simulations  
  
Models of parallel computing:  
  
* Available hardware drives the ability to split components - # CPU, Memory (including shared memory or distributed memory)  
	* Message passing software runs on distributed memory and allows for fully independent processes  
    * Shared memory allows for easier passing of data  
* Programming paradigms include master-worker and map-reduce (Hadoop or Scala or the like)  
* This course will cover the master-worker model  
	* The master process creates processes for the workers and then compiles the results that the workers return  
  
R packages for parallel computing:  
  
* The R core package parallel allows for code to be independent of other packages  
* Can instead work with iotools and sparklyr for working with the map-reduce process  
	* Further, pbdR allows for many parallel approaches within R  
* The master-worker paradigms can be implemented using foreach, future.apply, snow, snowFT, snowfall, future (currently under active development, more modern)  
* The parallel package can be used for basic parallel tasks  
	* ncores <- parallel::detectCores(logical = FALSE)  
    * cl <- parallel::makeCluster(ncores)  
    * parallel::clusterApply(cl, x = ncores:1, fun = rnorm)  # x is passed to the workers in order, so worker 1 will get ronorn(ncores)  
    * parallel::stopCluster(cl)  
  
Example code includes:  
```{r}

extract_words <- function(book_name) {
    # extract the text of the book
    text <- subset(austen_books(), book == book_name)$text
    # extract words from the text and convert to lowercase
    str_extract_all(text, boundary("word")) %>% unlist %>% tolower
}

janeausten_words <- function() {
    # Names of the six books contained in janeaustenr
    books <- austen_books()$book %>% unique %>% as.character
    # Vector of words from all six books
    words <- sapply(books, extract_words) %>% unlist
    words
}

austen_books <- function () 
{
    books <- list('Sense & Sensibility' = janeaustenr::sensesensibility, 
                  'Pride & Prejudice' = janeaustenr::prideprejudice, 
                  'Mansfield Park' = janeaustenr::mansfieldpark, 
                  'Emma' = janeaustenr::emma, 
                  'Northanger Abbey' = janeaustenr::northangerabbey, 
                  'Persuasion' = janeaustenr::persuasion
                  )
    ret <- data.frame(text = unlist(books, use.names = FALSE), stringsAsFactors = FALSE)
    ret$book <- factor(rep(names(books), sapply(books, length)))
    ret$book <- factor(ret$book, levels = unique(ret$book))
    structure(ret, class = c("tbl_df", "tbl", "data.frame"))
}

max_frequency <- function(letter, words, min_length = 1) {
    w <- select_words(letter, words = words, min_length = min_length)
    frequency <- table(w)    
    frequency[which.max(frequency)]
}

select_words <- function(letter, words, min_length = 1) {
    min_length_words <- words[nchar(words) >= min_length]
    grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Vector of words from all six books
words <- janeausten_words()

# Most frequent "a"-word that is at least 5 chars long
max_frequency(letter = "a", words = words, min_length = 5)

# Partitioning
result <- lapply(letters, FUN=max_frequency,
                 words = words, min_length = 5) %>% unlist()

# barplot of result
barplot(result, las = 2)


replicates <- 50
sample_size <- 10000

# Function that computes mean of normal random numbers
myfunc <- function(n, ...) mean(rnorm(n, ...))

# Init result, set seed & repeat the task sequentially
result <- rep(NA, replicates)
set.seed(123)
for(iter in 1:replicates) result[iter] <- myfunc(sample_size)

# View result
hist(result)

# Use sapply() with different distribution parameters
hist(sapply(rep(sample_size, replicates), FUN=myfunc, mean = 10, sd = 5))


# We'll now introduce a demographic model to be used throughout the course. It projects net migration rates via an AR(1) model, rate(t+1) -  = ?(rate(t) -) + error with variance s2
# An MCMC estimation for the USA resulted in 1000 samples of parameters , ? and s
# The task is to project the future distribution of migration rates

ar1_trajectory <- function(est, rate0, len = 15) {
    trajectory <- rep(NA, len)
    rate <- rate0
    for (time in seq_len(len)) {
        trajectory[time] <- ar1(est, r = rate)
        rate <- trajectory[time]
    }
    trajectory
}

ar1 <- function(est, r) {
    est['mu'] + est['phi'] * (r - est['mu']) + 
        rnorm(1, sd = est['sigma'])
}

ar1_block <- function(id, rate0 = 0.015, traj_len = 15, block_size = 10) {
    trajectories <- matrix(NA, nrow = block_size, ncol = traj_len)
    for (i in seq_len(block_size)) 
        trajectories[i,] <- ar1_trajectory(unlist(ar1est[id, ]), rate0 = rate0, len = traj_len)
    trajectories
}

show_migration <- function(trajs) {
    df <- data.frame(time = seq(2020, by = 5, len = ncol(trajs)),
                     migration_rate = apply(trajs, 2, median),
                     lower = apply(trajs, 2, quantile, 0.1),
                     upper = apply(trajs, 2, quantile, 0.9)
                    )
    g <- ggplot(df, aes(x = time, y = migration_rate)) + 
        geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey70") + 
        geom_line()
    print(g)
}


# Simulate from multiple rows of the estimation dataset
ar1_multblocks <- function(ids, ...) {
    trajectories <- NULL
    for (i in seq_along(ids)) {
        trajectories <- rbind(trajectories, ar1_block(ids[i], ...))
    }
    trajectories
}

ar1est <- data.frame(mu=c(0.0105, 0.0185, 0.022, 0.0113, 0.0144, 0.0175, -9e-04, 0.0093, 0.0111, -9e-04, -0.0024, 0.0086, 0.012, 0.0161, 0.0043, 0.0175, 0.0118, 0.0019, 0.0116, 0.0048, 0.0154, 0.0137, 0.0168, 0.0191, 0.0108, -0.0037, 0.0135, 0.0203, -0.0042, 0.0097, 0.0209, 0.0034, 0.0113, 0.0102, 0.0094, -0.0012, 0.008, 0.0082, 0.0123, 0.0175, 0.0054, -0.0087, 0.0161, 0.0155, 0.0126, 0.0181, 0.014, -0.0135, -0.0095, 0.0142, 0.011, 0.0194, 0.0149, 0.0115, 0.0129, -0.0124, 0.0116, 0.0136, 0.0161, 0.005, 0.0165, -0.0079, 0.0129, -0.0016, -7e-04, 0.0243, 0.0193, -0.004, 0.0145, 0.0078, 0.0156, 0.001, 0.0032, 0.0069, 0.0146, 0.0164, 0.0113, 0.0116, 0.0182, 0.0167, -0.0031, 0.0168, 0.0137, 0.012, -0.0212, -0.0092, 0.019, 0.0167, -0.0021, 0.0156, 0.0173, 0.0148, -0.0036, 0.0168, 0.0179, 0.0086, 0.0131, 0.015, 0.0106, 0.0132, 0.0119, 0.0156, 0.0159, 0.0256, 0.0071, 0.0163, 0.0107, 0.0139, 0.0228, 0.0139, 0.0117, 0.0133, 0.0127, -0.0162, 0.0115, 0.0095, 0.0183, 0.0183, -6e-04, 0.0177, 0.0145, 0.0041, 0.0143, 0.0135, -0.0078, 0.0036, 0.015, 0.018, 0.0158, 0.0054, -0.0204, 0.0193, 0.0051, 0.0144, 0.0129, 0.0134, 0.0116, 0.0102, 0.0203, 0.0154, 0.0106, 0.0184, 0.0096, -0.0032, 0.0143, 0.0158, 0.0093, 0.0159, 0.0112, 0.0106, 0.0075, 0.0133, 0.0171, 0.0133, 0.0139, 0.0167, 0.0131, -0.0078, 0.0135, 0.0145, 0.0104, 8e-04, 0.0205, 0.0046, 0.011, 0.0148, 0.0202, 8e-04, 0.0211, 0.0135, -8e-04, -0.0104, -0.0027, 0.0094, 0.0179, -0.0101, 0.0156, 0.0155, 0.014, 0.0149, 0.0165, 0.0168, 0.0155, 0.0136, 0.0156, 0.0149, 0.0191, 0.0176, 0.0094, -0.0076, 0.0162, 0.0143, 0.0182, 0.0102, 0.015, -0.0292, 0.0063, -0.0028, 0.0163, 0.015), 
                     sigma=c(0.0081, 0.0053, 0.0069, 0.0075, 0.0082, 0.006, 0.0101, 0.011, 0.0064, 0.0066, 0.0095, 0.0057, 0.0078, 0.005, 0.0076, 0.0064, 0.0067, 0.0049, 0.0086, 0.0067, 0.0063, 0.0054, 0.0063, 0.0077, 0.0072, 0.0074, 0.0067, 0.0047, 0.0125, 0.0069, 0.0052, 0.0073, 0.0063, 0.0072, 0.0086, 0.0079, 0.009, 0.006, 0.0077, 0.0061, 0.0082, 0.0072, 0.0054, 0.0056, 0.0072, 0.0085, 0.0064, 0.0058, 0.0064, 0.0084, 0.0075, 0.006, 0.0048, 0.0068, 0.0065, 0.0082, 0.0072, 0.0056, 0.0056, 0.0055, 0.0054, 0.0059, 0.0064, 0.0069, 0.0073, 0.0071, 0.0057, 0.0062, 0.0086, 0.0062, 0.0054, 0.0052, 0.0066, 0.0076, 0.0046, 0.0056, 0.0066, 0.0077, 0.0074, 0.0061, 0.0056, 0.0065, 0.0069, 0.0084, 0.0058, 0.007, 0.0074, 0.0077, 0.0081, 0.0083, 0.0054, 0.0057, 0.0076, 0.0119, 0.0056, 0.0078, 0.005, 0.0073, 0.0075, 0.0054, 0.0085, 0.011, 0.0063, 0.0056, 0.009, 0.0069, 0.008, 0.0063, 0.007, 0.0059, 0.0064, 0.006, 0.0103, 0.0085, 0.006, 0.0076, 0.0054, 0.0066, 0.0056, 0.0071, 0.0079, 0.007, 0.0085, 0.0075, 0.007, 0.0085, 0.006, 0.0067, 0.006, 0.0074, 0.0098, 0.0066, 0.0058, 0.0075, 0.0064, 0.0059, 0.0103, 0.0055, 0.0053, 0.0068, 0.0057, 0.009, 0.0118, 0.0096, 0.0085, 0.0075, 0.0078, 0.0041, 0.0056, 0.008, 0.0071, 0.006, 0.0046, 0.0061, 0.007, 0.0061, 0.0066, 0.0075, 0.0094, 0.0072, 0.008, 0.0064, 0.0079, 0.0068, 0.0069, 0.0058, 0.0056, 0.0057, 0.0065, 0.006, 0.0073, 0.0067, 0.0068, 0.0071, 0.0048, 0.0071, 0.0063, 0.0051, 0.0079, 0.0042, 0.0048, 0.0066, 0.0072, 0.0058, 0.0057, 0.0083, 0.0063, 0.0057, 0.0103, 0.0096, 0.0067, 0.0051, 0.0075, 0.0064, 0.0069, 0.007, 0.007, 0.0074, 0.0056, 0.006), 
                     phi=c(0.42, 0.3509, 0.8197, 0.5304, 0.1491, 0.3675, 0.9687, 0.7877, 0.7114, 0.9435, 0.9634, 0.9189, 0.4758, 0.5738, 0.8016, 0.0509, 0.8281, 0.8168, 0.7442, 0.9347, 0.1699, 0.3566, 0.8388, 0.7724, 0.7474, 0.7834, 0.6661, 0.5162, 0.9025, 0.5306, 0.6912, 0.7625, 0.8289, 0.6985, 0.9188, 0.9639, 0.3178, 0.7288, 0.4129, 0.2196, 0.9304, 0.9697, 0.193, 0.1474, 0.3111, 0.8844, 0.7386, 0.9674, 0.9983, 0.4863, 0.9338, 0.7999, 0.4696, 0.5078, 0.5141, 0.9958, 0.6404, 0.2886, 0.4171, 0.9856, 0.3261, 0.9713, 0.682, 0.7686, 0.8577, 0.9481, 0.6057, 0.934, 0.3161, 0.9414, 0.8349, 0.8325, 0.8913, 0.7726, 0.7327, 0.1403, 0.8144, 0.7506, 0.225, 0.4884, 0.9052, 0.2891, 0.1652, 0.7612, 0.9403, 0.9865, 0.4107, 0.6518, 0.893, 0.4981, 0.72, 0.3366, 0.8437, 0.2551, 0.7753, 0.5, 0.7857, 0.7107, 0.5643, 0.2887, 0.9621, 0.2384, 0.414, 0.86, 0.6917, 0.4946, 0.2325, 0.3419, 0.9219, 0.2706, 0.717, 0.2327, 0.7541, 0.9692, 0.5838, 0.9346, 0.4739, 0.3219, 0.9634, 0.3046, 0.9913, 0.8485, 0.3071, 0.0373, 0.9183, 0.7935, 0.0039, 0.5968, 0.3654, 0.595, 0.9712, 0.2745, 0.6027, 0.7441, 0.7641, 0.3582, 0.3397, 0.7748, 0.8188, 0.0604, 0.5076, 0.2856, 0.6859, 0.6705, 0.0326, 0.8749, 0.2596, 0.1138, 0.6072, 0.4, 0.9241, 0.612, 0.2375, 0.2495, 0.0661, 0.3234, 0.7651, 0.8581, 0.4818, 0.7303, 0.7458, 0.8925, 0.2861, 0.982, 0.0791, 0.2474, 0.4326, 0.8757, 0.5288, 0.6476, 0.8473, 0.9098, 0.9562, 0.8464, 0.5444, 0.9738, 0.706, 0.0795, 0.391, 0.3167, 0.3311, 0.5681, 0.27, 0.9046, 0.2299, 0.2299, 0.085, 0.4002, 0.7443, 0.9865, 0.7028, 0.9016, 0.6092, 0.2367, 0.5402, 0.9401, 0.8013, 0.993, 0.2473, 0.6414)
                     )
str(ar1est)


# Generate trajectories for all rows of the estimation dataset
trajs <- ar1_multblocks(seq_along(nrow(ar1est)), rate0 = 0.015,  block_size = 10, traj_len = 15)

# Show results
show_migration(trajs)


# Load package
library(parallel)

# How many physical cores are available?
ncores <- detectCores(logical = FALSE)

# Create a cluster
cl <- makeCluster(ncores)

# Process rnorm in parallel
clusterApply(cl, 1:ncores, fun = rnorm, mean = 10, sd = 2)

# Evaluate partial sums in parallel
part_sums <- clusterApply(cl, x = c(1, 51), fun = function(x) sum(x:(x + 49)))

# Total sum
total <- sum(unlist(part_sums))

# Check for correctness
total == sum(1:100)

# Stop the cluster
stopCluster(cl)


# Create a cluster and set parameters
cl <- makeCluster(2)
replicates <- 50
sample_size <- 10000

# Parallel evaluation
means <- clusterApply(cl, x = rep(sample_size, replicates), fun = myfunc)
                
# View results as histogram
hist(unlist(means))

```
  
  
  
***
  
Chapter 2 - The parallel package  
  
Cluster basics:  
  
* The parallel package consists of two parts - snow (Tuerney) and multicore (Urbanek)  
	* The snow can work on any operating system  
    * The multicore works on most systems but not on Windows  
* Supported backends for snow are managed automatically by the parallel package  
	* cl <- makeCluster(ncores, type = "PSOCK")  # default socket communication, works on all OS, all clusters start with a completely empty environment  
    * cl <- makeCluster(ncores, type = "FORK")  # all OS except Windows, all workers are complete copies of the master environment  
    * cl <- makeCluster(ncores, type = "MPI")  # interface provided by Rmpi and may be more efficient on machines where MPI is enabled  
  
Core of parallel:  
  
* The main processing functions are clusterApply and clusterApplyLB ("load balanced")  
* The wrapper functions include parApply, parLapply, parSapply, parRapply (rows of a matrix), parCapply (columns of a matrix)  
	* Further, parLapplyLB, parSapplyLB are wrappers on the clusterApplyLB data  
* Example of using clusterApply for work on a pre-defined cluster cl  
	* clusterApply(cl, x = arg.sequence, fun = myfunc)  # each element of x is passed to myfunc (length of x is the total number of tasks)  
* There are several overheads involved in creating parallel processing - starting/stopping clusters, messages sent between nodes/masters, size of messages  
	* Communications between master and workers is expensive, so long worker times are preferred in general  
    * The overheads may sometimes be so significant as to make parallel processing more time-consuming than serial processing  
  
Initialization of nodes:  
  
* Cluster nodes typically start with a clean, empty environment (default for sockets)  
* Repeated communications with the workers is expensive  
	* clusterApply(cl, rep(1000, n), rnorm, sd = 1:1000)  # master needs to send the vector to all the clusters (big overhead)  
* Good practice is to initialize workers at the beginning with everything that stays constant and/or is time consuming  
	* Sending static data or datasets, loading libraries, evaluating global functions, etc.  
* The clusterCall() will call the same function with the same arguments on all the nodes  
	* cl <- makeCluster(2)  
    * clusterCall(cl, function() library(janeaustenr))  # will be loaded in all the clusters  
    * clusterCall(cl, function(i) emma[i], 20)  # will call the 20th element of emma from janeausten  
* The clusterEvalQ() will evaluate a literal expression on all nodes  
	* cl <- makeCluster(2)  
    * clusterEvalQ(cl, { library(janeaustenr) ; library(stringr) ; get_books <- function() austen_books()$book %>% unique %>% as.character })  # all books in the package  
    * clusterCall(cl, function(i) get_books()[i], 1:3)  # function get_books is available in the environment due to the above  
* The clusterExport() will export objects from master to the workers  
	* books <- get_books()  
    * cl <- makeCluster(2)  
    * clusterExport(cl, "books")  # The books object is passed quoted  
    * clusterCall(cl, function() print(books))  # books will be available since it was passed by clusterExport()  
  
Subsetting data:  
  
* Each task is applied to a different data chunk; these can be made available to the worker in various ways  
	* Random numbers on the fly  
    * Arguments  
    * Chunking on the workers side  
* Example of random numbers being created on the fly by the workers (reproducibility covered in later chapters)  
	* myfunc <- function(n, ...) mean(rnorm(n, ...))  
    * clusterApply(cl, rep(1000, 20), myfunc, sd = 6)  
* Example of chunking the data on the master side and then passing the data to workers as an argument  
	* Incorporated in to parApply() by default  
    * cl <- makeCluster(4)  
    * mat <- matrix(rnorm(12), ncol=4)  
    * parCapply(cl, mat, sum)  # splits the matrix by column and passes to worker  
    * unlist(clusterApply(cl, as.data.frame(mat), sum))  # needs to be converted to data.frame first for clusterApply()  
* Example of chunking data on the worker side (each pre-populated with the full data, chunking on the worker side) - saves communication time  
	* n <- 100  
    * M <- matrix(rnorm(n * n), ncol = n)  
    * clusterExport(cl, "M")  
    * mult_row <- function(id) apply(M, 2, function(col) sum(M[id,] * col))  
    * clusterApply(cl, 1:n, mult_row) %>% do.call(rbind, .)  
  
Example code includes:  
```{r}

# Load parallel and create a cluster
library(parallel)
cl <- makeCluster(4)

# Investigate the cl object and its elements
typeof(cl)
length(cl)
typeof(cl[[3]])
cl[[3]]$rank

# What is the process ID of the workers
clusterCall(cl, Sys.getpid)

# Stop the cluster
stopCluster(cl)


# Define ncores and a print function
ncores <- 2
print_ncores <- function() print(ncores)

# Create a socket and a fork clusters
# cl_sock <- makeCluster(ncores, type = "PSOCK")
# cl_fork <- makeCluster(ncores, type = "FORK")  # this is possible only on OS other than Windows

# Evaluate the print function on each cluster
# clusterCall(cl_sock, print_ncores)  # this will fail since the socket has no knowledge of the main environment
# clusterCall(cl_fork, print_ncores)

# Change ncores and evaluate again
# ncores <- 4
# clusterCall(cl_fork, print_ncores)  # the fork is only of the original environment, so these clusters will still think the answer is 2


# In this exercise, you will take the simple embarrassingly parallel application for computing mean of random numbers (myfunc()) from the first chapter, and implement two functions:
# One that runs the application sequentially, mean_seq(), and one that runs it in parallel, mean_par()
# Both functions have three arguments, n (sample size), repl (number of replicates) and ... (passed to myfunc())
# Function mean_par() assumes a cluster object cl to be present in the global environment

# Function to run repeatedly
myfunc <- function(n, ...) mean(rnorm(n, ...))

# Sequential solution
mean_seq <- function(n, repl, ...) {
    res <- rep(NA, repl)
    for (it in 1:repl) res[it] <- myfunc(n, ...)
    res
}

# Parallel solution
mean_par <- function(n, repl, ...) {
    res <- clusterApply(cl, x = rep(n, repl), fun = myfunc, ...)
    unlist(res)
}


# Load packages 
library(parallel)
library(microbenchmark)

# Create a cluster
cl <- makeCluster(2)

# Compare run times
microbenchmark(mean_seq(3000000, repl = 4), 
               mean_par(3000000, repl = 4),
               mean_seq(100, repl = 100), 
               mean_par(100, repl = 100),
               times = 1, unit = "s")
# Stop cluster               
stopCluster(cl)


# Load extraDistr on master
library(extraDistr)

# Define myrdnorm 
myrdnorm <- function(n, mean = 0, sd = 1) 
    rdnorm(n, mean = mean, sd = sd)

# Run myrdnorm in parallel - should fail
# res <- clusterApply(cl, rep(1000, 20), myrdnorm, sd = 6)   # will error out


# Load extraDistr on all workers
cl <- makeCluster(2)
clusterEvalQ(cl, library(extraDistr))

# Run myrdnorm in parallel again and show results
res <- clusterApply(cl, rep(1000, 20), myrdnorm, sd = 6)
hist(unlist(res))


# myrdnorm that uses global variables
myrdnorm <- function(n) rdnorm(n, mean = mean, sd = sd)

# Initialize workers 
clusterEvalQ(cl, {
    library(extraDistr)
    mean=10
    sd=5
    })
    
# Run myrdnorm in parallel and show results
res <- clusterApply(cl, rep(1000, 100), myrdnorm)

# View results
hist(unlist(res))


# Set global objects on master
mean <- 20
sd <- 10

# Export global objects to workers
clusterExport(cl, c("mean", "sd"))

# Load extraDistr on workers
clusterEvalQ(cl, library(extraDistr))

# Run myrdnorm in parallel and show results
res <- clusterApply(cl, rep(1000, 100), myrdnorm)
hist(unlist(res))


select_words <- function(letter, words, min_length = 1) {
    min_length_words <- words[nchar(words) >= min_length]
    grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Export "select_words" to workers
clusterExport(cl, "select_words")

# Split indices for two chunks
ind <- splitIndices(length(words), 2)

# Find unique words in parallel
result <- clusterApply(cl, x = list(words[ind[[1]]], words[ind[[2]]]),  
            function(w, ...) unique(select_words("v", w, ...)), 
            min_length = 10)
            
# Show vectorized unique results
unique(unlist(result))


# Earlier you defined a function ar1_multblocks() that takes a vector of row identifiers as argument and generates migration trajectories using the corresponding rows of the parameter set ar1est
# ar1_multblocks() depends on ar1_block() which in turns depends on ar1_trajectory()
# These functions along with the cluster object cl of size 4, function show_migration(), the dataset ar1est (reduced to 200 rows) and packages parallel and ggplot2 are available in your workspace

ar1_block <- function(id, rate0 = 0.015, traj_len = 15, block_size = 10) {
    trajectories <- matrix(NA, nrow = block_size, ncol=traj_len)
    for (i in seq_len(block_size)) 
        trajectories[i,] <- ar1_trajectory(unlist(ar1est[id, ]), rate0 = rate0, len = traj_len)
    trajectories
}

ar1_trajectory <- function(est, rate0, len = 15) {
    ar1 <- function(est, r) {
        # simulate one AR(1) value
        est['mu'] + est['phi'] * (r - est['mu']) + 
        rnorm(1, sd = est['sigma'])
    }
    trajectory <- rep(NA, len)
    rate <- rate0
    for (time in seq_len(len)) {
        trajectory[time] <- ar1(est, r = rate)
        rate <- trajectory[time]
    }
    trajectory
}

ar1_multblocks <- function(ids, ...) {
    trajectories <- NULL
    for (i in seq_along(ids))
        trajectories <- rbind(trajectories, ar1_block(ids[i], ...))
    trajectories
}

# Export data and functions
clusterExport(cl, c("ar1est", "ar1_block", "ar1_trajectory"))

# Process ar1_multblocks in parallel
res <- clusterApply(cl, 1:nrow(ar1est), ar1_multblocks)

# Combine results into a matrix and show results
trajs <- do.call(rbind, res)
show_migration(trajs)


# The object res returned by clusterApply() in the previous exercise is also in your workspace, now called res_prev
res_prev <- res

# Split task into 5 chunks
ind <- splitIndices(nrow(ar1est), 5)

# Process ar1_multblocks in parallel
res <- clusterApply(cl, ind, ar1_multblocks)

# Dimensions of results 
(res_dim <- c(length(res), nrow(res[[1]])))
(res_prev_dim <- c(length(res_prev), nrow(res_prev[[1]])))

stopCluster(cl)

```
  
  
  
***
  
Chapter 3 - foreach, future.apply, and Load Balancing  
  
foreach:  
  
* The looping construct can be applied using the foreach package (Calaway and Weston), similar to previous examples  
* The foreach makes it possible to run parallel processing for loops - code can be written the same way for both sequential and parallel applications  
* The basic syntax is foreach(.) %do% .  
	* library(foreach)  
    * foreach(n = rep(5, 3)) %do% rnorm(n)  
    * foreach(n = rep(5, 3), m = 10^(0:2)) %do% rnorm(n, mean = m)  # can pass arguments  
    * The foreach() call will return a value - in the example above, this would be a list  
* Can combine results using post-processing arguments - the .combine argument  
	* foreach(n = rep(5, 3), .combine = rbind) %do% rnorm(n)  # rbind of the three lists  
    * foreach(n = rep(5, 3), .combine = '+') %do% rnorm(n)  # sum across the three lists  
* Can also use list comprehensions with the foreach() function - using the %:% operator  
	* foreach(x = sample(1:1000, 10), .combine = c) %:% when(x %% 3 == 0 || x %% 5 == 0) %do% x  
  
foreach and parallel backends:  
  
* The most popular backend is the doParallel() call for parallel foreach() processing  
	* Other backends include doFuture() using the future package and doSEQ() which allows switching between parallel and sequential  
* The doParallel package by Calaway et al is an interface between foreach and parallel, and requires initialization with registerDoParallel()  
	* library(doParallel)  
    * registerDoParallel(cores = 3)  # uses multicore for Unix and snow for Windows  
    * cl <- makeCluster(3)  # can make your own clusters and pass them also  
    * registerDoParallel(cl)  # passing the cl object rather than cores (will default to snow since that is makeCluster() default  
* Examples of converting a sequential loop to a parallel loop  
	* library(foreach)  
    * foreach(n = rep(5, 3)) %do% rnorm(n)  
    * library(doParallel)  
    * cl <- makeCluster(3)  
    * registerDoParallel(cl)  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  # note the conversion to %dopar% which is what engages the parallel processing  
* The doFuture package (Bengtsson) is built on top of the future package  
	* The central idea is that there is a future plan for how foreach should work behind the scenes - sequential, cluster, multicore, multiprocess, etc.  
    * Other packages are available from future.batchtools  
    * library(doFuture)  
    * registerDoFuture()  
    * plan(cluster, workers = 3)  # using the cluster plan  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  # same use of foreach, with the cluster plan applied  
    * plan(multicore)  # using the multicore plan  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  
  
future and future.apply - packages that are continually under development:  
  
* The future package intends to have a uniform API for sequential and parallel processing  
* The future construct is for an expression that may be used in the future  
	* Example in plain R - x <- mean(rnorm(n, 0, 1)) ; y <- mean(rnorm(n, 10, 5)) ; print(c(x, y))  
    * Example in implicit futures - x %<-% mean(rnorm(n, 0, 1)) ; y %<-% mean(rnorm(n, 10, 5)) ; print(c(x, y))  
    * Example in explicit futures - x <- future(mean(rnorm(n, 0, 1))) ; y <- future(mean(rnorm(n, 10, 5))) ; print(c(value(x), value(y)))  
    * Values can be managed asynchronously, meaning that the next line can start running while the current line is still in process  
* The same code can then be run either in parallel or sequentially - sequential example  
	* plan(sequential)  
    * x %<-% mean(rnorm(n, 0, 1))  
    * y %<-% mean(rnorm(n, 10, 5))  
    * print(c(x, y))  
* The same code can then be run either in parallel or sequentially - parallel example  
	* plan(multicore)  
    * x %<-% mean(rnorm(n, 0, 1))  
    * y %<-% mean(rnorm(n, 10, 5))  
    * print(c(x, y))  
* The package future.apply is a higher level API for all the apply packages in R using futures  
	* Sibling to foreach  
    * functions include future_lapply(),future_sapply(),future_apply()  
* Example of using future.apply()  
	* lapply(1:10, rnorm)  # base R  
    * plan(sequential)  
    * future_lapply(1:10, rnorm)  # future.apply implementation of base R above  
    * plan(cluster, workers=4)  
    * future_lapply(1:10, rnorm)  # future.apply implementation of base R above, now run in parallel  
* Separating the plan from the processing allows for processing across many systems  
	* Single CPU uses sequential, cluster plan for many cores, etc.  
  
Load balancing and scheduling:  
  
* There can be significant node waiting times if the master waits for all workers to finish before assigning new tasks  
	* The clusterApplyLB() is designed to speed up processing by sending new tasks to workers as soon as they finish their old tasks  
* Communication overhead can also be a big problem when the tasks are small  
	* Can instead give the workers many tasks at once, and have them communicate with the master only at the start and the finish  
    * Drawback is that idle workers will not be available to help busy workers  
* Can chunk in parallel using the splitIndices() function  
	* splitIndices(10, 2)  
    * clusterApply(cl, x = splitIndices(10, 2), fun = sapply, "*", 100)  # multiply all numbers by 100  
    * Can also use parApply with the chunk.size option in R 3.5+  
    * foreach(s = isplitVector(1:10, chunks = 2)) %dopar% sapply(s, "*", 100)  # itertools functions to implement in foreach  
    * future_sapply(1:10, `*`, 100, future.scheduling = 1)  # 1 chunk per worker  
    * future_sapply(1:10, `*`, 100, future.scheduling = FALSE)  # 1 chunk per task  
  
Example code includes:  
```{r}

# Recall the first chapter where you found the most frequent words from the janeaustenr package that are of certain minimum length
result <- lapply(letters, max_frequency, words = words, min_length = 5) %>% 
    unlist
# In this exercise, you will implement the foreach construct to solve the same problem
# The janeaustenr package, a vector of all words from the included books, words, and a function max_frequency() for finding the results based on a given starting letter are all available in your workspace

# Load the package
library(foreach)

# foreach construct
result <- foreach(l = letters, .combine=c) %do% max_frequency(l, words=words, min_length=5)
                
# Plot results 
barplot(result, las = 2)


# Specifically, your job is to modify the code so that the maximum frequency for the first half of the alphabet is obtained for words that are two and more characters long, while the frequency corresponding to the second half of the alphabet is derived from words that are six and more characters long
# Note that we are using an alphabet of 26 characters

# foreach construct and combine into vector
result <- foreach(l = letters, n = rep(c(2, 6), each=13), .combine = c) %do% 
    max_frequency(l, words=words, min_length=n)
          
# Plot results
barplot(result, las = 2)


# Register doParallel with 2 cores
doParallel::registerDoParallel(cores=2)

# Parallel foreach loop
res <- foreach(r = rep(1000, 100), .combine = rbind, 
            .packages = "extraDistr") %dopar% myrdnorm(r)
            
# Dimensions of res
dim_res <- dim(res)


# So far you learned how to search for the most frequent word in a text sequentially using foreach()
# In the course of the next two exercises, you will implement the same task using doParallel and doFuture for parallel processing and benchmark it against the sequential version
# The sequential solution is implemented in function freq_seq() (type freq_seq in your console to see it)
# It iterates over a global character vector chars and calls the function max_frequency() which searches within a vector of words, while filtering for minimum word length
# All these objects are preloaded, as is the doParallel package
# Your job now is to write a function freq_doPar() that runs the same code in parallel via doParallel

freq_seq <- function(min_length = 5)
    foreach(l = letters, .combine = c) %do% 
        max_frequency(l, words = words, min_length = min_length)

# Function for doParallel foreach
freq_doPar <- function(cores, min_length = 5) {
    # Register a cluster of size cores
    doParallel::registerDoParallel(cores=cores)
    
    # foreach loop
    foreach(l=letters, .combine=c, 
            .export = c("max_frequency", "select_words", "words"),
            .packages = c("janeaustenr", "stringr")) %dopar%
        max_frequency(l, words=words, min_length=min_length)
}

# Run on 2 cores
freq_doPar(cores=2)


# Now your job is to create a function freq_doFut() that accomplishes the same task as freq_doPar() but with the doFuture backend
# Note that when using doFuture, arguments .packages and .export in foreach() are not necessary, as the package deals with the exports automatically
# You will then benchmark these two functions, together with the sequential freq_seq()
# All the functions from the last exercise are available in your workspace
# In addition, the packages doFuture and microbenchmark are also preloaded
# To keep the computation time low, the global chars vector is set to the first six letters of the alphabet only

cores <- 2
min_length <- 5

# Error in tweak.function(strategy, ..., penvir = penvir) : 
# Trying to use non-future function 'survival::cluster': function (x)  { ... }
# For solution see https://github.com/HenrikBengtsson/future/issues/152

# Function for doFuture foreach
freq_doFut <- function(cores, min_length = 5) {
    # Register and set plan
    doFuture::registerDoFuture()
    future::plan(future::cluster, workers=cores)

    # foreach loop
    foreach(l = letters, .combine = c) %dopar%
        max_frequency(l, words = words, min_length = min_length)
}

# Benchmark
microbenchmark(freq_seq(min_length),
               freq_doPar(cores, min_length),
               freq_doFut(cores, min_length),
               times = 1)

# It is straight forward to swap parallel backends with foreach
# In this small example, you might not see any time advantage in running it in parallel
# In addition, doFuture is usually somewhat slower than doParallel
# This is because doFuture has a higher computation overhead
# We encourage you to test these frameworks on more time-consuming applications where an overhead become negligible relative to the overall processing time

extract_words_from_text <- function(text) {
    str_extract_all(text, boundary("word")) %>% 
        unlist %>% 
        tolower
}

# Main function
freq_fapply <- function(words, chars=letters, min_length=5) {
    unlist(future.apply::future_lapply(chars, FUN=max_frequency, words = words, min_length = min_length))
}

obama <- readLines("./RInputFiles/obama.txt")
obama_speech <- paste(obama[obama != ""], collapse=" ")

# Extract words and call freq_fapply
words <- extract_words_from_text(obama_speech)
res <- freq_fapply(words)

# Plot results
barplot(res, las = 2)


# Now imagine you are a user of the fictional package from the previous exercise
# At home you have a two-CPU Mac computer, and at work you use a Linux cluster with two 16-CPU computers, called "oisin" and "oscar"
# Your job is to write a function for each of the hardware that calls freq_fapply() while taking advantage of all available CPUs
# For the cluster, you set workers to a vector of computer names corresponding to the number of CPUs, i.e. 16 x "oisin" and 16 x "oscar"
# For a one-CPU environment, we have created a function fapply_seq()

# fapply_seq <- function(...) {
#     future::plan(strategy="sequential") 
#     freq_fapply(words, letters, ...)
# }

# multicore function
# fapply_mc <- function(cores=2, ...) {
#     plan(strategy="multicore", workers=cores)
#     freq_fapply(words, letters, ...)
# }

# cluster function
# fapply_cl <- function(cores=NULL, ...) {
#     # set default value for cores
#     if(is.null(cores))
#         cores <- rep(c("oisin", "oscar"), each = 16)
#         
#     # parallel processing
#     plan(strategy="cluster", workers=cores)
#     freq_fapply(words, letters, ...)
# }


# Note: Multicore does not work on Windows. We recommend using the 'multiprocess' or 'cluster' plan on Windows computers.

# Microbenchmark
# microbenchmark(fapply_seq = fapply_seq(),
#                fapply_mc_2 = fapply_mc(cores=2), 
#                fapply_mc_10 = fapply_mc(cores=10),
#                fapply_cl = fapply_cl(cores=2), 
#                times = 1)

# Which is the slowest?
# slowest1 <- "fapply_cl"


# This is because for a small number of tasks a sequential code can run faster than a parallel version due to the parallel overhead
# The cluster plan has usually the largest overhead and thus, should be used only for larger number of tasks
# The multicore may be more efficient when the number of workers is equal to the number of cores
# It uses shared memory, and thus is faster than cluster


# In your workspace there is a vector tasktime containing simulated processing times of 30 tasks (generated using runif())
# There is also a cluster object cl with two nodes
# Your job is to apply the function Sys.sleep() to tasktime in parallel using clusterApply() and clusterApplyLB() and benchmark them
# The parallel and microbenchmark packages are loaded
# We also provided functions for plotting cluster usage plots called plot_cluster_apply() and plot_cluster_applyLB()
# Both functions use functionality from the snow package

tasktime <- c(0.1328, 0.1861, 0.2865, 0.4541, 0.1009, 0.4492, 0.4723, 0.3304, 0.3146, 0.031, 0.1031, 0.0884, 0.3435, 0.1921, 0.3849, 0.2489, 0.3588, 0.496, 0.1901, 0.3887, 0.4674, 0.1062, 0.3259, 0.0629, 0.1337, 0.1931, 0.0068, 0.1913, 0.4349, 0.1702)

# plot_cluster_apply <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::clusterApply(cl, x, fun)),
#             title = "Cluster usage of clusterApply")

# plot_cluster_applyLB <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::clusterApplyLB(cl, x, fun)),
#             title = "Cluster usage of clusterApplyLB")

# Benchmark clusterApply and clusterApplyLB
# microbenchmark(
#     clusterApply(cl, tasktime, Sys.sleep),
#     clusterApplyLB(cl, tasktime, Sys.sleep),
#     times = 1
# )

# Plot cluster usage
# plot_cluster_apply(cl, tasktime, Sys.sleep)
# plot_cluster_applyLB(cl, tasktime, Sys.sleep)


# Now we compare the results from the previous exercise with ones generated using parSapply(), which represents here an implementation that groups tasks into as many chunks as there are workers available
# You first explore its cluster usage plot, using the function plot_parSapply() we defined for you
# We generated a version of the tasktime vector, called bias_tasktime that generates very uneven load
# Your job is to compare the run times of parSapply() with clusterApplyLB() applied to bias_tasktime

# plot_parSapply <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::parSapply(cl, x, fun)),
#             title = "Cluster usage of parSapply")

# bias_tasktime <- c(1, 1, 1, 0.1, 0.1, 0.1, 1e-04, 1e-04, 1e-04, 0.001, 1)

# Plot cluster usage for parSapply
# plot_parSapply(cl, tasktime, Sys.sleep)

# Microbenchmark
# microbenchmark(
#     clusterApplyLB(cl, bias_tasktime, Sys.sleep),
#     parSapply(cl, bias_tasktime, Sys.sleep),
#     times = 1
# )

# Plot cluster usage for parSapply and clusterApplyLB
# plot_cluster_applyLB(cl, bias_tasktime, Sys.sleep)
# plot_parSapply(cl, bias_tasktime, Sys.sleep)

```
  
  
  
***
  
Chapter 4 - Random Numbers and Reproducibility  
  
Are results reproducible?  
  
* Many statistical numbers require the use of random numbers - MCMC, boot, simulation, sample, rnorm, etc.  
	* Typically, would use a set.seed() to initialize the RNG to a known state  
* Setting the seed typically does not guarantee reproducibility of parallel processing  
	* library(parallel)  
    * cl <- makeCluster(2)  
    * set.seed(1234)  
    * clusterApply(cl, rep(3, 2), rnorm)  
    * set.seed(1234)  # only sets the RNG in the master node; thus not sent to the workers  
    * clusterApply(cl, rep(3, 2), rnorm)  # will get different sets of results 
* Can instead set the RNG for each of the workers  
	* clusterEvalQ(cl, set.seed(1234))  
    * clusterApply(cl, rep(3, 2), rnorm)  # both clusters will give the identicla results  
* There is another common and not recommended method of generating random numbers in parallel code - gives statistical properties that are not desirable  
	* for (i in 1:2) {  
    *     set.seed(1234)  
    *     clusterApply(cl, sample(1:10000000, 2), set.seed)  
    *     print(clusterApply(cl, rep(3, 2), rnorm))  
    * }  
  
Parallel random number generators:  
  
* A good RNG should comply with certain parameters - long period > 2**100, good structural properties in high dimensions, reproducible  
	* These parameters should hold in a distributed environment also  
* RNG streams with period 2**291 and seeds 2**127 steps apart is available based on research by L'Ecuyer  
	* Allows for each part of a parallel process to have reproducible streams with the proper properties  
    * Available in R through rlecuyer and rstream  
    * Also available in R Code using RNGkind("L'Ecuyer-CMRG")  
* The L'Ecuyer generator is the default when using parallel but needs to be initialized with a seed for cluster cl  
	* clusterSetRNGStream(cl, iseed = 1234)  # initializes a reproducible and independent seed for each of the workers  
* Reproducibility in parallel depends on the task flow - certain conditions need to apply  
	* Runs on clusters of the same size (different number of workers means different random numbers)  
    * Cannot use load balancing (balancing loads means non-deterministic scheduling and assignment)  
  
Reproducibility in foreach and future.apply:  
  
* Can achieve reproducibility in foreach using doRNG - one stream per task  
	* library(doRNG)  
    * library(doParallel)  
    * registerDoParallel(cores = 3)  
    * set.seed(1)  
    * res1 <- foreach(n = rep(2, 5), .combine = rbind) %dorng% rnorm(n)  
    * set.seed(1)  
    * res2 <- foreach(n = rep(2, 5), .combine = rbind) %dorng% rnorm(n)  
    * identical(res1, res2)  # TRUE  
* Can also use doRNG by way of %dopar%  
	* library(doRNG)  
    * library(doParallel)  
    * registerDoParallel(cores = 3)  
    * registerDoRNG(1)  # 1 is the seed  
    * res3 <- foreach(n = rep(2, 5), .combine = rbind) %dopar% rnorm(n)  
    * set.seed(1)  
    * res4 <- foreach(n = rep(2, 5), .combine = rbind) %dopar% rnorm(n)  
    * c(identical(res1, res3), identical(res2, res4))  # TRUE TRUE  
* Can also use independent streams in future.apply  
	* library(future.apply)  
    * plan(sequential)  
    * res5 <- future_lapply(1:5, FUN = rnorm, future.seed = 1234)  
    * plan(multiprocess)  
    * res6 <- future_lapply(1:5, FUN = rnorm, future.seed = 1234)  
    * identical(res5, res6)  # TRUE  
  
Next steps:  
  
* The parallel package is the baseline for many other packages  
	* Often not reproducible  
* The foreach package with doParallel and doFuture can be reproducible using doRNG  
* The future.apply package has an inutitive apply-like syntax and is always reproducible if future.seed is set  
* General best practices include  
	* Minimize overhead and master-worker communication frequencies  
    * Use scheduling and load balancing through effective use of chunking  
    * Be careful that parallel overhead can actually make tasks longer if the tasks are all small and the communication takes more time than the calculations  
  
Example code includes:  
```{r}

# In addition to the code in the previous exercise, we also created a FORK cluster for you.

# cl.fork <- makeCluster(2, type = "FORK")

# Your job is to register the two cluster objects with the preloaded doParallel package and compare results obtained with parallel foreach
# How do the results differ in terms of reproducibility?

library(doParallel)
cl.sock <- makeCluster(2, type = "PSOCK")
registerDoParallel(cl.sock)
set.seed(100)
foreach (i = 1:2) %dopar% rnorm(3)


# Register and use cl.sock
registerDoParallel(cl.sock)
replicate(2, {
    set.seed(100)
    foreach(i = 1:2, .combine = rbind) %dopar% rnorm(3)
    }, simplify = FALSE
)

# Register and use cl.fork
# registerDoParallel(cl.fork)
# replicate(2, {
#     set.seed(100)
#     foreach(i = 1:2, .combine = rbind) %dopar% rnorm(3)
#     }, simplify = FALSE
# )


# Create a cluster
cl <- makeCluster(2)

# Check RNGkind on workers
clusterCall(cl, RNGkind)

# Set the RNG seed on workers
clusterSetRNGStream(cl, iseed=100)

# Check RNGkind on workers
clusterCall(cl, RNGkind)


# Now you are ready to make your results reproducible
# You will use the simple embarrassingly parallel application for computing a mean of random numbers (myfunc) which we parallelized in the second chapter using clusterApply()
# The parallel package, myfunc() , n (sample size, set to 1000) and repl (number of replicates, set to 5) are available in your workspace
# You will now call clusterApply() repeatedly to check if results can be reproduced, without and with initializing the RNG

n <- 1000
repl <- 5

# Create a cluster of size 2
cl <- makeCluster(2)

# Call clusterApply three times
for(i in 1:3)
    print(unlist(clusterApply(cl, rep(n, repl), myfunc)))

# Create a seed object
seed <- 1234

# Repeatedly set the cluster seed and call clusterApply()
for(i in 1:3) {
    clusterSetRNGStream(cl, iseed = seed)
    print(unlist(clusterApply(cl, rep(n, repl), myfunc)))
}


# Create two cluster objects, of size 2 and 4
cl2 <- makeCluster(2)
cl4 <- makeCluster(4)

# Set seed on cl2 and call clusterApply
clusterSetRNGStream(cl2, iseed = seed)
unlist(clusterApply(cl2, rep(n, repl), myfunc))

# Set seed on cl4 and call clusterApply
clusterSetRNGStream(cl4, iseed = seed)
unlist(clusterApply(cl4, rep(n, repl), myfunc))


# Register doParallel and doRNG
library(doRNG)
registerDoParallel(cores = 2)
doRNG::registerDoRNG(seed)

# Call ar1_block via foreach
mpar <- foreach(r=1:5) %dopar% ar1_block(r)

# Register sequential backend, set seed and run foreach
registerDoSEQ()
set.seed(seed)
mseq <- foreach(r=1:5) %dorng% ar1_block(r)

# Check if results identical
identical(mpar, mseq)


# You are able to reproduce sequential and parallel applications! Remember to always use %dorng% if you use the doSEQ backend
# Also note that by default on the Linux DataCamp server, registerDoParallel() creates a FORK cluster if a number of cores is passed to it
# As a result, there was no need to export any functions to workers, as they were copied from the master
# On a different platform, the .export option may be needed


# Set multiprocess plan 
future::plan(strategy="multiprocess", workers = 2)

# Call ar1_block via future_lapply
mfpar <- future.apply::future_lapply(1:5, FUN=ar1_block, future.seed=seed)

# Set sequential plan and repeat future_lapply
future::plan(strategy="sequential")
mfseq <- future.apply::future_lapply(1:5, FUN=ar1_block, future.seed=seed)

# Check if results are identical
identical(mfpar, mfseq)


rm(mean)
rm(sd)

```
  
  
  
***
  
###_Marketing Analytics in R: Choice Modeling_  
  
Chapter 1 - Quickstart Guide  
  
Why choice?  
  
* Choice modeling (and conjoint) is a common and popular tool used in marketing  
	* Linear regression is about predicting a number based on features  
    * Frequently, though, we want to make a choose from a selection of objects (picking a show, purchasing a car, etc.)  
* Multinomial logit (logistic regressions) work well with choice data  
* Choice models can be helpful for feature selection, pricing, trade-offs between quality/cost, etc.  
  
Inspecting choice data:  
  
* Choice data does not fit in to normal predictive modeling formats  
	* For regression, data are typically one row per observation  
    * For choice datasets, data are typically stacked in to a few rows, where each row describes an alternative (rather than an observation), with a flag for which option was chosen  
* May want to count up the number of choices made (total or proportions)  
	* xtabs(choice ~ price, data=sportscar)  
  
Fitting and interpreting a choice model:  
  
* Fitting choice models is similar to fitting linear models  
	* my_model <- lm(y ~ x1 + x2 + x3, data=lm_data)  
    * library(mlogit)  # multinomial logit is needed rather than GLM  
    * mymodel <- mlogit(choice ~ feature1 + feature2 + feature3, data = choice_data)  # data must be choice data, including both ques, alt, and choice columns  
    * summary(mymodel)  
* Coefficients of magnitude greater than 1 are of very high impact in the decisions (rule of thumb)  
  
Using choice models to make decisions:  
  
* Can use the choice models to predict market shares  
    * predict_mnl(model, products)  # for mlogit models - written by instructor  
  
Example code includes:  
```{r eval=FALSE}

# Unload conflicting namespaces
unloadNamespace("rms")
unloadNamespace("quantreg")
unloadNamespace("MatrixModels")

unloadNamespace("lmerTest")
unloadNamespace("semPlot")
unloadNamespace("rockchalk")
unloadNamespace("qgraph")
unloadNamespace("sem")
unloadNamespace("mi")
unloadNamespace("arm")
unloadNamespace("mice")
unloadNamespace("mitml")
unloadNamespace("jomo")
unloadNamespace("arm")
unloadNamespace("jomo")
unloadNamespace("lme4")


# load the mlogit library 
library(mlogit)


scLong <- read.csv("./RInputFiles/sportscar_choice_long.csv")
scWide <- read.csv("./RInputFiles/sportscar_choice_wide.csv")
sportscar <- scLong
sportscar$alt <- as.factor(sportscar$alt)
sportscar$seat <- as.factor(sportscar$seat)
sportscar$price <- as.factor(sportscar$price)
sportscar$choice <- as.logical(sportscar$choice)
sportscar <- sportscar %>% rename(resp.id=resp_id)
sportscar$key <- rep(1:2000, each=3)
row.names(sportscar) <- paste(sportscar$key, sportscar$alt, sep=".")
sportscar <- mlogit.data(sportscar, shape="long", choice="choice", alt.var="alt")
str(sportscar)

# Create a table of chosen sportscars by transmission type
chosen_by_trans <- xtabs(choice ~ trans, data = sportscar)

# Print the chosen_by_trans table to the console
chosen_by_trans

# Plot the chosen_by_price object
barplot(chosen_by_trans)


# Crashes out due to issue with class "family" in MatrixModels and lme4
m1 <- mlogit(choice ~ seat + trans + convert + price, data=sportscar, seed=10)

# fit a choice model using mlogit() and assign the output to m1
# m1 <- mlogit::mlogit(choice ~ seat + trans + convert + price, 
#                      data=sportscar, 
#                      chid.var="key", 
#                      alt.var="alt", 
#                      choice="choice", 
#                      seed=10
#                      )

# summarize the m1 object to see the output of the choice model
summary(m1)


predict_mnl <- function(model, products) {
  # model: mlogit object returned by mlogit()
  # data: a data frame containing the set of designs for which you want to 
  #       predict shares.  Same format at the data used to estimate model. 
  data.model <- model.matrix(update(model$formula, 0 ~ .), data = products)[,-1]
  utility <- data.model%*%model$coef
  share <- exp(utility)/sum(exp(utility))
  cbind(share, products)
}

# inspect products
products <- data.frame(seat=factor("2", levels=c("2", "4", "5")), 
                       trans=factor(rep(c("manual", "auto"), each=2), levels=c("auto", "manual")), 
                       convert=factor(rep(c("no", "yes"), times=2), levels=c("no", "yes")), 
                       price=factor("35", levels=c("30", "35", "40"))
                       )
str(products)

# use predict_mnl to predict share for products
shares <- predict_mnl(m1, products)

# print the shares to the console
shares


barplot(shares$share, ylab="Predicted Market Share", 
        names.arg=c("Our Car", "Comp 1", "Comp 2", "Comp 3"))

```
  
  
  
***
  
Chapter 2 - Managing and Summarizing Choice Data  
  
Assembling choice data:  
  
* Choices made in the wild (revealed preference data) can be analyzed using the transaction record and product set available  
* Can instead run a survey with hypothetical decision making (conjoint)  
* Sometimes, data are provided in wide format, with sets of columns describing the choices  
  
Converting from wide to long:  
  
* Often helpful to convert the wide format data to long format instead  
	* sportscar <- reshape(sportscar_wide, direction="long", varying = list(seat=5:7, trans=8:10, convert=11:13, price=14:16), v.names = c("seat", "trans", "convert", "price"), timevar="alt")  # column labels are given in v.names; timevar="alt" means make an alt column  
    * new_order <- order(sportscar$resp_id, sportscar$ques, sportscar$alt)  
    * sportscar <- sportscar[new_order,]  # ordered by question  
    * sportscar$choice <- sportscar$choice == sportscar$alt  # make a boolean rather than an integer  
  
Choice data in two files:  
  
* Can receive choice data from two separate files - alternatives and choices  
	* sportscar <- merge(sportscar_choices, sportscar_alts, by=c("resp_id", "ques"))  
  
Visualizing choce data:  
  
* Data in long format can be summarized and visualized  
	* xtabs(~ trans, data = sportscar)  # just get the totals by transmission  
    * xtabs(~ trans + choice, data = sportscar)  # COUNT of transmission by choice  
    * xtabs(choice ~ trans, data=sportscar)  # SUM of choice by trans  
    * plot(xtabs(~ trans + choice, data=sportscar))  # mosaic plot  
    * plot(xtabs(~ trans + segment + choice, data=sportscar))  # mosaic plot split primarily by trans, then with segment and choice  
  
Designing a conjoint survey:  
  
* Conjoint surveys are popular for product design - can include any number of features  
	* Begin picking attributes of interest (commonly 8-10) and levels of interest (commonly 2-5 per level)  
* Need to decide which product profiles to show to which users and which questions  
* Can create a random design in R  
	* attribs <- list(Type=c("Milk", "Dark", "White"), Brand=c("Cadbury", "Toblerone", "Kinder"), Price=5:30/10)  
    * all_comb <- expand.grid(attribs)  
    * for (i in 1:100) {  
    *     rand_rows <- sample(1:nrow(all_comb), size=12*3)  
    *     rand_alts <- all_comb[rand_rows, ]  
    *     choc_survey[choc_survey$Subject==i, 4:6] <- rand_alts  
    * }  
* Can code the survey in html or use a platform like Google or Survey Monkey  
	* Can also use a firm like Qualtrics  
  
Example code includes:  
```{r}

chLong <- read.csv("./RInputFiles/chocolate_choice_long.csv")
chWide <- read.csv("./RInputFiles/chocolate_choice_wide.csv")
chocolate_wide <- chWide

# Look at the head() of chocolate_wide
head(chocolate_wide)

# Use summary() to see which brands and types are in chocolate_wide
summary(chocolate_wide)


# use reshape() to change the data from long to wide 
chocolate <- reshape(data= chocolate_wide , direction="long", 
                     varying = list(Brand=3:5, Price=6:8, Type=9:11), 
                     v.names=c("Brand", "Price", "Type"), timevar="Alt")
                     
# use head() to confirm that the data has been properly transformed
head(chocolate)


# Create the new order for the chocolate data frame
new_order <- order(chocolate$Subject, chocolate$Trial, chocolate$Alt)

# Reorder the chocolate data frame to the new_order
chocolate <- chocolate[new_order,]

# Look at the head() of chocolate to see how it has been reordered
head(chocolate)


# Use head(chocolate) and look at the Selection variable. 
head(chocolate)

# Transform the Selection variable to a logical indicator
chocolate$Selection <- chocolate$Alt == chocolate$Selection

# Use head(chocolate) to see how the Selection variable has changed. Now it is logical.
head(chocolate)


choc_choice <- chocolate %>%
    filter(Selection==TRUE) %>%
    mutate(Selection=Alt) %>%
    select(Subject, Trial, Response_Time, Selection)
choc_alts <- chocolate %>%
    select(Subject, Trial, Alt, Brand, Price, Type)

str(choc_choice)
str(choc_alts)


# Merge choc_choice and choc_alts
choc_merge <- merge(choc_choice, choc_alts, by=c("Subject", "Trial"))

# Convert Selection to a logical variable
choc_merge$Selection <- choc_merge$Selection == choc_merge$Alt

# Inspect chocolate_merge using head
head(choc_merge)


# Use xtabs to count up how often each Type is chosen
counts <- xtabs(~ Type + Selection, data=chocolate)

# Plot the counts
plot(counts, cex = 1.5)


# Modify this code to count up how many times each **Brand** is chosen
counts <- xtabs(~ Brand + Selection, data=chocolate)

# Plot the counts
plot(counts, cex = 1.5)


# Use xtabs to count up how often each Price is chosen
counts <- xtabs(~ Price + Selection, data=chocolate)

# Plot the counts
plot(counts, cex=0.6)

```
  
  
  
***
  
Chapter 3 - Building Choice Models  
  
Choice models - under the hood:  
  
* The multimonial logit model begins with a linear equation for utility which drives probabilities  
	* v1 <- alpha * 4 + beta * 100  # value  
    * v2 <- alpha * 5 + beta * 150  
    * v2 <- alpha * 2 + beta * 175  
    * u1 <- v1 + error1  # utility  
    * u2 <- v2 + error2  
    * u3 <- v3 + error3  
    * choice <- which.max(c(u1, u2, u3))  
    * p1 <- exp(v1) / ( exp(v1) + exp(v2) + exp(v3) )  
    * p2 <- exp(v2) / ( exp(v1) + exp(v2) + exp(v3) )  
    * p3 <- exp(v3) / ( exp(v1) + exp(v2) + exp(v3) )  
* We want to estimate the parameters that best calculate the v  
	* m1 <- mlogit(choice ~ 0 + seat + price, data=sportscar, print.level=3)  # find parameters that maximize likelihood, print all iterations as per print.level=3  
    * summary(m1)  
* The mlogit requires a specific data format  
	* sportscar <- mlogit.data(sportscar.df, shape="long", choice="choice", varying=5:8, alt.var="alt")  # what was chose, what attributes vary, and what is the alternative number column  
  
Interpreting choice model parameters:  
  
* Coefficients for each attribute are multiplied by the level of that attribute to create the v1/v2/v3  
* Factor variables are converted to numbers in the model.matrix process - need to understand what is the zero level  
	* head(model.matrix(m2))  
    * head(sportscar)  
* May want to convert to factors even if the data are numeric  
	* sportscar$seat <- as.factor(sportscar$seat)  
    * m3 <- mlogit(choice ~ 0 + seat + trans + convert + price, data=sportscar)  
    * summary(m3)  
* Can make price response non-linear (factor) or keep linear and assume Willingness to Pay  
	* coef(m3)  
    * coef(m3)/-coef(m3)[5]  # assumes that price is the 5th element of the coef vector  
  
Intercepts and interactions:  
  
* There is no intercept in the v1, v2, v3, since adding the same constant to each of them cancels out once the exponentials are taken  
	* The intercept is not identified, and so fixing the intercept using ~ 0 + is preferred (model will just pick a random one otherwise)  
* An interaction term is based on the multiplication of several terms from the original dataset  
	* m4 <- mlogit(choice ~ 0 + seat + trans + convert + price + trans:price, data=sportscar)  # interaction between transmission and price  
    * m4 <- mlogit(choice ~ 0 + seat + trans*convert + price, data=sportscar)  # same command as above, less typing  
* Interpreting the standard errors along with the z-values and p-values is similar to any other regression  
* Can also add segments, as long as they interact with at least one of the attributes  
	* m5 <- mlogit(choice ~ 0 + seat + convert + trans + price:segment, data=sportscar)  
  
Predicting shares:  
  
* Share predictions can be a good way to communicate preferences  
* Begin by creating vectors of attributes of interest  
	* price <- c(35, 30)  
    * seat <- factor(c(2, 2), levels=c(2,4,5))  
    * trans <- factor(c("manual", "auto"), levels=c("auto", "manual"))  
    * convert <- factor(c("no", "no"), levels=c("no", "yes"))  
    * segment <- factor(c("basic", "basic"), levels=c("basic", "fun", "racer"))  
    * prod <- data.frame(seat, trans, convert, price, segment)  
* Use a model prediction to make the share predictions  
	* m5 <- mlogit(choice ~ 0 + seat + convert + trans + price:segment, data=sportscar)  
    * prod.coded <- model.matrix(update(m5$formula, 0 ~ .), data = prod)[,-1]  
    * v <- prod.coded %*% m5$coef  
    * p <- exp(u) / sum(exp(u))  
    * cbind(p, prod)  
    * predict_mnl <- function(model, products) {  
    *     data.model <- model.matrix(update(model$formula, 0 ~ .), data = products)[,-1]  
    *     utility <- data.model%*%model$coef  
    *     share <- exp(utility)/sum(exp(utility))  
    *     cbind(share, products)  
    * }  
    * shares <- predict_mnl(m5, products)  
    * barplot(shares$share, horiz = TRUE, col="tomato2", xlab = "Predicted Market Share", names.arg = c("Our Sportscar", "Competitor 1"))  
  
Example code includes:  
```{r eval=FALSE}

# use mlogit.data() to convert chocolate to mlogit.data
chocolate_df <- mlogit.data(chocolate, shape = "long",
                            choice = "Selection", alt.var = "Alt", 
                            varying = 6:8)
                         
# use str() to confirm that chocolate is an mlogit.data object
str(chocolate_df)


# Fit a model with mlogit() and assign it to choc_m1
choc_m1 <- mlogit(Selection ~ Brand + Type + Price, data=chocolate_df, print.level=3)

# Summarize choc_m1 with summary()
summary(choc_m1)


# modify the call to mlogit to exclude the intercept
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data = chocolate_df, print.level=3)

# summarize the choc_m2 model
summary(choc_m2)


# compute the wtp by dividing the coefficient vector by the negative of the price coefficient
coef(choc_m2) / -coef(choc_m2)["Price"]


# change the Price variable to a factor in the chocolate data
chocolate$Price <- as.factor(chocolate$Price)

# fit a model with mlogit and assign it to choc_m3
choc_m3 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)

# inspect the coefficients
summary(choc_m3)


# likelihood ratio test comparing two models
lrtest(choc_m2, choc_m3)


# add the formula for mlogit
choc_m4 <- mlogit(Selection ~ 0 + Brand + Type + Price + Brand:Type, data=chocolate)

# use summary to see the coefficients
summary(choc_m4)


# add the formula for mlogit
choc_m5 <- mlogit(Selection ~ 0 + Brand + Type + Price + Price:Trial, data=chocolate)

# use summary to see the outputs
summary(choc_m5)


# add the formula for mlogit
choc_m5 <- mlogit(Selection ~ 0 + Brand + Type + Price + Price:Trial, data=chocolate)

# use summary to see the outputs
summary(choc_m5)


predict_mnl <- function(model, products) {
  data.model <- model.matrix(update(model$formula, 0 ~ .), 
                             data = products)[,-1]
  utility <- data.model%*%model$coef
  share <- exp(utility)/sum(exp(utility))
  cbind(share, products)
}

# modify the code below so that the segement is set to "racer" for both alternatives
price <- c(35, 30)
seat <- factor(c(2, 2), levels=c(2,4,5))
trans <- factor(c("manual", "auto"), levels=c("auto", "manual"))
convert <- factor(c("no", "no"), levels=c("no", "yes"))
segment <- factor(c("racer", "racer"), levels=c("basic", "fun", "racer"))
prod <- data.frame(seat, trans, convert, price, segment)

# predict shares for the "racer" segment
predict_mnl(model=m5, products=prod)


# fit the choc_m2 model
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)

# create a data frame with the Ghiradelli products
Brand <- factor(rep("Ghirardelli", 5), level = levels(chocolate$Brand))
Type <- levels(chocolate$Type)
Price <- 3   # treated as a number in choc_m2
ghir_choc <- data.frame(Brand, Type, Price)

# predict shares
predict_mnl(model=choc_m2, products=ghir_choc)


# compute and save the share prediction 
shares <- predict_mnl(choc_m2, ghir_choc)

# make a barplot of the shares
barplot(shares$share, 
        horiz = TRUE, col="tomato2",
        xlab = "Predicted Market Share", 
        main = "Shares for Ghiradelli chocolate bars at $3 each", 
        names.arg = levels(chocolate$Type)
        )

```
  
  
  
***
  
Chapter 4 - Hierarchical Choice Models  
  
What is a hierarchical choice model?  
  
* Hierarchical choice models (random coefficient models) account for differences in preferences across entities (heterogeneity)  
* An assumption is made that each individual is pulled from a distribution  
	* for (i in 1:n_resp) {  
    *     beta[i] <- mvrnorm(1, beta_0, Sigma)  # random normal vector  
    *     for (j in 1:n_task[i]) {  
    *         X <- X[X$resp == i & X$task == j, ]  
    *         u <- X %*% beta[i]  
    *         p[i,] <- exp(u) / sum(exp(u))  
    *     }  
    * }  
* Can fit the hierarchical model using the mlogit() function  
	* sportscar <- mlogit.data(sportscar, choice="choice", shape="long", varying=5:8, alt.var="alt", id.var = "resp_id")  
    * m7 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, rpar = c(price = "n"), panel = TRUE)  # rpar=c(price="n") will nornmally distribute price parameter across people  
    * summary(m7)  
    * plot(m7)  # plotting function for mlogit objects  
  
Heterogeneity in preferences for other features:  
  
* Might have heterogeneity in choices on many other attributes, including factor data  
* A different manner of coding factors can work better for hierarchical models  
	* Effects coding has -1, 0, 1 so that the factors are relative to average rather than relative to the first factor  
    * contrasts(sportscar$seat) <- contr.sum(levels(sportscar$seat))  # stores effects coding with the data frame  
    * dimnames(contrasts(sportscar$seat))[[2]] <- levels(sportscar$seat)[1:2]  # improve readability  
* Can make all of the coefficients heterogeneous  
	* my_rpar <- c("n", "n", "n", "n", "n")  # make them all normal  
    * m3 <- mlogit(choice ~ 0 + seat + trans + convert + price, data=sportscar)  # get coefficient names  
    * names(my_rpar) <- names(m3$coefficients)  # assign them to my_rpar  
    * m8 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, panel = TRUE, rpar = my_rpar)  # fit the model  
    * plot(m8, par=c("seat4", "seat5"))  
    * -sum(m8$coef[1:2])  # can get the 2-seat coefficient since it no longer needs to be 0  
  
Predicting shares with hierarchical models:  
  
* Can predict shares with a hierarchical model, including those where decision-making preferences may be correlated  
	* m10 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, rpar = myrpar, panel=TRUE, correlation = TRUE)  
    * cor.mlogit(m10)  
    * mean <- m10$coef[1:5] # hard coded  
    * Sigma <- cov.mlogit(m10)  
    * share <- matrix(NA, nrow=1000, ncol=nrow(prod.coded))  
    * for (i in 1:1000) {  
    *     coef <- mvrnorm(1, mu=mean, Sigma=Sigma)  
    *     utility <- prod.coded %*% coef  
    *     share[i,] <- exp(utility) / sum(exp(utility))  
    * }  
    * cbind(colMeans(share), prod)  
* Niche product shares tend to increase when heterogeneity is included - each element of the niche may be a small preference, but correlated with preference for other elements of the niche  
  
Wrap up:  
  
* Decisions need to be made in building a choice model - attributes, numeric vs. factors, hierarchical, distributions, nesting, etc.  
* Can start with basic models and build out as needed  
	* Inspect the data - investigate a few choices to confirm understanding of the data  
    * Run the model and inspect the standard errors - if too high, simplify  
    * Heterogeneity is a good idea whenever the decisions are being made by humans  
  
Example code includes:  
```{r eval=FALSE}

# Determine the number of subjects in chocolate$Subjects
length(levels(chocolate$Subject))


# add id.var input to mlogit.data call
chocolate <- mlogit.data(chocolate, choice = "Selection", shape="long", 
                         varying=6:8, alt.var = "Alt", id.var = "Subject"
                         )
                         
# add rpar and panel inputs to mlogit call
choc_m6 <- mlogit(Selection ~ 0 + Brand + Type + Price, data = chocolate, 
                  rpar = c(Price="n"), panel=TRUE)

# plot the model
plot(choc_m6)


# set the contrasts for Brand to effects code
contrasts(chocolate$Brand) <- contr.sum(levels(chocolate$Brand))
dimnames(contrasts(chocolate$Brand))[[2]] <- levels(chocolate$Brand)[1:4]
contrasts(chocolate$Brand)

# set the contrasts for Type to effects code
contrasts(chocolate$Type) <- contr.sum(levels(chocolate$Type))
dimnames(contrasts(chocolate$Type))[[2]] <- levels(chocolate$Type)[1:4]
contrasts(chocolate$Type)


# create my_rpar vector
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)
my_rpar <- rep("n", length(choc_m2$coef))
names(my_rpar) <- names(choc_m2$coef)
my_rpar

# fit model with random coefficients
choc_m7 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate, rpar=my_rpar, panel=TRUE)


# print the coefficients 
choc_m7$coef[5:8]

# compute the negative sum of those coefficients
-sum(choc_m7$coef[5:8])


# Extract the mean parameters and assign to mean
mean <- choc_m8$coef[1:9]   

# Extract the covariance parameters and assign to Sigma
Sigma <- cov.mlogit(choc_m8) 

# Create storage for individual draws of share
share <- matrix(NA, nrow=1000, ncol=nrow(choc_line_coded))

# For each draw (person)
for (i in 1:1000) { 
  # Draw a coefficient vector
  coef <- mvrnorm(1, mu=mean, Sigma=Sigma)
  # Compute utilities for those coef
  utility <- choc_line_coded %*% coef
  # Compute probabilites according to logit formuila
  share[i,] <- exp(utility) / sum(exp(utility))
}  

# Average the draws of the shares
cbind(colMeans(share), choc_line)

```
  
  
  
***
  
###_Single-Cell RNA-Seq Workflows in R_  
  
Chapter 1 - What Is RNA Single-Cell RNA-Seq?  
  
Background and utility:  
  
* Can get gene expression data at the cellular level - allows for better resolution of gene expressions  
	* Previous methods would get a mix of gener expressions across many cells - better for averages and distributions, but cannot identify a specific observation or cell type  
    * Implications are significant for personalized medicine and related areas  
* The data structure from the lab is a matrix - geners are rows and cells are columns  
	* ATCG counts by intersection  
    * Gene-level covariates  
    * Cell-level covariates  
* There are many zeroes - gene not expressed in cell, or dropouts (technical errors)  
  
Typical workflow:  
  
* There has been an exponential scaling in the ability to extract RNA data from 2009 - 2018  
	* Full-range technologies try to capture the full RNA sequence  
    * Tide-based (?) technologies try to capture the two ends of an RNA technology  
* Quality control is the process of removing problematic cells - library size and cell coverage metrics  
	* Library size is the total number of reads assigned to each cell  
    * Coverage is the total number of cells with reads assigned for that gene  
* Workflows then include normalization, dimensionality reduction, clustering, DE analysis (biomarkers with differential expression)  
  
Load, create, and access data:  
  
* The SingleCellExperiment (SCE) class is an S4 class for storing data from single-cell experiments  
	* source("https://bioconductor.org/biocLite.R")  
    * biocLite("SingleCellExperiment")  
    * library(SingleCellExperiment)  
    * counts <- matrix(rpois(8, lambda = 10), ncol = 2, nrow = 4)  
    * rownames(counts) <- c("Lamp5", "Fam19a1", "Cnr1", "Rorb")  
    * colnames(counts) <- c("SRR2140028", "SRR2140022")  
    * counts  
* Can create SCE data using the constructor  
	* sce <- SingleCellExperiment(assays = list(counts = counts), rowData = data.frame(gene = rownames(counts)), colData = data.frame(cell = colnames(counts)))  
* Can create SCE data using coercion  
	* se <- SummarizedExperiment(assays = list(counts = counts))  
    * sce <- as(se, "SingleCellExperiment")  
* Can apply process to a real dataset  
	* library(scRNAseq); data(allen)  # subset of mouse visual coretex data  
  
Example code includes:  
```{r eval=FALSE}

# head of count matrix
counts[1:3, 1:3]

# count of specific gene and cell
alignedReads <- counts['Cnr1', "SRR2140055"]

# overall percentage of zero counts 
pZero <- mean(counts == 0)

# cell library size
libsize <- colSums(counts)


# find cell coverage
coverage <- colMeans(counts > 0)
cell_info$coverage <- coverage

# load ggplot2
library(ggplot2)

# plot cell coverage
ggplot(cell_info, aes(x = names, y = coverage)) + 
  geom_point() +
  ggtitle('Cell Coverage') + 
  xlab('Cell Name') + 
  ylab('Coverage')


# mean of GC content
gc_mean <- mean(gene_info$gc)

# standard deviation of GC content
gc_sd <- sd(gene_info$gc)

# boxplot of GC content 
boxplot(gene_info$gc, main = 'Boxplot - GC content', ylab = 'GC content')


# batch
batch <- cell_info$batch

# patient
patient <- cell_info$patient

# nesting of batch within patient
batch_patient <- table(batch = batch, patient = patient)

# explore batch_patient
batch_patient


# load SingleCellExperiment
library(SingleCellExperiment)

# create a SingleCellExperiment object
sce <- SingleCellExperiment(assays = list(counts = counts ),
                            rowData = data.frame(gene_names = rownames(counts)),
                            colData = data.frame(cell_names = colnames(counts)))


# create a SummarizedExperiment object from counts
se <- SummarizedExperiment(assays = list(counts = counts))

# create a SingleCellExpression object from se
sce <- as(se, "SingleCellExperiment")


# create SingleCellExperiment object
sce <- as(allen, "SingleCellExperiment")

# cell information
cell_info <- colData(sce)

# size factors
sizeFactors(sce) <- colSums(assay(sce))

```
  
  
  
***
  
Chapter 2 - Quality Control and Normalization  
  
Quality Control:  
  
* Need to remove low quality cells and genes - identify first  
* Tung dataset includes three replicates  
	* sce  
    * library(scater)  
    * sce <- calculateQCMetrics(sce, feature_controls = list(ERCC = isSpike(sce, "ERCC"))  
* ERCC spiking genes are used for quality control - filter out cells with improper ratios (usually too much due to dead or stressed or the like) of "spiking"  
* Key functions used in the exercises include  
	* calculateQCMetrics()  
    * counts()  
    * rowSums()  
    * grepl()  
    * isSpike()  
    * plot(density(x))  
    * abline()  
  
Quality Control (continued):  
  
* Can filter based on library sizes using  
	* threshold <- 20000  
    * plot(density(sce$total_counts), main = "Density - total_counts")  
    * abline(v = threshold)  
    * keep <- (sce$total_counts > threshold)  
    * table(keep)  
* Can look at plots of the data  
	* scater::plotPhenoData(sce, aes_string(x = "total_counts", y = "total_counts_ERCC", colour = "batch"))  
* Can then filter based on data that do not meet key criteria in the plot  
	* keep <- (sce$batch != "NA19098.r2")  
    * table(keep)  
    * filter_genes <- apply(counts(sce), 1, function(x) length(x[x >= 2] >= 2)  # keep genes with counts of 2+ in at least 2+ cells  
    * table(filter_genes)  
  
Normalization:  
  
* Want to group cells based on their gene expression profiles; target with different drugs, for example  
* Technical artifacts can be introduced in the data  
	* Batch effect is a common problem with the data - cells may cluster by batch, even if they are from the same entity  
    * Can normalize based on dividing by library size (multiplied if needed to get count per million - CPM)  
* Exercise will use the following functions  
	* plotPCA()  
    * reducedDim(sce, "PCA")[, 1:2]  
    * computeSumFactors()  
    * sizeFactors()  
    * assays()  
    * normalize()  
    * plotRLE()  
  
Example code includes:  
```{r eval=FALSE}

# remove genes with only zeros
nonZero <- counts(sce) > 0
keep <- rowSums(nonZero) > 0
sce_2 <- sce[keep, ]

# spike-ins ERCC
isSpike(sce_2, "ERCC") <- grepl("^ERCC-", rownames(sce_2))


# load scater
library(scater)

# calculate QCs
sce <- calculateQCMetrics(sce, feature_controls = list(ERCC = isSpike(sce, "ERCC")))

# explore coldata of sce
colData(sce)


# set threshold
threshold <- 20000

# plot density
plot(density(sce@colData$total_counts), main = 'Density - total_counts')
abline(v = threshold)

# keep cells
keep <- sce@colData$total_counts > threshold

# tabulate kept cells
table(keep)


# set threshold
threshold <- 6000

# plot density
plot(density(sce$total_features), main = 'Density - total_features')
abline(v=threshold)

# keep cells
keep <- sce$total_features > threshold

# tabulate kept cells
table(keep)


#extract cell data into a data frame
cDataFrame <- as.data.frame(colData(sce))

# plot cell data
ggplot(cDataFrame, aes(x = total_counts, y = total_counts_ERCC, col = batch)) + 
  geom_point()

# keep cells
keep <- sce$batch != "NA19098.r2"

# tabulate kept cells
table(keep)


# load SingleCellExperiment
library(SingleCellExperiment)

# filter genes
filter_genes <- apply(counts(sce), 1, function(x){
  length(x[x > 1]) > 1
})

# tabulate the results of filter_genes
table(filter_genes)


# PCA raw counts
plotPCA(sce, exprs_values = "counts",
    colour_by = "batch", shape_by = "individual")

# PCA log counts
plotPCA(sce, exprs_values = "logcounts_raw",
        colour_by = "batch", shape_by = "individual")


#find first 2 PCs
pca <- reducedDim(sce, "PCA")[, 1:2]

#create cdata
cdata <- data.frame(PC1 = pca[, 1],
                    libsize = sce$total_counts,
                    batch = sce$batch)

#plot pc1 versus libsize
ggplot(cdata, aes(x = PC1, y = libsize, col = batch)) +
  geom_point()


# load scran
library(scran)

# find size factors
sce <- computeSumFactors(sce)

# display size factor histogram
hist(sizeFactors(sce))


# view assays
assays(sce)

# normalize sce
normalized_sce <- normalize(sce)

# view new assay for normalized logcounts
assays(normalized_sce)

```
  
  
  
***
  
Chapter 3 - Visualization and Dimensionality Reduction  
  
Mouse Epithelium Dataset:  
  
* Goal is to reduce the number of dimensions (from number of genes to something much smaller)  
* Mouse olfactory cell dataset - epithelium stem cell differentiation  
	* Dimensionlaity reduction makes for smaller data with preservation of signal much more so than noise  
  
Visualization:  
  
* Can visualize datasets using dimensionality reduction through any of several methods - PCA, tSNE, ZIFA, ZINB-WaVE  
	* plotPCA(sce, exprs_values = "logcounts", shape_by = "Batch", colour_by = "publishedClusters")  # los help reduce bias towards highly expressed genes  
    * plotTSNE(sce, exprs_values = "logcounts", shape_by = "Batch", colour_by = "publishedClusters", perplexity = 5)  # perplexity is a guess about the kNN parameter  
  
Dimensionality Reduction:  
  
* Can find the most variable genes using magrittr  
	* library(magrittr)  
    * vars <- assay(sce) %>% log1p %>% rowVars  
    * names(vars) <- rownames(sce)  
    * vars <- sort(vars, decreasing = TRUE)  
    * head(vars)  
    * sce_sub <- sce[names(vars[1:50]),]  
    * sce_sub  
* Can run dimensionality reduction using PCA  
	* logcounts <- log1p(assay(sce_sub))  
    * pca <- prcomp(t(logcounts))  
    * reducedDims(sce_sub) <- SimpleList(PCA = pca$x)  
    * sce_sub  
    * head(reducedDim(sce_sub, "PCA")[, 1:2])  
* Can then plot the PCA components  
	* pca <- reducedDim(sce_sub, "PCA")[, 1:2]  
    * col <- colData(sce)[, c("publishedClusters", "batch")]  
    * df <- cbind(pca, col)  
    * ggplot(df, aes(x = PC1, y = PC2, col = publishedClusters, shape = batch)) +  
    *     geom_point()  
  
Example code includes:  
```{r eval=FALSE}

# find dimensions
mydims <- dim(sce)

# extract cell and gene names
cellNames <- colnames(sce)
geneNames <- rownames(sce)


# cell data
cData <- colData(sce)

#print column names
colnames(cData)

# table batch & clusters
cData <- cData[, c('Batch', 'publishedClusters')]

#tabulate cData
table(cData)


# load scater
library(scater)

# plot pc1 and pc2 counts
plotPCA(
    object = sce,
    exprs_values = "counts",
    shape_by = "Batch",
    colour_by = "publishedClusters"
)


# explore initial assays
assays(sce)

# create log counts
logcounts <- log1p(assays(sce)$counts)

# add log counts
assay(sce, 'logcounts') <- logcounts
assays(sce)

# pca log counts
plotPCA(object = sce, exprs_values = "logcounts",
    shape_by = "Batch", colour_by = "publishedClusters")


# default tSNE
plotTSNE(
    sce,
    exprs_values = "counts",
    shape_by = "publishedClusters",
    colour_by = "Batch",
    perplexity = 5
)


# gene variance 
vars <- assay(sce) %>% log1p() %>% rowVars() 

#rename vars
names(vars) <- rownames(sce)

#sort vars
vars_2 <- sort(vars, decreasing = TRUE)
head(vars_2)

# subset sce 
sce_sub <- sce[names(vars[1:50]), ]
sce_sub


# log counts
logcounts <- log1p(assays(sce_sub)$counts)

# transpose
tlogcounts <- t(logcounts)

# perform pca
pca <- prcomp(tlogcounts)

# store pca matrix in sce
reducedDims(sce_sub) <- SimpleList(PCA = pca$x)
head(reducedDim(sce_sub, "PCA")[, 1:2])


# Extract PC1 and PC2 and create a data frame
pca <- reducedDim(sce_sub, "PCA")[, 1:2]
col_shape <- data.frame(publishedClusters = colData(sce)$publishedClusters, Batch = factor(colData(sce)$Batch))
df <- cbind(pca, col_shape)

# plot PC1, PC2
ggplot(df, aes(x = PC1, y = PC2, 
            colour = publishedClusters, 
            shape = Batch)) + 
  geom_point()

```
  
  
  
***
  
Chapter 4 - Cell Clustering and Differential Expression Analysis  
  
Clustering methods for scRNA-Seq:  
  
* Continuing to use the mouse epithelium dataset - cells color coded by cluster as per previous chapters  
* One of the goals of clustering is to group cells with similar gene expression, allowing for finding patterns in gene expression  
	* Hierarchical clustering  
    * k-means clustering  
* Challenges include setting the number of clusters, scalability to large datasets, etc.  
* Can begin by creating the Seurat object  
	* library(Seurat)  
    * library(SingleCellExperiment)  
    * seuset <- CreateSeuratObject(  
    *     raw.data = assay(sce),  
    *     normalization.method = "LogNormalize",  
    *     scale.factor = 10000,  
    *     meta.data = as.data.frame(colData(sce))  
    * )  
    * seuset <- ScaleData(object = seuset)  
    * seuset  
* Can then perform clustering on the seuset object  
	* seuset <- FindClusters( object = seuset, reduction.type = "pca", dims.use = 1:10, resolution = 1.8, print.output = FALSE )  
    * PCAPlot( object = seuset, group.by = "ident", pt.shape = "publishedClusters" )  
  
Differential expression analysis:  
  
* Differential expression (DE) analysis is to find differential expression of genes in various cells  
	* Methods include SCDE, MAST, edgeR, DESeq2, etc.  
* Can fit a MAST model using function zlm  
	* library(MAST)  
    * zlm <- zlm(~ celltype + cngeneson, sce)   
    * summary <- summary(zlm, doLRT = "celltype9")  
    * summary  
    * fit <- summary$datatable  
    * fit <- merge(fit[contrast=='celltype9' & component=='H', .(primerid, `Pr(>Chisq)`)],      fit[contrast=='celltype9' & component=='logFC', .(primerid, coef)], by='primerid')  
    * fit[, padjusted:=p.adjust(`Pr(>Chisq)`, 'fdr')]  
    * res = data.frame(gene = fit$primerid, pvalue = fit[,'Pr(>Chisq)'], padjusted = fit$padj, logFC = fit$coef)  
    * head(res)  
  
Visualization of DE genes:  
  
* Visualization is typically the final step of single-cell analysis  
* The volcano plot looks at fold-change and p-values simultaneously  
	* ggplot(res, aes(x=logFC, y=-log10(padjusted), color=mostDE)) + geom_point() +  
    *     ggtitle("Volcano") + xlab("log2 FC") + ylab("-log10 adjusted p-value")  
* Can also look at results of DE using a heatmap  
	* library(NMF)  
    * norm <- assay(sce[mostDE, ], "logcounts")  
    * norm <- as.matrix(norm)  
    * aheatmap(norm, annCol = colData(sce)$publishedClusters)  
* Course covered the typical workflow for analysis of single-cell RNA sequencing data  
	* Normalization  
    * Dimensionality reduction  
    * Clustering  
    * Differential expression analysis  
  
Example code includes:  
```{r eval=FALSE}

# load Seurat
library(Seurat)

#create seurat object
seuset <- CreateSeuratObject(
    raw.data = assay(sce),
    normalization.method = "LogNormalize", 
    scale.factor = 10000,
    meta.data = as.data.frame(colData(sce))
)

# scale seuset object
scaled_seuset <- ScaleData(object = seuset)


# perform pca
seuset <- RunPCA(
    object = seuset, 
    pc.genes = rownames(seuset@raw.data), 
    do.print = FALSE
)
# plot pca
PCAPlot(object = seuset,
        pt.shape = 'Batch',
        group.by = 'publishedClusters')


# load MAST
library(MAST)

# SingleCellAssay object 
sca

# fit zero-inflated regression 
zlm <- zlm(~ celltype + cngeneson, sca) 

# summary with likelihood test ratio
summary <- summary(zlm, doLRT = "celltype9")


# get summary table
fit <- summary$datatable

# pvalue df
pvalue <- fit[contrast == 'celltype9' & component == 'H', .(primerid, `Pr(>Chisq)`)]
  
# logFC df
logFC <- fit[contrast == 'celltype9' & component == 'logFC', .(primerid, coef)]

# pvalues and logFC
fit <- merge(pvalue, logFC, by = 'primerid')


# adjusted pvalues
fit[, padjusted:=p.adjust(`Pr(>Chisq)`, 'fdr')]

# result table
res <- data.frame(gene = fit$primerid,
                 pvalue = fit[,'Pr(>Chisq)'],
                 padjusted = fit$padj,
                 logFC = fit$coef)


# most DE 
res <- res[order(res$padjusted), ]
mostDE <- res$gene[1:20]
res$mostDE <- res$gene %in% mostDE

# volcano plot
ggplot(res, aes(x=logFC, y=-log10(padjusted), color=mostDE)) +
  geom_point() +
  ggtitle("Volcano plot") +
  xlab("log2 fold change") + 
  ylab("-log10 adjusted p-value")


# load NMF
library(NMF)

# normalize log counts
norm <- assay(sce[mostDE, ], "logcounts")
mat <- as.matrix(norm)

# heatmap
aheatmap(mat, annCol = colData(sce)$publishedClusters)

```
  
  
  
***
  
###_Differential Expression Analysis in R with limma_  
  
Chapter 1 - Differential Expression Analysis  
  
Differential expression analysis:  
  
* Analysis of data from functional genomics experiments  
* Example of having treated cells with phenotypes A and B  
	* The features can be genes or proteins or other molecular features of the cell - proxy for relative abundance (such as RNA)  
    * Upregulated - higher expression level  
    * Downregulated - lower expression level  
* Objectives are to look for novelty (genes that play an unexpected role), context (interpreting relevance of gene behaviors), systems level understanding (simultaneous allows for looking at pathways)  
* Many caveats to the analysis - study design is very important  
  
Differential expression data:  
  
* Data will be from the breast cancer data and CLL data - testing for differences in two groups of people within each  
	* x - Expression matrix  
    * f - Feature data - genes or proteins  
    * p - Phenotype data - description of each of the samples  
* Can begin by looking at the boxplot for a single gene  
	* boxplot(x[1, ] ~ p[, "er"], main = f[1, "symbol"])  
  
ExpressionSet class:  
  
* Data management can become precarious, especially when filtering and subsetting  
* Object-oriented programming can help - the class can hold the data, and has methods/functions that work specially on objectes of that class  
	* Accessors (getters) get the data  
    * Setters modify the stored data  
    * source("https://bioconductor.org/biocLite.R")  
    * biocLite("Biobase")  
    * library(Biobase)  
    * eset <- ExpressionSet(assayData = x, phenoData = AnnotatedDataFrame(p), featureData = AnnotatedDataFrame(f))  
* Can access data from an ExpressionSet object  
	* x <- exprs(eset)  
    * f <- fData(eset)  
    * p <- pData(eset)  
    * eset_sub <- eset[1000, 1:10]  
    * boxplot(exprs(eset)[1, ] ~ pData(eset)[, "er"], main = fData(eset)[1, "symbol"])  
  
The limma package:  
  
* Advantages of the limma package include replacing boiler-plate code and improved inference by sharing across genes (do not assume full independene - all from same experiment)  
	* Method used is empirical Bayes - convenience and better statistics (especially for smaller data sets)  
    * Good functions for pre-processing and post-processing  
* Specifying a linear model - Y = B0 + B1 * X1 + epsilon where Y is the expression level of the gene, B0 is the mean in ER- tumors, and B1 is the mean difference in expression in ER+ tumors  
	* model.matrix(~<explanatory>, data = <data frame>)  
    * design <- model.matrix(~er, data = pData(eset))  
    * colSums(design)  
* Can then test the design matrix using the standard limma pipeline  
	* library(limma)  
    * fit <- lmFit(eset, design)  
    * fit <- eBayes(fit)  
    * results <- decideTests(fit[, "er"])  
    * summary(results)  # -1 will be downregulated and +1 will be upregulated  
  
Example code includes:  
```{r eval=FALSE}

# Create a boxplot of the first gene in the expression matrix
boxplot(x[1, ] ~ p[, "Disease"], main = f[1, "symbol"])


# Load package
library(Biobase)

# Create ExpressionSet object
eset <- ExpressionSet(assayData = x,
                      phenoData = AnnotatedDataFrame(p),
                      featureData = AnnotatedDataFrame(f))

# View the number of features (rows) and samples (columns)
dim(eset)


# Subset to only include the 1000th gene (row) and the first 10 samples
eset_sub <- eset[1000, 1:10]

# Check the dimensions of the subset 
dim(eset_sub)

# Create a boxplot of the first gene in eset_sub
boxplot(exprs(eset_sub)[1, ] ~ pData(eset_sub)[, "Disease"],
        main = fData(eset_sub)[1, "symbol"])


# Create design matrix for leukemia study
design <- model.matrix(~Disease, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Calculate the t-statistics
fit <- eBayes(fit)

# Summarize results
results <- decideTests(fit[, "Diseasestable"])
summary(results)

```
  
  
  
***
  
Chapter 2 - Flexible Models for Common Study Designs  
  
Flexible linear models:  
  
* The models can be extended, for example to Y = B0 + B1*X1 + B2*X2 + eps  
	* This model is known as a treatment-contrast model  
* Can instead use a group-means parameterization - Y = B1*X1 + B2*X2 + eps  
	* Can then test whether B1-B2 == 0 since the intercept was exluded  
    * design <- model.matrix(~0 + er, data = pData(eset))  
    * cm <- limma::makeContrasts(status = erpositive - ernegative, levels = design)  # erpositive-ernegative means multiply erpositive by 1 and multiply ernegative by -1  
    * fit <- lmFit(eset, design)  # per above  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  # for the contrasts method  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
    * summary(results)  
  
Studies with more than two groups:  
  
* Dataset for this example has groups with leukemia types ALL, AML, CML - 20172x36 (12 leukemias of each type)  
* Desire to build a group-means model - Y = B1*X1 + B2*X2 + B3*X3 + eps  
	* design <- model.matrix(~0 + type, data = pData(eset))  
    * cm <- limma::makeContrasts(AMLvALL = typeAML - typeALL, CMLvALL = typeCML - typeALL, CMLvAML = typeCML - typeAML, levels = design)  
    * fit <- lmFit(eset, design)  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
* Exercises will look at gene expressions of stem cells grown in states of hypoxia  
  
Factorial experimental design:  
  
* Factorial designs look at every combination of experimental variables - for example, if there is a 2x2, then there would be 4 combinations examined  
* Example of 2x2 study in plants of types col, vte2 for temperatures of high, low - 11871 x 12  
* Can run the group-means model with the zero intercept; need to first create the type-temperature variable using paste  
	* group <- with(pData(eset), paste(type, temp, sep = "."))  
    * group <- factor(group)  # records the unique levels  
    * design <- model.matrix(~0 + group)  
    * colnames(design) <- levels(group)  
* May want to assess the interaction effect (difference in impact of temperature by type) as well as the direct effects (impact of temperature on a specific type, impact for same temperature across types)  
	* cm <- makeContrasts(type_normal = vte2.normal - col.normal, type_low = vte2.low - col.low, temp_vte2 = vte2.low - vte2.normal, temp_col = col.low - col.normal, interaction = (vte2.low - vte2.normal) - (col.low - col.normal), levels = design)  
    * fit <- lmFit(eset, design)  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
  
Example code includes:  
```{r eval=FALSE}

# Create design matrix with no intercept
design <- model.matrix(~0 + Disease, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(status = Diseaseprogres. - Diseasestable, levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)


# Create design matrix with no intercept
design <- model.matrix(~0 + oxygen, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(ox05vox01 = oxygenox05 - oxygenox01,
                    ox21vox01 = oxygenox21 - oxygenox01,
                    ox21vox05 = oxygenox21 - oxygenox05,
                    levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)


# Create single variable
group <- with(pData(eset), paste(type, water, sep = "."))
group <- factor(group)

# Create design matrix with no intercept
design <- model.matrix(~0 + group)
colnames(design) <- levels(group)

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(type_normal = nm6.normal - dn34.normal,
                    type_drought = nm6.drought - dn34.drought,
                    water_nm6 = nm6.drought - nm6.normal,
                    water_dn34 = dn34.drought - dn34.normal,
                    interaction = (nm6.drought - nm6.normal) - (dn34.drought - dn34.normal),
                    levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)

```
  
  
  
***
  
Chapter 3 - Pre-processing and post-processing  
  
Normalizing and filtering:  
  
* Need to convert raw data to analysis-ready data - generic approach for first-pass to new dataset  
	* Log transformation  
    * Quantile normalization  
    * Filtering  
* Can start with visualization of densities using the limma package  
	* limma::plotDensities(eset, legend = FALSE)  
    * exprs(eset) <- log(exprs(eset))  # log transform  
    * limma::plotDensities(eset, legend = FALSE)  
* Want to remove technical artifacts using quantile normalization  
	* exprs(eset) <- normalizeBetweenArrays(exprs(eset))  
    * limma::plotDensities(eset, legend = FALSE)  
    * abline(v = 5)  # from visualization, 5 may be a cutoff where the data should be kept  
    * keep <- rowMeans(exprs(eset)) > 5  
    * eset <- eset[keep, ]  
    * plotDensities(eset, legend = FALSE)  
  
Accounting for technical batch effects:  
  
* Technical batch effects are artifacts arising from differences in experiments - true for all experiment types, including functional genomics  
	* Need to balance variables of interest across batches - cannot just take type a from batch a and type b from type b and see if there are differences  
    * PCA and other dimension reduction techniques can help identify technical batch effects  
    * limma::plotMDS(eset, labels = pData(eset)[, "time"], gene.selection = "common")  
* Removing batch effects is also possible in limma  
	* exprs(eset) <- limma::removeBatchEffect(eset, batch = pData(eset)[, "batch"], covariates = pData(eset)[, "rin"])  
    * limma::plotMDS(eset, labels = pData(eset)[, "time"], gene.selection = "common")  
* For statistical analysis, it is better to include batch as a coefficient for analysis rather than to run the remove batch effect process  
  
Visualizing results:  
  
* Can inspect the results and visualize using limma  
	* results <- decideTests(fit2)  
    * topTable(fit2, number = 3)  
    * stats <- topTable(fit2, number = nrow(fit2), sort.by = "none")  
* Under the null hypothesis of no impact, the p-values should be uniformly distributed  
	* hist(runif(10000))  
    * hist(stats[, "P.Value"])  # should have many values near zero if there is an actual impact  
* Can examine results using a Volcano plot  
	* volcanoplot(fit2, highlight = 5, names = fit2$genes[, "symbol"])  
  
Enrichment testing:  
  
* Can use curated biological databases as a reference point  
* Can use the Fisher's exact test  
	* fisher.test(matrix(c(10, 100, 90, 900), nrow = 2))  
* Can also test for KEGG (reference set) enrichment, which requires a common ID  
	* entrez <- fit2$genes[, "entrez"]  
    * enrich_kegg <- kegga(fit2, geneid = entrez, species = "Hs")  # Hs is homo sapiens  
    * topKEGG(enrich_kegg, number = 3)  
* Can also test for GO (reference set) enrichment  
	* enrich_go <- goana(fit2, geneid = entrez, species = "Hs")  
    * topGO(enrich_go, ontology = "BP", number = 3)  
  
Example code includes:  
```{r eval=FALSE}

# Load package
library(limma)

# View the distribution of the raw data
plotDensities(eset, legend = FALSE)

# Log tranform
exprs(eset) <- log(exprs(eset))
plotDensities(eset, legend = FALSE)

# Quantile normalize
exprs(eset) <- normalizeBetweenArrays(exprs(eset))
plotDensities(eset, legend = FALSE)


# Load package
library(limma)

# View the normalized gene expression levels
plotDensities(eset, legend = FALSE); abline(v = 5)

# Determine the genes with mean expression level greater than 5
keep <- rowMeans(exprs(eset)) > 5
sum(keep)

# Filter the genes
eset <- eset[keep, ]
plotDensities(eset, legend = FALSE)


# Load package
library(limma)

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")

# Plot principal components labeled by batch
plotMDS(eset, labels = pData(eset)[, "batch"], gene.selection = "common")


# Load package
library(limma)

# Remove the batch effect
exprs(eset) <- removeBatchEffect(eset, batch = pData(eset)[, "batch"])

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")

# Plot principal components labeled by batch
plotMDS(eset, labels = pData(eset)[, "batch"], gene.selection = "common")


# Obtain the summary statistics for every gene
stats <- topTable(fit2, number = nrow(fit2), sort.by = "none")

# Plot a histogram of the p-values
hist(stats[, "P.Value"])


# Create a volcano plot. Highlight the top 5 genes
volcanoplot(fit2, highlight = 5, names = fit2$genes$symbol)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched KEGG Pathways
enrich_kegg <- kegga(fit2, geneid = entrez, species = "Hs")

# View the top 20 enriched KEGG pathways
topKEGG(enrich_kegg, number=20)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched GO categories
enrich_go <- goana(fit2, geneid = entrez, species = "Hs")

# View the top 20 enriched GO Biological Processes
topGO(enrich_go, ontology = "BP", number=20)

```
  
  
  
***
  
Chapter 4 - Case Study: Effect of Doxorubicin Treatment  
  
Pre-process data:  
  
* Doxorubicin is a commonly-prescribed cancer drug, with a strong side effect of cariotoxicity  
* Hypothesis for the MOA is that top2B is involved, and was tested in mice  
	* Wild mice tested against top2b null mice, each given DOX and a placebo  
    * The eset data is 29532 x 12 (3 replicates per combination of 2x2 factors)  
* Begin by inspecting and then pre-processing the data  
	* plotDensities(eset, group = pData(eset)[, "genotype"], legend = "topright")  
* Prep-processing steps include log-transform, quantile transform, and filter  
  
Model the data:  
  
* Can look at the main clusters of mice vs. treatments (top2b null seem to cluster together)  
	* plotMDS(eset, labels = pData(eset)[, "genotype"], gene.selection = "common")  
    * plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")  
* Follow the model.matrix process to run a differential expression analysis  
	* Contrasts will include wild to dox, top2b to dox, interaction effect of wild vs. top2b to dox  
    * Can also run hypothesis tests using the limma pipeline and a Venn diagram  
  
Inspect the results:  
  
* Initial results seem to support the hypothesis that top2b is the main vector for cardiotoxicity  
* The limma function topTable can be run, with a sepcified contrast  
	* coef = "dox_wt"  
    * coef = "dox_top2b"  
    * coef = "interaction"  
* Can use the volcanoplot for x-axis of log-fold change and y-axis for the log-odds of differential expression  
	* kegga and topKEGG functions  
    * species = "Mm"  # common house mouse  
  
Wrap up:  
  
* Pre-processing and visualization of gene data  
* Principal components analysis and plotMDS()  
* Fitting a group-means model for more interpretable contrasts  
* Investigation of p-values for uniformity vs. skew for low p-values  
* Use of volcanoplots  
* Testing for enrichment of differentially expressed genes  
  
Example code includes:  
```{r eval=FALSE}

# Log transform
exprs(eset) <- log(exprs(eset))
plotDensities(eset,  group = pData(eset)[, "genotype"], legend = "topright")

# Quantile normalize
exprs(eset) <- normalizeBetweenArrays(exprs(eset))
plotDensities(eset,  group = pData(eset)[, "genotype"], legend = "topright")

# Determine the genes with mean expression level greater than 0
keep <- rowMeans(exprs(eset)) > 0
sum(keep)

# Filter the genes
eset <- eset[keep, ]
plotDensities(eset, group = pData(eset)[, "genotype"], legend = "topright")


# Find the row which contains Top2b expression data
top2b <- which(fData(eset)["symbol"] == "Top2b")

# Plot Top2b expression versus genotype
boxplot(exprs(eset)[top2b, ] ~ pData(eset)[, "genotype"], main = fData(eset)[top2b, ])


# Plot principal components labeled by genotype
plotMDS(eset, labels = pData(eset)[, "genotype"], gene.selection = "common")

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")


# Create single variable
group <- with(pData(eset), paste(genotype, treatment, sep = "."))
group <- factor(group)

# Create design matrix with no intercept
design <- model.matrix(~0 + group)
colnames(design) <- levels(group)

# Count the number of samples modeled by each coefficient
colSums(design)


# Create a contrasts matrix
cm <- makeContrasts(dox_wt = wt.dox - wt.pbs,
                    dox_top2b = top2b.dox - top2b.pbs,
                    interaction = (top2b.dox - top2b.pbs) - (wt.dox - wt.pbs),
                    levels = design)

# View the contrasts matrix
cm


# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)

# Create a Venn diagram
vennDiagram(results)


# Obtain the summary statistics for the contrast dox_wt
stats_dox_wt <- topTable(fit2, coef = "dox_wt", number = nrow(fit2), sort.by = "none")
# Obtain the summary statistics for the contrast dox_top2b
stats_dox_top2b <- topTable(fit2, coef = "dox_top2b", number = nrow(fit2), sort.by = "none")
# Obtain the summary statistics for the contrast interaction
stats_interaction <- topTable(fit2, coef = "interaction", number = nrow(fit2), sort.by = "none")

# Create histograms of the p-values for each contrast
hist(stats_dox_wt[, "P.Value"])
hist(stats_dox_top2b[, "P.Value"])
hist(stats_interaction[, "P.Value"])


# Extract the gene symbols
gene_symbols <- fit2$genes[, "symbol"]

# Create a volcano plot for the contrast dox_wt
volcanoplot(fit2, coef = "dox_wt", highlight = 5, names = gene_symbols)

# Create a volcano plot for the contrast dox_top2b
volcanoplot(fit2, coef = "dox_top2b", highlight = 5, names = gene_symbols)

# Create a volcano plot for the contrast interaction
volcanoplot(fit2, coef = "interaction", highlight = 5, names = gene_symbols)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched KEGG Pathways for contrast dox_wt
enrich_dox_wt <- kegga(fit2, coef = "dox_wt", geneid = entrez, species = "Mm")

# View the top 5 enriched KEGG pathways
topKEGG(enrich_dox_wt, number = 5)

# Test for enriched KEGG Pathways for contrast interaction
enrich_interaction <- kegga(fit2, coef = "interaction", geneid = entrez, species = "Mm")

# View the top 5 enriched KEGG pathways
topKEGG(enrich_interaction, number = 5)

```
  
  
  
***
  
###_Interactive Data Visualization with bokeh_  
  
Chapter 1 - rbokeh Introduction  
  
Getting started with rbokeh:  
  
* rbokeh is the R interface to the Python Bokeh plot package - interactive and informative for end users  
* Data manipulation and pre-processing are needed, with tidyverse being integral for this course  
* Can run an example using the gapminder dataset  
	* library(gapminder)  
    * data_2002 <- gapminder %>% filter(year == 2002)  
    * gapminder_mod <- gapminder %>% mutate(pop_millions = pop/10^6)  
  
Layers for rbokeh:  
  
* The rbokeh plot is initialized using figure() with layers added using the pipe operator (note the contrast with the plus used in ggplot2)  
	* data_rwanda <- gapminder %>% filter(country == "Rwanda")  
    * figure() %>% ly_lines(x = year, y = lifeExp, data = data_rwanda)  
    * figure() %>% ly_lines(x = data_rwanda$year, y = data_rwanda$lifeExp)  # same output, but with different axis labels  
* Can look at the ggplot2::economics dataset  
	* plot_pop <- figure() %>% ly_lines(x = date, y = pop, data = economics)  
    * plot_pop  
  
Layers for rbokeh (continued):  
  
* There are many layers in rbokeh, and they all begin with ly_...()  
* Example for creating a one-layer plot  
	* dat_1982 <- gapminder %>% filter(year == 1982)  
    * figure() %>% ly_points(x = gdpPercap, y = lifeExp, data = dat_1982)  
* Example for creating a multi-layer plot - note that the data argument can be added to figure, and will inherit to the succeeding ly_() ommands  
	* data_oceania <- gapminder %>% filter(continent == "Oceania")  
    * figure(data = data_oceania, legend_location = "bottom_right") %>% ly_lines(x = year, y = gdpPercap , color = country) %>% ly_points(x = year, y = gdpPercap, color = country)  
  
Example code includes:  
```{r eval=FALSE}

## load rbokeh, gapminder and dplyr libraries
library(rbokeh)
library(gapminder)
library(dplyr)


## explore gapminder dataset 
str(gapminder)

## filter gapminder data by year 1982
dat_1982 <- gapminder %>% filter(year == 1982)


## plot life expectancy Vs GDP per Capita using data_1982
figure(legend_location = "bottom_right", title = "Life Expectancy Vs. GDP per Capita in 1982") %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_1982, 
              color = continent, hover = c(continent, country, pop)
              )


## filter the dataset for the continent Africa and and year 1967
data_africa <- gapminder %>% 
  filter(year==1967, continent=="Africa")
  
## view data_africa
data_africa


## plot life expectancy Vs GDP per Capita using data_africa   
figure(legend_location = "bottom_right",
       title = "Life Expectancy Vs. GDP per Capita in Africa - 1967"
       ) %>% 
       ly_points(x = gdpPercap, y = lifeExp, data = data_africa, hover = c(country, pop))


## add a new column with gdp in millions
gapminder_mill <- gapminder %>% 
  mutate(gdp_millions = gdpPercap * pop / 10^6)
  
## view the first 6 entries in gapminder after adding  gdp_millions
head(gapminder_mill)

## extract the entries for "Rwanda"
data_rwanda <- gapminder_mill %>% 
  filter(country=="Rwanda")

## explore data_rwanda
data_rwanda


## plot gdp over time
figure(data = data_rwanda) %>% 
    ly_lines(x = year, y = gdp_millions, width = 2)


## explore the economics dataset
data(economics)
str(economics)

## pass vectors to x & y
figure() %>%
  ly_lines(x = economics$date, y = economics$pce)

## pass columns names and dataframe
figure() %>%
  ly_lines(x = date, y = pce, data = economics)


## plot unemployment rate  versus time and change the default `ylab`
figure(ylab = "unemployment %") %>%
  ly_lines(x=date, y=100*unemploy/pop, data=economics)


dat_1992 <- gapminder %>%
    filter(year==1992)
str(dat_1992)

## plot lifeExp Vs. gdpPercap using rbokeh
plot_1992<- figure(legend_location = "bottom_right") %>%
  ly_points(x=gdpPercap, y=lifeExp, color=continent, data=dat_1992) 

## show the plot            
plot_1992


data_countries <- gapminder %>%
    filter(country %in% c("United Kingdom", "Australia", "Canada", "United States", "New Zealand"))
str(data_countries)

figure(data = data_countries, legend="top_left") %>% 
  ly_lines(x = year, y = gdpPercap , color = country) %>% 
  ly_points(x=year, y=gdpPercap, color=country)


data_countries <- gapminder %>% 
    filter(country %in% c("China", "India"))

## create a line plot with lifeExp vs. year 
fig_countries <- figure(legend="top_left") %>% 
  ly_lines(x=year, y=lifeExp, color=country, data=data_countries)


## View fig_countries
fig_countries

## modify fig_countries by adding a points layer with gdpPercap vs. year 
fig_countries %>% 
  ly_points(x=year, y=lifeExp, color=country, data=data_countries)

```
  
  
  
***
  
Chapter 2 - rbokeh Aesthetic Attributes and Figure Options  
  
Plot and Managed Attributes (Part I):  
  
* Can use aestehtic to modify areas like color, transparency, line type, shape, and the like  
	* figure(legend_location = "bottom_right", title = "Life Expectancy Vs. GDP per Capita in 1992" ) %>% ly_points(x = gdpPercap, y = lifeExp, data = dat_1992, color = continent)  
* Can use the Human Development Index (HDI) data from UNDP  
	* hdi_countries <- hdi_data %>% filter(country %in% c("Hungary", "Bulgaria", "Poland"))  
    * fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% ly_lines(x = year, y = human_development_index, color = country) %>% ly_points(x = year, y = human_development_index, color = country)  
* Can have varying color attributes by function - ly_points() has both fill_color() and line_color() which will both inherit from color  
	* Can make the fill_color explicit and set its alpha (default is 0.5) explicitly also  
    * fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% ly_points(x = year, y = human_development_index, fill_color = country, fill_alpha = 1) %>% ly_lines(x = year, y = human_development_index, color = country)  
* Can add a custom color palette also  
	* fig_col %>% set_palette(discrete_color = pal_color(c("#3182bd", "#31a354", "#de2d26")))  
  
Plot and Managed Attributes (Part II):  
  
* Bechdel dataset - movie data on finances and exclusion of women  
	* Bechdel criteria is a movie where two+ women have a discussion that is not about a male character  
    * figure() %>% ly_points(x = budget_2013, y = intgross_2013, data = dat_90_13)  # has an over-plotting problem and needs a log-transform  
    * figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13)  # log transform helps with a lot (but not all) of the over-plotting  
    * figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13, alpha = 0.4, size = 5)  # improves readability  
* May want to change the line widths for many countries  
	* hdi_countries <- hdi_data %>% filter(country %in% c("Rwanda", "Kenya", "Botswana"))  
    * figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country)  
    * Can use the width parameter to control the line width in ly_lines()  
    * (WRONG FROM VIDEO) figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country, size = 3)  
    * figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country, width = 3)  
  
Hover Info and Figure Options:  
  
* Can combine the HDI and the CPI (corruption perception index)  
	* The hover() argument added to the ly_points() can allow for hovering - see below  
    * figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = c(country, cpi_rank))  
* Can customize the hover commands using hover= where the @ means to place a variable from the frame at that point  
	* figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = "CPI Rank: @cpi_rank")  
    * Can also use basic html such as <b></b> and <br></br>  
    * figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = "<b>@country</b><br><b>CPI Rank</b>: @cpi_rank")  
* Can further add axis limits to the bokeh plots  
	* hdi_cpi_scatter <- figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015", ylim = c(0, 1), xlab = "CPI", ylab = "HDI", theme = bk_ggplot_theme()) %>% ly_points(x = corruption_perception_index_score, y = human_development_index, data = hdi_cpi_data, color = continent, size = 7)  
  
Example code includes:  
```{r eval=FALSE}

hdiRaw <- read.csv("./RInputFiles/Human Development Index (HDI).csv", skip=1)
str(hdiRaw)
hdi_data <- hdiRaw %>% 
    gather(key="year", value="human_development_index", -Country, -`HDI.Rank..2017.`) %>%
    mutate(country=str_trim(as.character(Country)), year=as.integer(str_sub(year, 2))) %>%
    filter(year %in% 1990:2105) %>%
    select(country, year, human_development_index)
str(hdi_data)

## extract "Namibia" and "Botswana" entries from hdi_data
hdi_countries <- hdi_data %>% 
    filter(country %in% c("Namibia", "Botswana"))
  
## plot human_development_index versus year
fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% 
    ly_lines(x = year, y = human_development_index, color = country) %>% 
    ly_points(x = year, y = human_development_index, 
              fill_color = "white", fill_alpha = 1,
              line_color = country, line_alpha = 1,
              size = 4
              )

## view plot 
fig_col


## use a custom palette with colors "green", "red"
fig_col %>% 
  set_palette(discrete_color = pal_color(c("green", "red")))

## define custom palette   
custom_pal <- pal_color(c("#c51b8a", "#31a354"))

## use custom_pal yp modify fig_col
fig_col %>% 
    set_palette(discrete_color=custom_pal)


## explore bechdel dataset using str
data(bechdel, package="fivethirtyeight")
str(bechdel)

## extract entries between 1980 - 2013
dat_80_13 <- bechdel %>% 
  filter(between(year, 1980, 2013))

dat_80_13 <- dat_80_13 %>% 
  mutate(roi_total = intgross_2013 / budget_2013) 
  
## plot
figure() %>% 
  ly_points(x=log(budget_2013), y=log(roi_total), data=dat_80_13)

## plot log(roi_total) versus log(budget_2013)
figure() %>% 
  ly_points(x=log(budget_2013), y=log(roi_total), size=5, line_alpha=0, fill_alpha=0.3, data=dat_80_13)


## filter the data by country = Syrian Arab Republic
hdi_countries <- hdi_data %>% 
  filter(country %in% c("Syrian Arab Republic", "Morocco"))

## change the color and line width
figure(title = "Human Development Index over Time", legend = "bottom_right") %>% 
    ly_lines(x=year, y=human_development_index, color=country, width=3, data=hdi_countries)


# explore hdi_cpi_data dataset
# str(hdi_cpi_2015)

## add multiple values as hover info (country, cpi_rank)
# figure(legend_location = "bottom_right") %>% 
#     ly_points(x=corruption_perception_index, y=human_development_index, color=continent, hover=c(country, cpi_rank), size=6, data=hdi_cpi_2015)


## modify the figure theme 
# figure(title = "Corruption Perception Index Vs. Human Development Index 2015",
#        legend_location = "bottom_right", xgrid = FALSE, ygrid = FALSE, 
#        xlab = "CPI", ylab = "HDI", theme=bk_ggplot_theme()) %>% 
#     ly_points(x = corruption_perception_index, y = human_development_index, 
#               data = hdi_cpi_2015, color = continent, size = 6, hover = c(country, cpi_rank)
#               )

```
  
  
  
***
  
Chapter 3 - Data Manipulation for Visualization and More rbokeh Layers  
  
Data Formats:  
  
* The proper data format for plotting can make rbokeh much easier  
* Frequently need to transform data from long format to wide format for easier plotting - tidyr will help (inverse of the gather function)  
	* hdi_cpi_wide <- hdi_cpi_long %>% spread(key = index, value = value)  
* May also want to transform data from wide format to long format, for example if time is a column  
	* hdi_data_long <- hdi_data_wide %>% gather(key = year, value = human_development_index, - country)  # year will become a new column, -country means leave country as its own column  
  
More rbokeh Layers:  
  
* Can create a scatter plot with a regression line as an added layer  
	* dat_90_13 <- bechdel %>% filter(between(year, 1990, 2013))  
    * p_scatter <- figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13, size = 5, alpha = 0.4)  
    * lin_reg <- lm(log(intgross_2013) ~ log(budget_2013), data = dat_90_13)  
    * summary(lin_reg)  
    * p_scatter %>% ly_abline(lin_reg)  # plots with the abline of the regression (the regression line) to the figure  
  
Interaction Tools:  
  
* Can use the interaction tool to pan, zoom, reset, and the like  
	* figure(tools=c("pan", "wheel_zoom", "box_zoom", "reset", "save", "help"), toolbar_location="right")  
* Tools can be any of "pan", "wheel_zoom", "box_zoom", "resize", "crosshair", "box_select", "lasso_select", "reset", "save", "help"  
	* Location can be any of 'above', 'below', 'left', 'right', NULL (remove the toolbar)  
* Example of customizing the available tools  
	* figure(tools = c("pan", "wheel_zoom", "box_zoom"), toolbar_location = "above", legend_location = "bottom_right", ylim = c(0, 100)) %>% ly_points(x = gdpPercap, y = lifeExp, data = gapminder_2002, color = continent, size = 6, alpha = 0.7)  
* Can create a plot and then use the widget2png tool to convert to PNG  
	* plot_scatter <- figure(title = "Life Expectancy Vs. GDP per Capita in 2002", legend_location = "bottom_right") %>% ly_points(x = gdpPercap, y = lifeExp, data = gapminder_2002)  
    * widget2png(p = plot_scatter, file = "plot_scatter.png")  
* Can also save as html  
	* rbokeh2html(fig = plot_scatter, file = "plot_scatter_interactive.html")  
    * browseURL("plot_scatter_interactive.html")  
  
Example code includes:  
```{r eval=FALSE}

ctry <- c('Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Bahrain', 'Bangladesh', 'Belgium', 'Benin', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Gambia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kenya', 'Kuwait', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Mali', 'Mauritania', 'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Rwanda', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovenia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Zambia', 'Zimbabwe', 'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Bahrain', 'Bangladesh', 'Belgium', 'Benin', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Gambia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kenya', 'Kuwait', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Mali', 'Mauritania', 'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Rwanda', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovenia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Zambia', 'Zimbabwe')
ctryCode <- c('AFG', 'ALB', 'DZA', 'AGO', 'ARG', 'AUS', 'AUT', 'BHR', 'BGD', 'BEL', 'BEN', 'BIH', 'BWA', 'BRA', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 'CRI', 'HRV', 'CUB', 'CZE', 'DNK', 'DJI', 'DOM', 'ECU', 'EGY', 'SLV', 'ERI', 'ETH', 'FIN', 'FRA', 'GAB', 'GMB', 'DEU', 'GHA', 'GRC', 'GTM', 'GIN', 'GNB', 'HTI', 'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KEN', 'KWT', 'LBN', 'LSO', 'LBR', 'LBY', 'MDG', 'MWI', 'MYS', 'MLI', 'MRT', 'MUS', 'MEX', 'MNG', 'MON', 'MAR', 'MOZ', 'MMR', 'NAM', 'NPL', 'NLD', 'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'OMN', 'PAK', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'ROM', 'RWA', 'STP', 'SAU', 'SEN', 'SCG', 'SLE', 'SGP', 'SVN', 'ZAF', 'ESP', 'LKA', 'SDN', 'SWE', 'CHE', 'THA', 'TGO', 'TTO', 'TUN', 'TUR', 'UGA', 'GBR', 'USA', 'URY', 'ZMB', 'ZWE', 'AFG', 'ALB', 'DZA', 'AGO', 'ARG', 'AUS', 'AUT', 'BHR', 'BGD', 'BEL', 'BEN', 'BIH', 'BWA', 'BRA', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 'CRI', 'HRV', 'CUB', 'CZE', 'DNK', 'DJI', 'DOM', 'ECU', 'EGY', 'SLV', 'ERI', 'ETH', 'FIN', 'FRA', 'GAB', 'GMB', 'DEU', 'GHA', 'GRC', 'GTM', 'GIN', 'GNB', 'HTI', 'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KEN', 'KWT', 'LBN', 'LSO', 'LBR', 'LBY', 'MDG', 'MWI', 'MYS', 'MLI', 'MRT', 'MUS', 'MEX', 'MNG', 'MON', 'MAR', 'MOZ', 'MMR', 'NAM', 'NPL', 'NLD', 'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'OMN', 'PAK', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'ROM', 'RWA', 'STP', 'SAU', 'SEN', 'SCG', 'SLE', 'SGP', 'SVN', 'ZAF', 'ESP', 'LKA', 'SDN', 'SWE', 'CHE', 'THA', 'TGO', 'TTO', 'TUN', 'TUR', 'UGA', 'GBR', 'USA', 'URY', 'ZMB', 'ZWE')
regn <- c('AP', 'ECA', 'MENA', 'SSA', 'AME', 'AP', 'WE/EU', 'MENA', 'AP', 'WE/EU', 'SSA', 'ECA', 'SSA', 'AME', 'WE/EU', 'SSA', 'SSA', 'AP', 'SSA', 'AME', 'SSA', 'SSA', 'AME', 'AP', 'AME', 'SSA', 'AME', 'WE/EU', 'AME', 'WE/EU', 'WE/EU', 'SSA', 'AME', 'AME', 'MENA', 'AME', 'SSA', 'SSA', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'WE/EU', 'SSA', 'WE/EU', 'AME', 'SSA', 'SSA', 'AME', 'AME', 'WE/EU', 'WE/EU', 'AP', 'AP', 'MENA', 'WE/EU', 'MENA', 'WE/EU', 'AME', 'AP', 'MENA', 'SSA', 'MENA', 'MENA', 'SSA', 'SSA', 'MENA', 'SSA', 'SSA', 'AP', 'SSA', 'SSA', 'SSA', 'AME', 'AP', 'ECA', 'MENA', 'SSA', 'AP', 'SSA', 'AP', 'WE/EU', 'AP', 'AME', 'SSA', 'SSA', 'WE/EU', 'MENA', 'AP', 'AME', 'AME', 'AME', 'AP', 'WE/EU', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'MENA', 'SSA', 'ECA', 'SSA', 'AP', 'WE/EU', 'SSA', 'WE/EU', 'AP', 'MENA', 'WE/EU', 'WE/EU', 'AP', 'SSA', 'AME', 'MENA', 'ECA', 'SSA', 'WE/EU', 'AME', 'AME', 'SSA', 'SSA', 'AP', 'ECA', 'MENA', 'SSA', 'AME', 'AP', 'WE/EU', 'MENA', 'AP', 'WE/EU', 'SSA', 'ECA', 'SSA', 'AME', 'WE/EU', 'SSA', 'SSA', 'AP', 'SSA', 'AME', 'SSA', 'SSA', 'AME', 'AP', 'AME', 'SSA', 'AME', 'WE/EU', 'AME', 'WE/EU', 'WE/EU', 'SSA', 'AME', 'AME', 'MENA', 'AME', 'SSA', 'SSA', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'WE/EU', 'SSA', 'WE/EU', 'AME', 'SSA', 'SSA', 'AME', 'AME', 'WE/EU', 'WE/EU', 'AP', 'AP', 'MENA', 'WE/EU', 'MENA', 'WE/EU', 'AME', 'AP', 'MENA', 'SSA', 'MENA', 'MENA', 'SSA', 'SSA', 'MENA', 'SSA', 'SSA', 'AP', 'SSA', 'SSA', 'SSA', 'AME', 'AP', 'ECA', 'MENA', 'SSA', 'AP', 'SSA', 'AP', 'WE/EU', 'AP', 'AME', 'SSA', 'SSA', 'WE/EU', 'MENA', 'AP', 'AME', 'AME', 'AME', 'AP', 'WE/EU', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'MENA', 'SSA', 'ECA', 'SSA', 'AP', 'WE/EU', 'SSA', 'WE/EU', 'AP', 'MENA', 'WE/EU', 'WE/EU', 'AP', 'SSA', 'AME', 'MENA', 'ECA', 'SSA', 'WE/EU', 'AME', 'AME', 'SSA', 'SSA')
cnt <- c('Asia', 'Europe', 'Africa', 'Africa', 'Americas', 'Oceania', 'Europe', 'Asia', 'Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'Americas', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Americas', 'Africa', 'Africa', 'Americas', 'Asia', 'Americas', 'Africa', 'Americas', 'Europe', 'Americas', 'Europe', 'Europe', 'Africa', 'Americas', 'Americas', 'Africa', 'Americas', 'Africa', 'Africa', 'Europe', 'Europe', 'Africa', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Africa', 'Africa', 'Americas', 'Americas', 'Europe', 'Europe', 'Asia', 'Asia', 'Asia', 'Europe', 'Asia', 'Europe', 'Americas', 'Asia', 'Asia', 'Africa', 'Asia', 'Asia', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Asia', 'Africa', 'Africa', 'Africa', 'Americas', 'Asia', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Asia', 'Europe', 'Oceania', 'Americas', 'Africa', 'Africa', 'Europe', 'Asia', 'Asia', 'Americas', 'Americas', 'Americas', 'Asia', 'Europe', 'Europe', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Europe', 'Africa', 'Asia', 'Europe', 'Africa', 'Europe', 'Asia', 'Africa', 'Europe', 'Europe', 'Asia', 'Africa', 'Americas', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Americas', 'Africa', 'Africa', 'Asia', 'Europe', 'Africa', 'Africa', 'Americas', 'Oceania', 'Europe', 'Asia', 'Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'Americas', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Americas', 'Africa', 'Africa', 'Americas', 'Asia', 'Americas', 'Africa', 'Americas', 'Europe', 'Americas', 'Europe', 'Europe', 'Africa', 'Americas', 'Americas', 'Africa', 'Americas', 'Africa', 'Africa', 'Europe', 'Europe', 'Africa', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Africa', 'Africa', 'Americas', 'Americas', 'Europe', 'Europe', 'Asia', 'Asia', 'Asia', 'Europe', 'Asia', 'Europe', 'Americas', 'Asia', 'Asia', 'Africa', 'Asia', 'Asia', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Asia', 'Africa', 'Africa', 'Africa', 'Americas', 'Asia', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Asia', 'Europe', 'Oceania', 'Americas', 'Africa', 'Africa', 'Europe', 'Asia', 'Asia', 'Americas', 'Americas', 'Americas', 'Asia', 'Europe', 'Europe', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Europe', 'Africa', 'Asia', 'Europe', 'Africa', 'Europe', 'Asia', 'Africa', 'Europe', 'Europe', 'Asia', 'Africa', 'Americas', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Americas', 'Africa', 'Africa')
idx <- rep(c("corruption_perception_index", "human_development_index"), each=121)
cpiRk <- c(166, 88, 88, 163, 106, 13, 16, 50, 139, 15, 83, 76, 29, 76, 69, 76, 150, 150, 130, 10, 145, 147, 23, 83, 83, 136, 40, 50, 56, 38, 1, 98, 102, 106, 88, 72, 154, 102, 3, 23, 98, 123, 11, 56, 58, 123, 139, 158, 158, 111, 50, 13, 76, 88, 161, 18, 32, 61, 69, 18, 45, 139, 55, 123, 61, 83, 161, 123, 111, 54, 95, 111, 45, 111, 72, 61, 88, 111, 147, 45, 130, 9, 1, 130, 98, 136, 5, 60, 117, 72, 130, 88, 95, 29, 28, 58, 43, 66, 48, 61, 71, 119, 7, 34, 61, 37, 83, 165, 4, 6, 76, 106, 72, 76, 66, 139, 11, 16, 21, 76, 150, 166, 88, 88, 163, 106, 13, 16, 50, 139, 15, 83, 76, 29, 76, 69, 76, 150, 150, 130, 10, 145, 147, 23, 83, 83, 136, 40, 50, 56, 38, 1, 98, 102, 106, 88, 72, 154, 102, 3, 23, 98, 123, 11, 56, 58, 123, 139, 158, 158, 111, 50, 13, 76, 88, 161, 18, 32, 61, 69, 18, 45, 139, 55, 123, 61, 83, 161, 123, 111, 54, 95, 111, 45, 111, 72, 61, 88, 111, 147, 45, 130, 9, 1, 130, 98, 136, 5, 60, 117, 72, 130, 88, 95, 29, 28, 58, 43, 66, 48, 61, 71, 119, 7, 34, 61, 37, 83, 165, 4, 6, 76, 106, 72, 76, 66, 139, 11, 16, 21, 76, 150)
vl <- c(0.479, 0.764, 0.745, 0.533, 0.827, 0.939, 0.893, 0.824, 0.579, 0.896, 0.485, 0.75, 0.698, 0.754, 0.794, 0.402, 0.404, 0.563, 0.518, 0.92, 0.352, 0.396, 0.847, 0.738, 0.727, 0.498, 0.776, 0.827, 0.775, 0.878, 0.925, 0.473, 0.722, 0.739, 0.691, 0.68, 0.42, 0.448, 0.895, 0.897, 0.697, 0.452, 0.926, 0.579, 0.866, 0.64, 0.414, 0.424, 0.493, 0.625, 0.836, 0.921, 0.624, 0.689, 0.649, 0.923, 0.899, 0.887, 0.73, 0.903, 0.742, 0.555, 0.8, 0.763, 0.497, 0.427, 0.716, 0.512, 0.476, 0.789, 0.442, 0.513, 0.781, 0.762, 0.735, 0.807, 0.647, 0.418, 0.556, 0.64, 0.558, 0.924, 0.915, 0.645, 0.353, 0.527, 0.949, 0.796, 0.55, 0.788, 0.693, 0.74, 0.682, 0.855, 0.843, 0.802, 0.498, 0.574, 0.847, 0.494, 0.776, 0.42, 0.925, 0.89, 0.666, 0.884, 0.766, 0.49, 0.913, 0.939, 0.74, 0.487, 0.78, 0.725, 0.767, 0.493, 0.91, 0.92, 0.795, 0.579, 0.516, 11, 36, 36, 15, 32, 79, 76, 51, 25, 77, 37, 38, 63, 38, 41, 38, 21, 21, 27, 83, 24, 22, 70, 37, 37, 26, 55, 51, 47, 56, 91, 34, 33, 32, 36, 39, 18, 33, 90, 70, 34, 28, 81, 47, 46, 28, 25, 17, 17, 31, 51, 79, 38, 36, 16, 75, 61, 44, 41, 75, 53, 25, 49, 28, 44, 37, 16, 28, 31, 50, 35, 31, 53, 31, 39, 44, 36, 31, 22, 53, 27, 84, 91, 27, 34, 26, 88, 45, 30, 39, 27, 36, 35, 63, 64, 46, 54, 42, 52, 44, 40, 29, 85, 60, 44, 58, 37, 12, 89, 86, 38, 32, 39, 38, 42, 25, 81, 76, 74, 38, 21)

hdi_cpi_data_long <- data.frame(country=ctry, year=2015L, country_code=ctryCode, cpi_rank=cpiRk, 
                                region=regn, continent=cnt, index=idx, value=vl,
                                stringsAsFactors = FALSE
                                )

## explore hdi_cpi_data_long
str(hdi_cpi_data_long)

## How many unique values are there in the index column?
unique(hdi_cpi_data_long$index)


## convert from long to wide
hdi_cpi_data_wide <- hdi_cpi_data_long %>% 
  spread(key=index, value=value)
  
## display the first 5 rows from hdi_cpi_data_wide
head(hdi_cpi_data_wide, 5)


## plot corruption_perception_index  versus human_development_index
figure(legend_location = "top_left") %>% 
    ly_points(x=human_development_index, y=corruption_perception_index, color=continent, alpha=0.7,
              hover=c(country, cpi_rank,corruption_perception_index, human_development_index), 
              data=hdi_cpi_data_wide
              )


## convert from wide to long
hdi_cpi_remake_long <- hdi_cpi_data_wide %>%
    gather(key="index", value="value", corruption_perception_index, human_development_index)
  
## display the first 5 rows of hdi_data_long
head(hdi_cpi_remake_long, 5)
all.equal(hdi_cpi_data_long, hdi_cpi_remake_long)


## explore the unique values in the movie_budget column
# unique(dat_90_13_long$movie_budget)

## spread the values in the `movie_budget` in two columns
# dat_90_13_wide <- dat_90_13_long %>% 
#   spread(key=movie_budget, value=value)
  
## View column names of dat_90_13_wide
# names(dat_90_13_wide)

## create a scatter plot with log(budget_2013) Vs log(intgross_2013) 
# p_scatter <- figure() %>%
#   ly_points(y=log(intgross_2013), x=log(budget_2013), size=4, alpha=0.5, data=dat_90_13_wide)
  
## View plot
# p_scatter

## fit a linear reg model
# lin_reg <- lm(log(intgross_2013) ~ log(budget_2013), data = dat_90_13)

## add the linear regression line layer to p_scatter
# p_scatter %>% 
#   ly_abline(lin_reg)


## extract entries for year 2007
dat_2007 <- gapminder %>% 
  filter(year == 2007)
dat_2002 <- gapminder %>% 
  filter(year == 2002)

## create scatter plot
figure(toolbar_location="above", legend_location="bottom_right") %>%
    ly_points(x=gdpPercap, y=lifeExp, color=continent, size=6, alpha=0.7, 
              data=dat_2007, hover=c(country, lifeExp, gdpPercap)
              )

figure(legend_location = "bottom_right", tools=c("resize", "save")) %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_2002, color = continent)

figure(legend_location = "bottom_right", tools=c("resize", "save"), toolbar_location=NULL) %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_2002, color = continent)

```
  
  
  
***
  
Chapter 4 - Grid Plots and Maps  
  
Intro to Grid Plots:  
  
* Example of dataset for TB by year by age group in the US - combine multiple figures in the same area  
	* figure() %>% ly_bar(x = year, y = count, data = tb_2534, color = gender, position = "stack")  # x should be a factor, default is stacked bars if position is missing (dodge or fill also available)  
* Can use the grid plot basics for multi-plotting  
	* fig_list <- list(bar_2534 = bar_2534, bar_3544 = bar_3544)  
    * grid_plot(fig_list, width = 1000, height = 500)  # will have different axis limits by default  
    * grid_plot(fig_list, width = 1000, height = 500, same_axes = TRUE)  # forces the axes to be on the same scale  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  # overrides the defaults of all in one row  
    * fig_list <- list(list(bar_1524 = bar_1524, bar_2534 = bar_2534), list(bar_3544 = bar_3544))  
    * grid_plot(fig_list, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  # list of plots plots one list per row, and NULL will place an empty plot in that column  
  
Facets with Grid Plots:  
  
* Facets can be helpful for slicing data, placing small batches of the data (segmented by factor) in a larger plot  
* Can start by creating a plot for each of the groups, though this is inefficient  
	* fig_list <- list(bar_1524 = bar_1524, bar_2534 = bar_2534, bar_3544 = bar_3544, bar_4554 = bar_4554, bar_5564 = bar_5564, bar_65 = bar_65)  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  
* Can instead use the split() function and a function for plotting to create the relevant lists  
	* tb_split_age <- split(tb, tb$age)  
    * plot_bar <- function(x){ figure() %>% ly_bar(y = count, year, data = x, color = gender, position = "dodge")}  
* Can instead use the lapply() functionality  
	* fig_list <- lapply(tb_split_age, plot_bar)  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  
  
rbokeh maps:  
  
* Can create interactive maps using rbokeh - transportation, density, population, and other areas where geography is key to understanding  
* NYC bike data available from the bikedata package  
* Can begin by initializing a map (appears to source Google Maps)  
	* ny_map <- gmap(lat = 40.73306, lng = -73.97351, zoom = 11)   
* Can use the map type argument to change the type of map - hybrid, satellite, road, terrain  
* Can also customize maps using gamp_style(), such as making the water blue  
	* ny_map <- gmap(lat = 40.73306, lng = -73.97351, zoom = 11, map_style = gmap_style("blue_water"))   
* Can then add points layers to the map and arrange in grids using grid_plot() as per previous chapters  
	* ny_map %>% ly_points(x = station_longitude, y = station_latitude, data = ny_bikedata_20170427, fill_color = start_count, line_alpha = 0, size = 8, hover = c(station_name, start_count))  
    * grid_plot(list(weekeend_April23 = map_weekend_20170423, weekday_April25 = map_weekday_20170425), width = 860, height = 420)  
  
Example code includes:  
```{r eval=FALSE}

tb <- data.frame(iso2="US", 
                 gender=rep(c("m", "f"), each=84),
                 year=factor(c(1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008)), 
                 age=c(1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65), 
                 count=c(355, 333, 330, 321, 331, 365, 320, 343, 365, 362, 383, 388, 414, 375, 876, 815, 701, 663, 616, 602, 613, 562, 526, 547, 535, 568, 490, 513, 1417, 1219, 1127, 1009, 1011, 906, 824, 813, 754, 728, 666, 659, 572, 495, 1121, 1073, 979, 1007, 930, 904, 876, 795, 828, 829, 767, 759, 744, 725, 742, 678, 679, 628, 601, 577, 524, 490, 487, 504, 499, 531, 533, 526, 1099, 1007, 944, 914, 801, 738, 649, 592, 650, 582, 624, 596, 562, 561, 280, 289, 269, 269, 232, 246, 239, 233, 277, 265, 241, 257, 257, 220, 579, 487, 449, 425, 391, 376, 410, 423, 353, 339, 348, 384, 338, 329, 499, 478, 447, 424, 394, 349, 346, 362, 310, 302, 276, 263, 260, 269, 285, 279, 254, 267, 245, 253, 247, 255, 269, 252, 242, 212, 225, 224, 202, 217, 201, 179, 244, 152, 176, 167, 169, 166, 161, 146, 135, 172, 591, 541, 514, 492, 444, 396, 389, 370, 354, 344, 322, 303, 308, 300), 
                 stringsAsFactors = FALSE
                 )
str(tb)
tb_2534 <- tb %>% filter(age==2534)
str(tb_2534)


## create a bar plot for age group tb_2534
bar_2534 <- figure() %>%
  ly_bar(x=year, y=count, color=gender, data=tb_2534, hover=TRUE)

## View figure
bar_2534


## create a bar plot for age group tb_2534 with % on the y-axis
bar_2534_percent <- figure(ylab = "share") %>% 
  ly_bar(x = year, y =  count,  tb_2534, color = gender, hover = TRUE,  position = "fill")
         
## View figure
bar_2534_percent


## create a list with bar_2534 and bar_2534_percent figures
fig_list <- list(bar_2534 = bar_2534, bar_2534_percent = bar_2534_percent)

## create a grid plot 
grid_plot(fig_list, width=1000, height=400)

## create a grid plot with same axes limits
grid_plot(figs = fig_list, width = 1000, height = 400, same_axes=TRUE)


plot_line <- function(x){
    figure() %>% 
        ly_lines(y =  count, year, data = x, color = age,  alpha = 1, width = 2)
}

## create two dataframes for female/male data           
tb_female <- tb %>% filter(gender=="f")
tb_male <- tb %>% filter(gender=="m")


## create two plots using plot_line
fig_female <- plot_line(tb_female)
fig_male <- plot_line(tb_male)

## create figure list
fig_list <- list(female = fig_female, male = fig_male)

## plot the two figures in a grid
grid_plot(fig_list, width=1000, height=600, same_axes=TRUE)


## split tb data by gender 
tb_split_gender <- split(tb, tb$gender)

## create a list of figures using lapply
fig_list <- lapply(tb_split_gender, FUN=plot_line)

## create a grid plot 
grid_plot(fig_list, width=1000, height=600, same_axes=TRUE)


## define a function to create a bar plot with the number of tb cases over time
plot_bar <- function(x){ 
    figure() %>% 
        ly_bar(y=count, x=year, data=x, color = gender, position = "dodge", hover=TRUE)
}

## split tb data by age
tb_split_age <- split(tb, tb$age)

## apply the function to the groups in tb_split_age
fig_list <- fig_list <- lapply(tb_split_age, plot_bar)

## create a grid plot 
grid_plot(fig_list, width=600, height=900, nrow=3, same_axes=TRUE) %>% 
    theme_axis("x", major_label_orientation = 90)


## initialize a map for NY center
# ny_map <- gmap(lat=40.73306, lng=-73.97351, zoom=11, map_style=gmap_style("blue_water"))
# ny_map


## filter ny_bikedata to get the entries for day "2017-04-25"
# ny_bikedata_20170425 <- ny_bikedata %>% filter(trip_date==as.Date("2017-04-25"))

## add a points layer to ny_map
# ny_map %>%
#     ly_points(y=station_latitude, x=station_longitude, 
#               size=8, fill_color=start_count, line_alpha=0, 
#               data=ny_bikedata_20170425, hover=c(station_name, start_count, end_count)
#               )

## create a names list with the two figures
# fig_list <- list(map_weekend=map_weekend_20170423, map_weekday=map_weekday_20170425)

## create a grid plot with the 2 maps
# grid_plot(fig_list, width=860, height=420)

```
  
  
  
***
  
###_A/B Testing in R_  
  
Chapter 1 - Mini Case Study in A/B Testing  
  
Introduction:  
  
* A/B testing is a powerful way to experiment with potential changes before implementing them  
	* Framework for testing new ideas to improve an existing design (often a website)  
* Hypothetical example - cat adoption website - could a change in home page improve conversion rate (clicks divided by views)?  
	* Question - does changing the photo improve conversion rate?  
    * Hypothesis - cat in hat will improve conversion rate  
    * Dependent variable - clicks  
    * Independent variable - homepage photo  
* Need to begin by assessing conversion rates in the current website  
	* click_data <- read_csv("click_data.csv")  
  
Baseline conversion rates:  
  
* Contnuning the previous example - hypothesis of cats in hats having "more" conversions  
	* Need to define "more" relative to some baseline - recent past, control group at same time, etc.  
    * click_data %>% summarize(conversion_rate = mean(clicked_adopt_today))  # mean from historical data  
    * click_data_sum <- click_data %>% group_by(lubridate::month(visit_date)) %>% summarize(conversion_rate = mean(clicked_adopt_today))  # mean by month from the historical data  
    * ggplot(click_data_sum, aes(x = `month(visit_date)`, y=conversion_rate)) + geom_point() + geom_line()  
  
Experimental design and power analysis:  
  
* Power analysis helps determine how many samples are needed (thus how long the experiment needs to run)  
	* Ideal is to run both conditions simultaneously - mitigate seasonality and other potential confounders  
    * Should know the planned test, baseline (control) value, and desired (test) value, as well as proportion of the data (typically 0.5), significance/alpha (typically 0.05), and power (typically 0.8)  
* Can use the powerMediation package to assess the power - note the function returns the total sample size, so each group is divided by 2  
	* library(powerMediation)  
    * total_sample_size <- SSizeLogisticBin(p1 = 0.2, p2 = 0.3, B = 0.5, alpha = 0.05, power = 0.8)  # note the function returns the total sample size, so each group is divided by 2  
  
Example code includes:  
```{r}


# Read in data
click_data <- readr::read_csv("./RInputFiles/click_data.csv")
click_data


# Find oldest and most recent date
min(click_data$visit_date)
max(click_data$visit_date)

# Calculate the mean conversion rate by day of the week
click_data %>%
  group_by(weekdays(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Calculate the mean conversion rate by week of the year
click_data %>%
  group_by(lubridate::week(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))


# Compute conversion rate by week of the year
click_data_sum <- click_data %>%
    mutate(weekOfYear = lubridate::week(visit_date)) %>%
    group_by(weekOfYear) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Build plot
ggplot(click_data_sum, aes(x = `weekOfYear`, y = conversion_rate)) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent)


# Compute and look at sample size for experiment in August
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.54, p2 = 0.64, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# Compute and look at sample size for experiment in August with 5% increase
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.54, p2 = 0.59, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size

```
  
  
  
***
  
Chapter 2 - Mini Case Study in A/B Testing - Part II  
  
Analyzing Results:  
  
* Can analyze the experiment data from the previous design - available in a new dataset  
	* experiment_data <- read_csv("experiment_data.csv")  
    * experiment_data %>% group_by(condition) %>% summarize(conversion_rate = mean(clicked_adopt_today))  
    * experiment_data_sum <- experiment_data %>% group_by(visit_date, condition) %>% summarize(conversion_rate = mean(clicked_adopt_today))  
    * ggplot(experiment_data_sum, aes(x = visit_date, y = conversion_rate, color = condition, group = condition)) + geom_point() + geom_line()  
* Can further assess the statistical significance of the outcomes  
	* glm(clicked_adopt_today ~ condition, family = "binomial", data = experiment_data) %>% broom::tidy()  
  
Designing follow-up experiments:  
  
* Can continue to refine and test new hypotheses, but typically still with one step at a time  
	* Experiments need to be unique, and each with their own control group  
    * Attempt to avoid confounding variables; hard to explain real-world outcomes if there were many changes at the same time  
  
Pre-follow-up-experiment assumptions:  
  
* Control conditions for seasonal products can be especially challenging - careful not to choose times that already have very extreme conversion rates  
  
Follow-up experiment assumptions:  
  
* May want to look at the differences in conversion rate by month  
	* eight_month_checkin_data_sum <- eight_month_checkin_data %>%  
    *     mutate(month_text = month(visit_date, label = TRUE)) %>% group_by(month_text, condition) %>%  
    *     summarize(conversion_rate = mean(clicked_adopt_today))  
    * eight_month_checkin_data_diff <- eight_month_checkin_data_sum %>%  
    *     spread(condition, conversion_rate) %>%  
    *     mutate(condition_diff = cat_hat - no_hat)  
    * mean(eight_month_checkin_data_diff$condition_diff)  
    * sd(eight_month_checkin_data_diff$condition_diff)  
  
Example code includes:  
```{r}

experiment_data <- read_csv("./RInputFiles/experiment_data.csv")
experiment_data
followup_experiment_data <- read_csv("./RInputFiles/eight_month_checkin_data.csv")
followup_experiment_data


# Group and summarize data
experiment_data_clean_sum <- experiment_data %>%
    group_by(condition, visit_date) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Make plot of conversion rates over time
ggplot(experiment_data_clean_sum, aes(x = visit_date, y = conversion_rate, 
                                      color = condition, group = condition
                                      )
       ) + 
    geom_point() +
    geom_line()


# View summary of results
experiment_data %>% 
    group_by(condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
experiment_results <- glm(clicked_adopt_today ~ condition, family = "binomial", 
                          data = experiment_data
                          ) %>%
    broom::tidy()
experiment_results


# Run logistic regression power analysis
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.39, p2 = 0.59, B = 0.5, 
                                                      alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# View conversion rates by condition
followup_experiment_data %>%
    group_by(condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
followup_experiment_results <- glm(clicked_adopt_today ~ condition, family = "binomial",
                                   data = followup_experiment_data
                                   ) %>%
    broom::tidy()
followup_experiment_results


# Compute monthly summary
eight_month_checkin_data_sum <- followup_experiment_data %>%
    mutate(month_text = lubridate::month(visit_date, label = TRUE)) %>%
    group_by(month_text, condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Plot month-over-month results
ggplot(eight_month_checkin_data_sum, aes(x = month_text, y = conversion_rate, 
                                         color = condition, group = condition
                                         )
       ) +
    geom_point() +
    geom_line()


# Plot monthly summary
ggplot(eight_month_checkin_data_sum, aes(x = month_text, y = conversion_rate,
                                         color = condition, group = condition
                                         )
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
    labs(x = "Month", y = "Conversion Rate")


# Compute difference over time
# no_hat_data_diff <- no_hat_data_sum %>% 
#     spread(year, conversion_rate) %>% 
#     mutate(year_diff = `2018` - `2017`)
# no_hat_data_diff

# Compute summary statistics
# mean(no_hat_data_diff$year_diff, na.rm = TRUE)
# sd(no_hat_data_diff$year_diff, na.rm = TRUE)


# Run power analysis for logistic regression
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.49, p2 = 0.64, B = 0.5, 
                                                      alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# View summary of data
# followup_experiment_data_sep %>% 
#     group_by(condition) %>% 
#     summarize(conversion_rate=mean(clicked_adopt_today))

# Run logistic regression
# followup_experiment_sep_results <- glm(clicked_adopt_today ~ condition,
#                                        family = "binomial",
#                                        data = followup_experiment_data_sep
#                                        ) %>%
#     broom::tidy()
# followup_experiment_sep_results

```
  
  
  
***
  
Chapter 3 - Experimental Design in A/B Testing  
  
A/B Testing Research Questions:  
  
* A/B testing combined experimental design and statistics - core building block basic principles  
* Any experimental design that compares two ideas can be run as an A/B test - conversion rates, engagement with a web site, drop-off rates, total amount of time or money spent  
* Example of looking at time spent on a websit  
	* str(viz_website_2017)  
    * viz_website_2017 %>% summarize(mean(time_spent_homepage_sec))  
    * viz_website_2017 %>% group_by(month(visit_date)) %>% summarize(mean(time_spent_homepage_sec))  
  
Assumptions and types of A/B testing:  
  
* Example of changing text in a website title, then checking the implications  
* Can look at within-group experiments (everyone sees both) or between-group experiments (everyone sees one or the other)  
	* The within experiment will often have better power, while the between experiment is easier to run when people may only interact once  
    * The between experiment needs to be appropriately random, so that whether the person sees A/B is not linked to other attributes of the person  
* There are several types of A/B testing  
	* A/B testing is test and control  
    * A/A testing is to verify that the control process is working well - should be no significant effects  
    * A/B/N testing is a control conditions with any number of test conditions - seems exciting and fast, but has more challenging statistics and requires more data points  
  
Confounding variables?  
  
* Confounding variables are elements of the environment that can confound your ability to find the real effect of A/B  
	* Sometimes the confounder is internal to the experiment - examples of word length/novelty being the real driver rather than the specific work chosen  
    * Sometimes the confounder is external to the experiment - examples of differing demographics by month, with the demographics having been a key driver of outcomes  
  
Side effects:  
  
* Side effects are unintended effects of a change that you made  
    * Example of changing from tools to tips if it changed the page loading times  
* Side effects can include load times and information "above the fold" (what a person sees without doing any scrolling)  
  
Example code includes:  
```{r eval=FALSE}

# Compute summary by month
viz_website_2017 %>%
    group_by(month(visit_date)) %>%
    summarize(article_conversion_rate = mean(clicked_article))


# Compute 'like' click summary by month
viz_website_2017_like_sum <- viz_website_2017 %>%
    mutate(month = month(visit_date, label = TRUE)) %>%
    group_by(month) %>%
    summarize(like_conversion_rate = mean(clicked_like))

# Plot 'like' click summary by month
ggplot(viz_website_2017_like_sum,
       aes(x = month, y = like_conversion_rate, group = 1)
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Plot comparison of 'like'ing and 'sharing'ing an article
ggplot(viz_website_2017_like_share_sum,
       aes(x = month, y = conversion_rate, color = action, group = action)
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Compute conversion rates for A/A experiment
viz_website_2018_01_sum <- viz_website_2018_01 %>%
    group_by(condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))
viz_website_2018_01_sum

# Plot conversion rates for two conditions
ggplot(viz_website_2018_01_sum, aes(x = condition, y = like_conversion_rate)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Run logistic regression
aa_experiment_results <- glm(clicked_like ~ condition, family = "binomial", data = viz_website_2018_01) %>%
    broom::tidy()
aa_experiment_results


# Compute 'like' conversion rate by week and condition
viz_website_2018_02 %>%
    mutate(week = week(visit_date)) %>%
    group_by(week, condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))

# Compute 'like' conversion rate by if article published and condition
viz_website_2018_02 %>%
    group_by(article_published, condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))


# Plot 'like' conversion rates by date for experiment
ggplot(viz_website_2018_02_sum,
       aes(x = visit_date, y = like_conversion_rate, color = condition,
           linetype = article_published, group = interaction(condition, article_published)
           )
       ) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = as.numeric(as.Date("2018-02-15"))) +
    scale_y_continuous(limits = c(0, 0.3), labels = percent)


# Compute 'like' conversion rate and mean pageload time by day
viz_website_2018_03_sum <- viz_website_2018_03 %>%
    group_by(visit_date, condition) %>%
    summarize(mean_pageload_time = mean(pageload_time), like_conversion_rate = mean(clicked_like))

# Plot effect of 'like' conversion rate by pageload time
ggplot(viz_website_2018_03_sum, aes(x = mean_pageload_time, y = like_conversion_rate, color = condition)) +
    geom_point()


# Plot 'like' conversion rate by day
ggplot(viz_website_2018_03_sum, aes(x = visit_date, y = like_conversion_rate, color = condition,
                                    linetype = pageload_delay_added, 
                                    group = interaction(condition, pageload_delay_added)
                                    )
       ) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = as.numeric(as.Date("2018-03-15"))) +
    scale_y_continuous(limits = c(0, 0.3), labels = percent)

```
  
  
  
***
  
Chapter 4 - Statistical Analyses in A/B Testing  
  
Power analyses:  
  
* Generally, the goal of a power analysis is to determine the sample size - dependent on alpha, power (1 minus beta), and effect size  
	* Effect size is often defined as the difference in the two groups divided by the standard deviation of the groups  
* The t-test is often used for significance, and can be planned using the library(pwr)  
	* pwr.t.test(power = 0.8, sig.level = 0.05, d = 0.6)  # will return the number of data points needed for power 0.8, alpha 0.05, effect size 0.6  
    * pwr.t.test(power = 0.8, sig.level = 0.05, d = 0.2)  # effect size change of ~3x drives sample size change of ~9x (delta-effect-size-squared)  
  
Statistical tests:  
  
* Logistic regression and t-tests are both common statistical methods used for A/B testing  
	* viz_website_2018_01 <- read_csv("viz_website_2018_01.csv")  
    * aa_experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = viz_website_2018_01)  
* Linear regression can be thought of as an extension of t-tests with more than 2 levels per variable  
	* However, for an A/B test with only 2-levels, you will get the same results  
  
Stopping rules and sequential analysis:  
  
* Stopping rules are procedures that allow for interim analysis (peaks in to the data) - also known as "sequential analysis"  
	* Can stop because the experiment worked, stop because the experiment failed, or continue experiment  
    * The p-value needs to be adjusted lower to account for the multiple peaks at the data  
    * Need to be very careful to prevent p-hacking by creating the stopping rules and points in advance  
* The library(gsDesign) can help with running sequrntial analysis in R  
	* library(gsDesign)  
    * seq_analysis <- gsDesign(k = 4, test.type = 1, alpha = 0.05, beta = 0.2, sfu = "Pocock")  # k=4 looks, test.type=1 is similar to one-sided test, alpha is significance, beta is 1-power so beta=0.2 is power=0.8, sfu is the spending function  
* Can then figure out the sample sizes using resource-based approaches  
	* max_n <- 1000  
    * max_n_per_group <- max_n / 2  
    * stopping_points <- max_n_per_group * seq_analysis$timing  
  
Multivariate testing:  
  
* Sometmes want to make comparisons that account for multiple changes  
	* multivar_results <- lm(time_spent_homepage_sec ~ word_one data = viz_website_2018_05) %>% tidy()  # single variable  
    * multivar_results <- lm(time_spent_homepage_sec ~ word_one * word_two, data = viz_website_2018_05) %>% tidy()  # full interaction effects  
* The default R order for regressions is to use the lowest alphanumeric as the baseline level - can modify this pre-regression though  
	* multivar_results <- viz_website_2018_05 %>% mutate(word_one = factor(word_one, levels = c("tips", "tools"))) %>% mutate(word_two = factor(word_two, levels = c("better", "amazing"))) %>% lm(time_spent_homepage_sec ~ word_one * word_two, data = .) %>% tidy()  
  
A/B Testing Recap:  
  
* Introduction to the basic concepts of A/B testing  
* New Ideas -> Experiments -> Statistical Analysis -> Implement Winners -> Repeat  
  
Example code includes:  
```{r eval=FALSE}

# Run power analysis for logistic regression
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.17, p2 = 0.27, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# Run power analysis for t-test
sample_size <- pwr::pwr.t.test(d = 0.3, sig.level = 0.05, power = 0.8)
sample_size


# Run logistic regression
ab_experiment_results <- glm(clicked_like ~ condition, family = "binomial", data = viz_website_2018_04) %>%
    broom::tidy()
ab_experiment_results


# Run t-test
ab_experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = viz_website_2018_04)
ab_experiment_results


# Run sequential analysis
seq_analysis_3looks <- gsDesign::gsDesign(k = 3, test.type = 1, 
                                          alpha = 0.05, beta = 0.2, sfu = "Pocock"
                                          )
seq_analysis_3looks


# Fill in max number of points and compute points per group and find stopping points
max_n <- 3000
max_n_per_group <- max_n / 2
stopping_points <- max_n_per_group * seq_analysis_3looks$timing
stopping_points


# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>% 
    group_by(word_one, word_two) %>% 
    summarize(mean_time_spent_homepage_sec = mean(time_spent_homepage_sec))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum, aes(x = word_one, y = mean_time_spent_homepage_sec, fill = word_two)) + 
    geom_bar(stat = "identity", position = "dodge")


# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>% 
    group_by(word_one, word_two) %>% 
    summarize(like_conversion_rate = mean(clicked_like))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum, aes(x = word_one, y = like_conversion_rate, fill = word_two)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Organize variables and run logistic regression
viz_website_2018_05_like_results <- viz_website_2018_05 %>%
    mutate(word_one = factor(word_one, levels = c("tips", "tools"))) %>%
    mutate(word_two = factor(word_two, levels = c("better", "amazing"))) %>%
    glm(clicked_like ~ word_one * word_two, family = "binomial", data = .) %>%
    broom::tidy()
viz_website_2018_05_like_results

```
  
  
  
***
  
###_Mixture Models in R_  
  
Chapter 1 - Introduction to Mixture Models  
  
Introduction to Model-Based Clustering:  
  
* Mixture models are a tool for model-based clustering (partitioning and segmentation)  
	* Objective for clusters to be homogenous within and heterogenous across  
* Common techniques include k-means (assign to nearest centers) and hierarchical (connect based on sililarity) and probabilistic model-based approaches (mixture models)  
	* gender <- read.csv("gender.csv")  
    * ggplot(gender, aes(x = Weight, y = BMI)) + geom_points()  
* May be helpful to have probabilities by gender rather than a hard cutoff for male vs. female  
	* This is the core of the mixture model - assumes an underlying probability distribution, with the outcome being the combination of these distributions  
  
Gaussian Distribution:  
  
* There are packages for fitting mixture models in R - mixtools (no Poisson), bayesmix (Bayesian), EMCluster (Gaussian only), flexmix (covered in this course)  
* The Gaussain distribution frequently plays a role in the mixture model  
	* Defined by the mean and standard deviation  
    * rnorm(n, mean, sd)  # sample from the Gaussian of mean and sd, taking n samples  
    * The mean is typically estimated as the sample mean, and the sd is typically estimated as root-mean-squared-delta-from-mean - sd()  
    * ggplot(data = population_sample) + geom_histogram(aes(x = x, y = ..density..)) + stat_function(geom = "line", fun = dnorm, args = list(mean = mean_estimate, sd = standard_deviation_estimate))  
  
Gaussian Mixture Models (GMM):  
  
* Can imagine two Gaussian distributions, and pick from each randomly with 50/50 probability  
	* number_of_obs <- 500  
    * coin <- sample(c(0,1), size = number_of_obs, replace = TRUE, prob = c(0.5, 0.5))  # can change the coin to be non-50/50 for other mixture simulations  
    * gauss_1 <- rnorm(n = number_of_obs, mean = 5, sd = 2)  
    * gauss_2 <- rnorm(n = number_of_obs)  
    * mixture_simulation <- ifelse(coin, gauss_1, gauss_2)  
    * head(cbind(coin, gauss_1, gauss_2, mixture_simulation))  
    * mixture_simulation <- data.frame(x = mixture_simulation)  
    * ggplot(mixture_simulation) + geom_histogram(aes(x = x, ..density..), bins = 40)  
* Can also create mixtures of 3+ underlying Gaussian  
	* proportions <- sample(c(0, 1, 2), number_of_obs, replace = TRUE, prob = c(1/3, 1/3, 1/3))  
    * gauss_3 <- rnorm(n = number_of_obs, mean = 10, sd = 1)  
    * mixture_simulation <- data.frame(x = ifelse(proportions == 0, gauss_1, ifelse(proportions == 1, gauss_2, gauss_3)))  
    * ggplot(mixture_simulation) + geom_histogram(aes(x = x, ..density..), bins = 40)  
  
Example code includes:  
```{r}

gender <- readr::read_csv("./RInputFiles/gender.csv")
glimpse(gender)

# Have a look to gender (before clustering)
head(gender)

# Scatterplot with probabilities
gender %>% 
  ggplot(aes(x = Weight, y = BMI, col = probability))+
  geom_point(alpha = 0.5)


# Set seed
set.seed(1313)

# Simulate a Gaussian distribution
simulation <- rnorm(n = 500, mean = 5, sd = 4)

# Check first six values
head(simulation)

# Estimation of the mean
mean_estimate <- mean(simulation)
mean_estimate

# Estimation of the standard deviation
standard_deviation_estimate <- sd(simulation)
standard_deviation_estimate

# Transform the results to a data frame
simulation <- data.frame(x = simulation)

# Plot the sample with the estimated curve
ggplot(simulation) + 
  geom_histogram(aes(x = x, y = ..density..)) + 
  stat_function(geom = "line", fun = dnorm,
                args = list(mean = mean_estimate, 
                sd = standard_deviation_estimate))


# Estimation of the mean
mean_estimate <- gender %>% 
  pull(Weight) %>% 
  mean()
mean_estimate

# Estimation of the standard deviation
sd_estimate <- gender %>% 
  pull(Weight) %>% 
  sd()
sd_estimate

# Plot the sample with the estimated curve
gender %>% 
  ggplot() + 
  geom_histogram(aes(x = Weight, y = ..density..), bins = 100) + 
  stat_function(geom = "line", fun = dnorm,
                args = list(mean = mean_estimate, sd = sd_estimate))


# Create coin object
coin <- sample(c(0, 1), size = 500, replace = TRUE, prob = c(0.2, 0.8))

# Sample from two different Gaussian distributions
mixture <- ifelse(coin == 1, rnorm(n = 500, mean = 5, sd = 2), rnorm(n = 500))

# Check the first elements
head(mixture)


# Transform into a data frame
mixture <- data.frame(x = mixture)

# Create histogram especifiying that is a density plot
mixture %>% ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 50)


number_observations <- 1000

# Create the assignment object
assignments <- sample(c(0, 1 , 2), size = number_observations, replace = TRUE, prob = c(0.3, 0.4, 0.3))

# Simulate the GMM with 3 distributions
mixture <- data.frame(
    x = ifelse(assignments == 1, rnorm(n = number_observations, mean = 5, sd = 2), 
               ifelse(assignments == 2, 
                      rnorm(n = number_observations, mean = 10, sd = 1), 
                      rnorm(n = number_observations)
                      )
               )
    )

# Plot the mixture
mixture %>% 
    ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 50)

```
  
  
  
***
  
Chapter 2 - Structure of Mixture Models and Parameter Estimation  
  
Structure of Mixture Models:  
  
* Three questions to be answered for clustering with mixture models  
	* Suitable probability distribution - depends on domain expertise  
    * Number of clusters - domain expertise or testing to see what best satisfies criteria  
    * Parameters and estimates - based on Expectation Maximization (EM) Algorithm  
* Example of the gender dataset - bivariate Gaussian, with two clusters, and mu/sigma/proportion for each  
* Example of handwritten digits (3 vs. 6) - Bernoulli distributions with two clusters, mean probability of being 1 for every dot  
* Example of crime types in Chicago - Poisson distribution with six clusters (crime types), average and proportion of crimes  
  
Parameter Estimation:  
  
* Suppose that we have an assumption of 2 clusters each from a Gaussian distribution and with each distribution having the same sigma  
	* If the probabilities are known, try to estimate the means  
    * If the means are known, try to estimate the probabilities  
    * means_estimates <- data_with_probs %>% summarise(mean_red = sum(x * prob_red) / sum(prob_red), mean_blue = sum(x * prob_blue) / sum(prob_blue))  
    * proportions_estimates <- data_with_probs %>% summarise(proportion_red = mean(prob_red), proportion_blue = 1 - proportion_red)  
    * data %>% mutate(prob_from_red = 0.3 * dnorm(x, mean = 3), prob_from_blue = 0.7 * dnorm(x,mean = 5), prob_red = prob_from_red/(prob_from_red + prob_from_blue), prob_blue = prob_from_blue/(prob_from_red + prob_from_blue)) %>%  
    *     select(x, prob_red, prob_blue) %>% head()  
  
EM Algorithm:  
  
* Can begin by making nave assumptions about the distributions, for later refinement  
	* means_init <- c(1, 2)  
    * props_init <- c(0.5, 0.5)  
* Can then run a first iteration of the probabilities - the expectations  
	* means_estimates <- data_with_probs %>% summarise(mean_red = sum(x * prob_red) / sum(prob_red), mean_blue = sum(x * prob_blue) / sum(prob_blue)) %>% as.numeric()  
    * props_estimates <- data_with_probs %>% summarise(proportion_red = mean(prob_red), proportion_blue = 1- proportion_red) %>% as.numeric()  
* Basically, the iterations continue  
	* Iteration 0: Initial Parameters -> Estimate Probabilities (1)  
    * Iteration 1: Estimated Probabilities (1) -> Estimated Parameters (2) -> Estimated Probabilities (2)  
    * Iteration 2: Estimated Probabilities (2) -> Estimated Parameters (3) -> Estimated Probabilities (3)  
    * Etc.  
* Can translate the iterative process in to a function that is called by way of a for loop  
	* expectation <- function(data, means, proportions){  
    *     data <- data %>%  
    *         mutate(prob_from_red = proportions[1] * dnorm(x, mean = means[1]),  
    *                prob_from_blue = proportions[2] * dnorm(x, mean = means[2]),  
    *                prob_red = prob_from_red/(prob_from_red + prob_from_blue),  
    *                prob_blue = prob_from_blue/(prob_from_red + prob_from_blue)
    *                ) %>%  
    *         select(x, prob_red, prob_blue)  
    *     return(data)  
    * }  
    * for(i in 1:10){  
    *     new_values <- maximization(expectation(data, means_init, props_init))  
    *     means_init <- new_values[[1]]  
    *     props_init <- new_values[[2]]  
    *     cat(c(i, means_init, proportions_init),"\n")  
    * }  
  
Example code includes:  
```{r}

digits <- readr::read_csv("./RInputFiles/digits.csv")
dim(digits)

digitData <- digits[, 1:256]
digitKey <- digits[, 257:266]

# keep a subset of 4 and 8
digitUse <- rowSums(digitKey[, c(5, 9)]==1)
digData <- digitData[digitUse, ]
digKey <- digitKey[digitUse, ]

show_digit <- function(arr256, col=gray(4:1/4), ...) {
    arr256 <- as.numeric(arr256)
    image(matrix(arr256, nrow=16)[,16:1],col=col,...)
}

# Dimension
# broom::glance(digits)

# Apply `glimpse` to the data
glimpse(digitData)

# Digit in row 50
show_digit(digitData[50, ])

# Digit in row 100
show_digit(digitData[100, ])


gaussian_sample_with_probs <- data.frame(
    x = c(54.5, 7.7, 55.9, 27.9, 4.6, 59.9, 6.4, 60.5, 32.6, 21.3, 0.5, 8.9, 70.7, 49.3, 40.1, 43, 8.1, 62.9, 56, 54.4, 42.5, 46.1, 58.3, 61.7, -11.6, 10.8, 27.5, 12.2, 67.7, -5.6, 13.3, 62.7, 37.2, 41.4, 47.4, 54.2, 31, 60.2, 69.9, 33.8, 25.4, 21.9, 17.9, 61.5, 49.8, 37.9, 55.8, 14.1, 53.3, 45.6, 44.7, 14.2, -5.7, 10.9, 63.7, -6.5, 50.3, 61.4, 35.1, -3.7, 68.4, -6.2, 64, 24.4, 65.7, 59.7, 52.7, 27.2, 17.5, 22.6, 14.7, 22.1, 61.5, 55.6, 62.6, 5.6, 52.3, 8, 25.4, 48.8, 58.4, 6.2, 52.3, 6.6, 64, 43, 60.6, 33.5, 45.8, 2.5, 63, 58.2, 50.9, 22.1, 36.5, 27.1, 61.4, 56.3, 63.5, 55.6, 53.8, 31.9, 30.7, 15.6, 14.8, 44.4, 51.9, 61.4, 11.8, 51.3, 58.6, 45.4, 8.3, 41.5, 52.7, 9.1, 60.8, 40.2, 20.5, 40.2, 59.2, 36.7, 47.5, 12.2, 7.7, 56.2, -13.2, 6, 58.7, 43.7, 67.3, 53.6, 37.6, 54.3, 37.7, 51.9, 10.5, 42, 24, -0.7, 53.1, 27.4, 57.2, 37.3, 28.6, 13.5, 35.2, 22.7, 35.8, 66.9, 45.9, 45.9, 56.7, 55.6, 58.3, 3.2, 45.9, 59.5, 50.8, 43.7, 42.8, 4.7, 29.5, 50.9, 7.8, 44.3, 53.6, 57, 57.8, 47.3, 56.8, 51.1, 27.7, 44.9, 33, 44, 42.1, 38, 52.3, 44, 28.1, 52.7, 53.6, 4.7, 42.1, 40.8, 5, 8, 49.1, 67.5, 16.2, 11.2, 14.6, 32.8, 61.3, 49.8, 51.5, 54.5, 51.6, 45.8, 55.9, 7.4, -10.2, 41.9, 27.4, 45.1, 17.7, 37.5, 53.5, 25.7, 18.1, 13.4, 40.5, 13.3, 2, 49.8, 66.7, 34.7, 11.4, 42.1, 54.4, 48.3, 38.3, 17.4, 48.2, 48.4, 57.4, 54.5, 13.6, 52.3, -0.1, 12.8, 29.3, 45.6, 62.3, 49.2, 32.6, 38.4, 15, 6.1, 12.2, 5.8, 17.7, 20.7, 43.6, 52.3, 42.4, 64.6, 34.3, 9.5, 3.6, 37.2, 45.7, 56.9, 67, 48.7, -3.1, 50.1, 45.4, 54.4, 38.1, 10.8, 7.4, 50.5, 24.7, 11.4, 59.5, 43.9, 4.4, 53.7, 41.9, 60.2, 49.5, 11.6, 51.1, 69.1, 46.2, 35.5, 15, -6.4, 59.9, 57.3, 49.1, 55.5, 55.6, 43.9, 52.5, 46.4, 5.8, 55.3, 22.2, 42.7, 51.3, 40.1, 62.1, 62.2, 48.8, 6.1, 0.6, 19.6, 36.8, 48, 33.8, 52.8, 66.6, 30.2, 45.9, 5.9, 52.7, 49.7, 37.7, 10.4, 60.1, 35.8, 62.1, 35, 38.7, 13.3, -4.9, 30.6, 55.9, 23.7, 12.6, 45.7, 38.1, 9.9, 39.6, 46.3, -3.5, 31.2, 8.3, -8.1, 31.4, 65.7, 10.7, 5.5, 54.4, 51.8, 59.8, 50.3, 45.1, 8.5, 15.3, 3.2, 19.3, 40.8, 48.4, 30.1, 32.7, 12.7, 59.2, 51.4, 55.3, 58.9, -19, 61.9, 30.3, 77.2, 39.8, 31.3, 23.1, 56, 41.9, 0.5, 33.4, 36.6, 54.4, 12.4, 16.4, 24.4, -2.4, 30.9, 56.4, 12.5, 65.2, 10, -1.7, 45.7, 49.5, 45.3, 17.5, 29, -8.7, 51.7, 17.3, 20.2, 14.6, 47.6, 55.3, 50.2, 4.1, 47.5, 71, 13.2, 75.4, 6.2, 53, 54.2, 40.6, 55.1, 67.4, 45, 47.3, 44.2, 8.4, 46.1, 48.7, 8.3, 40.4, 63, 49, 2.8, 50.4, 17.7, 40.4, 41.1, 56.6, 37.3, -0.1, 62.5, 47.7, 62.1, 16.6, 33.3, 4.1, 61, 49.4, 44.1, 18.7, -1.3, 42.1, -11.8, 40.6, 45.6, 14.9, 51.9, 57.4, 41.3, 59.2, 58.6, 50.5, -3.9, -0.6, 11.5, 54.5, 57.1, 46.2, 51.9, 58.2, 51.6, 50.3, 64.2, 8.3, 49, 42, 43.7, 53.4, 6.5, 36.6, -18.2, 41.8, -6.8, 35, 46.8, 43.8, 60.6, -11.3, 18.5, 0.3, 40.2, 73.3, 58.2, 43.9, 22.2, 12.8, 6.7, 36.3, 51.8, 33.6, 71, 56.8, 26, 43.3, 37.4, 60, 17.2, -10.3, 43.9, 69, 38.7, 57.9, 40.2, 48.6, 57.7, 45.8, 56.2, 7.3, 32.1, 41.2, 39.1), 
    prob_cluster1=c(0, 1, 0, 0.552, 1, 0, 1, 0, 0.158, 0.947, 1, 1, 0, 0, 0.01, 0.003, 1, 0, 0, 0, 0.004, 0.001, 0, 0, 1, 0.999, 0.591, 0.999, 0, 1, 0.998, 0, 0.03, 0.006, 0.001, 0, 0.268, 0, 0, 0.107, 0.773, 0.933, 0.985, 0, 0, 0.023, 0, 0.997, 0, 0.001, 0.002, 0.997, 1, 0.999, 0, 1, 0, 0, 0.065, 1, 0, 1, 0, 0.834, 0, 0, 0, 0.626, 0.988, 0.912, 0.996, 0.928, 0, 0, 0, 1, 0, 1, 0.773, 0, 0, 1, 0, 1, 0, 0.003, 0, 0.118, 0.001, 1, 0, 0, 0, 0.926, 0.038, 0.631, 0, 0, 0, 0, 0, 0.201, 0.286, 0.994, 0.996, 0.002, 0, 0, 0.999, 0, 0, 0.001, 1, 0.005, 0, 1, 0, 0.009, 0.961, 0.009, 0, 0.036, 0, 0.999, 1, 0, 1, 1, 0, 0.002, 0, 0, 0.025, 0, 0.024, 0, 0.999, 0.004, 0.855, 1, 0, 0.604, 0, 0.028, 0.484, 0.997, 0.062, 0.909, 0.05, 0, 0.001, 0.001, 0, 0, 0, 1, 0.001, 0, 0, 0.002, 0.003, 1, 0.398, 0, 1, 0.002, 0, 0, 0, 0.001, 0, 0, 0.576, 0.001, 0.138, 0.002, 0.004, 0.022, 0, 0.002, 0.533, 0, 0, 1, 0.004, 0.007, 1, 1, 0, 0, 0.993, 0.999, 0.996, 0.147, 0, 0, 0, 0, 0, 0.001, 0, 1, 1, 0.005, 0.606, 0.001, 0.987, 0.026, 0, 0.754, 0.985, 0.998, 0.008, 0.998, 1, 0, 0, 0.076, 0.999, 0.004, 0, 0, 0.019, 0.988, 0, 0, 0, 0, 0.997, 0, 1, 0.998, 0.417, 0.001, 0, 0, 0.162, 0.018, 0.995, 1, 0.998, 1, 0.987, 0.956, 0.002, 0, 0.004, 0, 0.087, 0.999, 1, 0.03, 0.001, 0, 0, 0, 1, 0, 0.001, 0, 0.021, 0.999, 1, 0, 0.82, 0.999, 0, 0.002, 1, 0, 0.005, 0, 0, 0.999, 0, 0, 0.001, 0.057, 0.995, 1, 0, 0, 0, 0, 0, 0.002, 0, 0.001, 1, 0, 0.925, 0.003, 0, 0.009, 0, 0, 0, 1, 1, 0.971, 0.035, 0, 0.104, 0, 0, 0.329, 0.001, 1, 0, 0, 0.024, 0.999, 0, 0.05, 0, 0.067, 0.016, 0.998, 1, 0.298, 0, 0.87, 0.998, 0.001, 0.021, 0.999, 0.012, 0.001, 1, 0.247, 1, 1, 0.233, 0, 0.999, 1, 0, 0, 0, 0, 0.001, 1, 0.995, 1, 0.975, 0.007, 0, 0.34, 0.157, 0.998, 0, 0, 0, 0, 1, 0, 0.325, 0, 0.011, 0.246, 0.895, 0, 0.005, 1, 0.12, 0.037, 0, 0.998, 0.992, 0.834, 1, 0.273, 0, 0.998, 0, 0.999, 1, 0.001, 0, 0.001, 0.987, 0.446, 1, 0, 0.989, 0.964, 0.996, 0, 0, 0, 1, 0, 0, 0.998, 0, 1, 0, 0, 0.008, 0, 0, 0.001, 0.001, 0.002, 1, 0.001, 0, 1, 0.008, 0, 0, 1, 0, 0.986, 0.008, 0.006, 0, 0.028, 1, 0, 0, 0, 0.991, 0.128, 1, 0, 0, 0.002, 0.98, 1, 0.004, 1, 0.008, 0.001, 0.996, 0, 0, 0.006, 0, 0, 0, 1, 1, 0.999, 0, 0, 0.001, 0, 0, 0, 0, 0, 1, 0, 0.004, 0.002, 0, 1, 0.037, 1, 0.005, 1, 0.068, 0.001, 0.002, 0, 1, 0.981, 1, 0.009, 0, 0, 0.002, 0.925, 0.998, 1, 0.042, 0, 0.114, 0, 0, 0.725, 0.003, 0.027, 0, 0.989, 1, 0.002, 0, 0.016, 0, 0.009, 0, 0, 0.001, 0, 1, 0.189, 0.006, 0.014)
)

gaussian_sample_with_probs <- gaussian_sample_with_probs %>%
    mutate(prob_cluster2 = 1-prob_cluster1)
glimpse(gaussian_sample_with_probs)


# Estimation of the means
means_estimates <- gaussian_sample_with_probs %>% 
    summarise(mean_cluster1= sum(x*prob_cluster1)/sum(prob_cluster1),
              mean_cluster2 = sum(x*prob_cluster2)/sum(prob_cluster2)
              )
means_estimates

# Estimation of the proportions
props_estimates <- gaussian_sample_with_probs %>% 
    summarise(props_cluster1 = mean(prob_cluster1),
              props_cluster2 = mean(prob_cluster2)
              )
props_estimates

# Transform to a vector
means_estimates <- as.numeric(means_estimates)

# Plot histogram with means estimates
ggplot(gaussian_sample_with_probs) + geom_histogram(aes(x = x), bins = 100) +
    geom_vline(xintercept = means_estimates)


gaussian_sample <- data.frame(
    x=c(6.4, 5.9, 57.8, 52.6, 54.3, 52.3, 4.4, 49.1, -4, 12.7, 19.8, 51.8, 35.4, 17.1, 38.8, 44.1, 45.6, 7.9, 57.7, 51.1, 14.1, 36.6, 51.6, 4.1, -1.8, 55.1, 52.4, 54.4, 47.9, 36.6, 53.9, 15, 68.8, 8.3, 40.8, 39.3, 37.1, 12.7, 54.6, 34.1, 24.9, 58.5, 50.8, 48.6, 60, 52.1, 61.5, 6.9, 63, 63.5, 54.1, 37.7, 52.6, 49.1, 53.7, 13.4, 23.6, 45.5, 33.4, 46.4, 46.6, 56.1, 37.8, 44.1, 62.4, 12, 54.4, 31.6, -1, 9.4, 16, 53.4, 71.1, 8.9, 64.4, 55.9, 50.5, 57.2, 45.9, 18.5, 53.9, 12.5, 12.2, 1.5, 0.3, 40.1, 13.9, 53.2, 12.1, 57.2, 2.3, -2.6, 2.7, 59.6, 3, 10.3, 66.9, 57.3, 57.6, 9.1, 43.8, 51.1, 7.7, 13.4, 46.3, 57.5, 0.2, 1.9, 43.8, 53.9, 9.3, 45.5, 15.4, -3.2, -1.2, 40.5, 1.9, 14.5, -2, 3.4, 54.1, 2.9, 58.2, 49.5, 49.1, 60.2, 45.3, 59.7, 38, 22.4, 42.6, 53.6, 7.3, 43.9, 2.8, 66.5, 56.5, 44.4, 53.5, 40.6, 57.1, 43.8, -3.1, 47.3, 42.5, 50.8, -12, -12, 15.2, 43.8, 57.3, 32.2, 61.1, 15.1, 5.8, 24.7, 51.5, 7.7, -5.1, 63.1, 50.1, 39.9, 38.7, -5.2, 50.3, 49.1, 58.1, 31.3, 54.6, 39.1, 4.4, 60.5, 45.6, 59.7, 39.5, 60.6, 42.8, 49.5, 12.9, 47.2, 50, 11.4, 50.9, 57.3, 46.7, 35.6, 38.8, 56, -5.7, 50.5, 21.2, 45.9, 60.7, 22.1, 46.7, 12.5, 55.2, 48.4, 36.6, 54, 47, 50.3, 51.7, 11, 56, 42.4, 61.8, 45.6, 60.5, 40.6, 8.8, 21, 5.6, 68.2, 21.3, 11.5, 47.2, 26.4, 35.8, 25.4, 19.6, 56, 9.1, 63.4, 48.5, 3.2, 57.1, 52.7, 11.3, 16.3, 49, 46.5, 12.4, 9.6, 45.5, 55.3, 72.9, 8.1, -3.8, 53.8, 34.1, 45.7, 56.3, 44, 23.4, 57.2, 0.5, 33.2, 63.4, 37.3, 57.3, 52.7, 9.7, 51.9, 39.4, 63.7, 23.3, 39.9, -0.5, 41.6, 11.3, 48, 38.2, 54.2, 41.3, 30.6, 55.2, 48.9, 34.4, 16.2, 45.7, 10.1, 42.7, 12.2, 39.5, 14.1, 64.9, 53.1, 50.4, 47, 58.5, 50.8, 43.9, 56.8, 12.6, 44.5, 54.6, 8.9, 15.5, 50.2, 4.8, 52.8, 14.4, 33.7, 5.4, -0.2, 19.8, 51, 59.4, -8.2, 10.4, 47.8, 31.2, 41.4, 9.4, -3.2, 21.1, 44.7, 22.9, 11.5, 49.6, 26.7, 11.5, 35.2, 9.4, 44.8, 63.1, 8.5, 21, 30.9, 16.1, 54.4, 53.4, 9.7, 49.8, 45.6, -3, 53, 43.4, 43.4, 43.9, 56.6, 33.5, 55.1, 54.4, 62.8, 37.9, 35.1, 8.6, 7.1, 46.1, 6.1, 27, -9.9, 6.4, 44.6, 49, 46, 42.4, 9.5, 47.1, 51.3, -4.7, 14, 64.8, 38, 33.6, -0.4, 53.5, 40.3, 47.2, 58.5, 45.4, 2.5, 52.9, 47.4, 56.1, 17.7, 3.9, 30.7, 44.6, 42.4, 55.4, 47.1, 11.5, 50.7, 47.6, 11.3, 45.1, 44.2, 46.6, 36.9, 47.4, 54.6, -2, 50.7, 63.6, 58.9, 7.6, -3.1, 31.1, 44.9, 55.7, 16.6, 64.3, 27.1, 23, 48.7, -0.8, 23.6, 72.8, 11.9, 57.3, 25.4, 47.1, 9.4, 57.6, 39.6, 25.3, 31.2, 52.4, 51.1, 1.6, 76.5, 50.7, 34.2, 7.6, 25.4, 11.7, 53.5, 17.5, 53.7, 61.2, 49.9, 48.8, 40.8, 61.2, 16.4, 48.6, 7.5, -2, 64.2, 26.2, 11.2, 3.2, -4.3, 37.9, 47.7, 26.3, 58, 66.9, 59.1, 35.8, 14.2, 53, 60.3, 63.3, 53.6, 47.6, 57.1, 37, 47.6, 61.6, 52.7, 0.8, 50.5, 48.1, -3.4, 53.6, 35.7, 49.8, 2.7, 59.9, 36.5, 63.6, 53.3, 3.8, 20.2, 19.7, 20.7, 45.6, 39.8, 37.2, 38.6, 12.4, 56.3, 59.6, 10.5, 11, -6.8, 58.8, 49.5, -3.6, 51.1, 53.1, 46, 57.9, 15.2, -2.3, 22.9, 32.8, 37.6, 52, 77.5, 2.2, 9.5, 40.4, 48.5, 27.2, 37.4)
)
str(gaussian_sample)


# Create data frame with probabilities
gaussian_sample_with_probs <- gaussian_sample %>% 
    mutate(prob_from_cluster1 = 0.35 * dnorm(x, mean = 10, sd = 10),
           prob_from_cluster2 = 0.65 * dnorm(x, mean = 50, sd = 10),
           prob_cluster1 = prob_from_cluster1/(prob_from_cluster1 + prob_from_cluster2),
           prob_cluster2 = prob_from_cluster2/(prob_from_cluster1 + prob_from_cluster2)) %>%
    select(x, prob_cluster1, prob_cluster2) 
head(gaussian_sample_with_probs)


expectation <- function(data, means, proportions, sds){
  # Estimate the probabilities
  exp_data <- data %>% 
      mutate(prob_from_cluster1 = proportions[1] * dnorm(x, mean = means[1], sd = sds[1]),
             prob_from_cluster2 = proportions[2] * dnorm(x, mean = means[2], sd = sds[2]),
             prob_cluster1 = prob_from_cluster1/(prob_from_cluster1 + prob_from_cluster2),
             prob_cluster2 = prob_from_cluster2/(prob_from_cluster1 + prob_from_cluster2)) %>%
      select(x, prob_cluster1, prob_cluster2)
    # Return data with probabilities
  return(exp_data)
}

maximization <- function(data_with_probs){
    means_estimates <- data_with_probs %>%
        summarise(mean_1 = sum(x * prob_cluster1) / sum(prob_cluster1),
                  mean_2 = sum(x * prob_cluster2) / sum(prob_cluster2)
                  ) %>% 
        as.numeric()
    props_estimates <- data_with_probs %>% 
        summarise(proportion_1 = mean(prob_cluster1), proportion_2 = 1 - proportion_1) %>% 
        as.numeric()
    list(means_estimates, props_estimates)   
}


means_init <- c(0, 100)
props_init <- c(0.5, 0.5)

# Iterative process
for(i in 1:10){
    new_values <- maximization(expectation(gaussian_sample, means_init, props_init, c(10, 10)))
    means_init <- new_values[[1]]
    props_init <- new_values[[2]]
    cat(c(i, means_init, props_init), "\n")
}


fun_gaussian <- function(x, mean, proportion){
    proportion * dnorm(x, mean, sd = 10)
}

means_iter10 <- means_init
props_iter10 <- props_init

gaussian_sample %>% ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 200) +
    stat_function(geom = "line", fun = fun_gaussian, 
                  args = list(mean = means_iter10[1], proportion = props_iter10[1])
                  ) +
    stat_function(geom = "line", fun = fun_gaussian,
                  args = list(mean = means_iter10[2], proportion = props_iter10[2])
                  )

```
  
  
  
***
  
Chapter 3 - Mixture of Gaussians with flexmix  
  
Univariate Gaussian Mixture Models:  
  
* Gaussian mixture models are formed by Gaussian distributions, which can have many potential parameters  
	* Simplest version is the univariate Gaussian, such as the BMI dataset with no labels for gender  
* Example of using the gender dataset for segmenting assuming that the data are not labeled by gender  
	* Weight should be a Gaussian - continuous, not linked to integer values, etc.  
    * Histogram looks like two Gaussians, so begin with the assumption of 2 univariate Gaussians with a resulting 2 segments  
    * Parameters can initially be estimated with a baseline mean, sd, and proportion (6 total parameters)  
    * The EM algorithm, implemented in flexmix, can then help to simplify the calculations  
  
Univariate Gaussian Mixture Models with flexmix:  
  
* Can begin by checking what the most suitable distributions might be  
	* gender %>% ggplot(aes(x = Weight)) + geom_histogram(bins = 100)  
    * Univariate Gaussian with 2 clusters  
* Can use the flexmix::flexmix(formula, data, k, models, control, ...) to make the estimates  
	* formula - describes the model to be fit (often variable ~ 1)  
    * data - data frame  
    * k - number of clusters  
    * models - distribution to be considered, such as FLXMCnorm1 for the univariate Gaussian  
    * control - maximum iterations and tolerance  
* Example of fitting to a dataset using flexmix  
	* fit_mixture <- flexmix(Weight ~ 1, data = gender, k = 2, model = FLXMCnorm1(), control = list(tol = 1e-15, verbose = 1, iter = 1e4))  
    * proportions <- prior(fit_mixture)  # proportions from the model  
    * parameters(fit_mixture)  # all the parameters  
    * comp_1 <- parameters(fit_mixture, component = 1)  # just the parameters for component 1  
    * posterior(fit_mixture) %>% head()  # probability of belonging to each cluster by observation  
    * posterior(fit_mixture) %>% head()  # assignment to the cluster with maximum probability  
    * table(gender$Gender, clusters(fit_mixture))  
  
Bivariate Gaussian Mixture Models with flexmix:  
  
* May want to use additional variables to improve the clustering - for example, using both height and weight from the gender dataset  
	* Bivariate Gaussian with 2 clusters, and needing to estimate additional means and standard deviations (which need to also be between the variables)  
* The flexmix library implements the Bivariate Gaussian distribution, which is conceptually like  
	* There would be two means - variable 1 and variable 2  
    * There would be a 2x2 covariance matrix - sd1, sd2, cov(1, 2), cov(1, 2) - really, three terms since the off-diagonals will be equal  
  
Bivariate Gaussian Mixture Models with flexmix:  
  
* Example of a covariance matrix without cross-terms  
	* fit_without_cov <- flexmix(cbind(Weight, BMI) ~ 1, k = 2, data = gender, model = FLXMCmvnorm(diag = TRUE), control = list(tolerance = 1e-15, iter.max = 1000))  # cbind() is because there are two variables; diag=TRUE means no covariance  
    * proportions <- prior(fit_without_cov)  
    * parameters(fit_without_cov)  
    * comp_1 <- parameters(fit_without_corr, component=1)  
    * comp_2 <- parameters(fit_without_corr, component=2)  
    * mean_comp_1 <- comp_1[1:2]  
    * mean_comp_2 <- comp_2[1:2]  
* Can then visualize the resulting clusters  
	* library(ellipse)  
    * ellipse_comp_1 <- ellipse(x = covariance_comp_1, centre = mean_comp_1, npoints = nrow(gender))  
    * ellipse_comp_2 <- ellipse(x = covariance_comp_2, centre = mean_comp_2, npoints = nrow(gender))  
    * gender %>%  
    *     ggplot(aes(x = Weight, y = BMI)) + geom_point() +  
    *         geom_path(data = data.frame(ellipse_comp_1), aes(x=x,y=y), col = "red") +  
    *         geom_path(data = data.frame(ellipse_comp_2), aes(x=x,y=y), col = "blue"  
* Need to include joint variability to improve the modeling  
	* fit_with_cov <- flexmix(cbind(Weight, BMI) ~ 1, k = 2, data = gender, model = FLXMCmvnorm(diag = FALSE), control = list(tolerance = 1e-15, iter.max = 1000))  # cbind() is because there are two variables; diag=FALSE means covariance  
  
Example code includes:  
```{r}

xExample <- c(7.3, 58.7, 9.7, 16.9, 6.3, 35.1, 33.5, 61.3, 28.3, 24.3, 58.6, 13.1, 58.7, 34, 29.1, 46.4, 54.6, 5.9, 30.6, 27.9, 27.5, -5.3, 37.6, 9.1, 44.5, 57.5, 30.5, 5, 51.9, 33.6, 37.4, 28.8, 47.9, 5.4, 64.1, 45.1, 41, 36.3, 28.2, 33.8, 9.8, 57.4, 48.4, 58.3, 27.7, 38.4, 36.4, 66.9, 30.7, 34.3, 25.9, 48.5, 52, 0.3, 45.3, 31.9, 21.6, 36.6, 29, 13.2, 41.5, 8.2, 46.6, 30.6, 48.6, 5.6, 39.3, 30.5, 34.2, 61.5, 4.2, 71.3, 42.5, 32.7, 54.4, 19.2, 13.3, 40.3, 72, 21.8, 49.5, 38.7, 9.6, 49.6, 32, 30.9, 28.6, 30.1, 29.8, 67.9, 60.8, 55, 34.6, 32.8, 11.9, 50.5, 32.1, 13.7, 48.6, 32.6, 9.1, 27.6, 35.6, 28.3, 15.1, 54.7, 30.8, 22.2, 27.5, 49.3, 56, 26.1, 57.2, 46.4, 50.3, 43.6, 51.8, 47.5, 15.5, 60.2, 63.6, 45.3, 14.1, 42.1, 31.4, 42.4, 61.7, 60.1, 27.7, 55.9, 3.3, 18.7, 58.1, 46, 14, 41.7, 28.9, 29.1, 56.9, 32.3, -0.8, 29.4, 27.3, 33.5, 39.1, 13.9, 28.7, 29.4, 10.3, 44.3, 57.1, 76, 49.4, 44.9, 23.2, 53.9, 33.6, 32.7, 30, 57, 63.6, 32.9, 8.6, 26.5, 26, 53.3, 40.8, 30.1, 10.5, 47.2, 30.2, 49.3, 52.4, 48.8, 51.4, 40.7, 33.8, 45.7, 28.1, 13.2, 28.4, 31.7, 30, 29.6, 49.5, 35, 62, 51.9, 39, 15.4, 59.1, 54.8, 9.2, 9.7, 35.4, 32.9, 31.3, 30.4, 64.4, 63.4, 32.9, 40.6, 37.5, 52.3, 35.3, 8.1, 6.4, 26.2, 29.2, 29.7, 27.8, 35.2, 34.1, 29.8, 49, 65.6, -1.1, 28.6, 33.7, 48.1, 45.7, 30.3, 32.7, 64.5, 29.8, 52.5, 48.4, 48.8, 26.4, 37.4, 33.2, 46.1, 29.5, -0.9, 49.8, 34.1, 48.9, 12.5, 36.6, 22.1, 57.3, 9.5, 9.4, 58.5, 50.2, 45.3, 25.3, 27.4, 4.5, 58.5, 63.4, 48.7, 42.6, 33, 47.9, 30.3, 54.9, 7.9, 50.2, 11.2, 59.7, 46.5, 57.5, 26.9, 28.5, 29.7, 52.5, 16.9, 29.8, 28.6, 31.2, 65.3, 1.7, 31.4, 52.5, 5.1, 66.1, 51.5, 9.5, 9.8, 41.6, 0.3, 10.4, 15.5, 34.8, 27.5, 43.6, 31.4, 46.3, 4.6, 45.8, 49.2, 10.7, 48.1, 7.3, 33.4, 10.7, 53.4, 28.9, 51.1, 52.4, 55.9, 56.8, 47.2, 46.8, 30.8, 60.3, 53.6, 30.9, 70.8, 11.2, 7.5, 55.8, 14.3, 25.8, 14.5, 30.9, 60.8, 26.8, 16.5, 31.4, 26.6, 10.6, 53.4, 33.1, 33.1, 46.3, 8.2, 56, 14.1, 25.5, 59.6, 61.9, 58.6, 63.1, 47.7, 30.5, 42.4, 56.2, 17, 13.4, 34.4, 1.1, 18.4, 63.9, 38.6, 15, 30.1, 23.9, 5.9, 53.8, 18.2, 22.7, 45.7, 29.2, 8.4, 52.5, 42, 28.7, 61.7, 35.4, 32.5, 5.5, 6.8, 60.1, 29.4, 31.5, 2.3, 28.3, 29.6, 34.9, 33.2, 28.9, 33.9, 51, 35.4, 52.3, 60, 27.1, 24.7, 57.7, 32.7, 52.5, 66.3, 37.8, 46.3, 38.1, 30.6, 55.6, 44.9, 28.4, 28.9, 19, 7.7, 9.4, 36, 49.9, 42.2, 28.2, 11.5, 52.4, 46.3, 52.4, 27.4, 15.6, 62.3, 51.7, 41.6, 6.2, 10.5, 14.7, 30.4, 23.9, 58.7, 36.1, 47.6, 31.2, 29.1, 60.1, 18, 30, 56.5, 42.7, 27.1, 45.5, 36.6, 46.4, 25.9, 15.4, 31.6, 3.3, 33.6, 63.3, 57.1, 32.3, 11.8, 32.9, 47.2, 31.2, 49.3, 61.7, 11.5, 9.7, 49.6, 45.7, 16.1, 27.4, 22.8, 8.5, 56.2, 26, 45.7, 29, 34.6, 29.4, 3.9, 45.7, 31.7, 52.6, 40.2, 35.5, 5.8, 56.4, 49.5, 30.6, 40.2, 20.8, 43.9, 32.1, 40.8, 45.6, 32.8, 7.4, 27.5, 29.4, 50.8, 43.9, 36.8, 5.5, 61.5, 41.5, 47.5, 13.9, 30.1, 67.3, 27.1, 50.8, 37.4, 28, 25, 37.1, 49.3, 25.3, 26.9, 34.9, 51.8, 33.9, 34.7, 44.2, 10.1, 71.3, 47.5, 23.4, 45.7, 49.4, 32.6, 6.9, 67.8, 56.8, 41.9, 50.7, 31.5, 55, 14.2, 34.8, 26.2, 25.8, 64, 63.8, 56.4, 42.1, 29.5, 49.4, 30.2, 16.2, 30, -0.2, 30.7, 29.6, 57, 41.5, 6.4, 9.7, 47.1, 19.4, 39.8)
xExample <- c(xExample, 32.9, 53.6, 8.4, 32.8, 63.1, 58.4, 7.5, 26, 41.8, 29, 36.9, 41.5, 39.5, 14.1, 27.4, 14.9, 48.4, 34.8, 72.8, 36.9, 27.8, 27.6, 6.1, 43.8, 36.9, 58.5, 55.1, 45.2, 2.6, 20.4, 59, 60.6, 57.7, 29.8, 60.2, 36.9, 29, 28, 46.5, 55, 29.6, 52.6, 38, 45.3, 5.7, 44.8, 35.3, 56.1, 30.3, 32.4, 56.9, 30.8, 44.8, 62.8, 46.1, 57.2, 50.5, 46.4, 37.6, 29.9, 8.6, 35.5, 47.4, 27.2, 36.4, 33.1, 29.4, 25.8, 46, 27.6, 45.7, 32.3, 12.8, 49.8, 13.7, 65.3, 48.5, 39.6, 4, 32.1, 49.6, 44, 74.5, 31, 52.6, 33.3, 56.8, 11.4, 33.7, 34.3, 25.8, 39.8, 7.3, 33.6, 7.9, 49.6, 52.6, 36.5, 43, 14.7, 43.5, 37, 50.8, 46.5, 46.9, 25.4, 32.7, 48.4, 40.3, 45.9, 51.3, 24, 48.3, 39.5, 21.2, 48.1, 56.9, 32.3, 10.2, 9.3, 40.3, 52.8, 34.5, 32.4, 30.1, 10.8, -3.8, 30.4, 58.2, 57.3, 48.9, 36.1, 46.2, 69, 67.8, 58.5, 41.9, 29.6, 51.7, 39.4, 50.8, 29.2, 56.1, 54.4, 17.2, 57.5, 54.1, 48.6, -0.9, 56.3, 27.7, 58.8, 57, 44.1, 6.3, 4.1, 35.9, 60.2, 44.1, 53.9, 33.3, 35.4, 32.1, 56, 56.8, 30.1, 43.1, 64.6, 27.7, 30.7, 53, 66, 29.1, 45, 12.3, 41.3, 54.7, 45.3, 13.3, 9.7, -2, 29.1, 29.5, 31.3, 29.2, 13.8, 26.7, 7.4, 36.8, 42.6, 54.7, 51.3, 42.6, 18, 34, 44.1, 53.6, 44.7, 28.9, 64.9, 60, 66.6, 32.9, 15.5, 37.6, 8.3, 28.5, 16.2, 39.7, 25.9, 8.8, 30.9, 9.9, 39.3, 66.4, 62.4, 53.8, 9.3, 44.7, 50.4, 57.8, 29, 50.1, 28.5, 62.9, 16.3, 54, 45.4, 60.6, 9, 7.7, 64.2, 54.4, 53.3, 45.5, 38, 5.2, 61.7, 10.8, 4.3, 24.8, 26.5, 32.2, 4.5, 49.3, 3.9, 39.6, 26.8, 36.3, 65.1, 59.6, 61.3, 30.1, 65.5, 55.8, 48.2, 49.8, 11.2, 64.2, 29, 44.6, 59.9, 12.6, 51.8, 14.5, 28.8, 49.8, 30.4, 42.7, 2.8, 31.1, 29.2, 27.4, 49.9, 28.2, 59.5, 28.7, 9.4, 30.2, 33.3, 30, 26, 65.1, 55.9, 30.5, 61.1, 50.3, 31.3, 58.2, 41.3, 33.4, 14.8, 51.2, 40.8, 34.1, 33.7, 29.4, 56, 26.4, 30.7, 55.1, 49.7, 37.7, 56.9, 38.5, 28.8, 50.3, 45.7, 13.2, 32.8, 30.5, 30.6, 61.5, 57.7, 33.6, 24.6, 53.9, 36.1, 37.4, 55.5, 27.4, 44.2, 15.4, 56.3, 28.1, 28.8, 67.6, 17.7, 48.5, 57.5, 33.7, 12.9, 19.5, 30.6, 56.8, 75.4, 26, 32.3, 28.3, 10.7, 9, 66.5, 51.6, 30.2, 46, 44.1, 53, 33.9, 28.4, 53.1, 42.3, 55.2, 42.4, 9.4, 36.3, 26.6, 41.2, 33, 42.1, 27, 25.4, 53.8, 56.7, 22.2, 29.5, 30.9, 9.3, 30.4, 48.1, 30.9, 28.4, 38.6, 28.8, 52, 16.5, 64.3, 56.1, 51.4, 50.2, 30.1, 67.3, 62.3, 12.9, 27.9, 38.9, 29.3, 17.4, 30, 62.5, 40.5, 48, 31.9, 54.7, 27.4, 28.2, 46.6, 14, 61.9, 59.4, 65.4, 30.2, 28.9, 35.4, 55.8, 51.4, 47.8, 34, 56.2, 26.5, 30.2, 8.4, 10.9, 63.9, 41.9, 31.3, 52.8, 36, 45.4, -2, 57.3, 80.3, 41, 13.8, 31.9, 33.8, 48.5, 16.7, 29.5, 6.7, 42.1, 32.2, 45.7, 18.9, 30.5, 30.9, 40.2, 14.6, 41.2, 27, 6.1, 34.9, 57.5, 30.1, 56.6, 62.4, 11.5, 25.7, 14.8, 28.2, 43.5, 37.7, 32.1, 44.4, 56.2, 7.6, 29.4, 63.4, 53, 14.6, 50.1, 62.6, 29.3, 33.5, 52.7)
mix_assign <- c(1, 2, 1, 1, 1, 2, 2, 2, 3, 3, 2, 1, 2, 2, 3, 2, 2, 1, 3, 3, 3, 1, 2, 1, 2, 2, 3, 1, 2, 3, 2, 3, 2, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 1, 2, 3, 1, 3, 3, 1, 2, 1, 2, 3, 2, 1, 2, 3, 3, 2, 1, 2, 2, 3, 2, 1, 1, 2, 2, 3, 2, 2, 1, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 1, 2, 3, 1, 2, 2, 1, 3, 2, 3, 1, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 3, 2, 2, 2, 3, 2, 1, 1, 2, 2, 1, 2, 3, 3, 2, 3, 1, 3, 3, 2, 2, 1, 3, 3, 1, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 2, 3, 1, 3, 3, 2, 2, 3, 1, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 1, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 1, 1, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2, 3, 2, 1, 3, 3, 2, 1, 1, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 1, 2, 2, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 1, 3, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 3, 3, 2, 3, 2, 1, 2, 2, 1, 2, 1, 3, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 1, 1, 2, 1, 3, 1, 3, 2, 3, 1, 3, 3, 1, 2, 3, 2, 2, 1, 2, 1, 3, 2, 2, 2, 2, 2, 3, 2, 2, 1, 1, 3, 1, 1, 2, 3, 1, 3, 3, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 2, 3, 1, 1, 2, 3, 2, 1, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 1, 1, 1, 2, 2, 2, 3, 1, 2, 2, 2, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 2, 3, 2, 3, 2, 2, 1, 3, 2, 2, 3, 2, 3, 2, 3, 1, 3, 1, 3, 2, 2, 3, 1, 3, 2, 3, 2, 2, 1, 1, 2, 2, 1, 3, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 1, 2, 2, 3, 2, 2, 3, 1, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 1, 3, 1, 3, 3, 2, 2, 1, 1, 2, 2, 2, 3, 2, 1, 3, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1)
mix_assign <- c(mix_assign, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 1, 2, 1, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2, 3, 2, 1, 3, 2, 3, 2, 1, 2, 1, 2, 2, 3, 2, 1, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 3, 1, 1, 2, 2, 3, 3, 3, 1, 1, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 1, 2, 3, 2, 2, 2, 1, 1, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 1, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2, 2, 2, 3, 1, 2, 1, 3, 1, 2, 3, 1, 3, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 3, 3, 3, 1, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 3, 2, 2, 1, 2, 1, 3, 2, 3, 2, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 1, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 1, 2, 3, 3, 2, 1, 2, 2, 2, 1, 1, 3, 2, 2, 3, 3, 3, 1, 1, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 2, 1, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 3, 3, 2, 1, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 1, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 1, 3, 3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 3, 2, 3, 2, 2, 1, 3, 1, 3, 2, 2, 3, 2, 2, 1, 3, 2, 2, 2, 2, 2, 3, 3, 2)

mix_example <- data.frame(x=xExample, assignment=mix_assign)
str(mix_example)

library(flexmix)
set.seed(1515)

# If wanting verbose output
# control = list(tolerance = 1e-15, verbose = 1, iter = 1e4)
fit_mix_example <- flexmix(x ~ 1, data = mix_example, k = 3, model = FLXMCnorm1(), 
                           control = list(tolerance = 1e-15, iter = 1e4)
                           )

proportions <- prior(fit_mix_example)
comp_1 <- parameters(fit_mix_example, component = 1)
comp_2 <- parameters(fit_mix_example, component = 2)
comp_3 <- parameters(fit_mix_example, component = 3)


fun_prop <- function(x, mean, sd, proportion){
    proportion * dnorm(x = x, mean = mean, sd = sd)
}

ggplot(mix_example) + 
    geom_histogram(aes(x = x, y = ..density..)) + 
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_1[1], sd = comp_1[2], proportion = proportions[1])
                  ) +
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_2[1], sd = comp_2[2], proportion = proportions[2])
                  ) +
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_3[1], sd = comp_3[2], proportion = proportions[3])
                  )


# Explore the first assignments
head(clusters(fit_mix_example))

# Explore the first real labels
head(mix_example$assignment)

# Create frequency table
table(mix_example$assignment, clusters(fit_mix_example))


genderData <- readr::read_csv("./RInputFiles/gender.csv")
str(genderData)

set.seed(1313)
fit_with_covariance <- flexmix(cbind(Weight, BMI) ~ 1, data = genderData, k = 2, 
                               model = FLXMCmvnorm(diag = FALSE), 
                               control = list(tolerance = 1e-15, iter.max = 1000)
                               )

# Get the parameters
comp_1 <- parameters(fit_with_covariance, component = 1)
comp_2 <- parameters(fit_with_covariance, component = 2)

# The means
mean_comp_1 <- comp_1[1:2]
mean_comp_1
mean_comp_2 <- comp_2[1:2]
mean_comp_2

# The covariance matrices
covariance_comp_1 <- matrix(comp_1[3:6], nrow = 2)
covariance_comp_1
covariance_comp_2 <- matrix(comp_2[3:6], nrow = 2)
covariance_comp_2


# Create ellipse curve 1
ellipse_comp_1 <- ellipse::ellipse(x = covariance_comp_1, centre = mean_comp_1, npoints = nrow(genderData))
head(ellipse_comp_1)

# Create ellipse curve 2
ellipse_comp_2 <- ellipse::ellipse(x = covariance_comp_2, centre = mean_comp_2, npoints = nrow(genderData))
head(ellipse_comp_2)


# Plot the ellipses
genderData %>% 
    ggplot(aes(x = Weight, y = BMI)) + geom_point()+
    geom_path(data = data.frame(ellipse_comp_1), aes(x=x,y=y), col = "red") +
    geom_path(data = data.frame(ellipse_comp_2), aes(x=x,y=y), col = "blue")

# Check the assignments
table(genderData$Gender, clusters(fit_with_covariance))

```
  
  
  
***
  
Chapter 4 - Mixture Models Beyond Gaussians  
  
Bernoulli Mixture Models:  
  
* Example of the handwritten images dataset, with images of 3 and 6  
	* The Bernoulli distribution is appropriate for a binary outcome when you do not want a continuous prediction but rather a probability of success  
    * p <- 0.7  
    * bernoulli <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p, p))  
* Every pixel in an image can be represented by a Bernoulli with outcomes being either black (filled) or white (empty), with a 16x16 considered to be a 256-length vector; example for a 3-pixel image  
	* p1 <- 0.7; p2 <- 0.5; p3 <- 0.4  
    * bernoulli_1 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p1, p1))  
    * bernoulli_2 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p2, p2))  
    * bernoulli_3 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p3, p3))  
    * multi_bernoulli <- cbind(bernoulli_1, bernoulli_2, bernoulli_3)  
    * p_vector <- c(p1, p2, p3)  
  
Bernoulli Mixture Models with flexmix:  
  
* Example of using flexmix to predict whether a digit is a 3 or a 6 in the 16x16 data that is stored in digits  
	* digits_sample <- as.matrix(digits)  # 320 x 256 (flattened array of pixels per image)  
    * show_digit(digits_sample[320,])  
* For fitting a Bernoulli mixture model with two segments, can proceed similar to before  
	* bernoulli_mix_model <- flexmix(digits_sample~1, k=2, model=FLXMCmvbinary(), control = list(tolerance = 1e-15, iter.max = 1000))  # digits_sample is the matrix, with each row as an image  
    * prior(bernoulli_mix_model)  # proportions in each cluster  
    * param_comp1 <- parameters(bernoulli_mix_model, component = 1)  
    * param_comp2 <- parameters(bernoulli_mix_model, component = 2)  
    * show_digit(param_comp1)  # will show probabilities of pixelation given segment is a 3  
  
Poisson Mixture Models:  
  
* Can use Chicago crimes data with a Poisson model, since the goal is a count of crimes by type by community, with segments by level of crime danger in the community  
	* lambda_1 <- 100; lambda_2 <- 200; lambda_3 <- 300  
    * poisson_1 <- rpois(n = 100, lambda = lambda_1)  
    * poisson_2 <- rpois(n = 100, lambda = lambda_2)  
    * poisson_3 <- rpois(n = 100, lambda = lambda_3)  
    * multi_poisson <- cbind(poisson_1, poisson_2, poisson_3)  
* Can extend the general concept to a crime dataset with 13 columns  
	* Multi-Poisson distribution  
    * Try from 1-15 clusters, and minimize BIC criteria  
    * Parameters include each lambda for each multi-Poisson distribution  
  
Poisson Mixture Models with flexmix:  
  
* Example of solving the Poisson mixture model using flexmix, with indeterminate clusters (try 1-15 and minimize BIC), and with parameters of lambdas by cluster and proportions by cluster  
	* crimes_matrix <- as.matrix(crimes[,-1])  # do not include the community names  
    * poisson_mix_model <- stepFlexmix(crimes_matrix ~ 1, k = 1:15, nrep = 5, model = FLXMCmvpois(), control = list(tolerance = 1e-15, iter = 1000))  # stepFlexMix is how to run 1:15  
    * best_fit <- getModel(poisson_mix_model, which = "BIC")  # can also use AIC or ICL  
    * prior(best_fit)  # proportions  
    * param_pmm <- data.frame(parameters(best_fit))  # parameters by cluster, converted to data frame  
    * param_pmm <- param_pmm %>% mutate(Type = colnames(crimes_matrix))  
    * head(param_pmm)  
    * param_pmm %>% gather(Components, Lambda, -Type) %>% ggplot(aes(x = Type, y = Lambda, fill = Type)) + geom_bar(stat = "identity") + facet_wrap(~ Components) + theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")  
    * crimes_c <- crimes %>% mutate(CLUSTER = factor(clusters(best_fit)))  
    * crimes_c %>% group_by(CLUSTER) %>% mutate(NUMBER = row_number()) %>% ggplot(aes(x = CLUSTER, y = NUMBER, col = CLUSTER)) + geom_text(aes(label = COMMUNITY), size = 2.3)+ theme(legend.position="none")  
  
Example code includes:  
```{r}

# Create the vector of probabilities
p_cluster_1 <- c(0.8, 0.8, 0.2, 0.9)

# Create the sample for each pixel
set.seed(18102308)
pixel_1 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[1], p_cluster_1[1]))
pixel_2 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[2], p_cluster_1[2]))
pixel_3 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[3], p_cluster_1[3]))
pixel_4 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[4], p_cluster_1[4]))

# Combine the samples
sample_cluster_1 <- cbind(pixel_1, pixel_2, pixel_3, pixel_4)

# Have a look to the sample
head(sample_cluster_1)


digitUse2 <- rowSums(digitKey[, c(1, 3, 10)]) == 1
digits_sample_2 <- digitData[digitUse2, ]
dim(digits_sample_2)


# transform into matrix
digits_sample_2 <- as.matrix(digits_sample_2)

# dimension
dim(digits_sample_2)

# look to the first observation
show_digit(digits_sample_2[1, ])

# look to the last observation
show_digit(digits_sample_2[nrow(digits_sample_2), ])


set.seed(1513)
# Fit Bernoulli mixture model
bernoulli_mix_model <- flexmix(digits_sample_2 ~ 1, k = 3, model = FLXMCmvbinary(), 
                               control = list(tolerance = 1e-15, iter.max = 1000)
                               )
prior(bernoulli_mix_model)

# Extract the parameters for each cluster
param_comp_1 <- parameters(bernoulli_mix_model, component = 1)
param_comp_2 <- parameters(bernoulli_mix_model, component = 2)
param_comp_3 <- parameters(bernoulli_mix_model, component = 3)

# Visualize the clusters
show_digit(param_comp_1)
show_digit(param_comp_2)
show_digit(param_comp_3)


set.seed(1541)

# Create the vector of lambdas
lambda_1 <- c(150, 300, 50)

# Create the sample of each crime
assault_1 <- rpois(n = 10, lambda = lambda_1[1])
robbery_1 <- rpois(n = 10, lambda = lambda_1[2])
battery_1 <- rpois(n = 10, lambda = lambda_1[3])

# Combine the results
cities_1 <- cbind(assault_1, robbery_1, battery_1)

# Check the sample
cities_1


crimes <- readr::read_csv("./RInputFiles/CoC_crimes.csv")
dim(crimes)
names(crimes) <- stringr::str_replace_all(stringr::str_to_lower(names(crimes)), " ", ".")

# Check with glimpse
glimpse(crimes)

# Transform into a matrix, without `community`
matrix_crimes <- crimes %>%
  select(-community) %>%  
  as.matrix()

# Check the first values
head(matrix_crimes)


set.seed(2017)
# Fit the Poisson mixture model
poisson_mm <- stepFlexmix(matrix_crimes ~ 1, k = 1:15, nrep = 5, model = FLXMCmvpois(), 
                          control = list(tolerance = 1e-15, iter.max = 1000)
                          )

# Select the model that minimize the BIC
best_poisson_mm <- getModel(poisson_mm, which = "BIC")

# Get the parameters into a data frame
params_lambdas <- data.frame(parameters(best_poisson_mm))

# Add the column with the type of crime
params_lambdas_crime <- params_lambdas %>% 
    mutate(crime = colnames(matrix_crimes))

# Plot the clusters with their lambdas
params_lambdas_crime %>% 
    gather(cluster, lambdas, -crime) %>% 
    ggplot(aes(x = crime, y = lambdas, fill = crime)) + 
    geom_bar(stat = "identity") +
    facet_wrap(~ cluster) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")

# Add the cluster assignments
crimes_with_clusters <- crimes %>% 
    mutate(cluster = factor(clusters(best_poisson_mm)))

# Plot the clusters with the communities
crimes_with_clusters %>% 
    group_by(cluster) %>% 
    mutate(number = row_number()) %>% 
    ggplot(aes(x = cluster, y = number, col = cluster)) + 
    geom_text(aes(label = community), size = 2.3) +
    theme(legend.position="none")

```
  
  
  
***
  
###_Developing R Packages_  
  
Chapter 1 - The R Package Structure  
  
Introduction to Package Building:  
  
* R packages can include functions, data, documentation, vignettes, tests, and the like  
* At a minimum, the package must have 1) R Directory, 2) man Directory, 3) NAMESPACE file, and 4) DESCRIPTION file  
* Can use devtools to help with package building, as well as roxygen2  
	* create()  
    * document()  
    * check()  
    * build()  
    * test()  
* The devtools functions have descriptive names closely related to their assigned tasks  
	* devtools::create("simutils")  # avoid names that are already on CRAN  
  
Description and Namespace Files:  
  
* The DESCRIPTION contains background information such as author, package name, version, license, and the like  
* The NAMESPACE file will be edited based on entries made in other locations  
	* import() will bring in from other packages  
    * export() will export to the calling environment  
  
Optional Directories:  
  
* The most common optional directories include data, vignettes, tests, compiled code, and translations  
* Example for adding data to the package  
	* sim_dat <- data.frame( ID = 1:10, Value = sample(1:11, 10), Apples = sample(c(TRUE, FALSE), 10, replace = TRUE) )  
    * devtools::use_data(sim_dat, pkg = "simutils")  
* Example for adding vignettes to the package  
	* use_vignette("my_first_vignette", pkg = "simutils")  
* Best practices for structuring code - may NOT include subdirectories  
	* Group similar functions together in a file  
    * Do not create a small file for every function  
  
Example code includes:  
```{r eval=FALSE}

# Use the create function to set up your first package
devtools::create("./RPackages/datasummary")

# Take a look at the files and folders in your package
dir("./RPackages/datasummary")


# Create numeric_summary() function
numeric_summary <- function(x, na.rm) {
    # Include an error if x is not numeric
    if(!is.numeric(x)){
        stop("Data must be numeric")
    }
    
    # Create data frame
    data.frame( min = min(x, na.rm = na.rm),
                median = median(x, na.rm = na.rm),
                sd = sd(x, na.rm = na.rm),
                max = max(x, na.rm = na.rm))
}

data(airquality)

# Test numeric_summary() function
numeric_summary(airquality$Ozone, TRUE)


# What is in the R directory before adding a function?
dir("./RPackages/datasummary/R")

# Use the dump() function to write the numeric_summary function
dump("numeric_summary", file = "./RPackages/datasummary/R/numeric_summary.R")

# Verify that the file is in the correct directory
dir("./RPackages/datasummary/R")


# a package should not have the same name as an existing package and its name must only contain letters, numbers, or dots.


# What is in the package at the moment?
dir("./RPackages/datasummary")

# Add the weather data
data(Weather, package="mosaicData")
devtools::use_data(Weather, pkg = "./RPackages/datasummary")

# Add a vignette called "Generating Summaries with Data Summary"
devtools::use_vignette("Generating_Summaries_with_Data_Summary", pkg = "./RPackages/datasummary")

# What directories do you now have in your package now?
dir("./RPackages/datasummary")


data_summary <- function(x, na.rm = TRUE){
  num_data <- select_if(x, .predicate = is.numeric) 
  map_df(num_data, .f = numeric_summary, na.rm = TRUE, .id = "ID")
}

# Write the function to the R directory
dump("data_summary", file = "./RPackages/datasummary/R/data_summary.R")
dir("./RPackages/datasummary")

```
  
  
  
***
  
Chapter 2 - Documenting Packages  
  
Introduction to roxygen2:  
  
* Good documentation is key to the package - all functions, usages, outputs, etc.  
* The roxygen2 package allows for creating this type of documentation  
	* Included above the function, with each line starting with #'  
    * Paragraph 1 is the title (should be short)  
    * Paragraph 2 is the brief description (should be a single sentence)  
    * Paragraph 3 is the longer description (an actual paragraph of a few sentences)  
    * Paragraphs 4+ need to all start with @tag where tag might be param or author or import or return or export or examples or etc  
* Packages need to import other packages rather than calling them by way of library(package)  
  
How to export functions:  
  
* Exported functions become visible to the end-user and are core to package functionality (documented)  
	* Flagged with a tag of #' @export  
* Non-exported functions are not visible to the end-user but instead serve as utility functions within the package  
	* Flagged by having no tag of #' @export  
* Only the exported functions get loaded, so calling a non-exported function requires a triple colon from the namespace  
	* simutils:::sum_na(airquality$Ozone)  
  
Documenting other elements:  
  
* Can include examples using the @examples tag with the example included on the line below  
	* If you do not want an example to be run during checking, then the \dontrun{} tag can be used with the example being inside {} and allowed to be multiple lines  
* The return from the function is specified using the @return tag, and with the \code{} tag being   available to signal that this refers to one of the arguments/parameters  
* Can use the @author and @seealso tags as needed  
  
Documenting a package:  
  
* All roxygen headers need to be followed by some form of R code  
	* Can use a single line "_PACKAGE" for this  
* The minimum level of documentation is title, description, arguments, and exported (for exported functionsl only)  
	* Other components may be useful to the end-user  
* Can put a data frame from the current environment in to the package  
	* use_data(sim_dat, pkg = "simutils")  
* Can also use the roxygen formatting and then conclude with "sim_dat" to put this in with headers and descriptors  
* Can create the manual files (create and/or update) using  
	* document("simutils")  
  
Example code includes:  
```{r eval=FALSE}

#' Summary of Numeric Columns
#'
#' Generate specific summaries of numeric columns in a data frame
#' 
#' @param x A data frame. Non-numeric columns will be removed
#' @param na.rm A logical indicating whether missing values should be removed
#' @import purrr
#' @import dplyr
#' @importFrom tidyr gather
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Summary of Numeric Columns
#'
#' Generate specific summaries of numeric columns in a data frame
#' 
#' @param x A data frame. Non-numeric columns will be removed
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Data Summary for Numeric Columns
#'
#' Custom summaries of numeric data in a provided data frame
#'
#' @param x A data.frame containing at least one numeric column
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
#' @examples
#' data_summary(iris)
#' data_summary(airquality, na.rm = FALSE)
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}



# For code you use \code{text to format}
# To link to other functions you use \link[packageName]{functioName}, although note the package name is only required if the function is not in your package
# To include an unordered list you use \itemize{}. Inside the brakets you mark new items with \item followed by the item text.

#' Data Summary for Numeric Columns
#'
#' Custom summaries of numeric data in a provided data frame
#'
#' @param x A data.frame containing at least one numeric column
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
#' @examples
#' data_summary(iris)
#' data_summary(airquality, na.rm = FALSE)
#'
## Update the details for the return value
#' @return 
#' This function returns a \code{data.frame} including columns:
#' \itemize{
#'  \item ID
#'  \item min
#'  \item median
#'  \item sd
#'  \item max
#' }
#'
#' @export
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Summary of Numeric Columns
#' Generate specific summaries of numeric columns in a data frame
#'
#' @param x A data frame. Non-numeric columns will be removed
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
#' @examples
#' data_summary(iris)
#' data_summary(airquality, na.rm = FALSE)
#' 
#' @return This function returns a \code{data.frame} including columns: 
#' \itemize{
#'  \item ID
#'  \item min
#'  \item median
#'  \item sd
#'  \item max
#' }
#'
## Add in the author of the `data_summary()` function. 
#' @author My Name <myemail@example.com>
## Update the header to link to the `summary()` function (in the `base` package).
#' @seealso \link[base]{summary}
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Custom Data Summaries
#' 
#' Easily generate custom data frame summaries
#' 
#' @docType package
#' @name datasummary
_PACKAGE 


#' Random Weather Data
#'
#' A dataset containing randomly generated weather data.
#'
#' @format A data frame of 7 rows and 3 columns
#' \describe{
#'  \item{Day}{Numeric values giving day of the week, 1 = Monday, 7 = Sunday}
#'  \item{Temp}{Integer values giving temperature in degrees Celsius}
#'  \item{Weather}{Character values giving precipitation type, Sun if none}
#' }
#' @source Randomly generated data
weather


# Generate package documentation
document("datasummary")

# Examine the contents of the man directory
dir("datasummary/man")

# View the documentation for the data_summary function
help("data_summary")

# View the documentation for the weather dataset
help("weather")

```
  
  
  
***
  
Chapter 3 - Checking and Building R Packages  
  
Why check an R package?  
  
* There are many checks for a typical R package, including many that are mandatory for submission to CRAN  
	* Package can be installed  
    * Description information is correct  
    * Dependencies make sense  
    * No code syntax errors  
    * Documentation is complete  
    * Unit tests can be run  
    * Vignettes can be built  
* A system-level R tool can check the package for you  
	* check("simutils")  # default setting is the same as for CRAN  
    * Errors need to be fixed, while warnings are less problematic (still should be investigated)  
  
Errors, warnings, and notes:  
  
* Package dependencies will be checked; will error out if these are not available  
* Often have documentation issues; need to update function and argument names to match what has been updated in code  
* The examples are all run, and need to return with an OK status  
* If LaTEX is not installed, there will be documentation build errors  
  
Differences in package dependencies:  
  
* The dependencies from "Depends:" are loaded in to the search() path, though this is not always the recommended approach due to masking  
* The dependencies from "Imports:" are loaded by a namespace  
	* use_package("dplyr") ## adds to imports  
* The "Suggests:" are not required for running the package but may be helpful (for example, for running the vignettes)  
	* use_package("ggplot2", "suggests") ## adds to suggests  
  
Building packages with continuous integration:  
  
* Can create package as either a source file or as a binary  
	* build("simutils")  # default builds the source file  
    * build("simutils", binary = TRUE)  # builds the binary version, needed for compiled code  
* Continuous integration can help with package maintenance  
	* Automatically checks packages whenever code is changed  
    * Useful with version control  
    * use_travis("simutils")  
  
Example code includes:  
```{r eval=FALSE}

# Check your package
check("datasummary")


#' Numeric Summaries
#' Summarises numeric data and returns a data frame containing the minimum value, median, standard deviation, and maximum value.
#'
#' @param x a numeric vector containing the values to summarize.
#' @param na.rm a logical value indicating whether NA values should be stripped before the computation proceeds.
numeric_summary <- function(x, na.rm){

  if(!is.numeric(x)){
    stop("data must be numeric")
  }

  data.frame( min = min(x, na.rm = na.rm),
              median = median(x, na.rm = na.rm),
              sd = sd(x, na.rm = na.rm),
              max = max(x, na.rm = na.rm))
}


# The way in which you define variables in tidyverse package functions can cause confusion for the R CMD check, which sees column names and the name of your dataset, and flags them as "undefined global variables".
# To get around this, you can manually specify the data and its columns as a vector to utils::globalVariables(), by including a line of code similar to the following in your package-level documentation:
# utils::globalVariables(c("dataset_name", "col_name_1", "col_name_2"))
# This defines dataset_name, col_name_1, and col_name_2 as global variables, and now you shouldn't get the undefined global variables error.

#' datasummary: Custom Data Summaries
#'
#' Easily generate custom data frame summaries
#'
#' @docType package
#' @name datasummary
_PACKAGE

# Update this function call
utils::globalVariables(c("weather", "Temp"))


# Add dplyr as an imported dependency to the DESCRIPTION file
use_package("dplyr", pkg = "datasummary")

# Add purrr as an imported dependency to the DESCRIPTION file
use_package("purrr", pkg = "datasummary")

# Add tidyr as an imported dependency to the DESCRIPTION file
use_package("tidyr", pkg = "datasummary")


# Build the package
build("datasummary")

# Examine the contents of the current directory
dir("datasummary")

```
  
  
  
***
  
Chapter 4 - Adding Unit Tests to R Packages  
  
What are unit tests and why write them?  
  
* Unit tests can help with checking that future function behavior remains as expected  
	* Changes in functionality of supporting code  
    * Later versions of R  
    * Different operating systems  
    * Different underlying data  
* Can add unit tests to the package using roxygen2  
	* Call use_testthat to set up the test framework  
    * This creates a test directory in the package root directory  
    * Within the test directory, there is a script testthat.R which contains code to run the tests  
    * Within the test directory is a directory testthat where you save all of your test scripts  
* Can then create individual tests that begin with expect_*()  
	* library(testthat)  
    * my_vector <- c("First" = 1, "Second" = 2)  
    * expect_identical(my_vector, c("First" = 1, "Second" = 2))  # will pass since types and values are the same  
    * expect_identical(myvector, c(1, 2))  # Error: `vec1` not identical to c(1, 2). names for target but not for current  
    * expect_equal(my_vector, c("First" = 1L, "Second" = 2L))  # checks only the values and attributes, so this will pass even if vector is c(1, 2)  
    * expect_equal(my_vector, c(First = 1.1, Second = 2.1), tolerance = 0.1)  # include a tolerance for differences  
    * expect_equivalent(my_vector, c(1, 2))  # compare only the values, not the types or the attributes  
  
Testing errors and warnings:  
  
* Functions can give warnings or errors for many reasons, such as sqrt(-1)  
* The testthat contains functions for expecting warnings and errors  
	* expect_warning(sqrt(-1)) # passes  
    * expect_error(sqrt("foo")) # passes  
    * expect_error(sqrt(-1))  # fails, since this returns a warning rather than an error  
    * expect_error(sqrt("foo"), "non-numeric argument to mathematical function")  # requires that the error thrown be exactly the string in the second argument  
    * expect_error(sqrt("foo"), "NaNs produced")  # fails, since that is not the message  
  
Testing specific output and non-exported functions:  
  
* Printed messages and plots are side effects, which can be tested using expect_output  
	* expect_output(str(airquality), "41 36 12 18 NA 28 23 19 8 NA")  # passes, since this is part of the output  
    * expect_output(str(airquality), "air")  # fails, since the word "air" is not in the output  
    * expect_output_file(str(airquality), "airq.txt", update = TRUE)  # creates the file, which will throw an error the first time that it is run  
    * expect_output_file(str(airquality), "airq.txt")  # next time run, it will compare the output to the file  
* Can use the library for headers from the roxygen headers  
	* expect_equivalent(na_counter(airquality), c(37, 7, 0, 0, 0, 0))  
    * expect_equal(simutils:::sum_na(airquality$Ozone), 37)  # three colons signal that this is a non-exported function  
  
Grouping and running tests:  
  
* Can organize and group tests for easier future use - can add many tests in a single test_that() call  
	* test_that("na_counter correctly counts NA values", { test_matrix = matrix(c(NA, 1, 4, NA, 5, 6), nrow = 2) ; air_expected = c(Ozone = 37, Solar.R = 7, Wind = 0, Temp = 0, Month = 0, Day = 0) ; mat_expected = c(V1 = 1, V2 = 1, V3 = 0) ;  expect_equal(na_counter(airquality), air_expected) ; expect_equal(na_counter(test_matrix), mat_expected) })  
    * context("na_counter checks")  # will then give the appropriate information about what failed in the checks  
* Need to fix any failed tests, either by fixing the code or by fixing the test  
  
Wrap up:  
  
* Sturcture of R packages, including NAMESPACE and DESCRIPTION  
* Documenting R packages and including roxygen2 for examples/documentation  
* Building integrated packages with checks and tests  
* Unit tests to ensure that packages run as expected  
  
Example code includes:  
```{r eval=FALSE}

# Set up the test framework
use_testthat("datasummary")

# Look at the contents of the package root directory
dir("datasummary")

# Look at the contents of the new folder which has been created 
dir("datasummary/tests")


# Create a summary of the iris dataset using your data_summary() function
iris_summary <- data_summary(iris)

# Count how many rows are returned
summary_rows <- nrow(iris_summary) 

# Use expect_equal to test that calling data_summary() on iris returns 4 rows
expect_equal(summary_rows, 4)


result <- data_summary(weather)

# Update this test so it passes
expect_equal(result$sd, c(2.1, 3.6), tolerance = 0.1)

expected_result <- list(
    ID = c("Day", "Temp"),
    min = c(1L, 14L),
    median = c(4L, 19L),
    sd = c(2.16024689946929, 3.65148371670111),
    max = c(7L, 24L)
)

# Write a passing test that compares expected_result to result
expect_equivalent(result, expected_result)


# Create a vector containing the numbers 1 through 10
my_vector <- 1:10

# Look at what happens when we apply this vector as an argument to data_summary()
data_summary(my_vector)

# Test if running data_summary() on this vector returns an error
expect_error(data_summary(my_vector))


# Run data_summary on the airquality dataset with na.rm set to FALSE
data_summary(airquality, na.rm=FALSE)

# Use expect_warning to formally test this
expect_warning(data_summary(airquality, na.rm = FALSE))


# Expected result
expected <- data.frame(min = 14L, median = 19L, sd = 3.65148371670111, max = 24L)

# Create variable result by calling numeric summary on the temp column of the weather dataset
result <- datasummary:::numeric_summary(weather$Temp, na.rm = TRUE)

# Test that the value returned matches the expected value
expect_equal(result, expected)


# Use context() and test_that() to group the tests below together
context("Test data_summary()")

test_that("data_summary() handles errors correctly", {

  # Create a vector
  my_vector <- 1:10

  # Use expect_error()
  expect_error(data_summary(my_vector))

  # Use expect_warning()
  expect_warning(data_summary(airquality, na.rm = FALSE))

})


# Run the tests on the datasummary package
test("datasummary")

```
  
  
  
***
  
###_Factor Analysis in R_  
  
Chapter 1 - Evaluating Your Measure with Factor Analysis  
  
Introduction to Exploratory Factor Analysis:  
  
* Psychometrics is the study of unobservable ("of the mind") variables  
* Factor analysis is a valuable tool in psychometric analysis  
* Factor analysis is mid-way between SEM (structural equation modeling) and Classical Test Theory  
	* Exploratory Factor Analysis (EFA) is used during measure development  
    * Confirmatory Factor Analysis (CFA) is used to validate a measure after development  
* This course will use library(psych) and the gcbs dataset on conspiracies  
	* EFA_model <- fa(gcbs)  
    * fa.diagram(EFA_model)  
    * EFA_model$loadings  
  
Overview of the Measure Development Process:  
  
* The development process includes  
	* Develop items foir your measure - start with a larger list than you need  
    * Collect pilot data from a representative sample  
    * Check what the dataset looks like - psych::describe()  
    * Consider whether to run EFA or CFA or both  
    * If running both EFA and CFA, split the dataset in to two samples  
    * Compare the two samples to make sure they are similar  
* Can use the psych package to view a dataset by the grouping variable  
	* gcbs_grouped <- cbind(gcbs, group_var)  
    * describeBy(gcbs_grouped, group = group_var)  # group= can be a vector  
    * statsBy(gcbs_grouped, group = "group_var")  # group= needs to be a name of a column  
  
Measure Features: Correlations and Reliability:  
  
* Can grab the lower-diagonal correlations using lowerCor(gcbs)  
* Can also get the p-values using corr.test(gcbs, use = "pairwise.complete.obs")$p  # gets the p-values for each correlation  
* Can also get the 95% CI for the correlations using corr.test(gcbs, use = "pairwise.complete.obs")$ci  
* Can also get the alpha using alpha(gcbs) # measure of the internal consistency or 'reliability' of the measure, with a target of 0.8+  
* Can look at the split-half reliability using splitHalf(gcbs)  
  
Example code includes:  
```{r}

# Load the psych package
library(psych)


gcbs <- readRDS("./RInputFiles/GCBS_data.rds")
glimpse(gcbs)


# Conduct a single-factor EFA
EFA_model <- fa(gcbs)

# View the results
EFA_model


# Set up the single-factor EFA
EFA_model <- fa(gcbs)

# View the factor loadings
EFA_model$loadings

# Create a path diagram of the items' factor loadings
fa.diagram(EFA_model)


# Take a look at the first few lines of the response data and their corresponding sum scores
head(gcbs)
rowSums(gcbs[1:6, ])

# Then look at the first few lines of individuals' factor scores
head(EFA_model$scores)

# To get a feel for how the factor scores are distributed, look at their summary statistics and density plot.
summary(EFA_model$scores)

plot(density(EFA_model$scores, na.rm = TRUE), main = "Factor Scores")


# Basic descriptive statistics
describe(gcbs)

# Graphical representation of error
error.dots(gcbs)

# Graphical representation of error
error.bars(gcbs)


# Establish two sets of indices to split the dataset
N <- nrow(gcbs)
indices <- seq(1, N)
indices_EFA <- sample(indices, floor((.5*N)))
indices_CFA <- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
gcbs_EFA <- gcbs[indices_EFA, ]
gcbs_CFA <- gcbs[indices_CFA, ]


# Use the indices from the previous exercise to create a grouping variable
group_var <- vector("numeric", nrow(gcbs))
group_var[indices_EFA] <- 1
group_var[indices_CFA] <- 2

# Bind that grouping variable onto the gcbs dataset
gcbs_grouped <- cbind(gcbs, group_var)

# Compare stats across groups
describeBy(gcbs_grouped, group = group_var)
statsBy(gcbs_grouped, group = "group_var")


# Take a look at some correlation data
lowerCor(gcbs, use = "pairwise.complete.obs")

# Take a look at some correlation data
corr.test(gcbs, use = "pairwise.complete.obs")$p

# Take a look at some correlation data
corr.test(gcbs, use = "pairwise.complete.obs")$ci


# Estimate coefficient alpha
alpha(gcbs)

# Calculate split-half reliability
splitHalf(gcbs)

```
  
  
  
***
  
Chapter 2 - Multidimensional EFA  
  
Determining dimensionality:  
  
* Can use factor analysis to find the "true" number of dimensions being reflected in the data  
* Can use the bfi dataset ("Big Five" personality trait dataset)  
	* Six point scale with 1 being very inaccurate and 6 being very accurate  
* Suppose that you do not have a theory underlying the data and instead want to use an empirical approach with eigenvalues  
	* bfi_EFA_cor <- cor(bfi_EFA, use = "pairwise.complete.obs")  
    * eigenvals <- eigen(bfi_EFA_cor)  
    * eigenvals$values  
    * scree(bfi_EFA_cor, factors = FALSE)  # eigenvalues greater than 1 are typically the best to use  
  
Understanding multidimensional data:  
  
* Theory and empirical data may lead to different outcomes - constructs for psychology are an example  
* Factors are the mathematical counterpart of a theoretical construct  
	* How well does the hypothesis fit with the data?  
* Can instead run exploratory analysis to try to come up with factors  
	* Lack of theoretical grounding can make interpretation of the results complicated  
* Can run the multidimensional analysis using any number of factors  
	* EFA_model <- fa(bfi_EFA, nfactors = 6)  
    * EFA_model$loadings  
    * head(EFA_model$scores)  
  
Investigating model fit:  
  
* Can look at absolute fit (adequate fit with typical ranges and cutoff values) and relative fit (no set ranges, used mainly for nested models from the same dataset)  
* Goal is to have a non-significant chi-squared test, though that is rare for large datasets  
* The TLI (Tucker-Lewis) should be 0.90+  
* The RMSEA (RMSE approximation) should be 0.05-  
* The BIC is a relative-fit statistic, so it is meaningful only to compare across models (lower BIC is better)  
	* bfi_theory <- fa(bfi_EFA, nfactors = 5)  
    * bfi_eigen <- fa(bfi_EFA, nfactors = 6)  
    * bfi_theory$BIC  
    * bfi_eigen$BIC  
  
Example code includes:  
```{r}

data(bfi, package="psych")
glimpse(bfi)


# Establish two sets of indices to split the dataset
N <- nrow(bfi)
indices <- seq(1, N)
indices_EFA <- sample(indices, floor((.5*N)))
indices_CFA <- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
bfi_EFA <- bfi[indices_EFA, ]
bfi_CFA <- bfi[indices_CFA, ]


# Calculate the correlation matrix first
bfi_EFA_cor <- cor(bfi_EFA, use = "pairwise.complete.obs")

# Then use that correlation matrix to calculate eigenvalues
eigenvals <- eigen(bfi_EFA_cor)

# Look at the eigenvalues returned
eigenvals$values

# Then use that correlation matrix to create the scree plot
scree(bfi_EFA_cor, factors = FALSE)


# Run the EFA with six factors (as indicated by your scree plot)
EFA_model <- fa(bfi_EFA, nfactors=6)

# View results from the model object
EFA_model


# Run the EFA with six factors (as indicated by your scree plot)
EFA_model <- fa(bfi_EFA, nfactors=6)

# View items' factor loadings
EFA_model$loadings

# View the first few lines of examinees' factor scores
head(EFA_model$scores)


# Run each theorized EFA on your dataset
bfi_theory <- fa(bfi_EFA, nfactors = 5)
bfi_eigen <- fa(bfi_EFA, nfactors = 6)

# Compare the BIC values
bfi_theory$BIC
bfi_eigen$BIC

```
  
  
  
***
  
Chapter 3 - Confirmatory Factor Analysis  
  
Setting up CFA:  
  
* Confirmatory analysis is based on explicitly defined factor relationships, to confirm a previously developed theory  
* Can use the results from an EFA as the baseline for a CFA; for example, reversing the wording of items with a negative loading  
	* EFA_syn <- structure.sem(EFA_model)  
    * EFA_syn  # Path goes from factor to item, while an NA for Value means that the starting parameter will be chosen randomly  
* Can also create CFA syntax from theory explicitly, for example  
	* theory_syn_eq <- "  
    * AGE: A1, A2, A3, A4, A5     #Agreeableness  
    * CON: C1, C2, C3, C4, C5     #Conscientiousness  
    * EXT: E1, E2, E3, E4, E5     #Extraversion  
    * NEU: N1, N2, N3, N4, N5     #Neuroticism  
    * OPE: O1, O2, O3, O4, O5     #Openness  
    * "  
    * theory_syn <- cfa(text = theory_syn_eq, reference.indicators = FALSE)  # sets the factor variances to 1 rather than estimating them  
  
Understanding the sem() syntax:  
  
* Syntax will show the Path, Parameter, and Starting Value  
	* Factor variances are shown using <-> arrows, with Start Value of 1 since they have been fixed as 1  
    * Factor covariances also use <-> arrows  
    * Item-level variances are also shown using <-> arrows  
* Can run the model using the sem() function with the syntax object  
	* theory_CFA <- sem(theory_syn, data = bfi_CFA)  
    * summary(theory_CFA)  
  
Investigating model fit:  
  
* The chi-squared test (log-likelihood test) is the only test printed by default, though it will usually be significant for a large dataset size  
* Can change the global options so that additional tests are run  
	* options(fit.indices = c("CFI", "GFI", "RMSEA", "BIC"))  
    * summary(theory_CFA)  
    * summary(theory_CFA)$BIC  # lower BIC is preferred, but is only useful for nested models on the same dataset  
  
Example code includes:  
```{r}

# Conduct a five-factor EFA on the EFA half of the dataset
EFA_model <- fa(bfi_EFA, nfactors = 5)

# Use the wrapper function to create syntax for use with the sem() function
EFA_syn <- structure.sem(EFA_model)


# Set up syntax specifying which items load onto each factor
theory_syn_eq <- "
AGE: A1, A2, A3, A4, A5
CON: C1, C2, C3, C4, C5
EXT: E1, E2, E3, E4, E5
NEU: N1, N2, N3, N4, N5
OPE: O1, O2, O3, O4, O5
"

library(sem)

# Feed the syntax in to have variances and covariances automatically added
theory_syn <- cfa(text = theory_syn_eq, reference.indicators = FALSE)

# Use the sem() function to run a CFA
theory_CFA <- sem(theory_syn, data = bfi_CFA)

# Use the summary function to view fit information and parameter estimates
summary(theory_CFA)


# CAUTION THAT THIS WILL SET GLOBAL OPTIONS
# Set the options to include various fit indices so they will print
origFit <- getOption("fit.indices")
options(fit.indices = c("CFI", "GFI", "RMSEA", "BIC"))

# Use the summary function to view fit information and parameter estimates
summary(theory_CFA)


# Run a CFA using the EFA syntax you created earlier
EFA_CFA <- sem(EFA_syn, data = bfi_CFA)

# Locate the BIC in the fit statistics of the summary output
summary(EFA_CFA)$BIC

# Compare EFA_CFA BIC to the BIC from the CFA based on theory
summary(theory_CFA)$BIC


# Reset to baseline
options(fit.indices = origFit)

```
  
  
  
***
  
Chapter 4 - Refining Your Measure and Model
  
EFA vs CFA Revisited:  
  
* EFA is exploratory and looks at many possible relationships  
* CFA is confirmatory and based only on the loadings defined by a theoretical relationship; will have different loadings than EFA dur to different number of variables  
* Due to the rotations involved in EFA, the variables may have non-intuitive names  
	* EFA_scores <- EFA_model$scores  
    * CFA_scores <- fscores(EFA_CFA, data = bfi_EFA)  
    * plot(density(EFA_scores[,1], na.rm = TRUE), xlim = c(-3, 3), ylim = c(0, 1), col = "blue")  
    * lines(density(CFA_scores[,1], na.rm = TRUE), xlim = c(-3, 3), ylim = c(0, 1), col = "red")  
  
Adding Loadings to Improve Fit:  
  
* Poor model fits are sometimes due to excluded loadings  
* Can alter the syntax to add the new loadings desired; OK to have some items theorized to load on to multiple factors  
* Can then run the updated model and look at the ANOVA  
	* anova(theory_CFA, theory_CFA_add)  
  
Improving Fit by Removing Loadings:  
  
* Can delete loadings rather than add loadings  
	* theory_syn_del <- "  
    * AGE: A1, A2, A3, A4, A5  
    * CON: C1, C2, C3, C4, C5  
    * EXT: E1, E2, E3, E4, E5  
    * NEU: N1, N2, N3, N4, N5  
    * OPE: O1, O2, O3, O5  
    * "  
    * theory_syn3 <- cfa(text = theory_syn_del, reference.indicators = FALSE)  
    * theory_CFA_del <- sem(model = theory_syn3, data = bfi_CFA)  
  
Wrap-Up:  
  
* Unidimensional and multideimensional models  
* EFA (exploratory) and CFA (confirmatory)  
* The psych and sem packages are available also  
  
Example code includes:  
```{r}

# CAUTION THAT THIS WILL SET GLOBAL OPTIONS
# Set the options to include various fit indices so they will print
origFit <- getOption("fit.indices")
options(fit.indices = c("CFI", "GFI", "RMSEA", "BIC"))

# View the first five rows of the EFA loadings
EFA_model$loadings[1:5, ]

# View the first five loadings from the CFA estimated from the EFA results
summary(EFA_CFA)$coeff[1:5, ]


# Extracting factor scores from the EFA model
EFA_scores <- EFA_model$scores

# Calculating factor scores by applying the CFA parameters to the EFA dataset
CFA_scores <- fscores(EFA_CFA, data = bfi_EFA)

# Comparing factor scores from the EFA and CFA results from the bfi_EFA dataset
plot(density(EFA_scores[,1], na.rm = TRUE), 
    xlim = c(-3, 3), ylim = c(0, 1), col = "blue")
lines(density(CFA_scores[,1], na.rm = TRUE), 
    xlim = c(-3, 3), ylim = c(0, 1), col = "red")


# Add some plausible item/factor loadings to the syntax
theory_syn_add <- "
AGE: A1, A2, A3, A4, A5
CON: C1, C2, C3, C4, C5
EXT: E1, E2, E3, E4, E5, N4
NEU: N1, N2, N3, N4, N5, E3
OPE: O1, O2, O3, O4, O5
"

# Convert your equations to sem-compatible syntax
theory_syn2 <- cfa(text = theory_syn_add, reference.indicators = FALSE)

# Run a CFA with the revised syntax
theory_CFA_add <- sem(model = theory_syn2, data = bfi_CFA)

# Conduct a likelihood ratio test
anova(theory_CFA, theory_CFA_add)

# Compare the comparative fit indices - higher is better!
summary(theory_CFA)$CFI
summary(theory_CFA_add)$CFI

# Compare the RMSEA values - lower is better!
summary(theory_CFA)$RMSEA
summary(theory_CFA_add)$RMSEA

# Compare BIC values
summary(theory_CFA)$BIC
summary(theory_CFA_add)$BIC


# Remove the weakest factor loading from the syntax
theory_syn_del <- "
AGE: A1, A2, A3, A4, A5
CON: C1, C2, C3, C4, C5
EXT: E1, E2, E3, E4, E5
NEU: N1, N2, N3, N4, N5
OPE: O1, O2, O3, O5
"

# Convert your equations to sem-compatible syntax
theory_syn3 <- cfa(text = theory_syn_del, reference.indicators = FALSE)

# Run a CFA with the revised syntax
theory_CFA_del <- sem(model = theory_syn3, data = bfi_CFA)


# Compare the comparative fit indices - higher is better!
summary(theory_CFA)$CFI
summary(theory_CFA_del)$CFI

# Compare the RMSEA values - lower is better!
summary(theory_CFA)$RMSEA
summary(theory_CFA_del)$RMSEA

# Compare BIC values
summary(theory_CFA)$BIC
summary(theory_CFA_del)$BIC


# Reset to baseline
options(fit.indices = origFit)

```
  
  
  
***
  
###_Generalized Linear Models in R_  
  
Chapter 1 - GLM - Extension of Regression Toolbox  

Limitations of linear models:  
  
* Linear models are workhorses of data science - explaining variability with linear combinations of variables  
	* lm(y ~ x, data = dat)  
* Linear models assume linear relationships, and normally distributed residuals  
	* lm(formula = weight ~ Diet, data = ChickWeightEnd)  # ChickWeightEnd is the FINAL endpoint  
* Sometimes want to model counts or survival or the like, where the basic linear model is not appropriate  
	* Poisson family for count data  
    * Binomial family for survival data  
    * Link functions to convert the linear model to the relevant family  
* Can run GLM in R  
	* glm( y ~ x, data = data, family = "gaussian")  # this is the same as lm()  
  
Poisson regression:  
  
* The Poisson model is good for modeling count data - scores, visitors, cells, etc.  
* The Poisson distribution is always a non-negative integer, and with the same mean and variance  
	* glm(goal ~ player, data = scores, family = "poisson")  # global intercept and then delta goals for player vs. reference level  
    * glm(goal ~ player -1, data = scores, family = "poisson")  # average goals per player  
  
Basic lm() functions with glm():  
  
* R gives some useful shortcuts when working with lm() and glm(); for example, automatic output printing  
* The summary() call on a regression model provides additional details about the regression  
	* Can further use broom::tidy() to extract key data in a tidy format  
    * Can use coef() and confint() to get the coefficients and the confidence intervals  
* Can also make predictions based on an existing model  
	* predict(model, newData)  
  
Example code includes:  
```{r}

data(ChickWeight, package="datasets")
ChickWeightEnd <- ChickWeight %>% 
    mutate(Chick=as.factor(as.integer(Chick))) %>%
    group_by(Chick) %>% 
    filter(Time==max(Time), !(Chick %in% c(1, 2, 3, 8, 41))) %>%
    ungroup()
glimpse(ChickWeightEnd)


# Fit a lm()
lm(formula = weight ~ Diet, data = ChickWeightEnd)

# Fit a glm()
glm( formula = weight ~ Diet , data = ChickWeightEnd, family = 'gaussian')


dat <- data.frame(time=1:30, 
                  count=c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 1, 1, 4, 1, 1, 1, 1, 0, 0)
                  )
dat


# fit y predicted by x with data.frame dat using the poisson family
poissonOut <- glm(count ~ time, data=dat, family="poisson")

# print the output
print(poissonOut)


# Fit a glm with count predicted by time using data.frame dat and gaussian family
lmOut <- glm(count ~ time, data=dat, family="gaussian")

summary(lmOut)
summary(poissonOut)


scores <- data.frame(player=rep(c("Sam", "Lou"), each=5), 
                     goal=c(1, 2, 0, 4, 3, 0, 0, 1, 0, 0)
                     )
scores


# Fit a glm() that estimates the difference between players
summary(glm(goal ~ player, data=scores, family="poisson"))

# Fit a glm() that estimates an intercept for each player 
summary(glm(goal ~ player - 1, data=scores, family="poisson"))


dat2 <- data.frame(Date=as.Date("2005-01-09")+1:4368, Number=0L) %>%
    mutate(Month=as.factor(lubridate::month(Date)))

eq1 <- c(1, 2, 6, 22, 42, 47, 48, 86, 96, 109, 113, 119, 190, 192, 208, 248, 264, 278, 306, 333, 334, 336, 368, 375, 392, 393, 408, 417, 424, 429, 439, 449, 455, 456, 500, 523, 536, 544, 545, 548, 550, 551, 586, 590, 597, 598, 673, 678, 700, 717, 740, 750, 755, 756, 767, 775, 793, 831, 859, 865, 866, 877, 885, 887, 895, 937, 1086, 1101, 1107, 1111, 1112, 1154, 1157, 1183, 1213, 1235, 1247, 1251, 1269, 1272, 1288, 1295, 1300, 1320, 1342, 1350, 1424, 1454, 1457, 1460, 1476, 1522, 1589, 1598, 1608, 1627, 1642, 1665, 1697, 1709, 1733, 1746, 1749, 1766, 1799, 1830, 1866, 1895, 1914, 1920, 1934, 1942, 1953, 1960, 1961, 1966, 1969, 1989, 2007, 2041, 2051, 2087, 2092, 2096, 2106, 2122, 2129, 2138, 2156, 2159, 2174, 2176, 2177, 2180, 2191, 2214, 2217, 2218, 2251, 2276, 2286, 2302, 2308, 2340, 2352, 2361, 2382, 2416, 2419, 2421, 2464, 2468, 2492, 2522, 2526, 2548, 2550, 2573, 2620, 2625, 2627, 2629, 2698, 2706, 2721, 2726, 2760, 2768, 2787, 2796, 2813, 2854, 2858, 2890, 2900, 2909, 2932, 2933, 2955, 2960, 2966, 2997, 3032, 3057, 3063, 3080, 3090, 3095, 3098, 3122, 3130, 3154, 3160, 3199, 3205, 3215, 3227, 3229, 3243, 3244, 3254, 3302, 3340, 3350, 3469, 3506, 3519, 3525, 3535, 3542, 3584, 3604, 3653, 3660, 3673, 3692, 3694, 3706, 3763, 3792, 3801, 3808, 3812, 3814, 3822, 3884, 3892, 4001, 4084, 4194, 4210, 4220, 4229, 4242, 4265, 4267, 4296, 4302, 4325, 4334, 4338, 4341, 4353, 4354, 4357, 4368)
eq2 <- c(21, 195, 308, 505, 522, 560, 913, 1202, 1353, 1439, 1473, 1484, 1614, 1717, 1808, 1940, 2110, 2391, 2407, 2535, 2716, 2748, 2949, 3313, 3421, 3671, 3967, 3991, 4281)
eq3 <- c(624, 776, 1364, 1585, 2063, 2109, 2196, 2569, 2576, 2607, 3399, 3533, 3607)
eq4 <- c(463, 1918, 2417, 3064, 3606)
eq5 <- c(13, 3826)
eq6 <- c(701, 2097)
eq7 <- c(2509, 4276)
eq9 <- c(1637)

dat2[eq1, "Number"] <- 1L
dat2[eq2, "Number"] <- 2L
dat2[eq3, "Number"] <- 3L
dat2[eq4, "Number"] <- 4L
dat2[eq5, "Number"] <- 5L
dat2[eq6, "Number"] <- 6L
dat2[eq7, "Number"] <- 7L
dat2[eq9, "Number"] <- 9L

str(dat2)
table(dat2$Number)
table(dat2$Month)


# build your models
lmOut <- lm(Number ~ Month, data=dat2) 
poissonOut <- glm(Number ~ Month, data=dat2, family="poisson")

# examine the outputs using print
print(lmOut)
print(poissonOut)

# examine the outputs using summary
summary(lmOut)
summary(poissonOut)

# examine the outputs using tidy
broom::tidy(lmOut)
broom::tidy(poissonOut)


# Extract the regression coefficients
coef(poissonOut)

# Extract the confidence intervals
confint(poissonOut)


# use the model to predict with new data 
newDat <- data.frame(Month=as.factor(6:8))
predOut <- predict(object = poissonOut, newdata = newDat, type = "response")

# print the predictions
print(predOut)

```
  
  
  
***
  
Chapter 2 - Logistic Regression  
  
Overview of logistic regression:  
  
* Commonly used for making win/loss or survive/die predictions - binary data such as 0/1, Coke/Pepsi, W/L, etc.  
* The logistic regression is the default for GLM with family "binomial"  
	* The logit link transforms probabilities to log-odds, while the inverse logit transforms log-odds to probabilities  
* Can fit the logistic regression in R using the default link in the glm()  
	* glm(y ~ x, data = dat, family = 'binomial')  
  
Bernoulli vs. Binomial Distribution:  
  
* Binomial and Bernoulli distributions are the foundation of logistic regression  
	* Bernoulli models a single event (for example, the likelihood of heads in 1 coin flip)  
	* Binomial models multiple events at the same time (for example, the number of heads in 10 coin flips)  
* Several options for entering data in R for use in logistic regression  
	* Long format (Bernoulli) - vector of outcomes  
    * Wide format (binomial) - proportions of success with weights, such as looking at groups  
* The appropriate input structure depends on the underlying data - groups vs. individuals  
  
Link functions - probit compared with logit:  
  
* Link functions are important for understanding and simulating GLMs  
* The probit link function is another option (rather than logit) for the link function of the binomial  
	* The "probit" is an abbreviation for probability unit, and is computattionally easier than the logit which was important when computers were slower  
    * The probit returns a z-score, but with thinner tails than the probit (so the logit is often better for modeling outliers)  
* Need to convert z-scores to probabilities, then can run the rbinom function  
  
Example code includes:  
```{r}

busData <- readr::read_csv("./RInputFiles/busData.csv")
bus <- busData %>%
    mutate(Bus=factor(Bus, levels=c("No", "Yes")))
glimpse(bus)


# Build a glm that models Bus predicted by CommuteDays
# using data.frame bus. Remember to use a binomial family.
busOut <- glm(Bus ~ CommuteDays, data=bus, family="binomial")

# Print the busOut (be sure to use the print() function)
print(busOut)

# Look at the summary() of busOut
summary(busOut)

# Look at the tidy() output of busOut
broom::tidy(busOut)


# Simulate 1 draw with a sample size of 100
binomialSim <- rbinom(n=1, size=100, prob=0.5)

# Simulate 100 draw with a sample size of 1 
BernoulliSim <- rbinom(n=100, size=1, prob=0.5)

# Print the results from the binomial
print(binomialSim)

# Sum the results from the Bernoulli
sum(BernoulliSim)


dataLong <- data.frame(x=factor(rep(c("a", "b"), each=14), levels=c("a", "b")), 
                       y=factor(c('fail', 'fail', 'fail', 'fail', 'success', 'fail', 'fail', 'fail', 'fail', 'fail', 'fail', 'fail', 'fail', 'success', 'success', 'fail', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'fail', 'success', 'fail'), levels=c("fail", "success"))
                       )
str(dataLong)

# Fit a a long format logistic regression
lr_1 <- glm(y ~ x, data=dataLong, family="binomial")
print(lr_1)


dataWide <- dataLong %>%
    group_by(x) %>%
    summarize(fail=sum(y=="fail"), success=sum(y=="success"), Total=n(), successProportion = success/Total)
dataWide

# Fit a wide form logistic regression
lr_2 <- glm(cbind(fail, success) ~ x, data=dataWide, family="binomial")

# Fit a a weighted form logistic regression
lr_3 <- glm(successProportion ~ x, weights=Total, data=dataWide, family="binomial")

# print your results
print(lr_2)
print(lr_3)


# Fit a GLM with a logit link and save it as busLogit
busLogit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "logit"))

# Fit a GLM with probit link and save it as busProbit
busProbit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "probit"))

# Print model summaries
summary(busLogit)
summary(busProbit)


# Convert from the logit scale to a probability
p <- dlogis(0)

# Simulate a logit 
rbinom(n=10, size=1, prob=p)


# Convert from the probit scale to a probability
p <- pnorm(0)

# Simulate a probit
rbinom(n=10, size=1, prob=p)

```
  
  
  
***
  
Chapter 3 - Interpreting and Visualizing GLMs  
  
Poisson Regression Coefficients:  
  
* Linear models are additive, but we may want to use the linear model with a link to an exponential (multiplicative)  
* The Poisson model is multiplicative while the linear model is additive  
	* poissonOut <- glm(y ~ x, family = 'poisson')  
    * tidy(poissonOut, exponentiate = TRUE)  # exponentiate the coefficients  
* A significant Poisson coefficient should be statistically different from 1 (since this is exp(0))  
  
Plotting Poisson Regression:  
  
* Can use either a geom_smooth() or a boxplot()  
* Example of using simulated does data on cancer cells  
	* ggplot(data = dat, aes(x = dose, y = cells)) + geom_point()  
    * ggplot(data = dat, aes(x = dose, y = cells)) + geom_jitter(width = 0.05, height = 0.05)  
    * ggplot(data=dat, aes(x = dose, y = cells)) + geom_jitter(width = 0.05, height = 0.05) geom_smooth()  
    * ggplot(data = dat, aes(x = dose, y = cells)) + geom_jitter(width = 0.05, height = 0.05) + geom_smooth(method = 'glm', method.args = list(family = 'poisson'))  # Poisson GLM  
  
Understanding output from logistic regression:  
  
* Linear model results are the easiest to communicate  
* Poisson model results are multiplicative rather than additive, and are relatively easy to communicate  
* Logistic model results are in log-odds form, which are harder to communicate  
	* The odds ratio is p(win) / p(loss)  
    * The log-odds are the logit, and are ln( p/(1-p) ), and this is the logit function  
    * The odds are exp(log-odds)  
    * The odds ratio is exp(Beta-1) - if the odds ratio is 1, there is no impact (odds ratios greater than 1 mean a greater chance of something occuring)  
* Can extract confidence intervals and coefficients from the binomial GLM  
	* glmOut <- glm(y ~ x, family = 'binomial')  
    * coef(glmOut)  
    * exp(coef(glmOut))  
    * confint(glmOut)  
    * exp(confint(glmOut))  
    * tidy(glmOut, exponentiate = TRUE, conf.int= TRUE)  # get everything at once from the broom::tidy() package  
  
ggplot2 and binomial regression:  
  
* Can look at plots for the underlying data for a ggplot2  
	* ggplot(bus, aes(x = MilesOneWay, y = Bus)) + geom_point()  
    * ggJitter <- ggplot(bus, aes(x = MilesOneWay, y = Bus)) + geom_jitter(width = 0, height = 0.05)  
    * ggJitter + geom_smooth()  # does not work!  
    * bus$Bus2 <- as.numeric(bus$Bus) - 1  # convert factor to numeric  
    * ggJitter + geom_smooth()  # still not really right  
    * ggJitter + geom_smooth(method = 'glm', method.args = list(family = "binomial"))  # much better!  
* Can use graphs to compare probit and logit  
	* ggJitter + geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'logit')), se = FALSE, color = 'red') + geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'probit')), se = FALSE, color = 'blue')  
  
Example code includes:  
```{r}

# extract the coeffients from lmOut
(lmCoef <- coef(lmOut))

# extract the coefficients from poisosnOut
(poissonCoef <- coef(poissonOut))

# take the exponetial using exp()
(poissonCoefExp <- exp(poissonCoef))


# This is because the Poisson coefficients are multiplicative
# Notice that 0.129 * 0.706 = 0.091 from the Poisson coefficents is the same as 0.129-0.038 = 0.091 from the linear model

cellData <- data.frame(dose=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10), 
                       cells=c(1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 3, 0, 2, 2, 1, 0, 1, 2, 2, 2, 2, 3, 5, 3, 0, 3, 6, 2, 4, 4, 2, 2, 8, 4, 4, 4, 7, 2, 6, 5, 2, 5, 8, 4, 7, 4, 4, 7, 9, 3, 6, 7, 9, 5, 3, 5, 5, 3, 4, 11, 2, 7, 9, 3, 4, 2, 6, 5, 5, 6, 4, 5, 8, 10, 11, 9, 8, 8, 11, 7, 10, 12, 9, 12, 10, 12, 9, 17, 6, 9, 15, 11, 11, 10, 4, 9, 13, 8, 8, 13)
                       )

# Use geom_smooth to plot a continuous predictor variable
ggplot(data = cellData, aes(x = dose, y = cells)) + 
    geom_jitter(width = 0.05, height = 0.05) + 
    geom_smooth(method = 'glm', method.args = list(family = 'poisson'))


# Extract out the coefficients 
coefOut <- coef(busOut)

# Convert the coefficients to odds-ratios 
exp(coefOut)

# use tidy on busOut and exponentiate the results and extract the confidence interval
broom::tidy(busOut, exponentiate=TRUE, conf.int=TRUE)


str(bus)
bus <- bus %>%
    mutate(Bus2 = as.integer(Bus)-1)
str(bus)

# add in the missing parts of the ggplot
ggJitter <- ggplot(data = bus, aes(x = MilesOneWay, y = Bus2)) + 
    geom_jitter(width = 0, height = 0.05)

# add in geom_smooth()
ggJitter + geom_smooth()

# add in the missing parts of the ggplot
ggJitter + geom_smooth(method =  "glm" , method.args = list(family="binomial"))


# add in the missing parts of the ggplot
ggJitter + 
    geom_smooth(method = 'glm', method.args = list(family = binomial(link="probit")), 
                color = 'red', se = FALSE
                ) +
    geom_smooth(method = 'glm', method.args = list(family = binomial(link="logit")), 
                color = 'blue', se = FALSE
                )

```
  
  
  
***
  
Chapter 4 - Multiple Regression with GLMs  
  
Multiple logistic regression:  
  
* Can use multiple predictors in the logistic regression  
	* Risks of over-fitting as the number of predictor variables increases - typical target of observations >= 10*predictors  
    * glm(Bus ~ CommuteDay + MilesOneWay, data = bus, family = 'binomial')  
* When there is correlation in the predictors, the coefficients may change depending on the order in which they appear in the equation  
  
Formulas in R:  
  
* The model.matrix() is the cornerstone of the regression process; often run behind the scenes  
* With multiple intercepts, the default is to have the global intercept as the first group, and all other groups being the intercept relative to the reference group  
  
Assumptions of multiple logistic regression:  
  
* Simpson's paradox can be a confounder - need to include all the relevant grouping variables  
	* Example of the UC Berkeley admission data - key to include the "by department" variable  
* Assumptions of linear and monotonic responses  
* Predictors and response variables should be independent  
* Over-dispersion can cause issues - too many zeroes, too many ones, changing variances over x, etc.  
  
Wrap up:  
  
* GLM extensions of LM - count data, logits, plotting, etc.  
  
Example code includes:  
```{r}

# Build a logistic regression with Bus predicted by CommuteDays and MilesOneWay
busBoth <- glm(Bus ~ CommuteDays + MilesOneWay, data=bus, family="binomial")

# Look at the summary of the output
summary(busBoth)


# Build a logistic regression with Bus predicted by CommuteDays
busDays <- glm(Bus ~ CommuteDays, data=bus, family="binomial")

# Build a logistic regression with Bus predicted by MilesOneWay
busMiles <- glm(Bus ~ MilesOneWay, data=bus, family="binomial")


# Build a glm with CommuteDays first and MilesOneWay second
busOne <- glm(Bus ~ CommuteDays + MilesOneWay, data=bus, family="binomial")

# Build a glm with MilesOneWay first and CommuteDays second
busTwo <- glm(Bus ~ MilesOneWay + CommuteDays, data=bus, family="binomial")

# Print model summaries
summary(busOne)
summary(busTwo)


size <- c(1.1, 2.2, 3.3)
count <- c(10, 4, 2)

# use model matrix with size
model.matrix(~ size)

# use model matirx with count
model.matrix(~ size + count)


color <- c("red", "blue", "green")

# create a matrix that includes a reference intercept
model.matrix(~ color)

# create a matrix that includes an intercept for each group
model.matrix(~ color - 1)


shape <- c("square", "square", "circle")

# create a matrix that includes color and shape  
model.matrix(~ color + shape - 1)

# create a matrix that includes shape and color 
model.matrix(~ shape + color - 1)


data("UCBAdmissions", package="datasets")
UCBdata <- as.data.frame(UCBAdmissions) %>%
    mutate(Gender=factor(Gender, levels=c("Female", "Male")), Dept=factor(Dept, levels=LETTERS[1:6])) %>%
    tidyr::spread(Admit, Freq) %>%
    arrange(Dept, Gender)

# build a binomial glm where Admitted and Rejected are predicted by Gender
glm1 <- glm(cbind(Admitted, Rejected) ~ Gender, data=UCBdata, family="binomial")
summary(glm1)

# build a binomial glm where Admitted and Rejected are predicted by Gender and Dept
glm2 <- glm(cbind(Admitted, Rejected) ~ Gender + Dept, data=UCBdata, family="binomial")
summary(glm2)


# Add a non-linear equation to a geom_smooth
ggJitter + 
  geom_smooth(method = 'glm', method.args = list(family = 'binomial'), formula = y~I(x^2), color = 'red')

```
  
  
  
***
  
###_Introduction to Bioconductor_  
  
Chapter 1 - What is Bioconductor?  
  
Introduction to the Bioconductor Project:  
  
* Bioconductor is open-source software - datasets and packages for analyzing biological data  
	* Typically for measuring either the structure or the function (or the interactions of these) for biological elements  
* Bioconductor has its own repository and means of installing packages  
	* source("https://bioconductor.org/biocLite.R")  
    * biocLite("packageName")  
* Bioconductor is constantly in development  
	* library(packageName)  
    * BiocInstaller::biocVersion()   
    * sessionInfo()  
    * packageVersion("packageName")  
    * BiocInstaller::biocValid()  
  
Role of S4 in Bioconductor:  
  
* R uses S3 and S4; Bioconductor inherits from S4  
* The S3 system is simple and powerful, and is typically used for CRAN  
	* S3 often uses generic functions such as plot() and print() which behave differently depending on the object type  
* The S4 system implements OOP by defining objects that are generalized to classes  
	* S4 classes have a formal definition and inheritance, making type checking much easier  
    * mydescriptor <- new("GenomeDescription")  
    * The S4 class is more complex than S3; higher setup costs but easier to share and reuse  
* Can check whether an object is from an S4 class  
	* isS4(mydescriptor)  
    * str(mydescriptor)  # will show a formal class with slots  
* The S4 class has a name, slots (methods/fields), and inheritance (often from "contains")  
* There are S4 accessors for getting the slots (methods/fields)  
	* .S4methods(class = "GenomeDescription")  
    * showMethods(classes = "GenomeDescription", where = search())  
    * show(myDescriptor)  # sort of like str() but for objects  
  
Biology of Genomic Datasets:  
  
* Organisms are complex and interconnected and can be unicellular or multicellular  
* All organisms have a genome (complete genetic material, stored mostly in the chromosomes) - the "blueprint"  
	* TAGC are the building blocks  
* The genome can be thought of as a genetic DNA alphabet  
	* Genes contain heredity instructions  
    * Coding genes are expressed through proteins - DNA to RNA (transcription) and RNA to protein (translation)  
    * Non-coding genes are not expressed  
* Yeast is a single-cell organism that is frequently used in food and beverage creation  
	* library(BSgenome.Scerevisiae.UCSC.sacCer3)  
    * yeast <- BSgenome.Scerevisiae.UCSC.sacCer3  
    * available.genomes()  
    * length(yeast)  
    * names(yeast)  
    * seqlengths(yeast)  
    * getSeq(yeast)  
    * getSeq(yeast, "chrM")  # get chromosomes  
    * getSeq(yeast, end = 10)  # first 10 base pairs of each chromosome  
  
Example code includes:  
```{r eval=FALSE}

# Load the BiocInstaller package
library(BiocInstaller)

# Explicit syntax to check the Bioconductor version
BiocInstaller::biocVersion() 

# When BiocInstaller is loaded use biocVersion alone
biocVersion()


# Load the BSgenome package
library(BSgenome)

# Check the version of the BSgenome package
packageVersion("BSgenome")

# Investigate about the a_genome using show()
# show(a_genome)

# Investigate some other accesors
# organism(a_genome)
# provider(a_genome)
# seqinfo(a_genome)


# Load the yeast genome
library(BSgenome.Scerevisiae.UCSC.sacCer3)

# Assign data to the yeastGenome object
yeastGenome <- BSgenome.Scerevisiae.UCSC.sacCer3

# Get the head of seqnames and tail of seqlengths for yeastGenome
head(seqnames(yeastGenome))
tail(seqlengths(yeastGenome))

# Select chromosome M, alias chrM
yeastGenome$chrM

# Count characters of the chrM sequence
nchar(yeastGenome$chrM)


# Assign data to the yeastGenome object
yeastGenome <- BSgenome.Scerevisiae.UCSC.sacCer3

# Get the first 30 bases of each chromosome
getSeq(yeastGenome, start=1, end=30)

```
  
  
  
***
  
Chapter 2 - Biostrings and When to Use Them  
  
Introduction to Biostrings:  
  
* Biostrings has method for quickly processing biological strings  
	* Memory efficient  
    * Conatiners that can be inherited  
    * showClass("XString")  
    * showClass("BString")  
    * showClass("BStringSet")  
* There are bases for DNA and RNA available for use  
	* DNA_BASES # DNA 4 bases  
    * RNA_BASES # RNA 4 bases  
    * AA_STANDARD # 20 Amino acids  
    * DNA_ALPHABET # contains IUPAC_CODE_MAP  
    * RNA_ALPHABET # contains IUPAC_CODE_MAP  
    * AA_ALPHABET # contains AMINO_ACID_CODE  
* The general process for gene expression includes  
	* Double-strand DNA splits, and is RNA transcribed (T becomes A, A becomes U, C -> G, G -> C)  
    * Each three RNA translate to an amino acid  
    * dna_seq <- DNAString("ATGATCTCGTAA")  
    * rna_seq <- RNAString(dna_seq)  
    * rna_seq  # will give seq: AUGAUCUCGUAA  
    * aa_seq <- translate(rna_seq) # Translation RNA to AA  
    * aa_seq  # seq: MIS*  
    * translate(dna_seq)  # translate() also goes directly from DNA to AA (shortcut to the RNA and translate process)  
  
Sequence handling:  
  
* Can use XString to store a single sequence  
* Can use XStrinSet to store multiple sequences, each of varying lengths  
	* zikaVirus <- readDNAStringSet("data/zika.fa")  
    * zikaVirus_seq <- unlist(zikaVirus)  # to collate the sequence use unlist  
    * zikaSet <- DNAStringSet(zikaVirus_seq, start = c(1, 101, 201), end = c(100, 200, 300))  # to create a new set from a single sequence  
    * complement(a_seq)  # the complementary sequence  
* Can use rev to reverse a sequence  
	* rev(zikaShortSet)  # the last list will become first  
    * reverse(zikaShortSet)  # reverse from right to left for each of the sequences in the set  
    * reverseComplement(rna_seq)  # same as reverseComplement(rna_seq) but more memory efficient  
  
Why we are interested in patterns:  
  
* Can learn more about patterns using sequencing - frequency, occurences, etc.  
* Can use Biostring string matching functions  
	* matchPattern(pattern, subject)  # 1 string to 1 string  
    * vmatchPattern(pattern, subject)  # 1 set of strings to 1 string OR 1 string to a set of strings  
* Palindromes can be important in biology - binding sites  
	* findPalindromes() # find palindromic regions in a single sequence  
* There are six possibilities with translation based on the start of the sequence - reverseComplements and amino acids (based on 3 bases) depending on where the window starts  
	* [1] 30 ACATGGGCCTACCATGGGAGCTACGAAGCC  # original sequence  
    * # 6 possible reading frames, DNAStringSet  
    * [1]    30 ACATGGGCCTACCATGGGAGCTACGAAGCC             + 1  
    * [2]    30 GGCTTCGTAGCTCCCATGGTAGGCCCATGT             - 1  
    * [3]    29  CATGGGCCTACCATGGGAGCTACGAAGCC             + 2  
    * [4]    29  GCTTCGTAGCTCCCATGGTAGGCCCATGT             - 2  
    * [5]    28   ATGGGCCTACCATGGGAGCTACGAAGCC             + 3  
    * [6]    28   CTTCGTAGCTCCCATGGTAGGCCCATGT             - 3  
  
Example code includes:  
```{r eval=FALSE}

# Load packages
library(Biostrings)

# Check the alphabet of the zikaVirus
alphabet(zikaVirus)

# Check the alphabetFrequency of the zikaVirus
alphabetFrequency(zikaVirus)

# Check alphabet of the zikaVirus using baseOnly = TRUE
alphabet(zikaVirus, baseOnly = TRUE)


# Unlist the set and select the first 21 letters as dna_seq, then print it
dna_seq <- DNAString(subseq(as.character(zikaVirus), end = 21))
dna_seq

# 1.1 Transcribe dna_seq as rna_seq, then print it
rna_seq <- RNAString(dna_seq) 
rna_seq

# 1.2 Translate rna_seq as aa_seq, then print it
aa_seq <- translate(rna_seq)
aa_seq

# 2.1 Translate dna_seq as aa_seq_2, then print it
aa_seq_2 <- translate(dna_seq)
aa_seq_2


# Create zikv with one collated sequence using `zikaVirus`
zikv <- unlist(zikaVirus)

# Check the length of zikaVirus and zikv
length(zikaVirus)
length(zikv)

# Check the width of zikaVirus
width(zikaVirus)

# Subset zikv to only the first 30 bases
subZikv <- subseq(zikv, end = 30)
subZikv


# The reverse of zikv is
reverse(zikv)

# The complement of zikv is
complement(zikv)

# The reverse complement of zikv is
reverseComplement(zikv)

# The translation of zikv is
translate(zikv)


# Find palindromes in zikv
findPalindromes(zikv)


# print the rnaframesZikaSet 
rnaframesZikaSet

# translate all 6 reading frames 
AAzika6F <- translate(rnaframesZikaSet)
AAzika6F

# Count the matches allowing 15 mistmatches
vcountPattern(pattern = ns5, subject = AAzika6F, max.mismatch = 15)

# Select the frame that contains the match
selectedSet <- AAzika6F[3]

#Convert this frame into a single sequence
selectedSeq <- unlist(selectedSet)


# Use vmatchPattern with the set
vmatchPattern(pattern = ns5, subject = selectedSet, max.mismatch = 15)

# Use matchPattern with the single sequence
matchPattern(pattern = ns5, subject = selectedSeq, max.mismatch = 15)

```
  
  
  
***
  
Chapter 3 - IRanges and GenomicRanges  
  
IRanges and Genomic Structures:  
  
* Can sequence millions of genes for cheap, so there is need for analyzing large sequence data  
* Sequence ranges are a core component of the analysis  
	* library(IRanges)  
    * myIRanges <- IRanges(start = 20, end = 30)  
    * (myIRanges_width <- IRanges(start = c(1, 20), width = c(30, 11)))  
    * (myIRanges_end <- IRanges(start = c(1, 20), end = 30))  
    * Note that width = end - start + 1  
* Can also use RLE - run length encoding  
	* General S4 containers for saving large and repetitive vectors  
    * (some_numbers <- c(3, 2, 2, 2, 3, 3, 4, 2))  
    * (Rle(some_numbers))  # numeric-Rle of length 8 with 5 runs  
* Can also create using logical vectors for keep or skip  
	* IRanges(start = c(FALSE, FALSE, TRUE, TRUE))  # will pull items 3 and 4 for a range of width 2  
    * gi <- c(TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE)  
    * myRle <- Rle(logi)  
* The Irange is hierarchical and can hold metadata  
  
Gene of Interest:  
  
* Genomic sequences can be split over numerous chromosomes; may want to extract with sequential ranges  
	* library(GenomicRanges)  
    * (myGR <- GRanges("chr1:200-300"))  # name:start-end as a character (each range is associated to a chromosome)  
    * methods(class = "GRanges") # to check available accessors  
    * seqnames(gr) # used for chromosome names   
    * ranges(gr) # returns an IRanges object for ranges  
    * mcols(gr) # stores metadata columns  
    * seqinfo(gr) # generic function to store sequence information  
    * genome(gr) # stores the genome name  
* Accessors can be inherited across S4 classes  
* One of the genes of interest is ABCD1 (end of the X chromosome long arm)  
	* library(TxDb.Hsapiens.UCSC.hg38.knownGene)  
    * hg <- TxDb.Hsapiens.UCSC.hg38.knownGene  
    * hg_chrXg <- genes(hg, filter = list(tx_chrom = c("chrX")))  
  
Manipulating collections of GRanges:  
  
* The GRangesList-class is a container for storing a collection of GRanges  
	* as(mylist, "GRangesList")  
    * GRangesList(myGranges1, myGRanges2, ...)  
    * unlist(myGRangesList)  # convert back to Granges  
    * methods(class = "GRangesList")  
* Multiple GRanges objects may be combined into a GRangesList  
	* GRanges in a list will be taken as compound features of a larger object  
    * transcripts by gene, exons by transcripts, read alignments, sliding windows  
* Can break a region in to smaller regions  
	* hg_chrX  
    * slidingWindows(hg_chrX, width = 20000, step = 10000)  # there is overlap of 10000 and the last range will (typically) be shorter  
* Can grab known genomic features  
	* library(TxDb.Hsapiens.UCSC.hg38.knownGene)  
    * (hg <- TxDb.Hsapiens.UCSC.hg38.knownGene)  
* Can then extract gebomic features  
	* seqlevels(hg) <- c("chrX")  
    * transcripts(hg, columns = c("tx_id", "tx_name"), filter = NULL)  
    * exons(hg, columns = c("tx_id", "exon_id"), filter = list(tx_id = "179161"))  
    * exonsBytx <- exonsBy(hg, by = "tx") # exons by transcript  
    * abcd1_179161 <- exonsBytx[["179161"]] # transcript id  
* Can also find genes of interest in the overlaps  
	* countOverlaps(query, subject)  
    * findOverlaps(query, subject)  
    * subsetByOverlaps(query, subject)  
  
Example code includes:  
```{r eval=FALSE}

# load package IRanges
library(IRanges)

# start vector 1 to 5 and end 100 
IRnum1 <- IRanges(start=1:5, end=100)

# end 100 and width 89 and 10
IRnum2 <- IRanges(end=100, width=c(89, 10))

# logical argument start = Rle(c(F, T, T, T, F, T, T, T))
IRlog1 <- IRanges(start = Rle(c(FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE)))

# Printing objects in a list
print(list(IRnum1 = IRnum1, IRnum2 = IRnum2, IRlog1 = IRlog1))


# Load Package Genomic Ranges
library(GenomicRanges)

# Print the GRanges object
myGR

# Check the metadata, if any
mcols(myGR)


# load human reference genome hg38
library(TxDb.Hsapiens.UCSC.hg38.knownGene)

# assign hg38 to hg, then print it
hg <- TxDb.Hsapiens.UCSC.hg38.knownGene
hg

# extract all the genes in chromosome X as hg_chrXg, then print it
hg_chrXg <- genes(hg, filter = list(tx_chrom = c("chrX")))
hg_chrXg

# extract all positive stranded genes in chromosome X as hg_chrXgp, then sort it
hg_chrXgp <- genes(hg, filter = list(tx_chrom = c("chrX"), tx_strand = "+"))
sort(hg_chrXgp)


# load the human transcripts DB to hg
library(TxDb.Hsapiens.UCSC.hg38.knownGene)
hg <- TxDb.Hsapiens.UCSC.hg38.knownGene

# prefilter chromosome X
seqlevels(hg) <- c("chrX")

# get all transcripts by gene
hg_chrXt <- transcriptsBy(hg, by="gene")

# select gene `215` from the transcripts
hg_chrXt[[215]]


# load the human transcripts DB to hg
library(TxDb.Hsapiens.UCSC.hg38.knownGene)
hg <- TxDb.Hsapiens.UCSC.hg38.knownGene

# prefilter chromosome X
seqlevels(hg) <- c("chrX")

# get all transcripts by gene
hg_chrXt <- transcriptsBy(hg, by="gene")

# select gene `215` from the transcripts
hg_chrXt[['215']]


# Store the overlapping range in rangefound
rangefound <- subsetByOverlaps(hg_chrX, ABCD1)

# Check names of rangefound
names(rangefound)

# Check the geneOfInterest 
ABCD1

# Check rangefound
rangefound

```
  
  
  
***
  
Chapter 4 - Introducing ShortRead  
  
Sequence Files:  
  
* Plant genomes tend to be large datasets; one of the plants has 135 million base pairs  
* Can store with text formats fastq (quality encoding per sequence letter) and fasta  
* The fastq format includes  
	* @ unique sequence identifier  
    * raw sequence string  
    * + optional id  
    * quality encoding per sequence letter  
* The fasta (fasta, fa, seq) format is shorter and includes  
	* > unique sequence identifier  
    * raw sequence string  
* Can read in the fasta files using  
	* library(ShortRead)  
    * fasample <- readFasta(dirPath = "data/", pattern = "fasta") # print fasample  
    * methods(class = "ShortRead")  
    * writeFasta(fasample, file = "data/sample.fasta")  
* Can read in the fastq files using  
	* fqsample <- readFastq(dirPath = "data/", pattern = "fastq")  
    * methods(class = "ShortReadQ")  
    * writeFastq(fqsample, file = "data/sample.fastq.gz")  
* Sometimes valuable to set the seed and run a FastqSampler()  
	* set.seed(123)  
    * sampler <- FastqSampler("data/SRR1971253.fastq", 500)  
    * sample_small <- yield(sampler)  
  
Sequence Quality:  
  
* Quality scores are lograithmic somewhat like earthquake magnitudes  
	* 10 = 1 in 10 wrong  
    * 20 = 1 in 100 wrong  
    * 30 = 1 in 1,000 wrong (often considered to be the cutoff for "good")  
    * 50 = 1 in 100,000 wrong  
    * etc.  
    * encoding(ShortRead::quality(fqsample))  
    * ShortRead::quality(fqsample)  
    * sread(fqsample)[1]  
    * quality(fqsample)[1]  # quality encoding values  
    * pq <- PhredQuality(quality(fqsample))  ## PhredQuality instance  
    * qs <- as(pq, "IntegerList")  # transform encoding into scores  
* Can run a quality assessment process on files that have been read in  
	* qaSummary <- qa(fqsample, lane = 1) # optional lane  
    * names(qaSummary)  
    * browseURL(report(qaSummary))  # HTML report  
    * alphabet(sread(fullSample))  
    * abc <- alphabetByCycle(sread(fullSample))  
    * nucByCycle <- t(abc[1:4,])  
    * nucByCycle <- nucByCycle %>%   
    *     as.tibble() %>% # convert to tibble  
    *     mutate(cycle = 1:50) # add cycle numbers  
  
Match and Filter:  
  
* Can run the match and filter process while reading in a large file  
* Duplicate sequences can result from a biological process, PCR amplification, sequencing more than onbce, etc.  
	* Duplicates should generally be marked, and an acceptable threshhold set  
    * table(srduplicated(dfqsample))  
    * cleanReads <- mydReads[srduplicated(mydReads) == FALSE]  
    * table(srduplicated(cleanReads))  
* Can create your own filters using srFilter  
	* readWidthCutOff <- srFilter(function(x) {width(x) >= minWidth}, name = "MinWidth")  
    * minWidth <- 51  
    * fqsample[readWidthCutOff(fqsample)]  
    * myFilter <- nFilter(threshold = 10, .name = "cleanNFilter")  
    * filtered <- readFastq(dirPath = "data", pattern = ".fastq", filter = myFilter)  
* Can also use idFilter and polynFilter  
	* myFilterID <- idFilter(regex = ":3:1")  
    * filtered <- readFastq(dirPath = "data", pattern = ".fastq", filter = myFilterID)  
    * myFilterPolyA <- polynFilter(threshold = 10, nuc = c("A"))  
    * filtered[myFilterPolyA(filtered)]  
  
Multiple Assessment:  
  
* Desire to save time and resources when reading in high volume data - parallel processing  
	* library(Rqc)  # uses many of the basic Bioconductor packages as well as some of the basic CRAN packages  
    * qaRqc <- rqcQA(files)  # pass a file list as the files object  
    * class(qaRqc) # "list"  
    * names(qaRqc) # name of the input files  
    * qaRqc[1]  
    * qaRqc <- rqcQA(files, workers = 4))  # run it in parallel, saving only the quality assessment  
* Can also save a sample of the parallel reads, using a seed for reproducibility  
	* set.seed(1111)  
    * qaRqc_sample <- rqcQA(files, workers = 4, sample = TRUE, n = 500))  
* Can also build paired files using the pair= argument  
	* pfiles <- "data/seq_11.fq" "data/seq1_2.fq" "data/seq2_1.fq" "data/seq2_2.fq"  
    * qaRqc_paired <- rqcQA(pfiles, workers = 4, pair = c(1, 1, 2, 2)))  
    * reportFile <- rqcReport(qaRqc, templateFile = "myReport.Rmd")  
    * browseURL(reportFile)  
    * qaRqc <- rqcQA(files, workers = 4))  
    * perFileInformation(qaRqc)  
  
Introduction to Bioconductor:  
  
* Installing packages from Bioconductor  
* Basic techniques for reading, manipulating, filtering, raw genomic data  
* BSGenome and TxDb built-in datasets  
* Check the quality of sequence files using ShortRead and Rqc  
* Explored variety of organisms  
  
Example code includes:  
```{r eval=FALSE}

# load ShortRead
library(ShortRead)

# print fqsample
fqsample

# class of fqsample
class(fqsample)

# class sread fqsample
class(sread(fqsample))

# id fqsample
id(fqsample)


qaSummary <- qa(fqsample, type = "fastq", lane = 1)

# load ShortRead
library(ShortRead)

# Check quality
quality(fqsample)

# Check encoding
encoding(quality(fqsample))

# Check baseQuality
qaSummary[["baseQuality"]]


# glimpse nucByCycle
glimpse(nucByCycle)

# make an awesome plot!
nucByCycle %>% 
  # gather the nucleotide letters in alphabet and get a new count column
  gather(key = alphabet, value = count , -cycle) %>% 
  ggplot(aes(x = cycle, y =  count, colour = alphabet)) +
  geom_line(size = 0.5 ) +
  labs(y = "Frequency") +
  theme_bw() +
  theme(panel.grid.major.x = element_blank())


myStartFilter <- srFilter(function(x) substr(sread(x), 1, 5) == "ATGCA")

# Load package ShortRead
library(ShortRead)

# Check class of fqsample
class(fqsample)

# filter reads into selectedReads using myStartFilter
selectedReads <- fqsample[myStartFilter(fqsample)]

# Check class of selectedReads
class(selectedReads)

# Check detail of selectedReads
detail(selectedReads)


# Load package Rqc
library(Rqc)

# Average per cycle quality plot
rqcCycleAverageQualityPlot(qa)

# Average per cycle quality plot with white background
rqcCycleAverageQualityPlot(qa) + theme_minimal()

# Read quality plot with white background
rqcReadQualityPlot(qa) + theme_minimal()

```
  
  
  
***
  
###_Non-Linear Modeling in R with GAM_  
  
Chapter 1 - Introduction to Generalized Additive Models  
  
Introduction:  
  
* There are trade-offs between model power (e.g., ML) and parsimony (e.g., linear regression), with GAM being a middle-ground solution  
* GAM allows for flexibly modeling non-linear relationships  
	* linear_mod <- lm(y ~ x, data = my_data)  
    * library(mgcv)  
    * gam_mod <- gam(y ~ s(x), data = my_data)  # s() is the smoothing function  
* The flexible smooth is built up from simpler basis functions  
	* The overall smooth is the sum of the simpler basis functions  
    * coef(gam_mod)  
  
Basis functions and smoothing:  
  
* Because the GAM often has many basis coefficients, there is a meaningful risk of over-fitting  
	* Fit = Likelihood - lambda * Wiggliness (lambda is the smoothing parameter, and is optimized while R fits the data to the GAM)  
* Can fit the smoothing parameter using arguments in the gam() function call  
	* gam(y ~ s(x), data = dat, sp = 0.1)  # global sp argument  
    * gam(y ~ s(x, sp = 0.1), data = dat)  # sp argument specific to a term  
    * gam(y ~ s(x), data = dat, method = "REML")  # select smoothing using Restricted Maximum Likelihood  
* The number of basis functions also drives both fit to the the training data and risk of over-fitting  
	* gam(y ~ s(x, k = 3), data = dat, method = "REML")  
    * gam(y ~ s(x, k = 10), data = dat, method = "REML")  # even if the k is "too high", the REML will help prevent overfits  
    * gam(y ~ s(x), data = dat, method = "REML")  # defaults  
  
Multivariate GAMs:  
  
* Can run the GAM using multiple independent variables - smooths, categoricals, etc.  
* Can start with a simple model  
	* model <- gam(hw.mpg ~ s(weight), data = mpg, method = "REML")  
* Can then extend the simple model to include additional predictors  
	* model2 <- gam(hw.mpg ~ s(weight) + s(length), data = mpg, method = "REML")  
    * model2 <- gam(hw.mpg ~ s(weight) + length, data = mpg, method = "REML")  # combined linear and non-linear terms since length is not enclosed in an s()  
    * model2b <- gam(hw.mpg ~ s(weight) + s(length, sp = 1000), data = mpg, method = "REML")  # will be linear since there is strong smoothing due to sp=1000  
* Linear terms are especially valuable with categorical predictors  
	* model3 <- gam(hw.mpg ~ s(weight) + fuel, data = mpg, method = "REML")  # fuel needs to be a factor; mgcv will not handle characters  
    * model4 <- gam(hw.mpg ~ s(weight, by = fuel), data = mpg, method = "REML")  # smooths will be by categories of fuel  
    * model4b <- gam(hw.mpg ~ s(weight, by = fuel) + fuel, data = mpg, method = "REML")  # add intercept as well as smooth by vategory of fuel  
  
Example code includes:  
```{r}

data(mcycle, package="MASS")

# Examine the mcycle data frame
head(mcycle)
plot(mcycle)


# Fit a linear model
lm_mod <- lm(accel ~ times, data = mcycle)

# Visualize the model
termplot(lm_mod, partial.resid = TRUE, se = TRUE)


# Load mgcv
library(mgcv)

# Fit the model
gam_mod <- gam(accel ~ s(times), data = mcycle)

# Plot the results
plot(gam_mod, residuals = TRUE, pch = 1)

# Extract the model coefficients
coef(gam_mod)


# Fit a GAM with 3 basis functions
gam_mod_k3 <- gam(accel ~ s(times, k = 3), data = mcycle)

# Fit with 20 basis functions
gam_mod_k20 <- gam(accel ~ s(times, k = 20), data = mcycle)

# Visualize the GAMs
par(mfrow = c(1, 2))
plot(gam_mod_k3, residuals = TRUE, pch = 1)
plot(gam_mod_k20, residuals = TRUE, pch = 1)
par(mfrow = c(1, 1))


# Extract the smoothing parameter
gam_mod <- gam(accel ~ s(times), data = mcycle, method = "REML")
gam_mod$sp

# Fix the smoothing paramter at 0.1
gam_mod_s1 <- gam(accel ~ s(times), data = mcycle, sp = 0.1)

# Fix the smoothing paramter at 0.0001
gam_mod_s2 <- gam(accel ~ s(times), data = mcycle, sp = 0.0001)

# Plot both models
par(mfrow = c(2, 1))
plot(gam_mod_s1, residuals = TRUE, pch = 1)
plot(gam_mod_s2, residuals = TRUE, pch = 1)
par(mfrow = c(1, 1))


# Fit the GAM
gam_mod_sk <- gam(accel ~ s(times, k=50), sp=0.0001, data=mcycle)

#Visualize the model
plot(gam_mod_sk, residuals = TRUE, pch = 1)


data(mpg, package="gamair")

# Examine the data
head(mpg)
str(mpg)


# Fit the model
mod_city <- gam(city.mpg ~ s(weight) + s(length) + s(price), data = mpg, method = "REML")

# Plot the model
plot(mod_city, pages = 1)


# Fit the model
mod_city2 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + fuel + drive + style, data = mpg, method = "REML")

# Plot the model
plot(mod_city2, all.terms = TRUE, pages = 1)


# Fit the model
mod_city3 <- gam(city.mpg ~ s(weight, by=drive) + s(length, by=drive) + s(price, by=drive) + drive, 
                 data = mpg, method = "REML"
                 )

# Plot the model
plot(mod_city3, pages = 1)

```
  
  
  
***
  
Chapter 2 - Interpreting and Visualizing GAMs  
  
Interpreting GAM Outputs:  
  
* Can get summaries of model output by way of the summary() function  
	* mod_hwy <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + s(comp.ratio) + s(width) + fuel + cylinders, data = mpg, method = "REML")  
    * summary(mod_hwy)  
    * The first component of the summary shows the model family, link, and formula  
    * The second component shows the parametric coefficients and significances  
    * The third components shows the approximate significance of the smooths  # edf of 1 is equivalent to a straight line, edf of 2 is equivalent to a parabola, etc.  
    * Generally, a significant smooth can be thought of as one where a straight, horizontal line cannot be drawn through the 95% confidence interval  
  
Visualizing GAMs:  
  
* Visualizations are a powerful way to inspect and communicate results  
	* ?plot.gam  
* The mgcv plots are "partial effects" plots - the components that add up to the overall model  
	* plot(gam_model, select = c(2, 3))  # default is that all smoothed terms are selected, can override with select  
    * plot(gam_model, pages = 1)  # default is as many pages as needed  
    * plot(gam_model, pages = 1, all.terms = TRUE)  # The all.terms will show the non-smoothed terms also  
    * plot(gam_model, residuals = TRUE)  # show the partial residuals  
    * plot(gam_model, rug = TRUE)  # show the rug on the x-axis  
    * plot(gam_model, rug = TRUE, residuals = TRUE, pch = 1, cex = 1)  # pch for shape and cex for size  
    * plot(gam_model, shade = TRUE)  # 95% CI is shaded rather than shown in dotted lines  
    * plot(gam_model, shade = TRUE, shade.col = "lightblue")  # color of shading  
    * plot(gam_model, seWithMean = TRUE)  
    * plot(gam_model, seWithMean = TRUE, shift = coef(gam_model)[1])  # brings in the intercept, which is the first coefficient of the model  
  
Model checking with gam.check():  
  
* There are many potential pitfalls to be checked  
	* Inadequate basis number  
* There is an automated call to look at the model results  
	* gam.check(mod)  
    * Convergence - if it has not converged, it is likely wrong  
    * The p-values for the residuals - should not be significant, though this is only an approximate test  
    * Standard regression residuals plots  
  
Checking concurvity:  
  
* There can be collinearity concerns with a linear model, which can result in poor models with large confidence intervals  
* The GAM can have a concurvity concern, where one variable is a smooth of another variable (such as x and x**2)  
	* concurvity(m1, full = TRUE)  
    * Generally, a concurvity worst case of 0.8+ is an area for concern  
    * concurvity(m1, full = FALSE)  # get the pairwise concurvities  
  
Example code includes:  
```{r}

# Fit the model
mod_city4 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + s(rpm) + s(width),
                 data = mpg, method = "REML")

# View the summary
summary(mod_city4)


# Fit the model
mod <- gam(accel ~ s(times), data = mcycle, method = "REML")

# Make the plot with residuals
plot(mod, residuals=TRUE)

# Change shape of residuals
plot(mod, residuals=TRUE, pch=1, cex=1)


# Fit the model
mod <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + comp.ratio, 
           data = mpg, method = "REML")

# Plot the price effect
plot(mod, select=c(3))

# Plot all effects
plot(mod, all.terms=TRUE, pages=1)

# Plot the weight effect with colored shading
plot(mod, select = 1, shade=TRUE, shade.col="hotpink")

# Add the intercept value and uncertainty
plot(mod, select = 1, shade=TRUE, shade.col="hotpink", seWithMean=TRUE, shift=coef(mod)[1])


dat <- data.frame(y=c(11.17, 2.81, 12.9, 5.68, 5.58, -1.09, 5.42, 12.13, 4.73, 6.29, 5.74, 8.32, 9.76, 4.78, 9.08, 10.5, 9.4, 9.51, 14.58, 13.84, 4.01, 3.31, 5.32, 6.6, 10.54, 13.19, 10.06, 8.6, -0.62, 4.78, 5.98, 2.75, 1.36, 8.51, 8.12, 4.18, 10.65, 5.92, -0.03, 6.48, 9.12, 6.57, 15.38, 11.76, 7.47, 12, 3.4, 3.39, 0.95, 5.49, 7.92, 8.04, 8.81, 6.65, 8.93, 0.55, 6.73, 3.38, 4.42, 8.23, 12.2, 14.45, 2.82, 5.58, 8.74, 14.14, 5.74, 4.59, 14.54, 6.65, 4.21, 8.71, 1.76, 6.22, 8.87, 10.3, 9.18, 5.05, 5.44, 4.86, 3.25, 4.59, 12.01, 6.69, 6.3, 6.85, 5.45, 15.43, -0.9, 3.43, 9.83, 1.04, 1.16, 16.7, 9.16, 8.46, 7.81, 4.97, 7.46, 1.49, 8.01, 9.48, 9.43, 3.92, 6.2, 7.63, 8.56, 11.53, 9.98, 2.49, 5.67, 3.48, 7.92, 8.62, 7.44, 6.35, 10.88, 9.74, 3.79, 15.43, 6.56, 2.5, 6.66, 9.75, 12.72, 14.64, 8.9, 10.74, 5.93, 2.53, 3.69, 15.25, 0.5, 11.8, 13.19, 6.05, -1.26, 9.09, 9.78, 7.23, 11.67, 12.54, -0.36, 9.4, 7.87, 13.46, 9.33, 2.55, 9.23, 5.95, 10.46, 3.39, 3.81, 7.25, 3.94, 10.18, 8.63, 11.51, 2.42, 9.44, 5.95, 7.75, 10.16, 16.11, 5.16, 3.13, 7.75, 9.96, 7.27, 14.62, 3.88, 10.2, 5.86, 16.18, 5.4, 1.55, 2.91, 9.16, 9.77, 2.25, 5.01, 8.79, 3.34, 7.09, 8.18, 3.34, 8.02, 8.12, 6.69, 3.22, 8.15, 5.01, 11.51, 6.62, 7.07, 0.52, 10.26, 7.99, 8.98, 9.87), 
                  x0=c(0.9, 0.27, 0.37, 0.57, 0.91, 0.2, 0.9, 0.94, 0.66, 0.63, 0.06, 0.21, 0.18, 0.69, 0.38, 0.77, 0.5, 0.72, 0.99, 0.38, 0.78, 0.93, 0.21, 0.65, 0.13, 0.27, 0.39, 0.01, 0.38, 0.87, 0.34, 0.48, 0.6, 0.49, 0.19, 0.83, 0.67, 0.79, 0.11, 0.72, 0.41, 0.82, 0.65, 0.78, 0.55, 0.53, 0.79, 0.02, 0.48, 0.73, 0.69, 0.48, 0.86, 0.44, 0.24, 0.07, 0.1, 0.32, 0.52, 0.66, 0.41, 0.91, 0.29, 0.46, 0.33, 0.65, 0.26, 0.48, 0.77, 0.08, 0.88, 0.34, 0.84, 0.35, 0.33, 0.48, 0.89, 0.86, 0.39, 0.78, 0.96, 0.43, 0.71, 0.4, 0.33, 0.76, 0.2, 0.71, 0.12, 0.25, 0.14, 0.24, 0.06, 0.64, 0.88, 0.78, 0.8, 0.46, 0.41, 0.81, 0.6, 0.65, 0.35, 0.27, 0.99, 0.63, 0.21, 0.13, 0.48, 0.92, 0.6, 0.98, 0.73, 0.36, 0.43, 0.15, 0.01, 0.72, 0.1, 0.45, 0.64, 0.99, 0.5, 0.48, 0.17, 0.75, 0.45, 0.51, 0.21, 0.23, 0.6, 0.57, 0.08, 0.04, 0.64, 0.93, 0.6, 0.56, 0.53, 0.99, 0.51, 0.68, 0.6, 0.24, 0.26, 0.73, 0.45, 0.18, 0.75, 0.1, 0.86, 0.61, 0.56, 0.33, 0.45, 0.5, 0.18, 0.53, 0.08, 0.28, 0.21, 0.28, 0.9, 0.45, 0.78, 0.88, 0.41, 0.06, 0.34, 0.72, 0.34, 0.63, 0.84, 0.86, 0.39, 0.38, 0.9, 0.64, 0.74, 0.61, 0.9, 0.29, 0.19, 0.89, 0.5, 0.88, 0.19, 0.76, 0.72, 0.94, 0.55, 0.71, 0.39, 0.1, 0.93, 0.28, 0.59, 0.11, 0.84, 0.32),
                  x1=c(0.78, 0.27, 0.22, 0.52, 0.27, 0.18, 0.52, 0.56, 0.13, 0.26, 0.72, 0.96, 0.1, 0.76, 0.95, 0.82, 0.31, 0.65, 0.95, 0.95, 0.34, 0.26, 0.17, 0.32, 0.51, 0.92, 0.51, 0.31, 0.05, 0.42, 0.85, 0.35, 0.13, 0.37, 0.63, 0.39, 0.69, 0.69, 0.55, 0.43, 0.45, 0.31, 0.58, 0.91, 0.14, 0.42, 0.21, 0.43, 0.13, 0.46, 0.94, 0.76, 0.93, 0.47, 0.6, 0.48, 0.11, 0.25, 0.5, 0.37, 0.93, 0.52, 0.32, 0.28, 0.79, 0.7, 0.17, 0.06, 0.75, 0.62, 0.17, 0.06, 0.11, 0.38, 0.17, 0.3, 0.19, 0.26, 0.18, 0.48, 0.77, 0.03, 0.53, 0.88, 0.37, 0.05, 0.14, 0.32, 0.15, 0.13, 0.22, 0.23, 0.13, 0.98, 0.33, 0.51, 0.68, 0.1, 0.12, 0.05, 0.93, 0.67, 0.09, 0.49, 0.46, 0.38, 0.99, 0.18, 0.81, 0.07, 0.4, 0.14, 0.19, 0.84, 0.72, 0.27, 0.5, 0.08, 0.35, 0.97, 0.62, 0.66, 0.31, 0.41, 1, 0.86, 0.95, 0.81, 0.78, 0.27, 0.76, 0.99, 0.29, 0.4, 0.81, 0.08, 0.36, 0.44, 0.16, 0.58, 0.97, 0.99, 0.18, 0.54, 0.38, 0.68, 0.27, 0.47, 0.17, 0.37, 0.73, 0.49, 0.06, 0.78, 0.42, 0.98, 0.28, 0.85, 0.08, 0.89, 0.47, 0.11, 0.33, 0.84, 0.28, 0.59, 0.84, 0.07, 0.7, 0.7, 0.46, 0.44, 0.56, 0.93, 0.23, 0.22, 0.42, 0.33, 0.86, 0.18, 0.49, 0.43, 0.56, 0.66, 0.98, 0.23, 0.24, 0.8, 0.83, 0.11, 0.96, 0.15, 0.14, 0.93, 0.51, 0.15, 0.35, 0.66, 0.31, 0.35), 
                  x2=c(0.15, 0.66, 0.19, 0.95, 0.9, 0.94, 0.72, 0.37, 0.78, 0.01, 0.94, 0.99, 0.36, 0.75, 0.79, 0.71, 0.48, 0.49, 0.31, 0.7, 0.82, 0.43, 0.51, 0.66, 0.14, 0.34, 0.41, 0.09, 0.93, 0.84, 0.88, 0.94, 0.07, 0.38, 0.54, 0.11, 0.8, 0.74, 0.05, 0.48, 0.92, 0.04, 0.29, 0.5, 0.61, 0.26, 0.42, 0.37, 0.94, 0.12, 0.07, 0.96, 0.44, 0.37, 0.14, 0.05, 0.66, 0.58, 0.99, 0.6, 0.06, 0.16, 0.48, 0, 0.44, 0.26, 0.94, 0.72, 0.16, 0.48, 0.69, 0.46, 0.96, 0.71, 0.4, 0.12, 0.24, 0.86, 0.44, 0.5, 0.69, 0.76, 0.16, 0.85, 0.95, 0.59, 0.5, 0.19, 0, 0.88, 0.13, 0.02, 0.94, 0.29, 0.16, 0.4, 0.46, 0.43, 0.52, 0.85, 0.06, 0.55, 0.69, 0.66, 0.66, 0.47, 0.97, 0.4, 0.85, 0.76, 0.53, 0.87, 0.47, 0.01, 0.73, 0.72, 0.19, 0.65, 0.54, 0.34, 0.64, 0.83, 0.71, 0.35, 0.13, 0.39, 0.93, 0.8, 0.76, 0.96, 0.99, 0.61, 0.03, 0.34, 0.28, 0.12, 0.04, 0.37, 0.34, 0.17, 0.62, 0.4, 0.96, 0.65, 0.33, 0.2, 0.12, 1, 0.38, 0.56, 0.73, 0.87, 0.57, 0.01, 0.91, 0.77, 0.38, 0.09, 0.05, 0.82, 0.83, 0.65, 0.13, 0.34, 0.73, 0.91, 0.7, 0.24, 0.64, 0.28, 0.96, 0.16, 0.42, 0.25, 0.09, 0.83, 0.53, 0.67, 0.41, 0.84, 0.74, 0.35, 0.95, 0.65, 0.04, 0.6, 0.42, 0.08, 0.53, 0.96, 0.71, 0.55, 0.24, 0.78, 0.65, 0.83, 0.65, 0.48, 0.5, 0.38), 
                  x3=c(0.45, 0.81, 0.93, 0.15, 0.75, 0.98, 0.97, 0.35, 0.39, 0.95, 0.11, 0.93, 0.35, 0.53, 0.54, 0.71, 0.41, 0.15, 0.34, 0.63, 0.06, 0.85, 0.21, 0.77, 0.14, 0.32, 0.62, 0.26, 0.63, 0.49, 0.94, 0.86, 0.37, 0.31, 0.83, 0.45, 0.32, 0.1, 0.06, 0.69, 0.67, 0.9, 0.3, 0.93, 0.2, 0.79, 0.22, 0.03, 0.86, 0.69, 0.94, 0.68, 0.84, 0.36, 0.39, 0.57, 0.1, 0.19, 0.59, 0.75, 0.87, 0.37, 0.8, 0.06, 0.62, 0.36, 0.59, 0.91, 0.2, 0.37, 0.67, 0.77, 0.52, 0.83, 0.53, 0.5, 0.42, 0.36, 0.12, 0.3, 0.28, 0.79, 0.78, 0.14, 0.52, 0.6, 0.51, 0.39, 0.43, 0.01, 0.92, 0.08, 0.51, 0.82, 0.6, 0.42, 0.56, 0.79, 0.17, 0.97, 0.47, 0.93, 0.9, 0.75, 0.68, 0.65, 0.07, 0.42, 0.53, 0.94, 0.71, 0.72, 0.47, 0.12, 0.78, 0.44, 0.43, 0.03, 0.15, 0.42, 0.77, 0, 0.6, 0.91, 0.71, 0.26, 0.85, 0.33, 0.58, 0.43, 0.05, 0.73, 0.55, 0.75, 0.05, 0.71, 0.3, 0.28, 0.83, 0.09, 0.04, 0.35, 0.54, 0.61, 0.27, 0.21, 0.38, 0.47, 0.84, 0.12, 0.68, 0.5, 0.9, 0.55, 0.13, 0.44, 0.19, 0.43, 0.23, 0.96, 0.45, 0.78, 0.16, 0.87, 0.21, 0.18, 0.16, 0.57, 0.73, 0.88, 0.71, 0.48, 0.82, 0.02, 1, 0.63, 0.43, 0.03, 0.75, 0.21, 1, 0.91, 0.71, 0.73, 0.47, 0.86, 0.17, 0.62, 0.29, 0.46, 0.05, 0.18, 0.06, 0.94, 0.34, 0.52, 0.63, 0.24, 0.52, 0.81)
                  )
str(dat)


# Fit the model
mod <- gam(y ~ s(x0, k = 5) + s(x1, k = 5) + s(x2, k = 5) + s(x3, k = 5),
           data = dat, method = "REML")

# Run the check function
gam.check(mod)


# Fit the model
mod <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 3) + s(x3, k = 3),
           data = dat, method = "REML")

# Check the diagnostics
gam.check(mod)


# Refit to fix issues
mod2 <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 12) + s(x3, k = 3),
           data = dat, method = "REML")

# Check the new model
gam.check(mod2)


# Fit the model
mod <- gam(hw.mpg ~ s(length) + s(width) + s(height) + s(weight), 
           data = mpg, method = "REML")

# Check overall concurvity
concurvity(mod, full=TRUE)

# Check pairwise concurvity
concurvity(mod, full=FALSE)

```
  
  
  
***
  
Chapter 3 - Spatial GAMs and Interactions  
  
Two-dimensional smooths and spatial data:  
  
* Can expand models to include smooths of multiple variables, including their interactions  
	* y = s(x1, x2)  
    * gam(y ~ s(x1, x2), data = dat, method = "REML")  
    * gam(y ~ s(x1, x2) + s(x3) + x4, data = dat, method = "REML")  
* Will use the meuse dataset, which is about heavy metals in the soil near a river  
	* ?sp::meuse  
  
Plotting and interpreting GAM interactions:  
  
* Can see interactions using plot(mod_2d)  
	* plot(mod_2d)  
    * Creates a topographic map of predicted values  
    * plot(mod_2d, scheme = 1)  # will show the 3D perspective plot  
    * plot(mod_2d, scheme = 2)  # heat map  
    * vis.gam(x, view = NULL, cond = list(), n.grid = 30, too.far = 0, col = NA, color = "heat", contour.col = NULL, se = -1, type = "link", plot.type = "persp", zlim = NULL, nCol = 50, ...) # customizes plots  
* Can run custom plots using the vis.gam() functions  
	* vis.gam(x = mod, view = c("x1", "x2"), plot.type = "persp")  # scheme=1  
    * vis.gam(x = mod, view = c("x1", "x2"), plot.type = "contour")  # scheme=2  
    * vis.gam(mod, view = c("x1", "x2"), plot.type = "contour", too.far = 0.1)  # set a range for not making predictions due to distance from training data  
    * vis.gam(x = mod, view = c("x1", "x2"), plot.type = "persp", se = 2)  # see confidence intervals for the plots  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "persp", theta = 220)  # horizontal rotation  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "persp", phi = 55)  # vertical rotation  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "persp", r = 0.1)  # zoom level (low r can lead to distortions or parallax)  
* Additional options are available for contour plots  
	* vis.gam(g, view = c("x1", "x2"), plot.type = "contour", color = "gray")  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "contour", contour.col = "blue")  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "contour", nlevels = 20)  
  
Visualizing categorical-continuous interactions:  
  
* The categorical-continuous interaction was previously modeled using s(x1, by="x2")  
* Can instead use a factor smooth with argument bs="fs" (best for controlling for categories that may have an impact but are not the primary categories of interest)  
	* model4c <- gam(hw.mpg ~ s(weight, fuel, bs = "fs") + fuel, data = mpg, method = "REML")  
    * plot(model4c)  
    * vis.gam(model4c, theta = 125, plot.type = "persp")  
  
Interactions with different scales: Tensors:  
  
* Tensor smooths allow for interactions on different scales, such as space and time  
* Within the meuse data, horizontal and vertical dstances may have very different wiggliness (incomparable impacts)  
* A tensor has two smoothing parameters, one for each of it variables  
	* gam(y ~ te(x1, x2), data = data, method = "REML")  
    * gam(y ~ te(x1, x2, k = c(10, 20)), data = data, method = "REML")  
* Tensor smooths can also help to tease out interactions and independent effects  
	* gam(y ~ te(x1) + te(x2) + ti(x1, x2), data = data, method = "REML")  
    * gam(y ~ s(x1) + s(x2) + ti(x1, x2), data = data, method = "REML")  
  
Example code includes:  
```{r}

# Inspect the data
data(meuse, package="sp")
head(meuse)
str(meuse)


# Fit the 2-D model
mod2d <- gam(cadmium ~ s(x, y), data=meuse, method="REML")

# Inspect the model
summary(mod2d)
coef(mod2d)


# Models of this form (s(x,y) + s(v1) + ...) are a great way to model spatial data because they incorporate spatial relationships as well as independent predictors

# Fit the model
mod2da <- gam(cadmium ~ s(x, y) +s(elev) + s(dist), 
              data = meuse, method = "REML")

# Inspect the model
summary(mod2da)

# Contour plot
plot(mod2da, pages = 1)

# 3D surface plot
plot(mod2da, scheme=1, pages = 1)

# Colored heat map
plot(mod2da, scheme=2, pages=1)


# Make the perspective plot with error surfaces
vis.gam(mod2d, view = c("x", "y"), plot.type="persp", se=2)

# Rotate the same plot
vis.gam(mod2d, view = c("x", "y"), plot.type="persp", se=2, theta=135)

# Make plot with 5% extrapolation
vis.gam(mod2d, view = c("x", "y"), plot.type = "contour", too.far=0.05)

# Overlay data
points(meuse)


# Make plot with 10% extrapolation
vis.gam(mod2d, view = c("x", "y"), plot.type="contour", too.far=0.1)

# Overlay data
points(meuse)

# Make plot with 25% extrapolation
vis.gam(mod2d, view = c("x", "y"), 
        plot.type = "contour", too.far = 0.25)

# Overlay data
points(meuse)


# Fit a model with separate smooths for each land-use level
mod_sep <- gam(copper ~ s(dist, by = landuse), data = meuse, method = "REML")

# Examine the summary
summary(mod_sep)

# Fit a model with a factor-smooth interaction
mod_fs <- gam(copper ~ s(dist, landuse, bs="fs"), data = meuse, method = "REML")

# Examine the summary
summary(mod_fs)


# Plot both the models with plot()
plot(mod_sep, pages=1)
plot(mod_fs, pages=1)

# Plot both the models with vis.gam()
vis.gam(mod_sep, view = c("dist", "landuse"), plot.type = "persp")
vis.gam(mod_fs, view = c("dist", "landuse"), plot.type = "persp")


# Fit the model
tensor_mod <- gam(cadmium ~ te(x, y, elev), data=meuse, method="REML")

# Summarize and plot
summary(tensor_mod)
plot(tensor_mod)


# Fit the model
tensor_mod2 <- gam(cadmium ~ ti(x, y) + te(elev) + ti(x, y, elev), data=meuse, method="REML")

# Summarize and plot
summary(tensor_mod2)
plot(tensor_mod2, pages = 1)

par(mfrow=c(1, 1))

```
  
  
  
***
  
Chapter 4 - Logistic GAM for Classification  
  
Types of model outcome:  
  
* Can also use GAM for making classifications (binary regression)  
	* GAM output is converted to probability using the logistic function - log-odds - plogis()  
    * The logit function converts probabilities to log-odds - qlogis()  
    * gam(y ~ x1 + s(x2), data = dat, family = binomial, # Add for binary outcomes method = "REML")  
* The csale dataset has anonymized insurance data  
  
Visualizing logistic GAMs:  
  
* Can plot the log-odds  
	* plot(binom_mod)  # on the log-odds scale  
    * plot(binom_mod, pages = 1, trans = plogis)  # convert to probability scale  
    * plot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1])  # add the intercept  
    * plot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1], seWithMean = TRUE)  # adds intercept uncertainty to smooth uncertainty  
    * plot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgreen", col = "purple")  
  
Making predictions:  
  
* Can use the fitted models for making predictions  
	* predict(log_mod2)  # vector of predictions for the model data; returned on the link scale by default (log-odds for logistic)  
    * predict(log_mod2, type="response")  # return on the probability scale  
    * predict(log_mod2, type = "link", se.fit = TRUE)  # first element of the list is the predictions and second element is approx. SE  
* For standard errors, use the link (log-odds) scale, then convert to the probability scales  
* Can also make predictions based on new data  
	* test_predictions <- predict(trained_model, type = "response", newdata = test_df)  
    * predict(log_mod2, type = "terms")  # will show a column for the impact of each of the smooths (sum across columns plus intercept would be prediction)  
    * plogis( sum(predict(log_mod2, type = "terms")[1, ]) + coef(log_mod2)[1] )  
  
Wrap up and next steps:  
  
* Basic theory of smooths for GAMs  
* Interpreting GAMs and plotting partial effects  
* Building and visualizing GAMs with interactions  
* Logistic GAMs for binary classification and predictions  
* Can extend to using the Tidyverse tools - broom, caret, etc.  
* Can further extend the smooths - see the help files in mgcv  
  
Example code includes:  
```{r}

csale <- readRDS("./RInputFiles/csale.rds")

# Examine the csale data frame
head(csale)
str(csale)


# Fit a logistic model
log_mod <- gam(purchase ~ s(mortgage_age), data = csale, family=binomial, method = "REML")

# Fit a logistic model
log_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) + s(avg_prem_balance) + 
    s(retail_crdt_ratio) + s(avg_fin_balance) + s(mortgage_age) + 
    s(cred_limit), data = csale, family = binomial, method = "REML")

# View the summary
summary(log_mod2)


# Plot on the log-odds scale
plot(log_mod2, pages=1)

# Plot on the probability scale
plot(log_mod2, pages = 1, trans = plogis)

# Plot with the intercept
plot(log_mod2, pages = 1, trans = plogis, shift = coef(log_mod2)[1])

# Plot with intercept uncertainty
plot(log_mod2, pages = 1, trans = plogis, shift = coef(log_mod2)[1], seWithMean = TRUE)


new_credit_data <- data.frame(matrix(data=c(1, 0, 0, 0, 0, 0, 0, 2, 19, 0, 0, 1, 6, 3, 0.3, 4.2, 36.095, 36.095, 25.7, 45.6, 10.8, 61, 967, 2494.414, 2494.414, 2494.414, 195, 2494.414, 11.491, 0, 11.491, 11.491, 11.491, 0, 11.491, 1767.197, 249, 1767.197, 1767.197, 1767.197, 0, 1767.197, 155, 65, 138.96, 138.96, 138.96, 13, 138.96, 0, 10000, 0, 0, 0, 13800, 0), ncol=8, nrow=7, byrow=FALSE))
names(new_credit_data) <- c('purchase', 'n_acts', 'bal_crdt_ratio', 'avg_prem_balance', 'retail_crdt_ratio', 'avg_fin_balance', 'mortgage_age', 'cred_limit')
new_credit_data


# Calculate predictions and errors
predictions <- predict(log_mod2, newdata = new_credit_data, 
                       type = "link", se.fit = TRUE)

# Calculate high and low predictions intervals
high_pred <- predictions$fit + 2*predictions$se.fit
low_pred <- predictions$fit - 2*predictions$se.fit

# Convert intervals to probability scale
high_prob <- 1 / (1/exp(high_pred) + 1)
low_prob <- 1 / (1/exp(low_pred) + 1)

# Inspect
high_prob
low_prob


# Predict from the model
prediction_1 <- predict(log_mod2, newdata = new_credit_data[1, ,drop=FALSE], type = "terms")

# Inspect
prediction_1

```
  
  
  
***
  
###_Machine Learning in the Tidyverse_  
  
Chapter 1 - Foundations of Tidy Machine Learning  
  
Introduction:  
  
* Tidyverse tools center around the tibble, which includes a "list" column that can store complex objects  
* There is also a list-column workflow of nest() - map() - unnest()  
* Course will use a more granular form of the gapminder dataset - 77 countries x 52 years x 6 features per observation  
* The nest makes each portion of the data in to a row of a new tibble  
	* nested <- gapminder %>% group_by(country) %>% nest()  # new column data is created; will contain the relevant data  
    * nested$data[[4]]  # will show the Austria data  
    * nested %>% unnest(data)  # will convert back to a normal tibble, using column data  
  
Map family of functions:  
  
* The map family of functions help to fulfill the second and third elements of the workflows  
* The map functions are all of the form map_(.x=, .f=)  
	* .x is the vector or list  
    * .f is the function  
    * output will be a list  
    * .f = ~mean(.x) and .f=mean will do the same thing  
* Can use the map functions on a nested file  
	* map(.x = nested$data, .f = ~mean(.x$population))  
    * pop_df <- nested %>% mutate(pop_mean = map(data, ~mean(.x$population)))  
    * pop_df %>% unnest(pop_mean)  # gets back the numbers rather than a column of lists  
* Can also use a map function that specifies a requested vector return of a specific type or a list of models such as an lm  
	* nested %>% mutate(pop_mean = map_dbl(data, ~mean(.x$population)))  # return will be a vector of doubles rather than a list  
    * nested %>% mutate(model = map(data, ~lm(formula = population~fertility, data = .x)))  
  
Tidy models with broom:  
  
* Several packages can help with the analysis of the list columns - broom, yardstick, rsample, etc.  
* The core of the broom is to extract relevant data from models  
	* tidy - statistical fundings such as coefficients  
    * glance - concise one-row summary  
    * augment - adds prediction columns to the data being modeled  
* Example of using tidy to extract data from the lm() used previously in the chapter  
	* tidy(algeria_model)  
    * glance(algeria_model)  
    * augment(algeria_model)  
* Plotting the augmented data can give a sense for model fits - predictions vs. actuals  
	* augment(algeria_model) %>% ggplot(mapping = aes(x = year)) + geom_point(mapping = aes(y = life_expectancy)) + geom_line(mapping = aes(y = .fitted), color = "red")  
  
Example code includes:  
```{r eval=FALSE}

# Explore gapminder
data(gapminder, package="gapminder")
head(gapminder)

# Prepare the nested dataframe gap_nested
gap_nested <- gapminder %>% 
  group_by(country) %>% 
  nest()

# Explore gap_nested
head(gap_nested)


# Create the unnested dataframe called gap_unnnested
gap_unnested <- gap_nested %>% 
  unnest()
  
# Confirm that your data was not modified  
identical(gapminder, gap_unnested)


# Extract the data of Algeria
algeria_df <- gap_nested$data[[which(gap_nested$country=="Algeria")]]

# Calculate the minimum of the population vector
min(algeria_df$pop)

# Calculate the maximum of the population vector
max(algeria_df$pop)

# Calculate the mean of the population vector
mean(algeria_df$pop)


# Calculate the mean population for each country
pop_nested <- gap_nested %>%
  mutate(mean_pop = map(.x=data, .f=~mean(.x$pop)))

# Take a look at pop_nested
head(pop_nested)

# Extract the mean_pop value by using unnest
pop_mean <- pop_nested %>% 
  unnest(mean_pop)

# Take a look at pop_mean
head(pop_mean)


# Calculate mean population and store result as a double
pop_mean <- gap_nested %>%
  mutate(mean_pop = map_dbl(.x=data, ~mean(.x$pop)))

# Take a look at pop_mean
head(pop_mean)


# Build a linear model for each country
gap_models <- gap_nested %>%
    mutate(model = map(.x=data, .f=~lm(formula = lifeExp ~ year, data = .x)))
    
# Extract the model for Algeria    
algeria_model <- gap_models$model[[which(gap_models$country=="Algeria")]]

# View the summary for the Algeria model
summary(algeria_model)


# Extract the coefficients of the algeria_model as a dataframe
broom::tidy(algeria_model)

# Extract the statistics of the algeria_model as a dataframe
broom::glance(algeria_model)


# Build the augmented dataframe
algeria_fitted <- broom::augment(algeria_model)

# Compare the predicted values with the actual values of life expectancy
algeria_fitted %>% 
  ggplot(aes(x = year)) +
  geom_point(aes(y = lifeExp)) + 
  geom_line(aes(y = .fitted), color = "red")

```
  
  
  
***
  
Chapter 2 - Multiple Models with broom  
  
Exploring coefficients across models:  
  
* The gapminder models contains data about all of the 77 countries  
	* gap_nested <- gapminder %>% group_by(country) %>% nest()  
    * gap_models <- gap_nested %>% mutate(model = map(data, ~lm(life_expectancy~year, data = .x)))  
* Can extract regression coefficients to better understand trends in the gapminder data  
	* gap_models %>% mutate(coef = map(model, ~tidy(.x))) %>% unnest(coef)  
  
Evaluating fit of many models:  
  
* Can also look at the R-squared across many models  
	* gap_models %>% mutate(fit = map(model, ~glance(.x))) %>% unnest(fit)  
  
Visually inspect the fit of many models:  
  
* Can compare predicted and actual observations from the gapminder dataset  
	* augmented_models <- gap_models %>% mutate(augmented = map(model,~augment(.x))) %>% unnest(augmented)  
    * augmented_model %>% filter(country == "Italy") %>% ggplot(aes(x = year, y = life_expectancy)) + geom_point() + geom_line(aes(y = .fitted), color = "red")  
  
Improve the fit of your models:  
  
* Can instead use multiple linear regressions and again explore the goodness of fits  
	* gap_fullmodels <- gap_nested %>% mutate(model = map(data, ~lm(formula = life_expectancy ~ ., data = .x)))  
    * tidy(gap_fullmodels$model[[1]])  
    * augment(gap_fullmodels$model[[1]])  
    * glance(gap_fullmodels$model[[1]])  
  
Example code includes:  
```{r eval=FALSE}

# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- gap_models %>% 
    mutate(coef = map(.x=model, .f=~broom::tidy(.x)))
    
# Simplify the coef dataframes for each model    
model_coef <- model_coef_nested %>%
    unnest(coef)

# Plot a histogram of the coefficient estimates for year         
model_coef %>% 
  filter(term=="year") %>% 
  ggplot(aes(x = estimate)) +
  geom_histogram()


# Extract the fit statistics of each model into dataframes
model_perf_nested <- gap_models %>% 
    mutate(fit = map(.x=model, .f=~broom::glance(.x)))

# Simplify the fit dataframes for each model    
model_perf <- model_perf_nested %>% 
    unnest(fit)
    
# Look at the first six rows of model_perf
head(model_perf)


# Plot a histogram of rsquared for the 77 models    
model_perf %>% 
  ggplot(aes(x=r.squared)) + 
  geom_histogram() 
  
# Extract the 4 best fitting models
best_fit <- model_perf %>% 
  top_n(n = 4, wt = r.squared)

# Extract the 4 models with the worst fit
worst_fit <- model_perf %>% 
  top_n(n = 4, wt = -r.squared)


best_augmented <- best_fit %>% 
  # Build the augmented dataframe for each country model
  mutate(augmented = map(.x=model, .f=~broom::augment(.x))) %>% 
  # Expand the augmented dataframes
  unnest(augmented)

worst_augmented <- worst_fit %>% 
  # Build the augmented dataframe for each country model
  mutate(augmented = map(.x=model, .f=~broom::augment(.x))) %>% 
  # Expand the augmented dataframes
  unnest(augmented)


# Compare the predicted values with the actual values of life expectancy 
# for the top 4 best fitting models
best_augmented %>% 
  ggplot(aes(x=year)) +
  geom_point(aes(y=lifeExp)) + 
  geom_line(aes(y=.fitted), color = "red") +
  facet_wrap(~country, scales = "free_y")

# Compare the predicted values with the actual values of life expectancy 
# for the top 4 worst fitting models
worst_augmented %>% 
  ggplot(aes(x=year)) +
  geom_point(aes(y=lifeExp)) + 
  geom_line(aes(y=.fitted), color = "red") +
  facet_wrap(~country, scales = "free_y")


# Build a linear model for each country using all features
gap_fullmodel <- gap_nested %>% 
  mutate(model = map(data, ~lm(formula = lifeExp ~ year + pop + gdpPercap, data = .x)))

fullmodel_perf <- gap_fullmodel %>% 
  # Extract the fit statistics of each model into dataframes
  mutate(fit = map(model, ~broom::glance(.x))) %>% 
  # Simplify the fit dataframes for each model
  unnest(fit)
  
# View the performance for the four countries with the worst fitting 
# four simple models you looked at before
fullmodel_perf %>% 
  filter(country %in% worst_fit$country) %>% 
  select(country, adj.r.squared)

```
  
  
  
***
  
Chapter 3 - Build, Tune, and Evaluate Regression Models  
  
Training, test, and validation splits:  
  
* Questions of how well the data would perform on new (unseen) data  
* The train-test split can be helpful for assessing out-of-sample performance - rsample has a function for this, with prop= being the proportion in the test data  
	* library(rsample)  
    * gap_split <- initial_split(gapminder, prop = 0.75)  
    * training_data <- training(gap_split)  
    * testing_data <- testing(gap_split)  
* The train data can be further split in to train and validate, with validate being used in model building (cross-validation)  
	* cv_split <- vfold_cv(training_data, v = 3)  # v=3 is for 3-fold cross-validation  
    * cv_data <- cv_split %>% mutate(train = map(splits, ~training(.x)), validate = map(splits, ~testing(.x)))  
  
Measuring cross-validation performance:  
  
* Can compare predictions made on the test or validate data to the actual values in the same datasets  
* The MAE captures the average magnitude of differences in the predictions and the actuals  
	* cv_prep_lm <- cv_models_lm %>% mutate(validate_actual = map(validate, ~.x$life_expectancy))  
* There is also a map2(.x=, .y=, .f=) function that allows for two input columns  
	* cv_prep_lm <- cv_eval_lm %>% mutate(validate_actual = map(validate, ~.x$life_expectancy), validate_predicted = map2(model, validate, ~predict(.x, .y)))  
    * library(Metrics)  
    * cv_eval_lm <- cv_prep_lm %>% mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))  
  
Building and tuning a random-forest model:  
  
* MAE for the gapminder model above is roughly 1.5 years  
* Can instead try a random forest model - can handle non-linear relationships and interactions  
	* rf_model <- ranger(formula = ___, data = ___, seed = ___)  
    * prediction <- predict(rf_model, new_data)$predictions  
    * rf_model <- ranger(formula, data, seed, mtry, num.trees)  # tuning the hyper-parameters  
* Can tune the mtry parameter using the crossing() function for a tidyverse approach  
	* cv_tune <- cv_data %>% crossing(mtry = 1:5)  # expands the frame for each hyperparameter of interest  
    * cv_model_tunerf <- cv_tune %>% mutate(model = map2(train, mtry, ~ranger(formula = life_expectancy~., data = .x, mtry = .y)))  
  
Measuring the test performance:  
  
* Can compare multiple models, including hyper-parameters  
* The final model is then built using ALL the training data, with the OOB error estimate based on the held-out test data  
	* best_model <- ranger(formula = life_expectancy~., data = training_data, mtry = 2, num.trees = 100, seed = 42)  
    * test_actual <- testing_data$life_expectancy  
    * test_predict <- predict(best_model, testing_data)$predictions  
    * mae(test_actual, test_predict)  
  
Example code includes:  
```{r eval=FALSE}

set.seed(42)

# Prepare the initial split object
gap_split <- rsample::initial_split(gapminder, prop = 0.75)

# Extract the training dataframe
training_data <- rsample::training(gap_split)

# Extract the testing dataframe
testing_data <- rsample::testing(gap_split)

# Calculate the dimensions of both training_data and testing_data
dim(training_data)
dim(testing_data)


set.seed(42)

# Prepare the dataframe containing the cross validation partitions
cv_split <- rsample::vfold_cv(training_data, v = 5)

cv_data <- cv_split %>% 
  mutate(
    # Extract the train dataframe for each split
    train = map(splits, ~rsample::training(.x)), 
    # Extract the validate dataframe for each split
    validate = map(splits, ~rsample::testing(.x))
  )

# Use head() to preview cv_data
head(cv_data)


# Build a model using the train data for each fold of the cross validation
cv_models_lm <- cv_data %>% 
  mutate(model = map(train, ~lm(formula = lifeExp ~ ., data = .x)))


cv_prep_lm <- cv_models_lm %>% 
  mutate(
    # Extract the recorded life expectancy for the records in the validate dataframes
    validate_actual = map(.x=validate, .f=~.x$lifeExp),
    # Predict life expectancy for each validate set using its corresponding model
    validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y))
  )


library(Metrics)
# Calculate the mean absolute error for each validate fold       
cv_eval_lm <- cv_prep_lm %>% 
  mutate(validate_mae = map2_dbl(.x=validate_actual, .y=validate_predicted, 
                                 .f=~mae(actual = .x, predicted = .y)
                                 )
         )

# Print the validate_mae column
cv_eval_lm$validate_mae

# Calculate the mean of validate_mae column
mean(cv_eval_lm$validate_mae)


library(ranger)

# Build a random forest model for each fold
cv_models_rf <- cv_data %>% 
  mutate(model = map(train, ~ranger(formula = lifeExp ~ ., data = .x,
                                    num.trees = 100, seed = 42)))

# Generate predictions using the random forest model
cv_prep_rf <- cv_models_rf %>% 
  mutate(validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y)$predictions))


# Calculate validate MAE for each fold
cv_eval_rf <- cv_prep_rf %>% 
  mutate(validate_actual=map(.x=validate, .f=~.x$lifeExp), 
         validate_mae = map2_dbl(.x=validate_actual, .y=validate_predicted, 
                                 .f=~mae(actual = .x, predicted = .y)
                                 )
         )

# Print the validate_mae column
cv_eval_rf$validate_mae

# Calculate the mean of validate_mae column
mean(cv_eval_rf$validate_mae)


# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>% 
  tidyr::crossing(mtry = 2:5) 

# Build a model for each fold & mtry combination
cv_model_tunerf <- cv_tune %>% 
  mutate(model = map2(.x=train, .y=mtry, ~ranger(formula = lifeExp ~ ., 
                                           data = .x, mtry = .y, 
                                           num.trees = 100, seed = 42)))


# Generate validate predictions for each model
cv_prep_tunerf <- cv_model_tunerf %>% 
  mutate(validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y)$predictions))

# Calculate validate MAE for each fold and mtry combination
cv_eval_tunerf <- cv_prep_tunerf %>% 
  mutate(validate_actual=map(.x=validate, .f=~.x$lifeExp),
         validate_mae = map2_dbl(.x=validate_actual, .y=validate_predicted, 
                                 .f=~mae(actual = .x, predicted = .y)
                                 )
         )

# Calculate the mean validate_mae for each mtry used  
cv_eval_tunerf %>% 
  group_by(mtry) %>% 
  summarise(mean_mae = mean(validate_mae))


# Build the model using all training data and the best performing parameter
best_model <- ranger(formula = lifeExp ~ ., data = training_data,
                     mtry = 4, num.trees = 100, seed = 42)

# Prepare the test_actual vector
test_actual <- testing_data$lifeExp

# Predict life_expectancy for the testing_data
test_predicted <- predict(best_model, testing_data)$predictions

# Calculate the test MAE
mae(test_actual, test_predicted)

```
  
  
  
***
  
Chapter 4 - Build, Tune, and Evaluate Classification Models  
  
Logistic Regression Models:  
  
* Binary classification models can also be run using the nest and map approach  
* The dataset of interest is "attrition" based on a fictional employer  
	* glm(formula = ___, data = ___, family = "binomial")  
    * head(cv_data)  
    * cv_models_lr <- cv_data %>% mutate(model = map(train, ~glm(formula = Attrition~., data = .x, family = "binomial")))  
  
Evaluating Classification Models:  
  
* Need actual and predicted classes, plus a relevant metric  
	* validate_prob <- predict(model, validate, type = "response")  
    * table(validate_actual, validate_predicted)  
    * accuracy(validate_actual, validate_predicted)  
    * precision(validate_actual, validate_predicted)  # of observations with predicted==TRUE, how often is actual==TRUE?  
    * recall(validate_actual, validate_predicted)  # of observations with actual==TRUE, how often is predicted==TRUE?  
  
Random Forest for Classification:  
  
* Can tune and build the random-forest model as per previous  
	* cv_tune <- cv_data %>% crossing(mtry = c(2, 4, 8, 16))  
    * cv_models_rf <- cv_tune %>% mutate(model = map2(train, mtry, ~ranger(formula = Attrition~., data = .x, mtry = .y, num.trees = 100, seed = 42)))  
    * validate_classes <- predict(rf_model, rf_validate)$predictions  
    * validate_predicted <- validate_classes == "Yes"  
  
Wrap Up:  
  
* List Column Workflow  
* Explore Models with Broom  
* Build, Tune, and Evaluate Regression Models  
* Build, Tune, and Evaluate Classification Models  
  
Example code includes:  
```{r eval=FALSE}

attrition <- readRDS("./RInputFiles/attrition.rds")
str(attrition)
head(attrition)


set.seed(42)

# Prepare the initial split object
data_split <- rsample::initial_split(data=attrition, prop=0.75)

# Extract the training dataframe
training_data <- rsample::training(data_split)

# Extract the testing dataframe
testing_data <- rsample::testing(data_split)

set.seed(42)
cv_split <- rsample::vfold_cv(training_data, v=5)

cv_data <- cv_split %>% 
  mutate(
    # Extract the train dataframe for each split
    train = map(.x=splits, .f=~rsample::training(.x)),
    # Extract the validate dataframe for each split
    validate = map(.x=splits, .f=~rsample::testing(.x))
  )


# Build a model using the train data for each fold of the cross validation
cv_models_lr <- cv_data %>% 
    mutate(model = map(train, ~glm(formula = Attrition ~ ., data = .x, family = "binomial")))


# Extract the first model and validate 
model <- cv_models_lr$model[[1]]
validate <- cv_models_lr$validate[[1]]

# Prepare binary vector of actual Attrition values in validate
validate_actual <- validate$Attrition == "Yes"

# Predict the probabilities for the observations in validate
validate_prob <- predict(model, validate, type = "response")

# Prepare binary vector of predicted Attrition values for validate
validate_predicted <- validate_prob > 0.5


library(Metrics)

# Compare the actual & predicted performance visually using a table
table(validate_actual, validate_predicted)

# Calculate the accuracy
accuracy(validate_actual, validate_predicted)

# Calculate the precision
precision(validate_actual, validate_predicted)

# Calculate the recall
recall(validate_actual, validate_predicted)


cv_prep_lr <- cv_models_lr %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(.x=validate, ~.x$Attrition == "Yes"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y, type = "response") > 0.5)
  )


# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>% 
  mutate(validate_recall = map2_dbl(.x=validate_actual, .y=validate_predicted, .f=~recall(actual = .x, predicted = .y)))

# Print the validate_recall column
cv_perf_recall$validate_recall

# Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)


library(ranger)

# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
  crossing(mtry = c(2, 4, 8, 16)) 

# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>% 
  mutate(model = map2(train, mtry, ~ranger(formula = Attrition~., 
                                           data = .x, mtry = .y,
                                           num.trees = 100, seed = 42)))


cv_prep_rf <- cv_models_rf %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(validate, ~.x$Attrition == "Yes"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x=model, .y=validate, ~predict(.x, .y, type = "response")$predictions=="Yes")
  )

# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_rf %>% 
  mutate(recall = map2_dbl(.x=validate_actual, .y=validate_predicted, ~recall(actual=.x, predicted=.y)))

# Calculate the mean recall for each mtry used  
cv_perf_recall %>% 
  group_by(mtry) %>% 
  summarise(mean_recall = mean(recall))


# Build the logistic regression model using all training data
best_model <- glm(formula = Attrition ~ ., 
                  data = training_data, family = "binomial")


# Prepare binary vector of actual Attrition values for testing_data
test_actual <- testing_data$Attrition == "Yes"

# Prepare binary vector of predicted Attrition values for testing_data
test_predicted <- predict(best_model, newdata=testing_data, type = "response") > 0.5


# Compare the actual & predicted performance visually using a table
table(test_actual, test_predicted)

# Calculate the test accuracy
accuracy(test_actual, test_predicted)

# Calculate the test precision
precision(test_actual, test_predicted)

# Calculate the test recall
recall(test_actual, test_predicted)

```
  
  
  
***
  
###_Predictive Analytics Using Networked Data in R_  
  
Chapter 1 - Introduction, Networks, and Labeled Networks  
  
Introduction:  
  
* Labeled networks and network structure for predicting node attributes  
	* Predicting Age, Gender, Fraud, Churn, etc., for unknown nodes  
* Course includes labeled social networks, homophily, network featurization, predicting using supervised learning  
* Example of course collaborations - network  
	* library(igraph)  
    * DataScienceNetwork <- data.frame( from = c('A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'D', 'E', 'F', 'F', 'G', 'G', 'H', 'H', 'I'), to = c('B','C','D','E','C','D','D', 'G','E', 'F','G','F','G','I', 'I','H','I','J','J'))  
    * g <- graph_from_data_frame(DataScienceNetwork, directed = FALSE)  
    * pos <- cbind(c(2, 1, 1.5, 2.5, 4, 4. 5, 3, 3.5, 5, 6), c(10.5, 9.5, 8, 8.5, 9, 7.5, 6, 4.5, 5.5, 4))  
    * plot.igraph(g, edge.label = NA, edge.color = 'black', layout = pos, vertex.label = V(g)$name, vertex.color = 'white', vertex.label.color = 'black', vertex.size = 25)  
    * V(g)$technology <- c('R','R','?','R','R', 'R','P','P','P','P')  
    * V(g)$color <- V(g)$technology  
    * V(g)$color <- gsub('R',"blue3", V(g)$color)  
    * V(g)$color <- gsub('P',"green4", V(g)$color)  
    * V(g)$color <- gsub('?',"gray", V(g)$color)  
* Will be using the network edgeList which has relationships among customers  
  
Labeled Networks, Social Influence:  
  
* Example of using node attributes "customers" which defines customers as having churned or not  
* Supposing that churn is a social phenomenon, then churn is likely predictable based on connections  
* Can use the relational neighbor classifier  
	* Percentage of neighbors with a certain trait used to predict trait of unlabeled node  
    * rNeighbors <- c(4,3,3,5,3,2,3,0,1,0)  
    * pNeighbors <- c(0,0,1,1,0,2,2,3,3,2)  
    * rRelationalNeighbor <- rNeighbors / (rNeighbors + pNeighbors)  
    * rRelationalNeighbor  
  
Challenges:  
  
* Desire to evaluate models using a test set (out-of-sample performance)  
	* Harder to do with networks, where connections are important (cannot just select 60% of the nodes)  
    * Often managed by training on one network and testing on another  
* Observations may not be iid (in fact, there tend to be strong correlations between connected nodes)  
* Collective inferencing is another challenge  
	* Infer the unknown nodes based on knowledge of how these nodes tend to interact with each other  
  
Example code includes:  
```{r}

library(igraph)


# load("./RInputFiles/StudentEdgelist.RData")

# Create edgeList
elFrom <- c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 8, 8, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 20, 20, 20, 21, 21, 22, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 27, 28, 28, 28, 29, 29, 29, 29, 32, 32, 32, 32, 32, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 583, 38, 38, 39, 39, 40, 40, 40, 40, 343, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 47, 47, 47, 47, 47, 48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 56, 56, 56, 56, 56, 684, 57, 57, 58, 58, 58, 58, 191, 59, 59, 59, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 62, 63, 63, 63, 64, 64, 64, 64, 64, 64, 99, 65, 66, 66, 67, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 71, 71, 71, 71, 72, 72, 72, 73, 73, 73, 73, 74, 75, 76, 76, 76, 76, 76, 76, 238, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 179, 80, 81, 707, 82, 83, 83, 83, 84, 84, 84, 84, 85, 85, 609, 85, 86, 86, 689, 86, 86, 87, 87, 87, 87, 87, 286, 459, 633, 88, 88, 89, 89, 89, 89, 90, 90, 90, 90, 91, 91, 92, 583, 92, 93, 93, 93, 94, 94, 358, 532, 95, 96, 97, 97, 707, 97, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 807, 976, 101, 101, 101, 102, 102, 102, 102, 102, 784, 103, 104, 104, 105, 105, 106, 106, 106, 106, 106, 107, 583, 294, 109, 109, 110, 110, 111, 111, 111, 112, 112, 278, 113, 114, 114, 114, 114, 114, 115, 115, 115, 406, 116, 116, 116, 116, 267, 388, 117, 117, 117, 117, 117, 118, 358, 118, 119, 119, 119, 119, 119, 343, 120, 120, 121, 121, 121, 138, 207, 122, 122, 122, 122, 122, 889, 123, 123, 123, 123, 123, 124, 125, 125, 125, 125, 125, 125, 126, 126, 793, 889, 127, 128, 128, 128, 128, 128, 129, 707, 129, 337, 130, 130, 131, 131, 131, 131, 132, 133, 133, 133, 133, 532, 135, 406, 135, 186, 136, 364, 139, 139, 736, 467, 141, 288, 317, 142, 142, 142, 976, 143, 143, 143, 144, 144, 144, 889, 988, 145, 145, 146, 146, 147, 288, 148, 149, 149, 149, 661, 149, 149, 149, 172, 150, 150, 150, 245, 152, 153, 153, 154, 154, 155, 155, 155, 186, 467, 156, 157, 157, 332, 642, 190, 159, 159, 159, 159, 159, 160, 160, 161, 161, 290, 162, 162, 163, 163, 163, 531, 164, 164, 164, 165, 406, 165, 165, 165, 166, 166, 166, 166, 167, 167, 168, 168, 233, 413, 683, 931, 170, 839, 170, 171, 171, 171, 171, 171, 172, 172, 400, 506, 184, 174, 174, 174, 174, 174, 175, 176, 176, 177, 177, 177, 177, 177, 177, 177, 177, 177, 393, 178, 178, 179, 179, 179, 180, 981, 181, 181, 181, 182, 183, 183, 185, 185, 186, 270, 187, 410, 625, 189, 189, 189, 190, 190, 190, 191, 191, 191, 191, 302, 192, 193, 193, 523, 741, 195, 195, 195, 424, 196, 349, 197, 197, 246, 198, 198, 198, 198, 199, 531, 200, 201, 201, 201, 201, 201, 411, 204, 205, 205, 205, 206, 206, 206, 206, 207, 207, 207, 207, 354, 208, 209, 587, 717, 211, 211, 211, 630, 666, 211, 212, 212, 212, 545, 212, 212, 456, 889, 214, 216, 216, 216)
elFrom <- c(elFrom, 216, 216, 927, 883, 422, 946, 219, 406, 734, 874, 221, 221, 222, 222, 853, 222, 222, 222, 223, 223, 223, 224, 224, 224, 274, 406, 227, 227, 228, 228, 229, 230, 230, 230, 964, 231, 231, 231, 864, 597, 232, 233, 675, 235, 964, 237, 416, 482, 568, 236, 237, 238, 238, 238, 239, 671, 734, 975, 240, 684, 240, 650, 241, 424, 817, 243, 683, 243, 243, 307, 456, 245, 245, 246, 968, 259, 247, 247, 669, 247, 487, 248, 248, 249, 689, 250, 250, 250, 251, 251, 251, 251, 891, 375, 855, 252, 253, 253, 253, 690, 789, 255, 255, 284, 587, 730, 256, 545, 257, 257, 281, 336, 614, 258, 258, 259, 259, 975, 274, 260, 260, 260, 260, 698, 261, 817, 262, 262, 262, 262, 803, 816, 406, 264, 264, 853, 265, 488, 601, 266, 267, 269, 269, 269, 269, 269, 270, 290, 271, 271, 927, 594, 274, 274, 274, 274, 963, 276, 276, 276, 454, 278, 278, 279, 279, 279, 329, 384, 280, 281, 822, 281, 482, 282, 284, 284, 284, 284, 285, 285, 285, 286, 286, 286, 286, 287, 287, 288, 288, 288, 289, 290, 449, 291, 669, 291, 788, 659, 292, 293, 522, 296, 296, 803, 296, 297, 299, 300, 300, 300, 300, 643, 301, 302, 302, 302, 302, 303, 303, 303, 303, 531, 704, 304, 305, 789, 306, 306, 306, 306, 308, 336, 309, 309, 745, 312, 312, 313, 313, 313, 637, 649, 790, 802, 960, 343, 524, 316, 317, 317, 318, 335, 319, 319, 319, 689, 320, 964, 321, 321, 321, 321, 323, 323, 323, 324, 324, 868, 698, 327, 529, 329, 329, 329, 329, 960, 332, 746, 811, 822, 865, 335, 334, 334, 334, 424, 335, 335, 336, 336, 336, 633, 931, 401, 341, 407, 412, 842, 343, 343, 343, 532, 637, 344, 553, 597, 399, 758, 346, 347, 347, 348, 348, 348, 348, 349, 349, 349, 817, 351, 351, 807, 855, 353, 353, 976, 354, 354, 354, 356, 356, 371, 358, 358, 707, 859, 994, 690, 360, 360, 361, 361, 361, 361, 973, 375, 362, 784, 365, 365, 365, 732, 890, 972, 637, 368, 369, 369, 369, 370, 643, 370, 370, 371, 372, 404, 373, 997, 375, 375, 376, 568, 377, 377, 378, 402, 840, 380, 425, 465, 569, 382, 382, 803, 894, 384, 384, 608, 526, 388, 388, 390, 390, 390, 390, 390, 390, 672, 392, 393, 660, 395, 395, 743, 397, 398, 399, 996, 460, 401, 401, 401, 402, 403, 422, 403, 413, 406, 406, 406, 406, 408, 408, 930, 409, 409, 409, 409, 410, 410, 410, 411, 411, 412, 413, 414, 415, 415, 855, 416, 416, 445, 417, 417, 417, 871, 420, 946, 420, 420, 422, 877, 423, 424, 424, 425, 426, 822, 430, 429, 429, 429, 732, 429, 429, 430, 433, 669, 433, 467, 433, 434, 435, 436, 436, 812, 437, 437, 853, 963, 438, 439, 446, 440, 608, 440, 914, 441, 692, 441, 862, 442, 442, 443, 446, 744, 446, 880, 449, 482, 556, 912, 957, 453, 453, 453, 994, 454, 454, 551, 561, 998, 457, 457, 788, 473, 458, 689, 459, 689, 460, 460, 679, 812, 884, 886, 717, 741, 466, 467, 467, 760, 834, 529, 738, 470, 470, 503, 471, 471, 472, 472, 473, 473, 999, 474, 474, 474, 643, 835, 476, 690, 859, 478, 479, 877, 480, 480, 480, 503, 774, 482, 483, 584, 733, 487, 487, 488, 488, 642, 488, 577, 491, 802, 492, 493, 494, 494, 498, 496, 496, 642, 726, 497, 914, 498, 499, 499, 499, 500, 500, 942, 581, 915, 501, 543, 701, 770, 503, 503, 748, 506, 506, 506, 506, 737, 982, 913, 510, 566, 511, 692, 767, 932, 737, 660, 742, 552, 518, 700, 854, 640, 583, 521, 521, 522, 522, 524, 683, 718, 525, 526, 527, 759, 529, 530, 530, 998, 531, 531, 531, 532, 532, 532, 532, 532, 577)
elFrom <- c(elFrom, 533, 533, 877, 534, 534, 534, 734, 535, 535, 561, 536, 538, 538, 538, 539, 614, 652, 541, 541, 541, 997, 543, 543, 543, 946, 545, 791, 549, 550, 963, 705, 809, 554, 554, 555, 912, 556, 556, 556, 556, 557, 958, 632, 665, 719, 560, 560, 561, 570, 628, 707, 777, 923, 976, 564, 564, 738, 564, 564, 565, 565, 566, 566, 567, 567, 567, 569, 570, 958, 679, 958, 908, 638, 736, 577, 653, 808, 607, 645, 774, 865, 636, 583, 583, 999, 585, 585, 587, 840, 609, 589, 589, 995, 592, 874, 625, 698, 594, 595, 596, 822, 732, 823, 607, 771, 724, 804, 600, 839, 601, 731, 800, 854, 645, 607, 607, 607, 607, 608, 609, 610, 610, 891, 943, 613, 615, 800, 839, 638, 619, 619, 883, 620, 620, 768, 622, 877, 623, 623, 623, 633, 892, 889, 787, 984, 748, 761, 929, 690, 930, 630, 630, 632, 633, 685, 634, 634, 635, 635, 635, 637, 968, 883, 638, 638, 816, 642, 669, 643, 643, 644, 646, 895, 740, 649, 650, 650, 961, 651, 651, 856, 730, 849, 680, 908, 960, 661, 662, 931, 955, 666, 666, 669, 669, 669, 670, 671, 671, 768, 672, 812, 818, 675, 677, 677, 677, 679, 679, 679, 889, 683, 684, 732, 685, 876, 770, 687, 788, 690, 692, 692, 692, 692, 692, 790, 916, 698, 718, 722, 743, 937, 702, 704, 707, 759, 917, 713, 713, 732, 715, 892, 717, 718, 874, 921, 942, 744, 722, 891, 968, 724, 724, 726, 726, 881, 732, 733, 734, 793, 892, 740, 970, 742, 743, 744, 744, 784, 746, 886, 807, 770, 996, 994, 856, 878, 813, 769, 771, 902, 956, 982, 780, 810, 874, 825, 873, 840, 924, 789, 988, 790, 811, 791, 875, 795, 795, 800, 802, 964, 805, 980, 808, 808, 808, 874, 874, 811, 922, 819, 868, 929, 881, 822, 822, 952, 865, 963, 981, 988, 872, 832, 839, 960, 851, 855, 855, 860, 918, 861, 875, 875, 984, 888, 890, 897, 901, 998, 905, 961, 907, 918, 988, 976, 982, 922, 923, 924, 947, 970, 974, 997, 999, 942, 952, 984, 992)
elTo <- c(250, 308, 413, 525, 803, 894, 332, 433, 474, 847, 963, 968, 147, 290, 337, 393, 474, 179, 193, 233, 737, 793, 838, 684, 718, 237, 404, 698, 724, 285, 641, 86, 285, 376, 689, 758, 889, 145, 410, 544, 583, 835, 96, 788, 924, 43, 91, 446, 181, 289, 378, 406, 547, 784, 189, 399, 482, 822, 262, 308, 817, 832, 260, 997, 81, 229, 839, 56, 840, 183, 186, 397, 676, 760, 344, 534, 980, 303, 343, 395, 925, 988, 483, 522, 132, 335, 506, 643, 304, 704, 871, 872, 466, 524, 567, 683, 997, 264, 279, 896, 105, 356, 460, 568, 726, 789, 865, 902, 951, 988, 138, 293, 38, 614, 633, 224, 550, 64, 224, 463, 521, 41, 347, 566, 746, 885, 99, 424, 442, 459, 571, 613, 689, 807, 84, 106, 257, 883, 191, 222, 265, 631, 681, 853, 207, 296, 546, 726, 866, 161, 665, 640, 816, 160, 669, 284, 313, 371, 973, 270, 407, 748, 230, 410, 445, 587, 644, 651, 936, 961, 964, 320, 804, 284, 476, 506, 755, 919, 57, 730, 754, 85, 259, 609, 975, 59, 278, 360, 413, 454, 589, 609, 889, 170, 184, 215, 365, 426, 707, 828, 548, 294, 479, 671, 473, 497, 642, 914, 942, 999, 65, 358, 491, 669, 326, 310, 531, 717, 852, 882, 960, 172, 331, 416, 552, 643, 453, 607, 732, 994, 245, 428, 943, 97, 255, 279, 570, 238, 412, 235, 271, 532, 722, 927, 964, 77, 286, 638, 736, 216, 351, 526, 745, 911, 927, 971, 79, 970, 839, 82, 814, 288, 556, 595, 456, 560, 563, 976, 98, 249, 85, 802, 149, 661, 86, 788, 953, 312, 630, 696, 740, 793, 88, 88, 88, 682, 864, 400, 424, 460, 947, 208, 354, 683, 744, 467, 946, 195, 92, 932, 402, 411, 828, 321, 670, 95, 95, 931, 766, 129, 458, 97, 968, 116, 161, 336, 422, 493, 802, 877, 281, 442, 651, 826, 907, 100, 100, 352, 364, 414, 190, 618, 839, 914, 924, 103, 887, 635, 961, 182, 772, 380, 488, 510, 662, 670, 520, 107, 109, 584, 597, 267, 388, 338, 862, 933, 691, 998, 113, 763, 475, 506, 790, 835, 898, 420, 494, 819, 116, 440, 535, 692, 886, 117, 117, 437, 661, 853, 876, 877, 219, 118, 798, 156, 672, 731, 766, 825, 120, 972, 981, 551, 842, 963, 122, 122, 349, 527, 529, 650, 770, 122, 150, 354, 377, 811, 922, 617, 130, 190, 337, 523, 742, 880, 409, 539, 126, 127, 937, 152, 159, 448, 527, 591, 317, 129, 973, 130, 702, 831, 324, 472, 492, 659, 436, 168, 172, 515, 538, 134, 329, 135, 714, 136, 706, 137, 241, 912, 140, 141, 660, 142, 142, 470, 735, 773, 142, 282, 705, 930, 300, 625, 715, 144, 144, 319, 774, 610, 741, 375, 148, 909, 383, 585, 593, 149, 718, 874, 894, 150, 178, 545, 873, 152, 487, 768, 999, 868, 995, 269, 560, 860, 156, 156, 645, 425, 849, 158, 158, 159, 342, 407, 595, 620, 849, 514, 645, 553, 802, 162, 549, 981, 239, 540, 864, 164, 537, 795, 800, 391, 165, 630, 761, 891, 333, 427, 581, 982, 855, 907, 398, 515, 169, 169, 169, 169, 184, 170, 890, 176, 356, 613, 679, 884, 426, 796, 173, 173, 174, 211, 274, 403, 734, 812, 511, 302, 363, 204, 246, 251, 283, 465, 557, 664, 704, 891, 178, 884, 915, 242, 302, 637, 273, 180, 289, 587, 696, 569, 671, 831, 305, 789, 277, 187, 341, 188, 188, 222, 439, 590, 620, 867, 880, 278, 356, 968, 989, 192, 419, 464, 673, 194, 194, 372, 403, 863, 196, 810, 197, 601, 666, 198, 401, 716, 719, 771, 594, 200, 756, 381, 666, 688, 788, 867, 202, 979, 498, 525, 898, 253, 539, 818, 859, 236, 448, 790, 880, 208, 879, 906, 210, 210, 261, 384, 545, 211, 211, 817, 346, 443, 451, 212, 752, 947, 213, 213, 674, 300, 322, 543, 671)
elTo <- c(elTo, 881, 216, 217, 218, 218, 239, 220, 220, 220, 649, 957, 265, 269, 222, 918, 920, 989, 675, 692, 809, 394, 463, 854, 225, 225, 658, 960, 307, 975, 695, 791, 811, 905, 230, 449, 452, 556, 231, 232, 769, 886, 234, 700, 235, 236, 236, 236, 236, 929, 503, 769, 822, 905, 517, 239, 239, 239, 297, 240, 808, 241, 769, 242, 242, 374, 243, 744, 930, 244, 244, 251, 642, 922, 246, 247, 291, 444, 247, 690, 248, 663, 944, 632, 249, 515, 769, 986, 334, 335, 465, 801, 251, 252, 252, 870, 322, 402, 721, 254, 254, 627, 852, 256, 256, 256, 987, 257, 909, 996, 258, 258, 258, 956, 996, 392, 845, 259, 260, 567, 600, 769, 916, 261, 781, 261, 306, 319, 323, 776, 263, 263, 264, 420, 795, 265, 862, 266, 266, 958, 876, 602, 743, 862, 888, 911, 441, 271, 324, 608, 271, 272, 437, 564, 600, 824, 274, 699, 743, 974, 277, 593, 821, 724, 865, 897, 280, 280, 992, 774, 281, 991, 282, 705, 398, 476, 730, 919, 746, 758, 823, 342, 635, 800, 821, 370, 390, 420, 556, 861, 779, 324, 291, 648, 291, 688, 291, 292, 954, 953, 296, 533, 577, 296, 849, 357, 596, 668, 715, 732, 881, 301, 753, 369, 419, 518, 987, 500, 655, 672, 825, 304, 304, 919, 653, 305, 434, 519, 540, 612, 894, 309, 485, 691, 311, 316, 617, 557, 596, 621, 314, 314, 314, 314, 314, 315, 316, 856, 537, 714, 895, 319, 361, 391, 534, 320, 804, 320, 590, 670, 875, 985, 447, 733, 812, 363, 655, 325, 326, 836, 328, 492, 621, 673, 984, 330, 901, 333, 333, 333, 333, 334, 417, 588, 966, 335, 589, 954, 614, 682, 802, 337, 338, 340, 368, 342, 342, 342, 588, 716, 892, 344, 344, 980, 345, 345, 346, 346, 834, 558, 623, 489, 559, 632, 850, 359, 588, 770, 351, 849, 971, 352, 352, 603, 854, 353, 819, 843, 856, 789, 823, 357, 536, 561, 359, 359, 359, 360, 884, 942, 450, 558, 701, 918, 361, 362, 461, 364, 580, 598, 685, 365, 366, 366, 367, 713, 499, 571, 700, 390, 370, 753, 921, 994, 863, 373, 471, 373, 508, 942, 435, 377, 645, 917, 948, 379, 379, 680, 382, 382, 382, 579, 768, 383, 383, 738, 837, 385, 386, 501, 636, 435, 462, 499, 606, 794, 923, 392, 845, 870, 394, 866, 987, 396, 909, 856, 758, 399, 400, 417, 722, 823, 721, 419, 403, 550, 405, 547, 886, 934, 999, 486, 870, 408, 470, 581, 600, 604, 502, 961, 964, 430, 441, 897, 803, 897, 628, 833, 415, 798, 908, 417, 588, 966, 984, 419, 861, 420, 983, 985, 479, 422, 777, 913, 954, 887, 796, 428, 429, 578, 652, 667, 429, 739, 808, 759, 431, 431, 452, 433, 920, 554, 923, 580, 673, 436, 445, 743, 437, 437, 480, 647, 440, 505, 440, 709, 440, 603, 441, 772, 441, 777, 878, 752, 668, 446, 955, 448, 577, 450, 450, 450, 452, 607, 678, 787, 453, 806, 980, 455, 455, 455, 468, 584, 457, 458, 530, 458, 538, 459, 902, 979, 461, 461, 462, 462, 463, 466, 939, 797, 813, 468, 468, 469, 469, 905, 949, 471, 516, 949, 701, 800, 521, 530, 473, 644, 759, 987, 475, 475, 495, 477, 477, 799, 676, 479, 507, 605, 634, 481, 481, 929, 610, 484, 484, 638, 682, 544, 571, 488, 918, 490, 510, 492, 950, 773, 634, 876, 495, 829, 955, 497, 497, 833, 497, 652, 606, 696, 767, 655, 825, 500, 501, 501, 938, 502, 502, 502, 558, 949, 504, 571, 724, 766, 790, 508, 508, 509, 615, 511, 654, 513, 513, 513, 516, 517, 517, 518, 598, 518, 518, 519, 520, 617, 622, 733, 952, 567, 524, 524, 528, 541, 820, 528, 699, 575, 903, 530, 697, 871, 882, 565, 575, 590, 682, 968, 533, 622, 753)
elTo <- c(elTo, 533, 903, 923, 980, 535, 926, 970, 536, 791, 600, 709, 934, 544, 540, 541, 703, 866, 944, 541, 701, 799, 824, 543, 850, 547, 786, 606, 551, 554, 554, 827, 939, 711, 555, 729, 855, 872, 966, 621, 558, 559, 559, 559, 582, 636, 998, 562, 562, 562, 562, 562, 563, 604, 651, 564, 786, 844, 782, 836, 654, 810, 612, 769, 916, 910, 777, 570, 573, 573, 575, 576, 576, 656, 578, 578, 579, 579, 579, 579, 582, 606, 968, 584, 851, 857, 636, 588, 589, 711, 967, 590, 677, 593, 594, 594, 825, 981, 621, 596, 598, 598, 599, 599, 600, 600, 934, 601, 875, 602, 603, 603, 604, 798, 849, 940, 993, 902, 967, 611, 967, 611, 611, 843, 686, 618, 618, 619, 764, 876, 619, 916, 926, 622, 778, 622, 703, 720, 819, 624, 624, 625, 626, 626, 627, 627, 627, 629, 629, 740, 789, 892, 779, 634, 765, 813, 728, 821, 873, 644, 637, 638, 895, 934, 640, 694, 643, 753, 861, 987, 708, 647, 648, 957, 722, 905, 650, 786, 844, 654, 656, 656, 657, 658, 658, 852, 786, 662, 664, 751, 817, 694, 711, 985, 786, 734, 831, 672, 825, 673, 674, 725, 775, 798, 945, 749, 884, 886, 679, 931, 754, 685, 814, 686, 687, 783, 688, 942, 695, 790, 809, 833, 910, 695, 697, 725, 699, 699, 699, 699, 928, 919, 777, 708, 709, 749, 935, 715, 935, 716, 852, 937, 719, 720, 721, 722, 905, 723, 723, 829, 865, 865, 966, 728, 881, 796, 755, 737, 740, 926, 740, 830, 974, 753, 803, 745, 791, 749, 754, 757, 758, 760, 762, 764, 765, 846, 873, 775, 775, 776, 977, 781, 781, 782, 785, 786, 788, 876, 789, 887, 791, 905, 792, 857, 858, 810, 869, 804, 990, 806, 875, 928, 955, 810, 811, 884, 811, 843, 819, 820, 821, 959, 991, 827, 829, 830, 831, 831, 832, 928, 965, 840, 857, 870, 872, 910, 860, 985, 941, 985, 887, 969, 977, 954, 904, 903, 949, 905, 933, 910, 910, 912, 921, 937, 1000, 940, 925, 926, 928, 933, 934, 940, 940, 966, 966)

edgeList <- data.frame(from=elFrom, 
                       to=elTo, 
                       stringsAsFactors = FALSE
                       )

# Inspect edgeList
str(edgeList)
head(edgeList)

# Construct the igraph object
network <- graph_from_data_frame(edgeList, directed = FALSE)

# View your igraph object
network


# load("./RInputFiles/StudentCustomers.RData")

custID <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651)
custID <- c(custID, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956)
custChurn <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
custChurn <- c(custChurn, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

customers <- data.frame(id=custID, churn=custChurn)
# Inspect the customers dataframe
str(customers)
head(customers)

# Count the number of churners and non-churners
table(customers$churn)

matchID <- match(V(network), customers$id)
churnID <- customers$churn[matchID]
table(churnID)
churnID[is.na(churnID)] <- 0
table(churnID)

# Add a node attribute called churn
V(network)$churn <- churnID


# useVerts <- c('1', '10', '100', '1000', '101', '102', '103', '104', '105', '106', '107', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '201', '202', '204', '205', '206', '207', '208', '209', '21', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '22', '220', '221', '222', '223', '224', '225', '227', '228', '229', '23', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '24', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '25', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '26', '260', '261', '262', '263', '264', '265', '266', '267', '269', '27', '270', '271', '272', '273', '274', '276', '277', '278', '279', '28', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '29', '290', '291', '292', '293', '294', '296', '297', '299', '3', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '32', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '34', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '35', '351', '352', '353', '354', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '37', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '38', '380', '381', '382', '383', '384', '385', '386', '388', '39', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '4', '40', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '41', '410', '411', '412', '413', '414', '415', '416', '417', '419', '42', '420', '422', '423', '424', '425', '426', '427', '428', '429', '43', '430', '431', '433', '434', '435', '436', '437', '438', '439', '44', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '45', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '47', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '48', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '49', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499')
# useVerts <- c(useVerts, '5', '50', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '51', '510', '511', '513', '514', '515', '516', '517', '518', '519', '52', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '53', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '54', '540', '541', '543', '544', '545', '546', '547', '548', '549', '55', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '56', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '57', '570', '571', '573', '575', '576', '577', '578', '579', '58', '580', '581', '582', '583', '584', '585', '587', '588', '589', '59', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '6', '60', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '61', '610', '611', '612', '613', '614', '615', '617', '618', '619', '62', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '63', '630', '631', '632', '633', '634', '635', '636', '637', '638', '64', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '65', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '66', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '67', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '68', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '69', '690', '691', '692', '694', '695', '696', '697', '698', '699', '7', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '71', '711', '713', '714', '715', '716', '717', '718', '719', '72', '720', '721', '722', '723', '724', '725', '726', '728', '729', '73', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '74', '740', '741', '742', '743', '744', '745', '746', '748', '749', '75', '751', '752', '753', '754', '755', '756', '757', '758', '759', '76', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '77', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '78', '780', '781', '782', '783', '784', '785', '786', '787', '788', '789', '79', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '8', '80', '800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '81', '810', '811', '812', '813', '814', '816', '817', '818', '819', '82', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '83', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '84', '840', '842', '843', '844', '845', '846', '847', '849', '85', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '86', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '87', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '88', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '89', '890', '891', '892', '894', '895', '896', '897', '898', '90', '901', '902', '903', '904', '905', '906', '907', '908', '909', '91', '910', '911', '912', '913', '914', '915', '916', '917', '918', '919', '92', '920', '921', '922', '923', '924', '925', '926', '927', '928', '929', '93', '930', '931', '932', '933', '934', '935', '936', '937', '938', '939', '94', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '95', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '96', '960', '961', '963', '964', '965', '966', '967', '968', '969', '97', '970', '971', '972', '973', '974', '975', '976', '977', '979', '98', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '99', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999')

# useVertNums <- match(useVerts, V(network))
# useNetwork <- induced_subgraph(network, useVertNums)
useNetwork <- network
useNetwork

# Visualize the network (pretty messy)
plot(useNetwork, vertex.label = NA, edge.label = NA, edge.color = 'black', vertex.size = 2)


# Add a node attribute called color
V(useNetwork)$color <- V(useNetwork)$churn

# Change the color of churners to red and non-churners to white
V(useNetwork)$color <- gsub("1", "red", V(useNetwork)$color) 
V(useNetwork)$color <- gsub("0", "white", V(useNetwork)$color)

# Plot the network (pretty messy)
plot(useNetwork, vertex.label = NA, edge.label = NA, edge.color = 'black', vertex.size = 2)


# Create a subgraph with only churners
churnerNetwork <- induced_subgraph(useNetwork, v = V(useNetwork)[which(V(useNetwork)$churn == 1)])
                    
# Plot the churner network 
plot(churnerNetwork, vertex.label = NA, vertex.size = 2)


ctNeighbors <- function(v) {
    tmp <- V(useNetwork)[neighbors(useNetwork, v, mode="all")]$churn
    c(sum(tmp==0), sum(tmp==1))
}
mtxNeighbors <- sapply(V(useNetwork), FUN=ctNeighbors)
NonChurnNeighbors <- mtxNeighbors[1, ]
ChurnNeighbors <- mtxNeighbors[2, ]


# Compute the churn probabilities
churnProb <- ChurnNeighbors / (ChurnNeighbors + NonChurnNeighbors)

# Find who is most likely to churn
mostLikelyChurners <- which(churnProb == max(churnProb))

# Extract the IDs of the most likely churners
customers$id[mostLikelyChurners]


# Find churn probability of the 44th customer
churnProb[44]

# Update the churn probabilties and the non-churn probabilities
AdjacencyMatrix <- as_adjacency_matrix(useNetwork)
nNeighbors <- colSums(mtxNeighbors)
churnProb_updated <- as.vector((AdjacencyMatrix %*% churnProb) / nNeighbors)

# Find updated churn probability of the 44th customer
churnProb_updated[44]


# Compute the AUC
pROC::auc(churnID, as.vector(churnProb))

# Write a for loop to update the probabilities
for(i in 1:10){
    churnProb <- as.vector((AdjacencyMatrix %*% churnProb) / nNeighbors)
}

# Compute the AUC again
pROC::auc(churnID, as.vector(churnProb))

```
  
  
  
***
  
Chapter 2 - Homophily  
  
Homophily:  
  
* Social networks tend to have reasons for connections (sharing of common properties such as interests, locations, etc.)  
* Homophily is a scientific term for "birds of a feather flock together" (similar nodes are more likely to connect than dissimilar nodes)  
* Can define different types of edges to classify the degree of homophily  
	* edge_rr<-sum(E(g)$label=='rr')  # connects r to r  
    * edge_pp<-sum(E(g)$label=='pp')  # connects p to p  
    * edge_rp<-sum(E(g)$label=='rp')  # connects r to p or p to r  
    * p <- 2*edges/nodes*(nodes-1)  
  
Dyadicity:  
  
* Dyadicity is a measure of connectedness among nodes with the same attributes  
	* Comparison is to a random configuration of the network (1 would be equal to expected, >1 would be greater than expected)  
    * Dyadic (>1) - Random (~1) - Anti-Dyadic (<1)  
  
Heterophilicity:  
  
* Connectedness among nodes with opposite labels  
	* Expected number of connections is nA * nB * p (where p is probability of connection across the network)  
* Heterophilic (>1), Random (~1), and Heterophobic (<1) networks are all possible  
  
Summary:  
  
* Need to evaluate whether node attributes have important relationships - is there structure among the node connections  
* Dyadicity (connectiveness to same type of nodes, relative to expected / average network connectiveness)  
* Hetrophilicity (connectedness to different type of nodes, relative to expected / average network connectiveness)  
  
Example code includes:  
```{r}

# Add the column edgeList$FromLabel
edgeList$FromLabel <- customers[match(edgeList$from, customers$id), 2]
edgeList$FromLabel[is.na(edgeList$FromLabel)] <- 0

# Add the column edgeList$ToLabel
edgeList$ToLabel <- customers[match(edgeList$to, customers$id), 2]
edgeList$ToLabel[is.na(edgeList$ToLabel)] <- 0

# Add the column edgeList$edgeType
edgeList$edgeType <- edgeList$FromLabel + edgeList$ToLabel
 
# Count the number of each type of edge
table(edgeList$edgeType)


# Count churn edges
ChurnEdges <- sum(edgeList$edgeType == 2)
 
# Count non-churn edges
NonChurnEdges <- sum(edgeList$edgeType == 0)
 
# Count mixed edges
MixedEdges <- sum(edgeList$edgeType == 1)
 
# Count all edges
edges <- ChurnEdges + NonChurnEdges + MixedEdges

#Print hte number of edges
edges


# Count the number of churn nodes
ChurnNodes <- sum(customers$churn == 1)
 
# Count the number of non-churn nodes
NonChurnNodes <- sum(customers$churn == 0)
 
# Count the total number of nodes
nodes <- ChurnNodes + NonChurnNodes
 
# Compute the network connectance
connectance <- 2 * edges / nodes / (nodes - 1)

# Print the value
connectance


# Compute the expected churn dyadicity
ExpectedDyadChurn <- ChurnNodes * (ChurnNodes - 1) * connectance / 2
 
# Compute the churn dyadicity
DyadChurn <- ChurnEdges / ExpectedDyadChurn
 
# Inspect the value
DyadChurn


# Compute the expected heterophilicity
ExpectedHet <- NonChurnNodes * ChurnNodes * connectance
 
# Compute the heterophilicity
Het <- MixedEdges / ExpectedHet
 
# Inspect the heterophilicity
Het

```
  
  
  
***
  
Chapter 3 - Network Featurization  
  
Basic Network Features:  
  
* Provided that the labels of the nodes depend on each other, they can be useful for predictive modeling  
* Can begin by getting neighborhood features  
	* degree(g)  # first order degree  
    * neighborhood.size(g, order=2)  # second order degree, or size of neighborhoods counting all 2+ away  
    * count_triangles(g)  # triangles that each node is part of (triangle has full connection among three nodes)  
* Can also look at the centrality features  
	* Betweenness - measure of how often the node is part of a quickest path between two other nodes  
    * betweenness(g)  
    * Closeness - measure of how easily a node can reach the other nodes  
    * closeness(g)  
* Can also look at the transitivity (clustering coefficient)  
	* transitivity(g,type = 'local')  # computed for each node separately  
    * Triangles vs. Triangles + Triads (triangles with a missing edge)  
  
Link-Based Features:  
  
* Link-based features are network features that depend on adjacencies  
	* The adjacency matrix has all nodes on both the x and y axis, with a 1 meaning they are linked by an edge  
    * A <- get.adjacency(g)  
    * Can then run the matrix multiplication (dot product) against (for example) a vector indicating preferences  
    * preference <- c(1,1,1,1,1,1,0,0,0,0)  
    * rNeighbors <- A %*% preference  
    * as.vector(rNeighbors)  
* Could instad get something like the average age of the neighbors  
	* age <- c(23,65,33,36,28,45,41,24,38,39)  
    * degree <- degree(g)  
    * averageAge <- A %*% age / degree  
  
Page Rank:  
  
* The page rank is the basis of search algorithms such as google  
* The page rank algorithm assumes that clicks on one page lead to the next page, and checks the degree of connectivity  
	* Page rank of pages that link to it  
    * Number of links that the referring pages include  
* The page rank formula for all pages simultaneously is PR = alpha * A * PR + (1 - alpha) * epsilon  
	* The alpha is the likelihood of a link being clicked, assumed to be uniform, assuming 85% as the default  
    * The A is the adjacency matrix  
    * The PR is the page rank, and is solved with either matrix inversions or iterations  
    * page.rank(g)  
    * page.rank(g, personalized = c(1,0,0,0,0,0,0,0,0,0))  # a personalized argument where the first node connections drive higher values  
  
Example code includes:  
```{r}

# Extract network degree
V(network)$degree <- degree(network, normalized=TRUE)

# Extraxt 2.order network degree
degree2 <- neighborhood.size(network, 2)

# Normalize 2.order network degree
V(network)$degree2 <- degree2 / (length(V(network)) - 1)

# Extract number of triangles
V(network)$triangles <- count_triangles(network)


# Extract the betweenness
V(network)$betweenness <- betweenness(network, normalized=TRUE)

# Extract the closeness
V(network)$closeness <- closeness(network, normalized=TRUE)

# Extract the eigenvector centrality
V(network)$eigenCentrality <- eigen_centrality(network, scale = TRUE)$vector


# Extract the local transitivity
V(network)$transitivity <- transitivity(network, type="local", isolates='zero')

# Compute the network's transitivity
transitivity(network)


# Extract the adjacency matrix
AdjacencyMatrix <- as_adjacency_matrix(network)

# Compute the second order matrix
SecondOrderMatrix <- AdjacencyMatrix %*% AdjacencyMatrix

# Adjust the second order matrix
SecondOrderMatrix_adj <- ((SecondOrderMatrix) > 0) + 0
diag(SecondOrderMatrix_adj) <- 0

# Inspect the second order matrix
SecondOrderMatrix_adj[1:10, 1:10]


# Compute the number of churn neighbors
V(network)$ChurnNeighbors <- as.vector(AdjacencyMatrix %*% V(network)$churn)

# Compute the number of non-churn neighbors
V(network)$NonChurnNeighbors <- as.vector(AdjacencyMatrix %*% (1 - V(network)$churn))

# Compute the relational neighbor probability
V(network)$RelationalNeighbor <- as.vector(V(network)$ChurnNeighbors / 
    (V(network)$ChurnNeighbors + V(network)$NonChurnNeighbors))


# Compute the number of churners in the second order neighborhood
V(network)$ChurnNeighbors2 <- as.vector(SecondOrderMatrix %*% V(network)$churn)

# Compute the number of non-churners in the second order neighborhood
V(network)$NonChurnNeighbors2 <- as.vector(SecondOrderMatrix %*% (1 - V(network)$churn))

# Compute the relational neighbor probability in the second order neighborhood
V(network)$RelationalNeighbor2 <- as.vector(V(network)$ChurnNeighbors2 / 
    (V(network)$ChurnNeighbors2 + V(network)$NonChurnNeighbors2))


degree <- degree(network)

# Extract the average degree of neighboring nodes
V(network)$averageDegree <- 
    as.vector(AdjacencyMatrix %*% V(network)$degree) / degree

# Extract the average number of triangles of neighboring nodes
V(network)$averageTriangles <- 
    as.vector(AdjacencyMatrix %*% V(network)$triangles) / degree

# Extract the average transitivity of neighboring nodes    
V(network)$averageTransitivity <-
    as.vector(AdjacencyMatrix %*% V(network)$transitivity) / degree

# Extract the average betweeness of neighboring nodes    
V(network)$averageBetweenness <- 
    as.vector(AdjacencyMatrix %*% V(network)$betweenness) / degree


# Compute one iteration of PageRank 
# iter1 <- page.rank(network, algo = 'power', options = list(niter = 1))$vector

# Compute two iterations of PageRank 
# iter2 <- page.rank(network, algo = 'power', options = list(niter = 2))$vector

# Inspect the change between one and two iterations
# sum(abs(iter1 - iter2))

# Inspect the change between nine and ten iterations
# sum(abs(iter9 - iter10))


# Create an empty vector
# value <- c()

# Write a loop to compute PageRank 
# for(i in 1:15){
#   value <- cbind(value, page.rank(network, algo = 'power',options = list(niter = i))$vector)
# }
  
# Compute the differences 
# difference <- colSums(abs(value[,1:14] - value[,2:15]))

# Plot the differences
# plot(1:14, difference)


# boxplots <- function(damping=0.85, personalized=FALSE){
#   if(personalized){
#     V(network)$pp<-page.rank(network,damping=damping,personalized = V(network)$Churn)$vector
#   }
#   else{
#   V(network)$pp<-page.rank(network,damping=damping)$vector
#   }
#   boxplot(V(network)$pp~V(network)$Churn)#
# }

# Look at the distribution of standard PageRank scores
# boxplots(damping = 0.85)

# Inspect the distribution of personalized PageRank scores
# boxplots(damping = 0.85, personalized = TRUE)

# Look at the standard PageRank with damping factor 0.2
# boxplots(damping = 0.2)

# Inspect the personalized PageRank scores with a damping factor 0.99
# boxplots(damping=0.99, personalized = TRUE)


# Compute the default PageRank score
# V(network)$pr_0.85 <- page.rank(network)$vector

# Compute the PageRank score with damping 0.2
# V(network)$pr_0.20 <- page.rank(network, damping=0.2)$vector

# Compute the personalized PageRank score
# V(network)$perspr_0.85 <- page.rank(network, damping=0.85, personalized = V(network)$Churn)$vector

# Compute the personalized PageRank score with damping 0.99
# V(network)$perspr_0.99 <- page.rank(network, damping=0.99, personalized = V(network)$Churn)$vector

```
  
  
  
***
  
Chapter 4 - Putting It All Together  
  
Extract Dataset:  
  
* May want to extract some of the features from the nodes in the igraph  
	* g  # prints the object  
    * as_data_frame(g,what='vertices')  # extracts to data frame - what= can be nodes (vertices) or edges  
* May want to preprocess, particularly for missing values (non-disclosed, non-relevant, processing error, etc.)  
	* sum(is.na(dataset$degree))  
* May want to understand the correlations among the node features  
	* M <- cor(dataset[,-1])  
    * corrplot::corrplot(M, method = 'circle')  
  
Building Predictive Models:  
  
* Building a predictive model using supervised learning  
* Can split the dataset from the graph in to a test and train dataset  
* Can use either logistic regression and random forests  
	* glm(R~degree+pageRank, dataset=training_set,family='binomial')  
    * library(randomForest)  
    * rfModel<-randomForest(R~., dataset=training_set)  
    * varImpPlot(rfModel)  
  
Evaluating Model Performance:  
  
* Can evaluate the model using the test dataset, along with the functions in the pROC library  
	* logPredictions <- predict(logModel, newdata = test_set, type = "response")  # probability based on the logistic regression  
    * rfPredictions<- predict(rfModel, newdata = test_set, type='prob')  # two column matrix, one for the probability of each of the factor states (two in this case)  
* Can evaluate the model based on AUC which is often between 0.5 (random) and 1.0 (perfect)  
	* library(pROC)  
    * auc(test_set$label, logPredictions)  
* Can also assess "top decile" lift - looks at actual churn among the top-10% churn probabilities  
	* library(lift)  
    * TopDecileLift(test_set$label, predictions, plot=TRUE)  
  
Wrap Up:  
  
* Accurately predicting labels for network data  
* Labeled networks that convert edgelists to networks using igraph  
* Homophily - idea that "birds of a feather flock together"  
	* Dyadicity - connectedness of nodes of same labels  
    * Heterophilicity - connectednedd of nodes of different labels  
* Can featurize the network using igraph  
* Can create datasets based on networks, then use that for modeling  
	* dataset <- as_data_frame(g, what='vertices')  
    * glm(R~., dataset=training_set, family='binomial')  
    * logPredictions <- predict(logModel, newdata=test_set, type="response")  
    * auc(test_set$label, logPredictions)  
    * TopDecileLift(test_set$label, predictions, plot=TRUE)  
  
Example code includes:  
```{r}

# Extract the dataset
dataset_full <- as_data_frame(network, what = "vertices")
dataset_full$Future <- 0
dsF1 <- c(404, 550, 41, 613, 48, 230, 294, 852, 93, 520, 617, 523, 714, 282, 705, 153, 995, 511, 204, 273, 194, 756, 979, 879, 843, 713, 837, 636, 469, 478, 938, 654, 751, 775)
dataset_full[match(dsF1, dataset_full$name), "Future"] <- 1

# Inspect the dataset
head(dataset_full)

# Remove customers who already churned
dataset_filtered <- dataset_full[-which(dataset_full$churn == 1), ]

# Remove useless columns
dataset <- dataset_filtered[, -c(1, 2)]


# Inspect the feature
summary(dataset$RelationalNeighbor2)

# Find the indeces of the missing values
toReplace <- which(is.na(dataset$RelationalNeighbor2))

# Replace the missing values with 0
dataset$RelationalNeighbor2[toReplace] <- 0

# Inspect the feature again
summary(dataset$RelationalNeighbor2)


# Generate the correlation matrix
M <- cor(dataset[,])

# Plot the correlations
corrplot::corrplot(M, method = "circle")

# Print the column names
colnames(dataset)

# Create toRemove
# toRemove <- c(10, 13, 19, 22)

# Remove the columns
# dataset <- dataset[, -toRemove]


# Set the seed
set.seed(7)

# Creat the index vector
index_train <- sample(1:nrow(dataset), round((2/3) * nrow(dataset), 0), replace=FALSE)

# Make the training set
training_set <- dataset[index_train,]

# Make the test set
test_set <- dataset[-index_train,]


# Make firstModel
firstModel <- glm(Future ~ degree + degree2 + triangles + betweenness + closeness + transitivity, 
                  family = "binomial", data = training_set
                  )

# Build the model
secondModel <- glm(Future ~ ChurnNeighbors + RelationalNeighbor + ChurnNeighbors2 + RelationalNeighbor2 + averageDegree + averageTriangles + averageTransitivity + averageBetweenness, 
                   family = "binomial", data = training_set
                   )

# Build the model
thirdModel <- glm(Future ~ ., data=training_set, family="binomial")


# Set seed
set.seed(863)

# Build model
rfModel <- randomForest::randomForest(as.factor(Future)~. ,data=training_set)

# Plot variable importance
randomForest::varImpPlot(rfModel)


# Predict with the first model
firstPredictions <- predict(firstModel, newdata = test_set, type = "response")

# Predict with the first model
secondPredictions <- predict(secondModel, newdata = test_set, type = "response")

# Predict with the first model
thirdPredictions <- predict(thirdModel, newdata = test_set, type = "response")

# Predict with the first model
rfPredictions<- predict(rfModel, newdata = test_set, type = "prob")

sapply(list(firstPredictions, secondPredictions, thirdPredictions, rfPredictions[, 2]), 
       FUN=function(x) { pROC::auc(test_set$Future, x) }
       )

```
  
  
***
  
  
###_Bayesian Regression Modeling with rstanarm_  
  
Chapter 1 - Introduction to Bayesian Linear Models  
  
Non-Bayesian Linear Regression:  
  
* Can use the "kidiq" dataset from the rstanarm package  
* Objective is to predict the child IQ from the mom IQ  
	* lm_model <- lm(kid_score ~ mom_iq, data = kidiq)  
    * summary(lm_model)  
    * broom::tidy(lm_model)  
* Challenge of the p-value is that it is only a comparison to the null hypothesis  
* Often interested in understanding the underlying population, as well the statistic calculated  
* Can use the "songs" dataset from Spotify  
  
Bayesian Linear Regression:  
  
* Bayesian methods sample from the posterior distribution, allowing for inferences about values the parameters might take  
* The "rstanarm" package allows a high-level interface to the Stan library  
	* library(rstanarm)  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * summary(stan_model)  
* There are several descriptive statistics in the output  
	* sigma: Standard deviation of errors  
    * mean_PPD: mean of posterior predictive samples  
    * log-posterior: analogous to a likelihood  
    * Rhat: a measure of within chain variance compared to across chain variance (stability of estimates, with a goal of less than 1.1 for all parameters)  
  
Comparing frequentist and Bayesian models:  
  
* The Bayesian and frequentist methods produce very similar outputs  
* The fundamental difference is that frequentists assume fixed parameters and random data while Bayesians assume fixed data and random parameters  
	* Frequentist - the p-value is the probability of a test statistics, given a specific null hypothesis  
    * Bayesian - probabilities of parameters vary in their ability to generate the given data  
* Bayesian approaches use the credible interval, which is very similar to the confidence interval  
	* Confidence interval: Probability that a range contains the true value  
    * Credible interval: Probability that the true value is within a range  
    * posterior_interval(stan_model)  # gives the credible intervals (90% by default)  
    * posterior_interval(stan_model, prob = 0.95)  # gives the 95% credible intervals  
* Can look at both the confidence intervals and the credible intervals for the same dataset and different models  
	* confint(lm_model, parm = "mom_iq", level = 0.95)  
    * posterior_interval(stan_model, pars = "mom_iq", prob = 0.95)  
* Can also look at probabilities that a parameter is between several key points  
	* posterior <- spread_draws(stan_model, mom_iq)  
    * mean(between(posterior_mom_iq, 0.60, 0.65))  # Bayesian methods allow for actual inferences about the parameter  
  
Example code includes:  
```{r eval=FALSE}

# Print the first 6 rows
head(songs)

# Print the structure
str(songs)


# Create the model here
lm_model <- lm(popularity ~ song_age, data = songs)

# Produce the summary
summary(lm_model)

# Print a tidy summary of the coefficients
tidy(lm_model)


# Create the model here
stan_model <- stan_glm(popularity ~ song_age, data = songs)

# Produce the summary
summary(stan_model)

# Print a tidy summary of the coefficients
tidy(stan_model)


# Create the 90% credible intervals
posterior_interval(stan_model)

# Create the 95% credible intervals
posterior_interval(stan_model, prob = 0.95)

# Create the 80% credible intervals
posterior_interval(stan_model, prob = 0.8)

```
  
  
  
***
  
Chapter 2 - Modifying a Bayesian Model  
  
What is in a Bayesian Model?  
  
* Many levers for modifying a Bayesian model  
* Posterior distributions are sampled in groups called chains (iterations) that begin in random areas  
	* Chains move to where there is a good combination of the priors and the parameters  
    * Lengths of the chains allow for more robust parameter estimates  
* Convergence is importance, since it ensures a consistent output - chains start at different places, and so the starting iterations (prior to convergence) are discarded  
    * The chains start with a few thousand warm-up or burn-in for each of the chains  
* The number of iterations is a balancing act - not enough (convergence problems) or too many (run-time problems)  
  
Prior Distributions:  
  
* Priors can inform the posterior distribution, given the same data in the experiment  
	* More informative priors typically have narrower distributions  
    * Can think of the prior as being like an additional data point - more meaningful the less data that we have  
    * Generally a best practice to use a weakly informative prior, absent good cause for a strong belief  
* Example of including prior distributions in rstanarm  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * prior_summary(stan_model){{1}}  
* Can calculate adjusted scales for the intercept as 10 * sd(y), since 10 is the default for the package  
* Can calculate adjusted scales for the coefficients as (2.5 / sd(x)) * sd(y), since 2.5 is the default for the package  
	* prior_summary(stan_model)  
* Can also use unadjusted priors, such as  
	* no_scale <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(autoscale = FALSE), prior = normal(autoscale = FALSE), prior_aux = exponential(autoscale = FALSE) )  
    * prior_summary(no_scale)  
  
User-Specified Priors:  
  
* Can use a specified prior distribution using the same arguments as in the previous paragraph  
	* Research may suggest the parameter should be near a specific value - take advantage of narrow prior  
    * Parameters may be constrained, such as a need to be always positive  
* Can specify the priors to be cast to specific values  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 0, scale = 10), prior = normal(location = 0, scale = 2.5), prior_aux = exponential(rate = 1) )  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE), prior = normal(location = 0, scale = 2.5, autoscale = FALSE), prior_aux = exponential(rate = 1, autoscale = FALSE) )  
    * The autoscale=FALSE is needed so that stan_glm does not rescale the data before applying the priors  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 3, scale = 2), prior = cauchy(location = 0, scale = 1), )  
* There are many types of prior distributions that can be used  
	* ?priors  
* Can also set a flat prior (prior provides zero information)  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = NULL, prior = NULL, prior_aux = NULL )  
    * This is usually a bad idea; rarely in a state where there is zero information, and a weakly informative prior with an adjusted scale is typically better  
  
Tuning Models for Stability:  
  
* May need to alter estimation parameters  
* Divergent transitions are when the steps are too big - need to take smaller steps (longer run time)  
	* stan_model <- stan_glm(popularity ~ song_age, data = songs, control = list(adapt_delta = 0.95))  # 0.95 is default, increase will decrease step size  
* May have already reached the maximum tree depth in some of the chains (indicates poor efficiency; no good stopping place, and insufficient sampling of the posterior)  
	* stan_model <- stan_glm(popularity ~ song_age, data = songs, control = list(max_treedepth = 10))  # 10 is the default, increases allow more sampling  
  
Example code includes:  
```{r eval=FALSE}

# 3 chains, 1000 iterations, 500 warmup
model_3chains <- stan_glm(popularity ~ song_age, data = songs,
    chains = 3, iter = 1000, warmup = 500)

# Print a summary of model_3chains
summary(model_3chains)

# 2 chains, 100 iterations, 50 warmup
model_2chains <- stan_glm(popularity ~ song_age, data = songs,
    chains = 2, iter = 100, warmup = 50)

# Print a summary of model_1chain
summary(model_2chains)


# Estimate the model
stan_model <- stan_glm(popularity ~ song_age, data = songs)

# Print a summary of the prior distributions
prior_summary(stan_model)


# Calculate the adjusted scale for the intercept
10 * sd(songs$popularity)

# Calculate the adjusted scale for `song_age`
(2.5 / sd(songs$song_age)) * sd(songs$popularity)

# Calculate the adjusted scale for `valence`
(2.5 / sd(songs$valence)) * sd(songs$popularity)


# Estimate the model with unadjusted scales
no_scale <- stan_glm(popularity ~ song_age, data = songs,
    prior_intercept = normal(autoscale = FALSE),
    prior = normal(autoscale = FALSE),
    prior_aux = exponential(autoscale = FALSE)
)

# Print the prior summary
prior_summary(no_scale)


# Estimate a model with flat priors
flat_prior <- stan_glm(popularity ~ song_age, data = songs,
    prior_intercept = NULL, prior = NULL, prior_aux = NULL)

# Print a prior summary
prior_summary(flat_prior)


# Estimate the model with an informative prior
inform_prior <- stan_glm(popularity ~ song_age, data = songs,
    prior = normal(location = 20, scale = 0.1, autoscale = FALSE))

# Print the prior summary
prior_summary(inform_prior)


# Estimate the model with a new `adapt_delta`
adapt_model <- stan_glm(popularity ~ song_age, data = songs,
  control = list(adapt_delta = 0.99))

# View summary
summary(adapt_model)

# Estimate the model with a new `max_treedepth`
tree_model <- stan_glm(popularity ~ song_age, data = songs,
  control = list(max_treedepth = 15))

# View summary
summary(tree_model)

```
  
  
  
***
  
Chapter 3 - Assessing Model Fit  
  
Using R-Squared Statistics:  
  
* The R-squared statistic is a measure of how well the model predicts the dependent variable (proportion of variance explained) - "coefficient of determination"  
	* R-squared = 1 - RSS/TSS where RSS is residual sum-squares and TSS is total sum-squares  
    * lm_model <- lm(kid_score ~ mom_iq, data = kidiq)  
    * lm_summary <- summary(lm_model)  
    * lm_summary$r.squared  
    * ss_res <- var(residuals(lm_model))  
    * ss_total <- var(residuals(lm_model)) + var(fitted(lm_model))  
    * 1 - (ss_res / ss_total)  
* The R-squared is not save by the stan_glm() call, but can be calculated  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * ss_res <- var(residuals(stan_model))  
    * ss_total <- var(fitted(stan_model)) + var(residuals(stan_model))  
    * 1 - (ss_res / ss_total)  
  
Posterior Predictive Model Checks:  
  
* Can use posterior distributions to check the model  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * spread_draws(stan_model, `(Intercept)`, mom_iq) %>% select(-.draw)  
    * predictions <- posterior_linpred(stan_model)  # predicted scores using each of the sets of parameter values and each data point  
* Can compare the distributions of predicted and observed scores  
	* iter1 <- predictions[1,]  
    * iter2 <- predictions[2,]  
    * summary(kidiq$kid_score)  
    * summary(iter1)  
    * summary(iter2)  
  
Model Fit with Posterior Predictive Model Checks:  
  
* Can use Bayesian functions as part of the post-processing  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * r2_posterior <- bayes_R2(stan_model)  
    * summary(r2_posterior)  
    * quantile(r2_posterior, probs = c(0.025, 0.975))  
    * hist(r2_posterior)  
    * pp_check(stan_model, "dens_overlay")  # compare densities  
    * pp_check(stan_model, "stat")  # compare statistic to histogram  
* The mean is only one aspect - can look at many aspects of the dependent variable  
	* pp_check(stan_model, "stat_2d")  # mean and sd plotted on a 2-D plot  
  
Bayesian Model Comparisons:  
  
* Can compare two or more models produced using rstanarm  
* The LOO (leave one out) package runs a modified (approximated) form of LOO cross-validation  
	* library(loo)  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * loo(stan_model)  
* The values from LOO mainly have value in comparison to similar models (much like ANOVA for comparing nested linear regression)  
	* model_1pred <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * model_2pred <- stan_glm(kid_score ~ mom_iq * mom_hs, data = kidiq)  
    * loo_1pred <- loo(model_1pred)  
    * loo_2pred <- loo(model_2pred)  
    * compare(loo_1pred, loo_2pred)  
* The compare() function provides the difference in loo, along with an SE  
	* Positive means "prefer second model"  
    * Negative means "prefer first model"  
    * The SE helps assess whether it is meaningful - rule of thumb is to require change in loo greater than SE, otherwise prefer the simpler model  
  
Example code includes:  
```{r eval=FALSE}

# Print the R-squared from the linear model
lm_summary$r.squared

# Calulate sums of squares
ss_res <- var(residuals(lm_model))
ss_fit <- var(fitted(lm_model))

# Calculate the R-squared
1 - (ss_res / (ss_res + ss_fit))


# Save the variance of residulas
ss_res <- var(residuals(stan_model))

# Save the variance of fitted values
ss_fit <- var(fitted(stan_model))

# Calculate the R-squared
1 - (ss_res / (ss_res + ss_fit))


# Calculate posterior predictive scores
predictions <- posterior_linpred(stan_model)

# Print a summary of the observed data
summary(songs$popularity)

# Print a summary of the 1st replication
summary(predictions[1,])

# Print a summary of the 10th replication
summary(predictions[10,])


# Calculate the posterior distribution of the R-squared
r2_posterior <- bayes_R2(stan_model)

# Make a histogram of the distribution
hist(r2_posterior)


# Create density comparison
pp_check(stan_model, "dens_overlay")

# Create scatter plot of means and standard deviations
pp_check(stan_model, "stat_2d")


# Estimate the model with 1 predictor
model_1pred <- stan_glm(popularity ~ song_age, data = songs)

# Print the LOO estimate for the 1 predictor model
loo(model_1pred)

# Estimate the model with both predictors
model_2pred <- stan_glm(popularity ~ song_age * artist_name, data = songs)

# Print the LOO estimates for the 2 predictor model
loo(model_2pred)

```
  
  
  
***
  
Chapter 4 - Presenting and Using Bayesian Regression  
  
Visualizing Bayesian Models:  
  
* Can save the model coefficients using tidy() and then plot the regression against the underlying point  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * tidy(stan_model)  
    * tidy_coef <- tidy(stan_model)  
    * model_intercept <- tidy_coef$estimate[1]  
    * model_slope <- tidy_coef$estimate[2]  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point() + geom_abline(intercept = model_intercept, slope = model_slope,)  
* Can also plot uncertainty using the posterior distribution  
	* draws <- spread_draws(stan_model, `(Intercept)`, mom_iq)  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point()  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point() geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = mom_iq), size = 0.2, alpha = 0.1, color = "skyblue")  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point() geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = mom_iq), size = 0.2, alpha = 0.1, color = "skyblue") + geom_abline(intercept = model_intercept, slope = model_slope)  
  
Making Predictions:  
  
* Can make predictions for the observed data  
	* stan_model <- stan_glm(kid_score ~ mom_iq + mom_hs, data = kidiq)  
    * posteriors <- posterior_predict(stan_model)  
    * predict_data <- data.frame( mom_iq = 110, mom_hs = c(0, 1) )  
    * new_predictions <- posterior_predict(stan_model, newdata = predict_data)  
  
Visualizing Predictions:  
  
* Can plot predictions based on the new data  
	* stan_model <- stan_glm(kid_score ~ mom_iq + mom_hs, data = kidiq)  
    * predict_data <- data.frame( mom_iq = 110, mom_hs = c(0, 1) )  
    * posterior <- posterior_predict(stan_model, newdata = predict_data)  
    * posterior <- as.data.frame(posterior)  
    * colnames(posterior) <- c("No HS", "Completed HS")  
    * plot_posterior <- gather(posterior, key = "HS", value = "predict")  
    * ggplot(plot_posterior, aes(x = predict)) + facet_wrap(~ HS, ncol = 1) + geom_density()  
  
Conclusion:  
  
* One solution to inferences - implementing Bayesian models  
    * Differences between frequentist and Bayesian  
    * Importance of making correct inferences  
* Modifying a Bayesian model  
* Evaluating fit of a Bayesian model  
* Using the model to make predictions and communicate results  
* Additional topics for exploration include  
  
Example code includes:  
```{r eval=FALSE}

# Save the model parameters
tidy_coef <- tidy(stan_model)

# Extract intercept and slope
model_intercept <- tidy_coef$estimate[1]
model_slope <- tidy_coef$estimate[2]

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
  geom_point() +
  geom_abline(intercept = model_intercept, slope = model_slope)


# Save the values from each draw of the posterior distribution
draws <- spread_draws(stan_model, `(Intercept)`, `song_age`)

# Print the `draws` data frame to the console
draws

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
  geom_point()

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
    geom_point() +
    geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age), 
                size = 0.1, alpha = 0.2, color = "skyblue"
                )

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
    geom_point() +
    geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age), 
                size = 0.1, alpha = 0.2, color = "skyblue"
                ) +
    geom_abline(intercept = model_intercept, slope = model_slope)

# Estimate the regression model
stan_model <- stan_glm(popularity ~ song_age + artist_name, data = songs)

# Print the model summary
summary(stan_model)

# Get posteriors of predicted scores for each observation
posteriors <- posterior_predict(stan_model)

# Print 10 predicted scores for 5 songs
posteriors[1:10, 1:5]


# Create data frame of new data
predict_data <- data.frame(song_age = 663, artist_name = "Beyonc")

# Create posterior predictions for Lemonade album
new_predictions <- posterior_predict(stan_model, newdata = predict_data)

# Print first 10 predictions for the new data
new_predictions[1:10, ]

# Print a summary of the posterior distribution of predicted popularity
summary(new_predictions[, 1])


# View new data predictions
new_predictions[1:10, ]

# Convert to data frame and rename variables
new_predictions <- as.data.frame(new_predictions)
colnames(new_predictions) <- c("Adele", "Taylor Swift", "Beyonc")

# Create tidy data structure
plot_posterior <- gather(new_predictions, key = "artist_name", value = "predict")

# Print formated data
head(plot_posterior)


# Create plot of 
ggplot(plot_posterior, aes(x = predict)) +
    facet_wrap(~ artist_name, ncol = 1) +
    geom_density()

```
  
  
  
***
  
###_ChIP-seq Workflows in R_  
  
Chapter 1 - Introduction to ChIP-seq  
  
What is ChIP-seq?  
  
* The core of ChIP-seq is understanding how the cells in the body know what to do  
* The function of a cell is largely determined by the expressed genes (DNA - RNA - Proteins)  
	* Inhibitors will stop the process  
    * Regualtors help keep the cell producing the right proteins - disorders lead to diseases like cancer  
* Can use ChIP-seq to find over-represented genes in various patient cohorts (such as cancer patients)  
* Dataset for the course will be prostate cancer - 5 primary tumors, 3 treatment resistent  
	* library(GenomicAlignments)  
    * reads <- readGAlignments('file_name')  
    * seqnames(reads)  # obtaining read coordinates  
    * start(reads)  # obtaining read coordinates  
    * end(reads)  # obtaining read coordinates  
    * coverage(reads)  # computing coverage  
* Can also access the peack cells in the ChIP-seq experiment  
	* library(rtracklayer)  
    * peaks <- import.bed('file_name')  
    * chrom(peaks)  
    * ranges(peaks)  
    * score(peaks)  
  
ChIP-seq Workflow:  
  
* First step of the workflow is read mapping - mapping reads to genomes  
* Then, create a coverage profile (number of overlapping reads)  
* Then, import the mapped reads and verify the quality (these are usually the first steps in R - the above steps are usually in more specialized software)  
* Then, identify peaks by comparing samples - AR sample sites that are over-used  
* Then, interpret the findings for a better understanding of the associated biological processes  
	* Heat maps - differences in cells and similarities in cells  
    * Creating UpSet plots, such as upset(fromList(peak_sets))  
  
ChIP-seq Results Summary:  
  
* The heatmap plot is useful for assessing sample quality - check for expected patterns  
* Heights of individual peaks across samples can be more informative - height of peak by group can be particularly insightful  
  
Example code includes:  
```{r eval=FALSE}

# Print a summary of the 'reads' object
print(reads)

# Get the start position of the first read
start_first <- start(reads)[1]

# Get the end position of the last read
end_last <- end(reads)[length(reads)]

# Compute the number of reads covering each position in the selected region
cvg <- coverage(reads)


# Print a summary of the 'peaks' object
print(peaks)

# Use the score function to find the index of the highest scoring peak
max_idx <- which.max(score(peaks))

# Extract the genomic coordinates of the highest scoring peak using the `chrom` and `ranges` functions
max_peak_chrom <- chrom(peaks)[max_idx]
max_peak_range <- ranges(peaks)[max_idx]


# Create a vector of colors to label groups (there are 2 samples per group)
group <- c(primary = rep("blue", 2), TURP = rep("red", 2))

# Plot the sample correlation matrix `sample_cor` as a heat map
heatmap(sample_cor, ColSideColors = group, RowSideColors = group, 
        cexCol = 0.75, cexRow = 0.75, symm = TRUE)

# Create a heat map of peak read counts
heatmap(read_counts, ColSideColors = group, labRow = "", cexCol = 0.75)


# Take a look at the full gene sets
print(ar_sets)

# Visualise the overlap between the two groups using the `upset` function
upset(fromList(ar_sets))

# Print the genes with differential binding
print(db_sets)

# Visualise the overlap between the two groups using the `upset` function
upset(fromList(db_sets))

```
  
  
  
***
  
Chapter 2 - Back to Basics - Preparing ChIP-seq Data  
  
Importing Data:  
  
* Read mapping and peak calling are typically carried out with specialized (non-R) tools and then stored in a BAM (Binary Sequence Alignment Map) format  
* Can use Rsamtools package to interact with BAM files  
	* Rsamtools provides functions for indexing, reading, filtering and writing of BAM files  
* Use readGAlignments to import mapped reads  
	* library(GenomicAlignments)  
    * reads <- readGAlignments(bam_file)  
* Use BamViews to define regions of interest  
	* library(GenomicRanges)  
    * library(Rsamtools)  
    * ranges <- GRanges(...)  
    * views <- BamViews(bam_file, bamRanges=ranges)  
* Use import.bed to load peak calls from a BED file  
	* library(rtracklayer)  
    * peaks <- import.bed(peak_bed, genome="hg19")  
    * bams <- BamViews(bam_file, bamRanges=peaks)  
    * reads <- readGAlignments(bams)  
  
Closer Look at Peaks:  
  
* Can use Gvix to combine data from multiple sources in to a single plot  
	* Where are the reads located, and the associated coverage  
    * Annotations of key features of read coverage  
* Need to load the key library and set context for data to be plotted  
	* library(Gviz)  
    * ideogram <- IdeogramTrack("chr12", "hg19")  
    * axis <- GenomeAxisTrack()  
    * plotTracks(list(ideogram, axis), from=101360000, to=101380000)  
* Can then add data to the track  
	* cover_track <- DataTrack(cover_ranges,window=100000,type='h',name="Coverage")  
    * plotTracks(list(ideogram, cover_track, axis), from=101360000, to=101380000)  
    * peak_track <- AnnotationTrack(peaks, name="Peaks")  
    * plotTracks(list(ideogram, cover_track, peak_track, axis), from=101360000, to=101380000)  
    * library(TxDb.Hsapiens.UCSC.hg19.knownGene)  
    * tx <- GeneRegionTrack(TxDb.Hsapiens.UCSC.hg19.knownGene, chromosome="chr12", start=101360000, end=101380000, name="Genes")  
    * plotTracks(list(ideogram, cover_track, peak_track, tx, axis), from=101360000, to=101380000)  
  
Cleaning ChIP-seq Data:  
  
* Incorrectly mapped reads can produce false peaks - "genomic repeats"  
	* Particularly problematic if the reference and the sample differ in the number of false reads  
    * Low complexity regions (such as end of chromosomes) tend to have poorer quality  
* Amplification bias can be a concern - just prior to sequencing  
* Quality Control reports can help diagnose the potential issues  
	* library(ChIPQC)  
    * qc_report <- ChIPQC(experiment="sample_info.csv", annotation="hg19")  
    * ChIPQCreport(qc_report)  
    * Input is a CSV file mapping samples to input files and descriptions  
* Cleaning the data has many steps  
	* Remove duplicate reads  
    * Remove reads with multiple hits or low mapping quality  
    * Remove peaks in "blacklisted" regions -- available from the ENCODE project  
  
Assessing Enrichment:  
  
* Reading finds the ends where a protein is created, meaning there will also be more than one read representing the ends of the fragment (?)  
* Can be helpful to do read coverage forward and backwards and to then aggregate findings  
	* reads <- readGAlignments(bam)  
    * reads_gr <- granges(reads[[1]])  
    * frag_length <- fragmentlength(qc_report)["GSM1598218"]  
    * reads_ext <- resize(reads_gr, width=frag_length)  
    * cover_ext <- coverage(reads_ext)  
* Question is how does coverage look in peaks relative to the rest of the genome  
	* bins <- tileGenome(seqinfo(reads), tilewidth=200, cut.last.tile.in.chrom=TRUE)  # create 200 bins along the genome  
    * peak_bins_overlap <- findOverlaps(bins, peaks)  # can find the overlaps  
    * peak_bins <- bins[from(peak_bins_overlap), ]  # subset to just the overlaps  
    * peak_bins$score <- countOverlaps(peak_bins, reads)  # count number of overlapping reads  
* Can create a function for the binning reads process  
	* count_bins <- function(reads, target, bins){  
    *   # Find all bins overlapping peaks  
    *   overlap <- from(findOverlaps(bins, target))
    *   target_bins <- bins[overlap, ]  
    *   # Count the number of reads overlapping each peak bin  
    *   target_bins$score <- countOverlaps(target_bins, reads)  
    * target_bins  
    * }  
* Can find coverage for the blacklisted regions in much the same way  
	* peak_bins <- count_bins(reads_ext, peaks, bins)  
    * bl_bins <- count_bins(reads_ext, blacklist.hg19, bins)  
* Can then get the background coverage by considering all of the remaining bins  
	* bkg_bins <- subset(bins, !bins %in% peak_bins & !bins %in% bl_bins)  
    * bkg_bins$score <- countOverlaps(bkg_bins, reads_ext)  
  
Example code includes:  
```{r eval=FALSE}

# Load reads form chr20_bam file
reads <- readGAlignments(chr20_bam)

# Create a `BamViews` object for the range 29805000 - 29820000 on chromosome 20
bam_views <- BamViews(chr20_bam, bamRanges=GRanges("chr20", IRanges(start=29805000, end=29820000)))

# Load only the reads in that view
reads_sub <- readGAlignments(bam_views)

# Print the `reads_sub` object
str(reads_sub)


# Load peak calls from chr20_peaks
peaks <- import.bed(chr20_peaks, genome="hg19")

# Create a BamViews object
bam_views <- BamViews(chr20_bam, bamRanges=peaks)

# Load the reads
reads <- readGAlignments(bam_views)


# Create tracks
peak_track <- AnnotationTrack(peak_calls, name="Peaks")
cover_track <- DataTrack(cover_ranges, window=10500, type="polygon", name="Coverage", fill.mountain=c("lighgrey", "lightgrey"), col.mountain="grey")

# Highlight peak locations across tracks
peak_highlight <- HighlightTrack(trackList = list(cover_track, peak_track), range = peak_calls)

# Produce plot
plotTracks(list(ideogram, peak_highlight, GenomeAxisTrack()), chromosome="chr20", from=start_pos, to=end_pos)


# Load reads with mapping qualities by requesting the "mapq" entries
reads <- readGAlignments(bam_file, param=ScanBamParam(what="mapq"))

# Identify good quality alignments
high_mapq <- mcols(reads)$mapq >= 20

# Examine mapping quality distribution for high and low quality alignments
boxplot(mcols(reads)$mapq ~ high_mapq, xlab="good quality alignments", ylab="mapping quality")

# Remove low quality alignments
reads_good <- subset(reads, high_mapq)

```
  
  
  
***
  
Chapter 3 - Comparing ChIP-seq Samples  
  
Introduction to Differential Binding:  
  
* Objective is to find differences in prostate cancercells that respond or are resistant to treatment  
* Can use PCA to find differences in the groups - directions of the components with the most variation  
	* Points can be projected on to the plane  
    * qc_result <- ChIPQC("sample_info.csv", "hg19")  
    * counts <- dba.count(qc_results, summits=250)  
    * plotPrincomp(counts)  
* Can use hierarchical clustering to cluster similar samples for easier future visualization  
	* distance <- dist(t(coverage))  
    * dendro <- hclust(distance)  
    * plot(dendro)  
    * dba.plotHeatmap(peaks, maxSites = peak_count, correlations = FALSE)  
  
Testing for Differential Binding:  
  
* Can use the DiffBind package for either DESeq2 or edgeR  
* Can count the records in a peak dataset  
	* peak_counts <- dba.counts(qc_output, summits=250)  
* Can add a contrast for how the sample should be split  
	* peak_counts <- dba.contrast(peak_counts, categories = DBA_CONDITION)  
* Want to assess the coverage differences in comparison to a control sequence  
	* bind_diff <- dba.analyze(peak_counts)  
	* dba.plotPCA(bind_diff, DBA_Condition, contrast=1)  
    * dba.plotHeatmap(bind_diff, DBA_Condition, contrast=1)  
  
Closer Look at Differential Binding:  
  
* Plotting is a good way to look at the results - can use the DiffBind library  
* Can create MA plots - x-axis for concentration, y-axis for resistant vs. log-fold-change of responsive  
	* dba.plotMA(dba_object)  # MA plots  
    * May need to normalize data in the MA plot to eliminate systemic downward bias  
* Can create volcano plots - FDR as a function of log-fold-change  
	* dba.plotVolcano(dba_object)  
* Can create box plots  
	* dba.plotBox(dba_object)  
  
Example code includes:  
```{r eval=FALSE}

# Compute the pairwise distances between samples using `dist`
cover_dist <- dist(t(cover))

# Use `hclust()` to create a dendrogram from the distance matrix
cover_dendro <- hclust(cover_dist)

# Plot the dendrogram
plot(cover_dendro)


# Print the `peaks` object
print(peaks)

# Obtain the coordinates of the merged peaks
merged_peaks <- peaks$merged

# Extract the number of peaks present in the data
peak_count <- nrow(merged_peaks)

# Create a heatmap using the `dba.plotHeatmap()` function
dba.plotHeatmap(peaks, maxSites = peak_count, correlations = FALSE)


# Examine the ar_binding object
print(ar_binding)

# Identify the category corresponding to the tumor type contrast
contrast <- DBA_CONDITION

# Establish the contrast to compare the two tumor types
ar_binding <- dba.contrast(ar_binding, categories=contrast, minMembers=2)

# Examine the ar_binding object again to confirm that the contrast has been added
print(ar_binding)


# Examine the `ar_binding` object to confirm that it contains the required contrast
print(ar_binding)

# Run the differential binding analysis
ar_diff <- dba.analyze(ar_binding)

# Examine the result
print(ar_diff)


# Create a PCA plot using all peaks
dba.plotPCA(ar_diff, DBA_CONDITION)

# Create a PCA plot using only differentially bound peaks
dba.plotPCA(ar_diff, DBA_CONDITION, contrast = 1)

# Create a heatmap using all peaks
dba.plotHeatmap(ar_diff, DBA_CONDITION, correlations = FALSE, maxSites = 440)

# Create a heatmap using only differentially bound peaks
dba.plotHeatmap(ar_diff, DBA_CONDITION, contrast=1, correlations = FALSE)


# Create an MA plot
dba.plotMA(ar_diff)


# Create a volcano plot
dba.plotVolcano(ar_diff)


# Create a box plot of the peak intensities
compare_groups <- dba.plotBox(ar_diff, notch=FALSE)

# Inspect the returned p-values
print(compare_groups)

```
  
  
  
***
  
Chapter 4 - From Peaks to Genes to Function  
  
Interpreting ChIP-seq Peaks:  
  
* Want to find the genes that impact a particular binding site - no way to know for sure, but often look for "closest" genes  
	* Obtain information about gene locations  
    * Assign peaks to genes  
    * Identify genes associated with stronger peaks in one of the conditions  
* Transcript annotations are helpful  
	* library(TxDb.Hsapiens.UCSC.hg19.knownGene)  
    * genes(TxDb.Hsapiens.UCSC.hg19.knownGene)  
    * library(org.Hs.eg.db)  # may be easier for reading/intepreting  
    * select(org.Hs.eg.db, keys=gene_id, columns="SYMBOL", keytype="ENTREZID")  
* Gene symbols are tricky - can have multiple genes given the same symbol  
* Can annotate peaks using the transcript data  
	* library(ChIPpeakAnno)  
    * annoPeaks(peaks, human_genes, bindingType="startSite", bindingRegion=c(-5000,5000))  
    * library(DiffBind)  
    * dba.plotVenn(peaks, mask=1:2)  
* Can use UpSet plots for better interpretation in larger datasets  
	* library(UpSetR)  
    * called_peaks <- as.data.frame(peaks$called)  
    * upset(called_peaks, sets=colnames(peaks$called), order.by='freq')  
  
Interpreting Gene Lists:  
  
* Can use a Gene Set image for very long lists of genes - associations with genes of interest  
	* library(chipenrich)  
	* chipenrich(peaks, genome='hg19', genesets = 'hallmark', locusdef = 'nearest_tss')  
  
Advanced ChIP-seq Analyses:  
  
* Loading and analyzing ChIP-seq data in R  
* First step is to import data and then visualize read coverage, peaks, etc.  
* Need to run quality control procedures  
* Can investigate differential binding with DiffBind  
* Can continue to explore additional datasets  
	* Find datasets to explore at GEO https://www.ncbi.nlm.nih.gov/geo/  
    * The data you worked with in this course is available under accession GSE65478  
    * Bioconductor website: https://bioconductor.org/  
    * Bioconductor support: https://support.bioconductor.org/  
  
Example code includes:  
```{r eval=FALSE}

# Extract peaks from ChIPQCexperiment object
peak_calls <- peaks(ar_calls)

# Only keep samples that passed QC
peak_passed <- peak_calls[qc_pass]

# Find overlaps between peak sets
peaks_combined <- findOverlapsOfPeaks(peak_passed[[1]], peak_passed[[2]], 
                                      peak_passed[[3]], peak_passed[[4]], 
                                      maxgap=50
                                      )

# Examine merged peak set
print(peaks_combined)


# Annotate peaks with closest gene
peak_anno <- annoPeaks(peaks_merged, human_genes, bindingType="startSite", bindingRegion=c(-5000,5000))

# How many peaks were found close to genes?
length(peak_anno)

# Where are peaks located relative to genes?
table(peak_anno$insideFeature)


# Create Venn diagram
dba.plotVenn(ar_diff, mask=1:4)

# Convert the matrix of called peaks into a data frame
called_peaks <- as.data.frame(ar_diff$called)

# Create UpSet plot
upset(called_peaks, keep.order = TRUE, sets=colnames(ar_diff$called), order.by="freq")


# Select all peaks with higher intensity in treatment resistant samples
turp_peaks <- peaks_binding[, "GSM1598218"] + peaks_binding[, "GSM1598219"] < peaks_binding[, "GSM1598223"] + peaks_binding[, "GSM1598225"]

# Run enrichment analysis
enrich_turp <- chipenrich(peaks_comb[turp_peaks, ], genome="hg19", 
                   genesets = "hallmark", out_name = NULL, 
                   locusdef = "nearest_tss", qc_plots=FALSE)

# Print the results of the analysis
print(enrich_turp$results)


# Examine the top gene sets
head(enrich_primary$results)

# Extract the gene IDs for the top ranking set
genes <- enrich_primary$results$Geneset.Peak.Genes[1]

# Split gene IDs into a vector
genes_split <- strsplit(genes, ', ')[[1]]

# Convert gene IDs to gene symbols
gene_symbol <- select(org.Hs.eg.db, keys=genes_split, columns="SYMBOL", keytype="ENTREZID")

# Print the result
print(gene_symbol)


# This is the base URL for all KEGG pathways
base_url <- "https://www.kegg.jp/pathway/"

# Add pathway ID to URL
path_url <- paste0(base_url, top_path, collapse="+")

# Collapse gene IDs into selection string
gene_select <- paste(genes, collapse="+")

# Add gene IDs to URL
path_url <- paste(path_url, gene_select, sep="+")

```
  
  
  
***
  
###_Designing and Analyzing Clinical Trials in R_  
  
Chapter 1 - Principles  
  
Fundamentals:  
  
* Clinical trials are scientific experiments used to evaluate the safety and efficacy of one or more treatments in humans  
	* Pharma, medical devices, medical procedures, etc.  
* Four general phases of clinical trials  
	* Phase I - small group of healthy volunteers to look for effects and side effects  
    * Phase II - small groups of patients with the disease - optimal doses for safety and efficiacy  
    * Phase III - chosen dose evaluated for efficacy and safety against a control  
    * Phase IV - post-marketing surveillance  
* Randomized control trials are considered the gold-standard for treatment - reduces the impact of confoudning variables (such as in cohort studies)  
	* Aim is to have similar patient charcteristics in each of the groups  
    * Blinding is when the patient does not know what they are receiving  
    * Double-blinding is when the researcher and the patient BOTH do not know what they are receiving  
  
Types of Data and Endpoints:  
  
* Clinical trials are highly regulated and must meet various international standards  
* Clinical measures are pre-defined in the protocol (endpoints) and can be a mix of primary or secondary measures related to both efficacy and safety  
* Endpoints can be continuous or categorical - example of continuous  
	* ggplot(data=exercise, aes(x=sbp_change)) + geom_histogram(fill="white", color="black") + xlab("SBP Change, mmHg")  
    * exercise %>% summarise(mean_sbp = mean(sbp_baseline), sd_spb = sd(sbp_baseline))  
* Endpoints can also be categorical, such as did the patient recover in 30 days  
	* finaldata %>% group_by(treatment) %>% filter(!is.na(response)) %>% summarise (n = n()) %>% mutate(prop = n / sum(n))  
    * table(finaldata$response, finaldata$treatment)  
* While binary and continuous endpoints are both common, there are other types also  
	* Discrete values, such as drinks per week or years to progression  
  
Basic Statistical Analysis:  
  
* There is typically a target population (all patients with a disease), a sample population drawn from the target population, and inferences drawn about the target population from the sample population  
* Hypothesis testing is typically run against a null hypothesis - alternative hypothesis can be one-sided or two-sided  
	* The p-value is the probability of observing something at least as extreme as our data if the null hypothesis is true - typical hurdles are set at 0.05  
* For comparing distributions, can use the Wilcox test or the Chi-squared test  
	* wilcox.test(outcome.variable~ group.variable, data=dataset)  
    * table1<-table(care.trial$group, care.trial$recover)  
    * prop.test(table1, correct=FALSE)  
  
Example code includes:  
```{r}

Acupuncture <- readRDS("./RInputFiles/Ex1_1_1.Rds")

#Explore the Acupuncture dataset with the str() function 
str(Acupuncture)

#Display the treatment group frequencies
table(Acupuncture$treatment.group)


#Generate summaries of the variables by treatment group and save results as baselines
baselines <- compareGroups::compareGroups(treatment.group ~ score.baseline + age + sex, data = Acupuncture)

#Use the createTable function to display the results saved in baselines
baseline.table <- compareGroups::createTable(baselines, show.ratio = FALSE, show.p.overall=FALSE)

#Display the created summary table
baseline.table


#Generate a variable for the change from baseline at 12 months
Acupuncture$diff.month12 <- Acupuncture$score.month12 - Acupuncture$score.baseline

#Use the new variable to generate the percentage change from baseline at 12 months
Acupuncture$pct.month12 <- Acupuncture$diff.month12 / Acupuncture$score.baseline * 100

#Generate a histogram for percentage change from baseline within each treatment group
ggplot(data=Acupuncture, aes(x=pct.month12)) + 
  geom_histogram(fill="white", color="black") + facet_wrap( ~ treatment.group) +
  xlab("Percentage Change from Baseline at Month 12")


#Generate the binary response variable. 
Acupuncture$resp35.month12 <- ifelse(Acupuncture$pct.month12 < (-35), 1, 0)

#Encode this new variable as a factor.
Acupuncture$resp35.month12 <- factor(Acupuncture$resp35.month12, 
                                     levels = c(1,0), 
                                     labels=c("greater than 35%", "less than or eq to 35%")
                                     )

#Tabulate the numbers and percentages of patients in each category. 
Acupuncture %>% 
  group_by(resp35.month12) %>% 
  filter(!is.na(resp35.month12)) %>%
  summarise(n = n()) %>% 
  mutate(pct = n / sum(n)*100)


#Dichotomize the variable for complementary therapist visits into 0 or at least 1 visit.
Acupuncture$any.therap.visits <- ifelse(Acupuncture$total.therap.visits == 0, 0, 1)

#Encode the new variable as a factor
Acupuncture$any.therap.visits <- factor(Acupuncture$any.therap.visits, 
                                        levels = c(0,1), 
                                        labels=c("Did not visit CT", "Visited CT")
                                        )

#Dichotomize the variable for complementary therapist visits into 0 or at least 1 visit.
Acupuncture$any.gp.visits <- ifelse(Acupuncture$total.gp.visits == 0, 0, 1)

#Encode the new variable as a factor
Acupuncture$any.gp.visits <- factor(Acupuncture$any.gp.visits, 
                                        levels = c(0,1), 
                                        labels=c("Did not visit GP", "Visited GP")
                                        )

#Dichotomize the variable for complementary therapist visits into 0 or at least 1 visit.
Acupuncture$any.spec.visits <- ifelse(Acupuncture$total.spec.visits == 0, 0, 1)

#Encode the new variable as a factor
Acupuncture$any.spec.visits <- factor(Acupuncture$any.spec.visits, 
                                        levels = c(0,1), 
                                        labels=c("Did not visit specialist", "Visited specialist")
                                        )

#Generate a combined binary endpoint for having any professional visits. 
Acupuncture$combined <- ifelse(Acupuncture$any.therap.visits=="Did not visit CT" &
                                   Acupuncture$any.gp.visits=="Did not visit GP" & 
                                   Acupuncture$any.spec.visits=="Did not visit specialist", 0, 1
                               )

#Encode the new variable as a factor
Acupuncture$combined <- factor(Acupuncture$combined, 
                               levels = c(0,1), 
                               labels=c("No visits", "At least one visit")
                               )

#Tabulate the new composite endpoint.
table(Acupuncture$combined, useNA="ifany")


#Perform the t-test, assuming the variances are equal in the treatment groups
t.test(pct.month12 ~ treatment.group, var.equal=TRUE, data = Acupuncture)

#Use the compareGroups function to save a summary of the results in pct.month12.test
pct.month12.test <- compareGroups::compareGroups(treatment.group ~ pct.month12, data = Acupuncture)

#Use the createTable function to summarize and store the results saved in pct.month12.test.
pct.month12.table <- compareGroups::createTable(pct.month12.test, show.ratio = FALSE, show.p.overall=TRUE)

#Display the results of pct.month12.table
pct.month12.table


#Use a boxplot to visualize the total days off sick by treatment group.  
ggplot(data=Acupuncture, aes(x=treatment.group, y=total.days.sick)) + 
  geom_boxplot(fill="white", color="black") +
  ylab("Total days off sick") +  xlab("Treatment group")

#Use the Wilcoxon Rank Sum test to compare the two distributions.
wilcox.test(total.days.sick ~ treatment.group, data=Acupuncture)


#Perform the test of proportions on resp35.month12 by treatment.group.
prop.test(table(Acupuncture$treatment.group, Acupuncture$resp35.month12), correct=FALSE)

#Use the tidy function to store and display a summary of the test results.
resp35.month12.test <- broom::tidy(prop.test(table(Acupuncture$treatment.group, 
                                                   Acupuncture$resp35.month12
                                                   ), correct=FALSE
                                             )
                                   )
resp35.month12.test

#Calculate the treatment difference
resp35.month12.test$estimate1 - resp35.month12.test$estimate2

```
  
  
  
***
  
Chapter 2 - Trial Designs  
  
Randomization Methods:  
  
* Good randomization ensures the groups are appropriately stratified  
* Simple Randomization has every patient with a 50/50 chance of assignment to either group, regardless of how many are already in each group  
	* treatment <- c("A","B")  
    * simple.list <- sample(treatment, 20, replace=TRUE)  
    * cat(simple.list,sep="\n")  
* May want to instead use blocks, where each block has equal numbers  
	* library(blockrand)  
    * block.list <- blockrand(n=20, num.levels = 2,block.sizes = c(2,2))  
    * Can further randomize the block sizes to avoid the issue of predictability at the end of the block  
    * block.list2 <- blockrand(n=20, num.levels = 2,block.sizes = c(1,2))  
* May want to instead used stratified randomization  
	* Age group, geographical region, disease severity, etc.  
    * over50.severe.list <- blockrand(n=100, num.levels = 2, block.sizes = c(1,2,3,4), stratum='Over 50, Severe', id.prefix='O50_S', block.prefix='O50_S')  
  
Crossover, Factorial, Cluster Randomized Trials:  
  
* Sometimes a desire to have each patient be their own control after a "washout" period (long enough to reverse effects) - called a crossover trial  
	* May increase precision, eliminate inter-pateitn variability, and allow for smaller sample sizes  
    * However, orders of treatments may impact outcomes, washout periods may be too short, and patients may fall out before finishing  
* Factorial designs are such that patients may receive A, B, A and B, or placebo only  
* Sometimes desirable to calculate the odds ratios, especially if the treatments are independent  
	* Odds of Recovery = nRecover / nNotRecover  
    * epitools::oddsratio.wald(recovery.trial$A, recovery.trial$recover)  
* May also want to run cluster-level trials, such as by school or by hospital  
	* Can run in to problems with sample sizes (need to be large, may not have enough, etc.)  
  
Equivalence and Non-Inferiority Trials:  
  
* Objective of an equivalence trial is to show similar efficacy - for example, where a generic is being released  
	* Need to pre-specify a maximum acceptable difference (delta) among the groups  
    * Equivalence is when the confidence intervals are all inside the deltas  
    * Non-inferiority is when at least one of the confidence intervals is outside delta, even with the point estimate being inside  
    * prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "less", conf.level = 0.95, correct=FALSE)  
    * prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "greater", conf.level = 0.95, correct=FALSE)  
    * prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "two.sided", conf.level = 0.90, correct=FALSE)  
* Sometimes the only objective is to show non-inferiority with a one-sided test (Ha: new treatment wose then existing), often at the 2.5% level  
	* prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "less", conf.level = 0.975, correct=FALSE)  
* Need to state in advance the delta, the number of sides, and the significance levels  
	* Lack of superiority does NOT imply equivalence  
  
Bioequivalence trials:  
  
* Bioequivalence is determined by blood draws after someone has taken a drug - pharmacokinetics (PK)  
	* Absorbtion, Excretion  
    * Assumption is often made that similar PK profiles will lead to similar safety and efficacy, saving time on Phase III trials  
* PK profiles are often assessed based on key statistics  
	* Cmax - highest concentration  
    * Tmax - time to highest concentration  
    * T1/2 - half-life  
    * AUC - area under the curve  
    * Crossover designs are frequently used, with washouts being many times greater than the half-life  
* The AUC is often calculated using the trapezoidal method - objective is to be between (0.8, 1.25) of the reference drug for the 90% CI  
    * library(PKNCA)  
    * pk.calc.auc(PKData$plasma.conc.n, PKData$rel.time, interval=c(0.25, 12), method="linear")  
  
Example code includes:  
```{r}

#Generate a vector to store treatment labels "A" and "B"
set.seed(123)
arm<-c("A", "B")

#Randomly select treatment arm 14 times with the sample function and store in a vector
simple <- sample(arm, 14, replace=TRUE)

#Display the contents of the vector
simple

#Tabulate the numbers assigned to each treatment.
table(simple)


#Use the blockrand function for 14 patients, two arms and block size 2.
set.seed(123)
block2 <- blockrand::blockrand(n=14, num.levels = 2,  block.prefix='B', block.sizes = c(1,1))

#Display the list.
block2

#Tabulate the numbers per treatment arm.
table(block2$treatment)


#Use block randomization to produce lists of length 100 and random block sizes between 2 and 8.
set.seed(123)
under55 <- blockrand::blockrand(n=100, num.levels = 2, block.sizes = 1:4, 
                                id.prefix='U55', block.prefix='U55', stratum='<55y'
                                )
above55 <- blockrand::blockrand(n=100, num.levels = 2, block.sizes = 1:4, 
                                id.prefix='A55', block.prefix='A55',stratum='>=55y'
                                )

#Explore the two lists 
head(under55)
head(above55)

#Tabulate the numbers assigned to each treatment within each strata
table(under55$treatment)
table(above55$treatment)


fact.data <- readRDS("./RInputFiles/fact.data.Rds")
str(fact.data)

#Explore the fact.data using the head function.
head(fact.data)

#Display the numbers with and without infections by supplement combination.
fact.data %>% 
    count(glutamine, selenium, infection)

#Display the numbers and proportions with infections for those who received glutamine.
fact.data %>% 
    group_by(glutamine) %>% 
    filter(infection=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))

#Display the numbers and proportions with infections for those who received selenium.
fact.data %>% 
    group_by(selenium) %>% 
    filter(infection=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))


#Display the numbers with and without infections by supplement combination.
fact.data %>% 
    count(glutamine, selenium, infection)

#Display the numbers and proportions with infections for those who received glutamine.
fact.data %>% 
    group_by(infection) %>% 
    filter(glutamine=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))

#Display the numbers and proportions with infections for those who received selenium.
fact.data %>% 
    group_by(infection) %>% 
    filter(selenium=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))


#Calculate the effect of glutamine on infection
epitools::oddsratio.wald(fact.data$glutamine, fact.data$infection)

#Calculate the effect of selenium on infection
epitools::oddsratio.wald(fact.data$selenium, fact.data$infection)


relapse.trial <- data.frame(Treatment=rep(c("New", "Standard"), times=c(264, 263)), 
                            Relapse=rep(rep(c("At least one relapse", "No relapse"), times=2), 
                                        times=c(184, 80, 169, 94)
                                        ), 
                            stringsAsFactors = TRUE
                            )
str(relapse.trial)
table(relapse.trial)

#Use the head function to explore the relapse.trial dataset
head(relapse.trial)

#Calculate the number of percentages of relapse by treatment group
relapse.trial %>% 
    group_by(Treatment, Relapse) %>% 
    summarise(n = n()) %>% 
    mutate(pct = (n / sum(n))*100)

#Calculate the two-sided 90% confidence interval for the difference
prop.test(table(relapse.trial$Treatment, relapse.trial$Relapse), 
          alternative = "two.sided", conf.level=0.9, correct=FALSE
          )


PKData <- readRDS("./RInputFiles/PKData.Rds")
str(PKData)

#Display the dataset contents
head(PKData)

#Store a numeric version of the concentration variable in plasma.conc.n
PKData$plasma.conc.n <- as.numeric(PKData$plasma.conc)

#Use ggplot to plot the concentration levels against relative time
ggplot(data=PKData, aes(x=rel.time, y=plasma.conc.n)) + 
    geom_line() +
    geom_point() + ggtitle("Individual Concentration Profile") +
    xlab("Time Relative to First Dose, h") + 
    ylab("Plasma Concentration, ng/mL")


#Use the summary function to find the max concentration level
summary(PKData$plasma.conc.n)

#Use pk.calc.tmax to find when Cmax occurred, specifying the concentration and time.
PKNCA::pk.calc.tmax(PKData$plasma.conc.n, PKData$rel.time)

#Use pk.calc.cmax to estimate AUC between 0.25 and 12hrs.
PKNCA::pk.calc.auc(PKData$plasma.conc.n, PKData$rel.time, interval=c(0.25, 12), method="linear")

```
  
  
  
***
  
Chapter 3 - Sample Size and Power  
  
Introduction to Sample Size and Power:  
  
* Only a sample from the target population is appropriate, and it needs to be appropriately sized (too big and too small are both problems)  
* Need to understand the requirements of the trial, endpoints, statistical techniques to be used, smallest clinical meaningful difference, variability, and the significance level as well as the power  
* Type I error is rejecting a true null hypothesis - significance level, often set to 0.05  
* Type II error is failing to reject a false null hypothesis - power is 1 minus Type II error and is usually targeted as 0.8 or 0.9  
	* power.t.test(delta=3, sd=10, power=0.8, type = "two.sample", alternative = "two.sided")  
    * power.t.test(delta=3, sd=10, power=0.9, type = "two.sample", alternative = "two.sided")  
* May want to instead run a test of proportions  
	* power.prop.test(p1=0.3, p2=0.15, power=0.8)  
  
Sample Size Adjustments:  
  
* The alternative hypothesis is often, but not always, based on a two-sided test  
* If the alternative hypothesis is one-sided, then this should be incproproated in the study design  
	* power.t.test(delta=3, sd=10, power=0.8, type = "two.sample", alternative = "one.sided")  
* May want to have unequal group sizes (non 1:1 ratio) if it produces better recruitment or compliance  
	* n.ttest(power = 0.8, alpha = 0.05, mean.diff = 3, sd1 = 10, sd2 = 10, k = 0.5, design = "unpaired", fraction = "unbalanced")  # k of 0.5 means a ratio of 2  
* Can also make adjustments for unequal variances  
	* n.ttest(power = 0.8, alpha = 0.05, mean.diff = 3, sd1 = 9.06, sd2 = 9.06, k = 1, design = "unpaired", fraction = "balanced")  
* There are inevitably drop-outs from clinical trials - ratio is called Q  
	* Sample size needs to be inflated by 1 / (1 - Q)  
    * orig.n <- power.t.test(delta=3, sd=10, power=0.8, type = "two.sample", alternative = "one.sided")$n  
    * ceiling(orig.n/(1-0.1))  # assuming Q = 0.1  
  
Interim Analyses and Stopping Rules:  
  
* Patient recruitment often occurs over a time period of years; can regularly monitor the study prior to completion  
	* May want to stop early if the evidence is very strong for superiority, inferiority, side effects, futility, etc.  
* Interim analyses typically require increasing the same size - Type I error increases with more chances to reject  
* The Pocock rule is also known as the "fixed nominal" rule  
	* library(gsDesign)  
    * Pocock <- gsDesign(k=3, test.type=2, sfu="Pocock")  # sfu is the spending function and k=3 means 2 interim and 1 final  
    * 2*(1-pnorm(Pocock$upper$bound))  
    * Pocock.ss <- gsDesign(k=3, test.type=2, sfu="Pocock", n.fix=200, beta=0.1)  
    * ceiling(Pocock.ss$n.I)  
* Can instead use the O'Brien-Fleming rule which has increasing p-value hurdles as the sample size increases (most of the budget is saved for the full and final sample)  
	* OF <- gsDesign(k=3, test.type=2, sfu="OF")  
    * 2*(1-pnorm(OF$upper$bound))  
    * OF.ss <- gsDesign(k=3, test.type=2, sfu="OF", n.fix=200, beta=0.1)  
    * ceiling(OF.ss$n.I)  
  
Sample Size for Alternative Trial Designs:  
  
* Goal of an equivalence trial is to prove similarity to within a maximum specified delta  
	* library(TOSTER)  
    * powerTOSTtwo.prop(alpha = 0.05, statistical_power = 0.9, prop1 = 0.7, prop2 = 0.7, low_eqbound_prop = -0.05, high_eqbound_prop = 0.05)  
    * powerTOSTtwo.raw(alpha=0.05, statistical_power=0.8, sdpooled=15, low_eqbound=-3,high_eqbound=3)  
* May want to instead run a cluster-level randomized trial  
	* CRTSize::n4means(delta=1, sigma=2.5, m=25, ICC=0.1, alpha=0.05, power=0.90)  
* May want to instead run a factorial design randomized trial - powered such that it assumes independence (would not detect an interaction effect)  
	* power.prop.test(p1=0.40, p2=0.25, power=0.9)  
    * power.prop.test(p1=0.40, p2=0.23, power=0.9)  
  
Example code includes:  
```{r}

#Generate the sample size for delta of 1, with SD of 3 and 80% power.
ss1 <- power.t.test(delta=1, sd=3, power=0.8)
ss1

#Round up and display the numbers needed per group
ceiling(ss1$n)

#Use the sample size from above to show that it provides 80% power
power.t.test(n=ceiling(ss1$n), delta=1, sd=3)


#Generate a vector containing values between 0.5 and 2.0, incrementing by 0.25
delta <- seq(0.5, 2, 0.25)
npergp <- NULL

#Specify the standard deviation and power
for(i in 1:length(delta)){
  npergp[i] <- ceiling(power.t.test(delta = delta[i], sd = 3, power = 0.8)$n)
}

#Create a data frame from the deltas and sample sizes
sample.sizes <- data.frame(delta, npergp)

#Plot the patients per group against the treatment differences
ggplot(data=sample.sizes, aes(x=delta, y=npergp)) + 
    geom_line() + 
    geom_point() + 
    ggtitle("Sample Size Scenarios") + 
    xlab("Treatment Difference") + 
    ylab("Patients per Group")


#Use the power.prop.test to generate sample sizes for the proportions
power.prop.test(p1 = 0.4, p2 = 0.6, power = 0.8)

#Find the minimum detectable percentage for the above using 150 patients per group.
power.prop.test(p1 = 0.4, power = 0.8, n = 150)$p2*100


#Use 90% power, delta 1.5, standard deviations of 2.5, fraction of 0.5
unequalgps <- samplesize::n.ttest(power = 0.9, alpha = 0.05, mean.diff = 1.5, 
                                  sd1 = 2.5, sd2 = 2.5, k = 0.5, 
                                  design = "unpaired", fraction = "unbalanced"
                    )
unequalgps


#Generate sample sizes comparing the proportions using a two-sided test
two.sided <- power.prop.test(p1=0.1, p2=0.3, power=0.8, alternative = "two.sided")
two.sided
ceiling(two.sided$n)

#Repeat using a one-sided test
one.sided <- power.prop.test(p1=0.1, p2=0.3, power=0.8, alternative = "one.sided")
one.sided
ceiling(one.sided$n)

#Display the reduction per group
ceiling(two.sided$n)- ceiling(one.sided$n)


#Use the gsDesign function to generate the p-values for four analyses under the Pocock rule
Pocock <- gsDesign::gsDesign(k=4, test.type=2, sfu="Pocock")
Pocock
2*(1-pnorm(Pocock$upper$bound))

#Repeat for the the O'Brein & Fleming rule
OF <- gsDesign::gsDesign(k=4, test.type=2, sfu="OF")
OF
2*(1-pnorm(OF$upper$bound))


#Use the gsDesign function to generate the sample sizes at each stage under the Pocock rule
Pocock.ss <- gsDesign::gsDesign(k=4, test.type=2, sfu="Pocock", n.fix=500, beta=0.1)
ceiling(Pocock.ss$n.I)

#Repeat for the the O'Brein-Fleming rule
OF.ss <- gsDesign::gsDesign(k=4, test.type=2, sfu="OF", n.fix=500, beta=0.1)
ceiling(OF.ss$n.I)


#Find the sample size  per group for expected rates of 60%, 4% delta, 90% power and 5% significance level.
TOSTER::powerTOSTtwo.prop(alpha = 0.05, statistical_power = 0.9, prop1 = 0.6, prop2 = 0.6, 
                          low_eqbound_prop = -0.04, high_eqbound_prop = 0.04
                          )

#Find the power if the above trial is limited to 2500 per group
TOSTER::powerTOSTtwo.prop(alpha = 0.05, N=2500, prop1 = 0.6, prop2 = 0.6, 
                          low_eqbound_prop = -0.04, high_eqbound_prop = 0.04
                          )

#Find the sample size for a standard deviation of 10, delta of 2, 80% power and 5% significance level.
TOSTER::powerTOSTtwo.raw(alpha=0.05, statistical_power=0.8, sdpooled=10, low_eqbound=-2, high_eqbound=2)


#Find the sample sizes based on standard deviations between 7 and 13.
stdev <- seq(7, 13, 1)
npergp <- NULL
for(i in 1:length(stdev)){
    npergp[i] <- ceiling(TOSTER::powerTOSTtwo.raw(alpha=0.05, statistical_power=0.8, sdpooled=stdev[i],
                                                  low_eqbound=-2, high_eqbound=2
                                                  )
                         )
}
sample.sizes <- data.frame(stdev, npergp)

#Plot npergp against stdev
ggplot(data=sample.sizes, aes(x=stdev, y=npergp)) + 
    geom_line() +
    geom_point() + 
    ggtitle("Equivalence Sample Size Scenarios") +
    xlab("Standard Deviation") + 
    ylab("Patients per Group")

```
  
  
  
***
  
Chapter 4 - Statistical Analysis  
  
Regression Analysis:  
  
* May be additional explanatory variables associated with outcomes of interest; these could differ by treatment arms ("explanatory variables")  
* May be interested to see whether particular variables are associated with trial endpoints of particular interest  
* Can use simple linear regression to investigate the treatment effects  
	* asthma.trial$group <- relevel(asthma.trial$group, ref="Placebo")  
    * asthma.reg1 <- lm(fev.change ~group, asthma.trial)  
    * summary(asthma.reg1)  
    * t.test(fev.change~group, var.equal=TRUE, data=asthma.trial)  
    * asthma.reg2<-lm(fev.change ~group + age, asthma.trial)  
    * summary(asthma.reg2)  
* May want to use logistic regression to model binary outcomes  
	* asthma.logreg1=glm(attack~group + age, family=binomial(link="logit"), asthma.trial)  
    * summary(asthma.logreg1)  
    * exp(coefficients(asthma.logreg1)[2])  
    * exp(confint(asthma.logreg1)[2,])  
  
Analysis Sets, Subgroups, and Interactions:  
  
* Patient adherence may be imperfect for many reasons; as such, analysis is often re-done for sub-groups  
* Intention to treat (ITT) means looking at patient outcomes according to planned treatments rather than just those who complied and completed treatment  
	* Full Analysis Set (FAS) follows IT principles  
    * Per-Protocol Set (PPS) does not follow ITT principles and instead includes everyone  
* Can look at both FAS and PPS to compare outcomes  
	* asthma.fas<-lm(fev.change ~group , asthma.trial)  
    * asthma.pp<-lm(fev.change ~group , asthma.trial, subset = pp==1)  
* Can also look at subgroup analyses  
	* asthma.u65<-lm( fev.change ~group , asthma.trial, subset = age<65)  
    * asthma.o65<-lm( fev.change ~group , asthma.trial, subset = age>=65)  
* There is a risk of p-hacking with subgroup analyses, so they should be pre-specified rather than ad hoc  
* May also want to consider interaction effects for the trial  
	* asthma.ageg <- lm( fev.change ~group + agegroup , asthma.trial)  
    * summary(asthma.ageint)  
  
Multiplicity of Data:  
  
* Multiple subgroups of data - patients, looks, sub-groups, etc.  
* Goal is to keep the probability of Type I error low - roughly 0.05  
	* Multiplicity may make Type I errors cumulatively much more likely  
* Can adjust the p-values to maintain the desired Type I error rate  
	* p / n (Bonferroni) for n tests with a Type I error rate of p  
* There is typically a lack of power within the subgroups due to how the study is designed  
	* Subgroups should thus be limited, and based on hypotheses driven by previous research  
* There are often multiple endpoints for a study, so there needs to be clarity about which are confirmatory and which are exploratory  
* Composite endpoints can increase statistical power - for example, adding the causes of cardiac death  
* May also have repeated measurements due to collecting data at multiple timepoints  
  
Wrap up:  
  
* Well-conducted human clinical trials are common in the medical indistry and need to follow rigorous protocols and analyses  
	* Randomization, including several different methods  
    * Clinical trial designs such as cross-over, equivalence, and non-inferiority  
    * Sample size determination  
    * Statistical analysis for the overll group and sub-groups  
    * T-tests, Wilcoxon rank tests, test for equal proportions, logistic regression  
  
Example code includes:  
```{r}

#Explore the variable names with the str function
str(Acupuncture)

#Use the relevel function to set Control as reference group
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")


#Use lm to run and store the model in linreg1
linreg1 <- lm(pct.month12 ~ treatment.group + sex + score.baseline.4, data=Acupuncture, 
              na.action = na.exclude
              )

#Display the results of linreg1
summary(linreg1)


#Use lm to run and store the model in linreg2
linreg2 <- lm(pct.month12 ~ treatment.group + score.baseline.4, data=Acupuncture, na.action = na.exclude)

#Display the results of linreg2
summary(linreg2)

#Add the predicted values to the Acupuncture dataset for linreg2 using the predict function 
Acupuncture$pred.linreg2 <- predict(linreg2)


#Plot the predicted values against baseline score quartile grouping by treatment.
ggplot(data = subset(Acupuncture, !is.na(pred.linreg2)), 
       aes(x = score.baseline.4, y = pred.linreg2, group = treatment.group)
       ) + 
    geom_line(aes(color = treatment.group)) +
    geom_point(aes(color = treatment.group)) + 
    ggtitle("Predicted Values from Linear Regression Model") + 
    xlab("Baseline Score Quartile") + 
    ylab("Percentage Change from Baseline at M12")


#Use the relevel function to set "Control" as the reference for treatment
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")

#Use the relevel function to set "less than or eq to 35%" as the reference for resp35.month12
Acupuncture$resp35.month12 <- relevel(Acupuncture$resp35.month12, ref="less than or eq to 35%")

#Use glm to run and store the model in logreg1
logreg1 <- glm(resp35.month12 ~ treatment.group + sex  + score.baseline.4, 
               family=binomial(link="logit"), data=Acupuncture
               )

#Display the results of logreg1
summary(logreg1)

#Display the odds ratio and 95% CI for Acupuncture vs Control
exp(coefficients(logreg1)[2])
exp(confint(logreg1)[2,])


#Tabulate withdrawal.reason
table(Acupuncture$withdrawal.reason,  useNA="ifany")

#Tabulate completedacupuncturetreatment by treatment.group
table(Acupuncture$completedacupuncturetreatment, Acupuncture$treatment.group,  useNA="ifany") 

#Create a per protocol flag that is TRUE if patients met the criteria
Acupuncture <- Acupuncture %>%
    mutate(pp = is.na(withdrawal.reason) & 
               ((completedacupuncturetreatment==1 & treatment.group=="Acupuncture") | 
                    (is.na(completedacupuncturetreatment) & treatment.group=="Control")
                )
           )
Acupuncture$pp[is.na(Acupuncture$pp)] <- FALSE
Acupuncture$pp <- as.factor(Acupuncture$pp)

#Tabulate the per protocol flag
table(Acupuncture$pp)


#Use the relevel function to set Control as reference group
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")

#Use lm to run and store the model without interaction in linregnoint
linregnoint <- lm(pct.month12 ~ treatment.group + score.baseline.4, Acupuncture, na.action = na.exclude)

#Use lm to run and store the model with interaction in linregint
linregint <- lm(pct.month12 ~ treatment.group*score.baseline.4, Acupuncture, na.action = na.exclude)

#Display the results of linregnoint and linregint
summary(linregnoint)
summary(linregint)

#Compare the models with the anova command
anova(linregnoint, linregint)


#Tabulate the age group variable to view the categories
table(Acupuncture$age.group)

#Display the adjusted significance level
0.05/4

#Run the Wilcoxon Rank Sum test in each of the age subgroups
age <- c("18-34", "35-44", "45-54", "55-65")
for(group in age){
  subgroup <- broom::tidy(wilcox.test(total.days.sick ~ treatment.group, 
                                      data = subset(Acupuncture, age.group==group), 
                                      exact=FALSE
                                      )
                          )
  print(group)
  print(subgroup)
}


#Tabulate the combined endpoint by treatment group
table(Acupuncture$combined, Acupuncture$treatment.group, useNA="ifany")

#Use the relevel function to set Control as reference group
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")

#Use compareGroups to generate and save the treatment effect for the composite endpoint amd each component
combined.test <- compareGroups::compareGroups(treatment.group ~ combined + any.therap.visits + any.gp.visits + any.spec.visits, data = Acupuncture)

# Store the results in a table
combined.test.table <- compareGroups::createTable(combined.test, show.ratio = TRUE, show.p.overall=FALSE)

#Display the results
combined.test.table

```
  
  
  
***
  
###_Financial Analytics in R_  
  
Chapter 1 - Intro to Valuations (Cash is King)  
  
Valuations Overview:  
  
* Can value economic value based on cash flows using R and the tidyverse  
* Focus for the course is valuation of projects within a company  
* Discounted cash flows use an interest rate to discount future expenditures and revenues to the present value  
* Cash flow statements typically have time as the column, with variables such as spend or revenue as rows  
	* May want to tidy data for analysis where the column is the variable and the row is the observation  
  
Business Models and Writing R Functions:  
  
* The business model includes the timing of earning revenue and incurring expenses  
	* Operating revenue is driven by the units sold and the price per unit (for consumer products)  
    * Operating revenue is driven by enrolment, churn, monthly fees, etc. (for subscription products)  
    * Direct Expenses are driven by the units sold and the cost per unit (marginal direct costs)  
    * Operating expenses (overhead) are costs required to run the business but not directly tied to a specific sale - e.g., SG&A, depreciation, etc.  
* The timing of revenues and expenses are on an accrual basis - for example, raw materials may be purchased well before production, but accrual would be when producing  
* Gross Profit = Operative Revenue minus Direct Expenses  
* Can use a function for calculating the business model  
	* calc_business_model <- function(assumptions, price_per_unit, cost_per_unit){  
    *     model <- assumptions  
    *     model$revenue <- model$sales * price_per_unit  
    *     model$direct_expense <- model$sales * cost_per_unit  
    *     model$gross_profit <- model$revenue - model$direct_expenses  
    * model  
    * }  
    * calc_business_model( assumptions, price_per_unit = 10, cost_per_unit = 2 )  
  
Pro-Forma Income Statements:  
  
* The income statement (P&L) translates revenues and expenses to net income  
	* overhead_expense <- sga + depreciation + amortization  
    * operating_profit <- gross_profit - overhead_expense  
* Depreciation is the idea that large capital expenditures should be spread over their useful lifetimes  
	* Amortization is a similar concept for non-physical investments (such as R&D)  
    * Depreciation per Year = (Book Value - Salvage Value) / Years of Useful Life  
    * depreciation_per_period <- (book_value - salvage_value)/useful_life  
    * depreciation <- rep(depreciation_per_period, useful_life)  
* Can calculate valuations as either levered (financing, including tax considerations and interest) or unlevered (ignores financing considerations)  
* Generally need to include the tax considerations  
	* tax <- operating_income * tax_rate  
    * net_income <- operating_income - tax  
  
Income to Cash:  
  
* Net income needs to be adjusted to create free cash flow  
* Cash is king - need to be able to convert the income in to no-string-attached cash  
* Income is recognized based on accrual while cash is recognized when it is received or spent  
	* The income statement may majorly differ from the cash flow particularly due to depreciation  
* To convert from income to free cash flow, convert as follows  
	* Add back depreciation and amortization (only care about actual spending)  
    * Subtract out CAPEX (care about recognizing all of the capital asset when it is purchased)  
    * Adjust for net working capital (NWC)  
    * cashflow <- net_income + depreciation_exp - capex + nwc_changes  
  
Example code includes:  
```{r}

# define inputs
price <- 20
print_cost <- 0.5
ship_cost <- 2

assumptions <- data.frame(year=1:5, sales=c(175, 200, 180, 100, 50))

# add revenue, expense, and profit variables
cashflow <- assumptions
cashflow$revenue <- cashflow$sales * price
cashflow$direct_expense <- cashflow$sales * (print_cost + ship_cost) 
cashflow$gross_profit <- cashflow$revenue - cashflow$direct_expense

# print cashflow
print(cashflow)

prem_ts <- data.frame(MONTH=1:60, COST_PER_SONG=0.01, SONG_LENGTH=3, REV_PER_SUB=10) %>%
    mutate(PCT_ACTIVE=0.95**(MONTH-1), HOURS_PER_MONTH=528-0.08*(MONTH-40)**2)

# premium business models
premium_model <- prem_ts
premium_model$SONGS_PLAYED <- prem_ts$PCT_ACTIVE * prem_ts$HOURS_PER_MONTH * 1 / prem_ts$SONG_LENGTH
premium_model$REV_SUBSCRIPTION <- prem_ts$PCT_ACTIVE * prem_ts$REV_PER_SUB
premium_model$COST_SONG_PLAYED <- premium_model$SONGS_PLAYED * prem_ts$COST_PER_SONG

# inspect results
head(premium_model)


free_ts <- data.frame(MONTH=1:60, PROP_MUSIC=0.95, REV_PER_AD=0.02, REV_PER_CLICK=10, 
                      COST_PER_SONG=0.01, SONG_LENGTH=3, AD_LENGTH=0.25, CLICK_THROUGH_RATE=0.001
                      ) %>%
    mutate(PCT_ACTIVE=0.97**(MONTH-1), HOURS_PER_MONTH=480-0.08*(MONTH-40)**2)

# freemium business models
freemium_model <- free_ts
freemium_model$SONGS_PLAYED <- free_ts$PCT_ACTIVE * free_ts$HOURS_PER_MONTH * free_ts$PROP_MUSIC / free_ts$SONG_LENGTH
freemium_model$ADS_PLAYED <- free_ts$PCT_ACTIVE * free_ts$HOURS_PER_MONTH * (1-free_ts$PROP_MUSIC) / free_ts$AD_LENGTH
freemium_model$REV_AD_PLAYED <- freemium_model$ADS_PLAYED * free_ts$REV_PER_AD
freemium_model$REV_AD_CLICKED <- freemium_model$ADS_PLAYED * free_ts$CLICK_THROUGH_RATE * free_ts$REV_PER_CLICK
freemium_model$COST_SONG_PLAYED <- freemium_model$SONGS_PLAYED * free_ts$COST_PER_SONG

# examine output
head(freemium_model)


# Define function: calc_business_model
calc_business_model <- function(assumptions, price, print_cost, ship_cost){
    cashflow <- assumptions
    cashflow$revenue <- cashflow$sales * price
    cashflow$direct_expense <- cashflow$sales * (print_cost + ship_cost) 
    cashflow$gross_profit <- cashflow$revenue - cashflow$direct_expense
    cashflow
}

# Call calc_business_model function for different sales prices
assumptions
calc_business_model(assumptions, 20, 0.5, 2)$gross_profit
calc_business_model(assumptions, 25, 0.5, 2)$gross_profit


# Inputs
production <- data.frame(Month=1:60, Units=rep(c(60, 50, 40, 30), times=15))

cost <- 100000
life <- 60
salvage <- 10000

# Compute depreciation
production$Depr_Straight <- (cost - salvage)/life
production$Depr_UnitsProd <- (cost - salvage)*(production$Units) / sum(production$Units)

# Plot two depreciation schedules
ggplot(production, aes(x = Month)) + 
    geom_line(aes(y = Depr_Straight)) + 
    geom_line(aes(y = Depr_UnitsProd))


# Business model
cashflow

cashflow$revenue <- cashflow$revenue + 2 * cashflow$sales
cashflow$gross_profit <- cashflow$revenue - cashflow$direct_expense

# Income statement
cashflow$depr_sl <- (1000 - 0) / 5
cashflow$operating_profit <- cashflow$gross_profit - cashflow$depr_sl
cashflow$tax <- cashflow$operating_profit * 0.3
cashflow$net_income <- cashflow$operating_profit - cashflow$tax

# Inspect dataset
cashflow


# Calculate income statement
assumptions <- data.frame(unit_sales=100000*c(1, 2, 4, 8), machines_purchased=c(1, 1, 2, 4), 
                          depreciation=10000000*c(4, 8, 16, 32)
                          )
assumptions
price_per_unit <- 1000
cogs_per_unit <- 450
labor_per_unit <- 50

income_statement <- assumptions
income_statement$revenue <- income_statement$unit_sales * price_per_unit
income_statement$expenses <- income_statement$unit_sales * (cogs_per_unit + labor_per_unit)
income_statement$earnings <- income_statement$revenue - income_statement$expenses - income_statement$depreciation

# Summarize cumulative earnings
sum(income_statement$earnings)
sum(income_statement$earnings) / sum(income_statement$revenue)


# calculate free cashflow
cashflow <- income_statement
cashflow$operating_cf <- cashflow$earnings + cashflow$depreciation
cashflow$capex <- cashflow$machines_purchased * 160000000
cashflow$free_cf <- cashflow$operating_cf - cashflow$capex

# summarize free cashflow
sum(cashflow$free_cf)

```
  
  
  
***
  
Chapter 2 - Key Financial Concepts (Time is Money)  
  
Time Value of Money:  
  
* Money today is generally more valuable than money tomorrow  
	* Can use a compounding interest rate, so V(n) = V(0) * (1 + r)**n  
    * Therefore, PV = FV(n) / (1+r)**n - the discounting rate is r, applied over n time periods  
    * mutate(data, pv = fv / (1 + r)^n)  
  
Different Discount Rates:  
  
* Need to clarify the types of discount rates - need to ensure the same time periods as the cash flows  
* Can convert discount rates between time periods using appropriate compounding  
	* r2 = [(1 + r1)^(# r1 units per 1 r2 unit) ] - 1  
    * r_quart <- (1 + r_mth)^3 - 1  
    * r_quart <- (1 + r_ann)^(1/4) - 1  
* Need to also consider the differences in real and nominal rates  
	* Can think about inflation or deflation as relates to purchaing power for an item  
* Generally easiest for cash flows to reflect real cash-flows and discounted by real interest rates  
	* r_real=r_nominal / (1+inflation_rate)  
    * r_nominal=r_real*(1+inflation_rate)  
  
Discounting Multiple Cash Flows:  
  
* Cash flow differs dramatically based on time in the future when it is received  
* Need to discount the cash flows back to present values prior to summing across them  
	* pv <- calc_pv(fv = 100, r = 0.01, n = 3)  # single caseh flow  
    * cashflows <- c(0, -50, 25, 100, 175, 250, 250)  
    * discounted_cashflows <- calc_pv(cashflows, r = 0.01, n = 0:6)  # vectorized calculation, once for each of 0:6  
* Vectorized functions work well with the Tidyverse  
	* many_cashflows %>% group_by(option) %>% summarize( PV = sum(calc_pv(cashflow, 0.08, n = time))  
  
Example code includes:  
```{r}

# Assign input variables
fv <- 100
r <- 0.08

# Calculate PV if receive FV in 1 year
pv_1 <- 100 / (1 + r)**1
pv_1

# Calculate PV if receive FV in 5 years
pv_5 <- 100 / (1 + r)**5
pv_5

# Calculate difference
pv_1 - pv_5


# Define PV function: calc_pv
calc_pv <- function(fv, r, n){
    pv <- fv / (1+r)**n
    pv
}

# Use PV function for 1 input
calc_pv(100, 0.08, 5)

# Use PV function for range of inputs
n_range <- 1:10
pv_range <- calc_pv(100, 0.08, n_range)
pv_range


# Calculate present values in dataframe
present_values <- data.frame(n = 1:10) %>% mutate(pv = 100 / (1 + 0.08)**n)

# Plot relationship between time periods versus present value
ggplot(present_values, aes(x = n, y = pv)) +
    geom_line() +
    geom_label(aes(label = paste0("$",round(pv,0)))) +
    ylim(0,100) +
    labs(
        title = "Discounted Value of $100 by Year Received", 
        x = "Number of Years in the Future",
        y = "Present Value ($)"
        )


# Calculate present values over range of time periods and discount rates
present_values <- 
    expand.grid(n = 1:10, r = seq(0.05,0.12,0.01)) %>%
    mutate(pv = calc_pv(100, r, n))
     
# Plot present value versus time delay with a separate colored line for each rate
ggplot(present_values, aes(x = n, y = pv, col = factor(r))) +
    geom_line() +
    ylim(0,100) +
    labs(
        title = "Discounted Value of $100 by Year Received", 
        x = "Number of Years in the Future",
        y = "Present Value ($)",
        col = "Discount Rate"
        )


# Convert monthly to other time periods
r1_mth <- 0.005
r1_quart <- (1 + r1_mth)^3 - 1
r1_semi <- (1 + r1_mth)^6 - 1
r1_ann <- (1 + r1_mth)^12 - 1

# Convert years to other time periods
r2_ann <- 0.08
r2_mth <- (1 + r2_ann)^(1/12) - 1
r2_quart <- (1 + r2_ann)^(1/4) - 1


# Convert real to nominal
r1_real <- 0.08
inflation1 <- 0.03
(r1_nom <- (1 + r1_real) * (1 + inflation1) - 1)

# Convert nominal to real
r2_nom <- 0.2
inflation2 <- 0.05
(r2_real <- (1 + r2_nom) / (1 + inflation2) - 1)


# Define cashflows
cashflow_a <- c(5000, rep(0,6))
cashflow_b <- c(0, rep(1000, 6))

# Calculate pv for each time period
disc_cashflow_a <- calc_pv(cashflow_a, 0.06, 0:6)
disc_cashflow_b <- calc_pv(cashflow_b, 0.06, 0:6)

# Calculate and report total present value for each option
(pv_a <- sum(disc_cashflow_a))
(pv_b <- sum(disc_cashflow_b))


# Define cashflows
cashflow_old <- rep(-500, 11)
cashflow_new <- c(-2200, rep(-300, 10))
options <- data.frame(time = rep(0:10, 2),
                      option = c(rep("Old",11), rep("New",11)),
                      cashflow = c(cashflow_old, cashflow_new)
                      )
                
# Calculate total expenditure with and without discounting
options %>%
    group_by(option) %>%
    summarize(sum_cashflow = sum(cashflow),
              sum_disc_cashflow = sum(calc_pv(cashflow, 0.12, 0:10))
              )

```
  
  
  
***
  
Chapter 3 - Prioritizing Profitability (Financial Metrics)  
  
Profitability Metrics and Payback Period:  
  
* Profitability metrics help to quantify how projects bring value to firms  
* Decision rules help to interpret metrics for decisions and comparisons  
	* Absolute vs. relative  
    * Constrained vs. unconstrained  
* All summary metrics have shortcomings; none are perfect in all situations  
	* Decision rules are communication tools, not hard and fast "laws"  
    * There are many corporate objectives over and above discounted cash flows; important input but not the sole answer  
* Payback periods help to assess the amount of time needed before full payback - sooner is better  
	* Does not consider TVM and does not consider profits after the payback period  
    * cashflows <- c(-10000, 2500, 3000, 5000, 6000, 1000)  
    * cumsum(cashflows) + init_investment  
  
NPV, IRR, Profitability Index:  
  
* NPV - Net Present Value is net of the investment costs  
	* In an unconstrained world, any NPV-positive project would be pursued  
    * n <- 0:(length(cashflows) - 1)  
    * npv <- sum( calc_pv(cashflows, r, n) )  
* IRR - Internal Rate of Return is the required rate of return for an investment to break-even over a stated time period  
	* Also called the "hurdle rate" since it is the rate required for a project to hurdle in to profitability  
    * # assume we have calc_npv function with signature:  
    * # calc_npv(cashflows, r)  
    * uniroot(calc_npv, interval = c(0, 1), cashflows = cashflows)$root  
* Profitability index - ratio between sum of discounted cash flows over the cost of the initial investment  
	* A value of 1 is a break-even that only recoups the investment  
    * npv_fcf <- calc_npv(future_cashflow, r)  
    * profitability_index <- npv_fcf / abs(initial_investment)  
  
Terminal Value:  
  
* Terminal Value (TV) is residual value outside the specified time period of the cash flow analysis  
	* Can add TV as a summary metric for the cash flows (discounted) after the final time period  
    * final_cashflow <- cashflow[n]  
    * terminal_value_period_n <- final_cashflow / (discount_rate - growth_rate)  
    * terminal_value_as_present <- terminal_value_period_n / (1 + discount_rate)^n  
* Can also use the exit multiplier method, in which a benchmark is applied to the cash flows (sales, revenues, etc.)  
  
Comparing and Computing Metrics:  
  
* NPV and IRR are important metrics with related information - however, they may give different constrained project decisions  
	* By definition, IRR is the discount rate that lets a project break-even on an NPV basis  
* NPV focuses on profit (favors large investments), while IRR focuses on return (favors small investments with high returns)  
	* Best solution can be to examine both metrics  
    * options %>% group_by(option) %>% summarize(npv=calc_npv(cf,0.08))  
  
Wrap up:  
  
* Strengths, weaknesses, and blind spots of various metrics  
	* Summary metrics alone can be deceptive  
* Additional metrics include ROE (equity) and ROA (assets)  
* Industries and even companies may have their own key metrics of interest  
  
Example code includes:  
```{r}

cashflows <- c(-50000, 1000, 5000, 5000, 5000, 10000, 10000, 10000, 10000, 10000, 10000)

# Inspect variables
cashflows

# Calculate cumulative cashflows
cum_cashflows <- cumsum(cashflows)

# Identify payback period
payback_period <- min(which(cum_cashflows >= 0)) - 1

# View result
payback_period


# Define payback function: calc_payback
calc_payback <- function(cashflows) {
  cum_cashflows <- cumsum(cashflows)
  payback_period <- min(which(cum_cashflows >= 0)) - 1
  payback_period
}

# Test out our function
cashflows <- c(-100, 50, 50, 50)
calc_payback(cashflows) == 2


cashflows <- c(-50000, 1000, 5000, 5000, 5000, 10000, 10000, 10000, 10000, 10000, 10000)

# normal payback period
payback_period <- calc_payback(cashflows)

# discounted payback period
discounted_cashflows <- calc_pv(cashflows, r = 0.06, n = 0:(length(cashflows)-1) )
payback_period_disc <- calc_payback(discounted_cashflows)

# compare results
payback_period
payback_period_disc


# Define NPV function: calc_npv
calc_npv <- function(cashflows, r) {
  n <- 0:(length(cashflows) - 1)
  npv <- sum( calc_pv(cashflows, r, n) )
  npv
}


# The base R function stats::uniroot can help find values between a lower bound (lower) and an upper bound (upper) where the value of a function is zero
# This can help us calculate the internal rate of return (IRR) for which NPV = 0.

# Define IRR function: calc_irr
calc_irr <- function(cashflows) {
    uniroot(calc_npv, 
        interval = c(0, 1), 
        cashflows = cashflows)$root
}

# Try out function on valid input
cashflows <- c(-100, 20, 20, 20, 20, 20, 20, 10, 5)
calc_irr(cashflows)


# Define profitability index function: calc_profitability_index
calc_profitability_index <- function(init_investment, future_cashflows, r) {
    discounted_future_cashflows <- calc_npv(future_cashflows, r)
    discounted_future_cashflows / abs(init_investment)
}

# Try out function on valid input
init_investment <- -100
cashflows <- c(0, 20, 20, 20, 20, 20, 20, 10, 5)
calc_profitability_index(init_investment, cashflows, 0.08)


# pull last year cashflow from vector of cashflows
last_year_cashflow <- cashflow[length(cashflow)]
last_period_n <- length(cashflow) - 1

# calculate terminal value for different discount raes
terminal_value_1 <- last_year_cashflow / ((0.15 - 0.1)*(1 + 0.15)^last_period_n)
terminal_value_2 <- last_year_cashflow / ((0.15 - 0.01)*(1 + 0.15)^last_period_n)
terminal_value_3 <- last_year_cashflow / ((0.15 + 0.05)*(1 + 0.15)^last_period_n)

# inspect results
terminal_value_1 
terminal_value_2
terminal_value_3


cashflow1 <- c(-50000, 100, 2000, 2000, 5000, 10000, 10000, 10000, 10000, 10000, 10000)
cashflow2 <- c(-1e+05, 20000, 20000, 20000, 20000, 20000)
cashflow3 <- c(-8000, 6000, 5000, 4000, 3000, 2000, 1000, 0)

# calculate internal rate of return (IRR) for each stream of cashflows
r1 <- calc_irr(cashflow1)
r2 <- calc_irr(cashflow2)
r3 <- calc_irr(cashflow3)

# calculate net present value (NPV) for each stream of cashflows, assuming r = irr
npv1 <- calc_npv(cashflow1, r1)
npv2 <- calc_npv(cashflow2, r2)
npv3 <- calc_npv(cashflow3, r3)

# examine results
npv1
npv2
npv3


cf1 <- c(-5000, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450)
cf2 <- c(-5000, 2000, 2000, 2000, 2000, 2000, 2000, -2000, -2000, -2000, -2000)
rates <- c(0, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1, 0.105, 0.11, 0.115, 0.12, 0.125, 0.13, 0.135, 0.14, 0.145, 0.15, 0.155, 0.16, 0.165, 0.17, 0.175, 0.18, 0.185, 0.19, 0.195, 0.2, 0.205, 0.21, 0.215, 0.22, 0.225, 0.23, 0.235, 0.24, 0.245, 0.25)

# create dataset of NPV for each cashflow and rate
npv_by_rates <- data.frame(rates) %>%
    group_by(rates) %>%
    mutate(npv1 = calc_npv(cf1, rates), npv2 = calc_npv(cf2, rates))
   
# plot cashflows over different discount rates     
ggplot(npv_by_rates, aes(x = rates, y = npv1)) + 
    geom_line() +
    geom_line(aes(y = npv2)) +
    labs(title = "NPV by Discount Rate", subtitle = "A Tale of Two Troubling Cashflows",
         y = "NPV ($)",x = "Discount Rate (%)"
         ) +
    annotate("text", x = 0.2, y = -500, label = "Two break-even points") +
    annotate("text", x = 0.2, y = -2500, label = "No break-even point")


cashflows <- data.frame(option=rep(1:4, each=11), time=rep(0:10, times=4), 
                        cashflow=c(-10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -1000, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, -1e+05, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, -10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                        )

# calculate summary metrics
cashflow_comparison <-
  cashflows %>%
  group_by(option) %>%
  summarize(npv = calc_npv(cashflow, 0.1), irr = calc_irr(cashflow))
      
# inspect output
cashflow_comparison
             
# visualize summary metrics
ggplot(cashflow_comparison, aes(x = npv, y = irr, col = factor(option))) +
    geom_point(size = 5) +
    geom_hline(yintercept = 0.1) +
    scale_y_continuous(label = scales::percent) +
    scale_x_continuous(label = scales::dollar) +
    labs(title = "NPV versus IRR for Project Alternatives",
         subtitle = "NPV calculation assumes 10% discount rate",
         caption = "Line shows actual discount rate to asses IRR break-even",
         x = "NPV ($)", y = "IRR (%)",col = "Option"
         )

```
  
  
  
***
  
Chapter 4 - Understanding Outcomes  
  
Building a Business Case:  
  
* Can combine metrics to evaluate potential business models  
* Example of working for a data-driven coffee shop - move from coffee to nitro-brew  
	* Consider only the incremental costs; for example rent and sunk costs (prior to the project) should not be considered  
    * May want to consider side-effects such as self-cannibalization  
* Business modeling is part art and part science; need a good blend  
  
Scenario Analysis with tidyr and purrr:  
  
* May want to consider additional projects, and there may not be funding for pursuing all of these ideas  
* May be some variability in the accuracy of our estimates - "what if analysis", aka "scenario analysis"  
	* scenario1 <- mutate(assumptions, var1 = 1.2 * var1)  
    * cashflow1 <- calc_model(scenario1)  
    * calc_npv(cashflow1)  
* Can use tidyr and purr to automate some of this  
	* all_scenarios %>% nest(-scenario)  
    * all_scenarios %>% nest(-scenario) %>% mutate( cashflow = map_df( data, calc_model) )  # data frame in column 'data' will be passed to calc_model  
    * all_scenarios %>% nest(-scenario) %>% mutate( cashflow = map_df( data, calc_model) ) %>% mutate( npv = map_dbl( cashflow, calc_npv) )  
  
Sensitivity analysis:  
  
* Sensitivity analysis is an approach to uncertainty that assesses the impact of each base assumption on the outcomes  
	* sensitivity <- expand.grid( factor = c(0.5, 1, 1.5), metric = c("vbl1", "vbl2") )  
    * sensitivity <- expand.grid( factor = c(0.5, 1, 1.5), metric = c("vbl1", "vbl2") ) %>% mutate(scenario = map2(metric, factor, ~factor_data(assumptions, .x, .y)))  # The ~ is for the anaonymous function  
* Can then visualize the impact of the sensitivity analysis; change in overall NPV vs. % change in base metric  
* Caution that errors are often correlated - this is just a univariate sensitivity analysis  
* Caution that not all metrics make the same magnitudes of change; analysis could be misinterpreted as a result  
  
Communicating Cashflow Concepts:  
  
* Long data is tidy - one column per metric and one row per observation (time)  
* Financial stakeholders will be more accustomed to non-tidy data  
	* long_cashflow <- gather(cashflow, key = Month, value = Value, -Metric)  # make the data long with key column Month and values in Value and Metric left as a column  
    * tidy_cashflow <- spread(long_cashflow, key = Metric, value = Value, -Metric)  
* Waterfall diagrams may be useful for communicating outcomes  
	* ggplot(data) + geom_rect( aes( xmin = , xmax = , ymin = , ymax = ) )  
    * ggplot(waterfall_data, aes( xmin = rn - 0.25, xmax = rn + 0.25, ymin = start, ymax = end) ) + geom_rect() + scale_x_continuous( breaks = waterfall_data$rn, labels = waterfall_data$category )  
  
Advanced Topics in Cashflow Modeling:  
  
* Many additional topics can be covered in financial modeling  
	* Debt vs. equity financing  
    * Decisions made today change the options available to us in the future  
    * Probabilistic simulation (impacts of uncertainty)  
  
Example code includes:  
```{r}

assumptions <- data.frame(year=0:10, 
                          unit_sales_per_day=c(0, 10, 12, 14, 15, 16, 17, 18, 18, 18, 18),
                          capex=c(5000, rep(0, 10)),
                          pct_cannibalization=c(0, rep(0.25, 10)), 
                          maintenance_cost=c(0, rep(250, 10)), 
                          depreciation_cost=c(0, rep(500, 10)), 
                          profit_margin_per_nitro=3, 
                          profit_margin_per_regular=1, 
                          labor_cost_per_hour=8, 
                          days_open_per_year=250
                          )

# Check the first few rows of the data
head(assumptions)

# Check the variable names of the data
names(assumptions)

# Plot the trend of unit_sales_per_day by year
ggplot(assumptions, aes(x = year, y = unit_sales_per_day)) + 
    geom_line()

tax_rate <- 0.36

# Create the cashflow_statement dataframe
cashflow_statement <-
  mutate(assumptions,
    sales_per_year = unit_sales_per_day * days_open_per_year,
    sales_revenue = sales_per_year * profit_margin_per_nitro,
    total_revenue = sales_revenue,
    labor_cost = days_open_per_year * 0.5 * labor_cost_per_hour,
    cannibalization_cost = sales_per_year * pct_cannibalization * profit_margin_per_regular,
    direct_expense = labor_cost + cannibalization_cost + maintenance_cost,
    gross_profit = total_revenue - direct_expense,
    operating_income = gross_profit - depreciation_cost,
    net_income = operating_income * (1 - tax_rate), 
    cashflow = net_income + depreciation_cost - capex    
  )


# build individual scenarios
optimist <- mutate(assumptions, unit_sales_per_day = unit_sales_per_day * 1.2, pct_cannibalization = 0.1)
pessimist <- mutate(assumptions, unit_sales_per_day = unit_sales_per_day * 0.8, profit_margin_per_nitro = 1)

# combine into one dataset
scenarios <-
  bind_rows(
    mutate(pessimist, scenario = "pessimist"),
    mutate(assumptions, scenario = "realist"),
    mutate(optimist, scenario = "optimist")
  )


calc_model <- function(assumptions){
  mutate( assumptions,
    sales_per_year = unit_sales_per_day * days_open_per_year,
    sales_revenue = sales_per_year * profit_margin_per_nitro,
    total_revenue = sales_revenue,
    labor_cost = days_open_per_year * 0.5 * labor_cost_per_hour,
    cannibalization_cost = sales_per_year * pct_cannibalization * profit_margin_per_regular,
    direct_expense = labor_cost + cannibalization_cost + maintenance_cost,
    gross_profit = total_revenue - direct_expense,
    operating_income = gross_profit - depreciation_cost,
    net_income = operating_income * (1 - 0.36), 
    cashflow = net_income + depreciation_cost - capex    
  )
}

calc_npv_from_cashflow <- function(cashflow, r){
  cashflow_line <- cashflow$cashflow
  sum(calc_pv(cashflow_line, r, 0:(length(cashflow_line)-1)))
}

# calculate scenario NPVs
scenario_analysis <- scenarios %>%
    nest(-scenario) %>%
    mutate(cashflow = map(data, calc_model)) %>%
    mutate(npv = map_dbl(cashflow, calc_npv_from_cashflow, 0.2))

# inspect results
select(scenario_analysis, scenario, npv)


# scenario analysis bar chart
ggplot(data = scenario_analysis, aes(x = scenario, y = npv, fill = scenario)) + 
    geom_bar(stat = "identity") +
    scale_y_continuous(labels = scales::dollar) +
    labs(title = "NPV Scenario Analysis of Nitro Coffee Expansion") +
    guides(fill = FALSE)


# define sensitivity factor function
factor_data <- function(data, metric, factor){
  data[[metric]] <- data[[metric]] * factor
  data
}

# create sensitivity analysis
sensitivity <-
  expand.grid(
    factor = seq(0.5,1.5,0.1), 
    metric = c("profit_margin_per_nitro", "labor_cost_per_hour", "pct_cannibalization", "unit_sales_per_day")) %>%
  mutate(scenario = map2(metric, factor, ~factor_data(assumptions, .x, .y))) %>%
  mutate(cashflow = map(scenario, calc_model)) %>% 
  mutate(npv = map_dbl(cashflow, calc_npv_from_cashflow, r = 0.2))


ggplot(sensitivity,
       aes(x = factor, y = npv, col = metric)
       ) +
  geom_line() +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Sensivity Analysis",
    x = "Factor on Original Assumption",
    y = "Projected NPV",
    col = "Metric"
  )


# examine current cashflow strucutre
tidy_cashflow <- data.frame(Month=1:6, 
                            Received=c(100, 200, 300, 400, 500, 500), 
                            Spent=c(150, 175, 200, 225, 250, 250)
                            )

# create long_cashflow with gather
# long_cashflow <- tidyr::gather(cashflow, key = Month, value = Value, -Metric)

# create tidy_cashflow with spread
# tidy_cashflow <- tidyr::spread(long_cashflow, key = Metric, value = Value)

# examine results
tidy_cashflow


# create long_cashflow with gather
long_cashflow <- tidyr::gather(tidy_cashflow, key = Metric, value = Value, -Month)

# create untidy_cashflow with spread
untidy_cashflow <- tidyr::spread(long_cashflow, key = Month, value = Value)

# examine results
untidy_cashflow


gross_profit_summary <- data.frame(metric=c("Sales Revenue", "Keg Cost", "Cannibalization Cost", "Labor Cost", "Maintenance Cost"), 
                                   value=c(187200, -78240, -31200, -10000, -2500)
                                   )

# compute min and maxes for each line item
waterfall_items <-
  mutate(gross_profit_summary,
         end = cumsum(value), 
         start = lag(cumsum(value), 1, default = 0))

# compute totals row for waterfall metrics
waterfall_summary <- 
  data.frame(metric = "Gross Profit", 
             end = sum(gross_profit_summary$value), 
             start = 0)

# combine line items with summary row
waterfall_data <-
  bind_rows(waterfall_items, waterfall_summary) %>%
  mutate(row_num = row_number())


# Plot waterfall diagram
ggplot(waterfall_data, aes(fill = (end > start))) +
  geom_rect(aes(xmin = row_num - 0.25, xmax = row_num + 0.25, 
                ymin = start, ymax = end)) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = waterfall_data$row_num, labels = waterfall_data$metric) +
  # Styling provided for you - check out a ggplot course for more information!
  scale_y_continuous(labels = scales::dollar) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  guides(fill = FALSE) +
  labs(
      title = "Gross Profit for Proposed Nitro Coffee Expansion",
      subtitle = "Based on pro forma 10-year forecast")

```
  
  
  
***
  
###_Visualizing Big Data with Trelliscope_  
  
Chapter 1 - General Strategies for Visualizing Big Data  
  
Visualizing summaries:  
  
* Start with summarization plots for large datasets  
* Univariate variable descriptions from the gapminder dataset  
	* Continuous - histograms  
    * ggplot(gapminder, aes(lifeExp)) + geom_histogram()  
    * Discrete - bar chart for counts  
    * ggplot(gapminder, aes(continent)) + geom_bar()  
    * Temporal - bin by time, compute number of observations  
    * by_year <- gapminder %>% group_by(year) %>% summarise(medianGdpPercap = median(gdpPercap, na.rm = TRUE))  
    * ggplot(by_year, aes(year, medianGdpPercap)) + geom_line()  
* Binning is a common strategy for summarizing large datasets - particularly the dplyr groupby function  
* Can extend to the NYC taxi cab dataset - millions of records on the number of taxi trips taken in NYC  
  
Adding more detail to summaries:  
  
* Can bin pairs of variables in two dimensions, and facet vary on additional variables  
* The geom_hex() can be a useful means for showing the 2x2 plot  
	* ggplot(tx, aes(tip_amount, total_amount)) + geom_hex(bins = 75) + scale_x_log10() + scale_y_log10() + geom_abline(slope = 1, intercept = 0)  
* Can use the facet_wrap to see visual subsets of the data - all elements of the main call are added to each of the facets automatically  
	* ggplot(daily_count, aes(pickup_date, n_rides)) + geom_point() + facet_grid(~ pickup_dow)  
  
Visualizing subsets:  
  
* May need to look in more detail than what is available in the summaries  
* A useful technique is to take a natural subset of a large dataset; for example, all of the price data for a single stock  
* Example of examining the "zero tip" nature of the cash rides shown in the NYC taxi dataset; subset to 5,000 rides from UES South to UES North  
	* ggplot(tx_pop, aes(trip_duration, total_amount)) + geom_point(alpha = 0.2)  
    * ggplot(tx_pop, aes(sample = total_amount, color = payment_type)) + geom_qq(distribution = stats::qunif) + ylim(c(3, 20))  # quantile plot against the uniform distribution  
  
Visualizing all subsets:  
  
* May want to extend the analysis to all of the routes in the taxi dataset  
* Faceting will not work with over 20k total panels as in the taxi dataset  
* Refined approaches using trelliscope help enable these higher-volume visualizations  
  
Example code includes:  
```{r cache=TRUE}

load("./RInputFiles/tx_sub.RData")
glimpse(tx)

tx <- tx %>%
    rename(pickup_date=pick_day, pickup_dow=pick_dow)
glimpse(tx)


# Summarize taxi ride count by pickup day
daily_count <- tx %>%
  group_by(pickup_date) %>%
  summarise(n_rides=n())

# Create a line plot
ggplot(daily_count, aes(x=pickup_date, y=n_rides)) +
  geom_line()


# Create a histogram of total_amount
ggplot(tx, aes(x=total_amount)) +
  geom_histogram() +
  scale_x_log10()


# Create a bar chart of payment_type
ggplot(tx, aes(x=payment_type)) +
  geom_bar()


# Create a hexagon-binned plot of total_amount vs. trip_duration
ggplot(tx, aes(x=trip_duration, y=total_amount)) +
  geom_hex(bins=75) +
  scale_x_log10() +
  scale_y_log10()


# Summarize taxi rides count by payment type, pickup date, pickup day of week, and payment type
daily_count <- tx %>%
  filter(payment_type %in% c("Card", "Cash")) %>%
  group_by(payment_type, pickup_date, pickup_dow) %>%
  summarise(n_rides=n())

# Plot the data
ggplot(daily_count, aes(x=pickup_date, y=n_rides)) +
  geom_point() +
  facet_grid(payment_type ~ pickup_dow) +
  coord_fixed(ratio = 0.4)


# Histogram of the tip amount faceted on payment type
ggplot(tx, aes(x=tip_amount+0.01)) +
  geom_histogram() +
  scale_x_log10() +
  facet_wrap(~ payment_type, ncol=1, scales="free_y")


# Get data ready to plot
amount_compare <- tx %>%
    mutate(total_no_tip = total_amount - tip_amount) %>%
    filter(total_no_tip <= 20) %>%
    sample_n(size=round(nrow(.)/20), replace=FALSE) %>%
    select(total_amount, total_no_tip, payment_type) %>%
    tidyr::gather(amount_type, amount, -payment_type)

# Quantile plot
ggplot(amount_compare, aes(sample=amount, color=payment_type)) +
  geom_qq(distribution=stats::qunif, shape = 21) +
  facet_wrap(~ amount_type) +
  ylim(c(3, 20))

```
  
  
  
***
  
Chapter 2 - ggplot2 + Trelliscope JS  
  
Faceting with Trelliscope JS:  
  
* The trelliscope is a powerful tool for viewing moderate and large datasets - will again explore gapminder as a starting point  
	* ggplot(gapminder, aes(year, lifeExp, group = country)) + geom_line()  # enormouse over-plotting problem  
    * ggplot(gapminder, aes(year, lifeExp, group = country, color = continent)) + geom_line() + facet_wrap(~ continent, nrow = 1) + guides(color = FALSE)  # somewhat better to see insights by continent  
    * ggplot(gapminder, aes(year, lifeExp)) + geom_line() + facet_wrap(~ country + continent)  # labels overplot the data (impossible to view)  
* The trelliscope is just a replacement of a single call in the ggplot  
	* It's as easy as swapping out facet_wrap() for facet_trelliscope()  
    * As with facet_wrap(), control rows and columns with nrow and ncol  
  
Interacting with Trelliscope JS displays:  
  
* Can look at just one panel as per the JS user controls; focus in on the most interesting areas of a visualization  
* Can page through the panels using the Prev and Next panels and/or the arrow buttons  
* Can customize the grid layout by clicking on the "Grid" widget on the side of the plot  
* The "cognostics" are also clickable as far as which labels will be shown  
	* Can subset by selecting portions of a histogram and/or categories for viewing  
    * Default sorting order are low-to-high, though this can be customized  
  
Additional Trelliscope JS features:  
  
* Can use plotly for additional interactivity within panels - adds tooltips and pop-ups  
	* gap_life <- select(gapminder, year, lifeExp, country, continent)  
    * ggplot(gap_life, aes(year, lifeExp)) + geom_point() + facet_trelliscope(~ country + continent, name = "lifeExp_by_country", desc = "Life expectancy vs. year for 142 countries.", nrow = 2, ncol = 3, as_plotly = TRUE)  
* Can calculate cognostics automatically for additional useful metrics  
	* ggplot(gap_life, aes(year, lifeExp)) + geom_point() + facet_trelliscope(~ country + continent, name = "lifeExp_by_country", desc = "Life expectancy vs. year for 142 countries.", nrow = 2, ncol = 3, auto_cog = TRUE)  
* Multi-panel displays require good usage of axis limits  
	* Similar to the scales argument in facet_wrap  
    * Default behavior is scales = "same"  
    * When scales = "sliced", the scales have the same ranges but with potentially different starting points (these are currently only in trelliscopejs rather than in ggplot2)  
    * When scales = "free", the scales are independent by facet  
  
Adding your own cognostics:  
  
* Can add new variables as cognostics by adding new variables to the data  
* Example of creating the latest observed life expectancy  
	* gap <- gapminder %>% group_by(country) %>% mutate(latestLifeExp = tail(lifeExp, 1))  # do not summarize; maintain the original data  
* Can create links within the displays; for example  
	* gap <- gapminder %>% group_by(country, continent) %>% mutate(wiki = paste0("https://en.wikipedia.org/wiki/", country))  
* Customizing custom cognostics  
	* A function cog() can be wrapped around a variable to fine-tune how a cognostic is handled in Trelliscope  
    * desc: a meaningful description for the cognostic  
    * default_label: boolean specifying whether the cognostic should be shown as a label by default or not  
  
Example code includes:  
```{r eval=FALSE}

library(trelliscopejs)

data(gapminder, package="gapminder")
glimpse(gapminder)


# Create the plot
gapminder %>%
    filter(continent=="Oceania") %>%
    ggplot(aes(x=year, y=lifeExp)) +
    geom_line() +
    # Facet on country and continent
    facet_trelliscope(~ country + continent, data=gapminder)

gapminder %>%
    filter(continent=="Oceania") %>%
    ggplot(aes(x=year, y=lifeExp)) +
    geom_line() +
    facet_trelliscope(~ country + continent, name = "lifeExp_by_country",
                      desc = "Life expectancy vs. year per country", nrow = 1, ncol = 2
                      )


# Create the plot
gapminder %>%
    filter(continent=="Oceania") %>%
    ggplot(aes(x=year, y=lifeExp)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    facet_trelliscope(~ country + continent, name = "lifeExp_by_country",
                      desc = "Life expectancy vs. year for 142 countries.",
                      nrow = 2, ncol = 3,
                      # Set the scales
                      scales="sliced",
                      # Specify automatic cognistics
                      auto_cog=TRUE
                      )


# Group by country and create the two new variables
gap <- gapminder %>%
    filter(continent=="Oceania") %>%
    group_by(country) %>%
    mutate(delta_lifeExp = tail(lifeExp, 1) - head(lifeExp, 1),
           ihme_link = paste0("http://www.healthdata.org/", country)
           )

# Add the description
gap$delta_lifeExp <- cog(gap$delta_lifeExp, desc = "Overall change in life expectancy")

# Specify the default label
gap$ihme_link <- cog(gap$ihme_link, default_label = TRUE)

ggplot(gap, aes(year, lifeExp)) +
    geom_point() +
    facet_trelliscope(~ country + continent,
                      name = "lifeExp_by_country",
                      desc = "Life expectancy vs. year for 142 countries.",
                      nrow = 2, ncol = 3, scales = c("same", "sliced")
                      )

```
  
  
  
***
  
Chapter 3 - Trelliscope in the Tidyverse  
  
Trelliscope in the Tidyverse:  
  
* Stock market dataset called "stocks" has price data for the 500 highest-traded NASDAQ stocks  
* Can use ggplot2 to understand stock price changes by year  
* Might instead want to use the canbdlestick chart from plotly - can also zoom, plot, and hover based on the plotly characteristics  
	* candlestick_plot <- function(d) plot_ly(d, x = ~date, type = "candlestick", open = ~open, close = ~close, high = ~high, low = ~low)  
    * candlestick_plot(filter(stocks, symbol == "AAPL"))  
* Can use nesting to make for a smaller data frame with column data holding a tibble  
	* by_symbol <- stocks %>% group_by(symbol) %>% nest()  
    * by_symbol <- mutate(by_symbol, last_close = map_dbl(data, function(x) tail(x$close, 1)))  
    * by_symbol <- mutate(by_symbol, panel = map_plot(data, candlestick_plot))  # map_plot is from the trelliscope package; one plot for each stock  
    * trelliscope(by_symbol, name = "candlestick_top500", nrow = 2, ncol = 3)  # show the plot with the given initial layout  
  
Cognostics:  
  
* Can add variables, which will then be included as cognostics in the trelliscope  
* Can add metadata to the stocks database, then use nest to have the metadata as a new variable  
	* by_symbol <- left_join(by_symbol, stocks_meta)  
    * by_symbol <- mutate(by_symbol, volume_stats = map(data, function(x) { data_frame( min_volume = min(x$volume), max_volume = max(x$volume) ) }))  
  
Trelliscope options:  
  
* Can further customize multiple options related to a trelliscope plot  
* Can specify the output directory for sharing the display with others - default is just a temporary directory, and will over-write anything in the directory with the same name  
	* trelliscope(dat, path = "...", ...)  
    * ggplot(...) + ... + facet_trelliscope(path = "...", ...)  
* May want to provide more detailed descriptions of the data - shows in the viewer icon  
	* trelliscope(by_symbol, name = "candlestick_top500", desc = "Candlestick plot of the 500 most-traded NASDAQ stocks in 2016", md_desc = " ## Candlestick Plot A [candlestick plot](https://en.wikipedia.org/wiki/Candlestick_chart) is a financial plot... ... ")  # multi-line markdown in md_desc  
* Plot aspect ratio is important for strong visual displays - default is a square of 500px x 500px  
	* trelliscope(dat, width = 600, height = 300, ...)  # specified in units of pixels  
    * ggplot(...) + ... + facet_trelliscope(width = 600, height = 300, ...)  
* Default state can be further specified - interface is currently undergoing active improvement  
	* trelliscope(dat, state = ..., ...)  
    * ggplot(...) + ... + facet_trelliscope(state = ..., ...)  
  
Visualizing databases of images:  
  
* May want to view large collections of images, which is a strong use case for trelliscope - example of the pokemon dataset  
	* select(pokemon, url_image)  
    * pokemon <- mutate(pokemon, panel = img_panel(url_image))  
* May have local images for comparisons instead  
	* path <- file.path(tempdir(), "pokemon_local")  
    * dir.create(path)  
    * for (url in pokemon$url_image)  
    *     download.file(url, destfile = file.path(path, basename(url)))  
    * pokemon$image <- basename(pokemon$url_image)  
    * pokemon <- mutate(pokemon, panel = img_panel_local(image))  
    * trelliscope(pokemon, name = "pokemon", nrow = 3, ncol = 6, path = path)  
  
Example code includes:  
```{r eval=FALSE}

# do not have dataset 'stocks'
by_symbol <- stocks %>%
  group_by(symbol) %>%
  nest()

min_volume_fn <- function(x) min(x$volume)

# Create new column
by_symbol_min <- by_symbol %>%
  mutate(min_volume = map_dbl(data, min_volume_fn))


ohlc_plot <- function(d) {
  plot_ly(d, x = ~date, type = "ohlc",
    open = ~open, close = ~close,
    high = ~high, low = ~low)
}

by_symbol_plot <- mutate(by_symbol, panel=map_plot(data, ohlc_plot))

trelliscope(by_symbol_plot, name="ohlc_top500")


# Create market_cap_log
by_symbol <- mutate(by_symbol,
  market_cap_log = cog(
    val = log10(market_cap),
    desc = "log base 10 market capitalization"
  )
)


annual_return <- function(x)
  100 * (tail(x$close, 1) - head(x$open, 1)) / head(x$open, 1)

# Compute by_symbol_avg
by_symbol_avg <- mutate(by_symbol,
  stats = map(data, function(x) {
    data_frame(
      mean_close = mean(x$close),
      mean_volume = mean(x$volume),
      annual_return = annual_return(x)
      )
    }
  )
)


# Create the trelliscope display
p <- trelliscope(by_symbol, width=600, height=300, name="ohlc_top500", desc="Example aspect 2 plot")


pokemon %>%
  # Reduce the variables in the dataset
  select(pokemon, type_1, attack, generation_id, url_image) %>%
  mutate(
    # Respecify pokemon
    pokemon = cog(pokemon, default_label=TRUE),
    # Create panel variable
    panel = img_panel(url_image)
  ) %>%
  # Create the display
  trelliscope(name="pokemon", nrow=3, ncol=6)

```
  
  
  
***
  
Chapter 4 - Case Study: Exploring Montreal BIXI Bike Data  
  
Montreal BIXI Bike Data:  
  
* Bike riding data for 2017 for BIXI Montreal - desire to understand patterns of usages  
* Aggregations and summary plots for the big picture, followed by deep-dives with trelliscope  
* Randomly sub-samples down to 1,000,000 records for the examples  
  
Summary Visualization Recap:  
  
* Clear differences in weekday usage (peaks at 08h00 and 17h00) and weekend usage (peak around 15h00)  
* Non-members tend to ride as much on weekends (over 2 days) rather than weekdays (over 5 days)  
	* Hypothesis that members are more likely to be commuters and non-members more likely to be tourists  
  
Top 100 routes dataset:  
  
* May want to examine route-specific behaviors  
* Considering a route as a from-to, we want to look at the top-100 routes, ignoring round-trip routes  
	* route_tab <- bike %>% filter(start_station_code != end_station_code) %>% group_by(start_station_code, end_station_code) %>% summarise(n = n()) %>% arrange(-n)  
    * top_routes <- paste( route_tab$start_station_code[1:100], route_tab$end_station_code[1:100])  
    * top100 <- bike %>% filter(paste(start_station_code, end_station_code) %in% top_routes)  
* Preparing data for visualization  
	* route_hod <- top100 %>% group_by(start_station_code, end_station_code, start_hod, weekday) %>% summarise(n = n())  
    * route_hod <- route_hod %>% left_join(start_stations) %>% left_join(end_stations)  
  
Wrap up:  
  
* Can filter and sort to find key information in the trelliscope facets  
* Can further investigate routes with rush hour peaks (AM, PM, or both) as well as routes with differences in weekend and weekday  
  
Example code includes:  
```{r cache=TRUE}

# DO NOT HAVE FULL BIXI Data
bike04 <- read_csv("./RInputFiles/BIXIData/OD_2017-04.csv")
bike05 <- read_csv("./RInputFiles/BIXIData/OD_2017-05.csv")
bike06 <- read_csv("./RInputFiles/BIXIData/OD_2017-06.csv")
bike07 <- read_csv("./RInputFiles/BIXIData/OD_2017-07.csv")
bike08 <- read_csv("./RInputFiles/BIXIData/OD_2017-08.csv")
bike09 <- read_csv("./RInputFiles/BIXIData/OD_2017-09.csv")
bike10 <- read_csv("./RInputFiles/BIXIData/OD_2017-10.csv")
bike11 <- read_csv("./RInputFiles/BIXIData/OD_2017-11.csv")
stations <- read_csv("./RInputFiles/BIXIData/Stations_2017.csv")

bike <- rbind(bike04, bike05, bike06, bike07, bike08, bike09, bike10, bike11) %>%
    mutate(membership=factor(is_member, levels=c(1, 0), labels=c("member", "non-member")), 
           start_day=as.Date(start_date), 
           start_dow=factor(lubridate::wday(start_date), levels=1:7, labels=c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")),
           weekday=factor(ifelse(start_dow %in% c("Sat", "Sun"), 2, 1), levels=1:2, labels=c("workweek", "weekend")), 
           start_hod=lubridate::hour(start_date), 
           start_mon=lubridate::month(start_date), 
           start_wk=lubridate::week(start_date)
           )
glimpse(bike)


# Compute daily counts
daily <- bike %>%
  group_by(start_day, weekday) %>%
  summarise(n = n())

# Plot the result
ggplot(daily, aes(x=start_day, y=n, color=weekday)) +
  geom_point()


# Compute week_hod
week_hod <- bike %>%
  group_by(start_wk, start_hod, weekday) %>%
  summarize(n=n())

# Plot the result
ggplot(week_hod, aes(x=start_wk, y=n, color=weekday)) +
  geom_point() +
  facet_grid(cols=vars(start_hod)) +
  scale_y_sqrt()


# Compute wk_memb_hod
wk_memb_hod <- bike %>%
  group_by(start_wk, start_hod, weekday, membership) %>%
  summarize(n=n())

# Plot the result
ggplot(wk_memb_hod, aes(x=start_wk, y=n, color=weekday)) +
  geom_point() +
  facet_grid(membership ~ start_hod) +
  scale_y_sqrt()


# Compute daily_may
daily_may <- bike %>%
  filter(start_mon == 5) %>%
  group_by(start_day, start_hod, membership) %>%
  summarise(n = n())

# Plot the result
ggplot(daily_may, aes(x=start_hod, y=n, color=membership)) +
  geom_point() + 
  facet_wrap(~ start_day, ncol=7)


# ggplot(daily_may, aes(x=start_hod, y=n, color = membership)) +
#   geom_point() +
  # Facet on start_day
#   facet_trelliscope(~ start_day, nrow=2, ncol=7)


# Function to construct a Google maps URL with cycling directions
make_gmap_url <- function(start_lat, start_lon, end_lat, end_lon) {
  paste0("https://www.google.com/maps/dir/?api=1",
    "&origin=", start_lat, ",", start_lon,
    "&destination=", end_lat, ",", end_lon,
    "&travelmode=bicycling")
}

load("./RInputFiles/route_hod.RData")
glimpse(route_hod)

# Compute tot_rides, weekday_diff, and map_url
route_hod_updated <- route_hod %>%
  group_by(start_station_code, end_station_code) %>%
  mutate(
    tot_rides = sum(n),
    weekday_diff = mean(n[weekday == "workweek"]) - mean(n[weekday == "weekend"]),
    map_url = make_gmap_url(start_lat, start_lon, end_lat, end_lon))


# Create the plot
# ggplot(route_hod, aes(x=start_hod, y=n, color=weekday)) +
  # geom_point(size=3) + 
  # facet_trelliscope(~start_station_name + end_station_name, nrow=2, ncol=4) + 
  # theme(legend.position = "none")

```
  
  
  
***
  
###_Visualization Best Practices in R_  
  
Chapter 1 - Proportions of a Whole  
  
Course/Grammar of Graphics Information:  
  
* General objective is to think deeply about the data and make the best visualization based on the data at hand  
* Will cover standard visualization techniques and alternative visualization techniques for improvement  
	* Topics are not cut and dry, rules will have exceptions, and the emphasis should be on thinking through the specific issue at hand  
* Dataset will be available from WHO - measles, mumps, etc., for 7 diseases  
	* who_disease  
    * amr_region <- who_disease %>% filter(region == 'AMR')  
    * ggplot(amr_region, aes(x = year, y = cases, color = disease)) + geom_point(alpha = 0.5)  
  
Pie Chart and Friends:  
  
* May want to examine the proportions of a single population - adds up to 100%  
* The pie chart is often one of the first techniques learned; but, often abused (too many slices, 3D, not adding up to 100%, and the like)  
	* Data encoded in angles, but humans are better at comparing lengths  
    * After three slices, it typically becomes hard to compare (not just angles, but offset angles)  
    * who_disease %>% mutate( region = ifelse( region %in% c('EUR', 'AFR'), region, 'Other') ) %>% ggplot(aes(x = 1, fill = region)) + geom_bar(color = 'white') + coord_polar(theta = "y") + theme_void()  
* The waffle chart can be more precise; encoding data by area (add the squares) rather than by angle  
	* obs_by_region <- who_disease %>% group_by(region) %>% summarise(num_obs = n()) %>% mutate(percent = round(num_obs/sum(num_obs)*100))  
    * percent_by_region <- obs_by_region$percent  
    * names(percent_by_region) <- obs_by_region$region  
    * waffle::waffle(percent_by_region, rows = 5)  
  
When to use Bars:  
  
* May want to compare multiple proportions to each other - facets are not ideal for pie or waffle charts  
* Can instead use stacked bar charts with a common y-axis  
	* who_disease %>% filter(region == 'SEAR') %>% ggplot(aes(x = countryCode, y = cases, fill = disease)) + geom_col(position = 'fill')  # position="fill" makes it a proportion chart  
* Stacked bars tend to be worse in isolation by themselves (only the first and last class have a good anchor)  
* Generalized best practices include 1) do not make a stacked bar chart in isolation, and 2) keep the number of groups reasonably small  
  
Example code includes:  
```{r}

who_disease <- readr::read_csv("./RInputFiles/who_disease.csv")
glimpse(who_disease)


# set x aesthetic to region column
ggplot(who_disease, aes(x=region)) +
  geom_bar()


# filter data to AMR region. 
amr_region <- who_disease %>%
    filter(region=="AMR")

# map x to year and y to cases. 
ggplot(amr_region, aes(x=year, y=cases)) + 
    # lower alpha to 0.5 to see overlap.   
    geom_point(alpha=0.5)


# Wrangle data into form we want. 
disease_counts <- who_disease %>%
    mutate(disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other')) %>%
    group_by(disease) %>%
    summarise(total_cases = sum(cases))


ggplot(disease_counts, aes(x = 1, y = total_cases, fill = disease)) +
    # Use a column geometry.
    geom_col() + 
    # Change coordinate system to polar and set theta to 'y'.
    coord_polar(theta="y")


ggplot(disease_counts, aes(x = 1, y = total_cases, fill = disease)) +
    # Use a column geometry.
    geom_col() +
    # Change coordinate system to polar.
    coord_polar(theta = "y") +
    # Clean up the background with theme_void and give it a proper title with ggtitle.
    theme_void() +
    ggtitle('Proportion of diseases')


disease_counts <- who_disease %>%
    group_by(disease) %>%
    summarise(total_cases = sum(cases)) %>% 
    mutate(percent = round(total_cases/sum(total_cases)*100))

# Create an array of rounded percentages for diseases.
case_counts <- disease_counts$percent
# Name the percentage array with disease_counts$disease
names(case_counts) <- disease_counts$disease

# Pass case_counts vector to the waffle function to plot
waffle::waffle(case_counts)


disease_counts <- who_disease %>%
    mutate(disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other')) %>%
    group_by(disease, year) %>% # note the addition of year to the grouping.
    summarise(total_cases = sum(cases))

# add the mapping of year to the x axis. 
ggplot(disease_counts, aes(x=year, y = total_cases, fill = disease)) +
    # Change the position argument to make bars full height
    geom_col(position="fill")


disease_counts <- who_disease %>%
    mutate(
        disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other') %>%
            factor(levels=c('measles', 'other', 'mumps')) # change factor levels to desired ordering
        ) %>%
    group_by(disease, year) %>%
    summarise(total_cases = sum(cases))

# plot
ggplot(disease_counts, aes(x = year, y = total_cases, fill = disease)) +
    geom_col(position = 'fill')


disease_counts <- who_disease %>%
    # Filter to on or later than 1999  
    filter(year >= 1999) %>% 
    mutate(disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other')) %>%
    group_by(disease, region) %>%    # Add region column to grouping
    summarise(total_cases = sum(cases))

# Set aesthetics so disease is the stacking variable, region is the x-axis and counts are the y
ggplot(disease_counts, aes(x=region, y=total_cases, fill=disease)) +
    # Add a column geometry with the proper position value.
    geom_col(position="fill")

```
  
  
  
***
  
Chapter 2 - Point Data  
  
Point Data:  
  
* Switching from proportions to points (numerical observations for multiple categories) - single observations per item  
* Most common visualization is a bar chart  
	* ggplot(who_disease) + geom_col(aes(x = disease, y = cases))  
* Bar charts are frequently used incorrectly for the task at hand; really best for accumulating measures (money can be thought of a stack of coins)  
	* However, if the bars are something non-stackable (such as likelihood of error), then bars are not appropriate  
    * Humans tend to perceive bars as meaning "everything inside the bar is included, and everything outside the bar is excluded"  
  
Point Charts:  
  
* When a point is not really a stackable quantity (percentile, temperature, log-transform, etc.), alternative to bar charts should be used  
* Instead, a point right at the top of the bar may work best; also known as a dot chart; high precision and simple  
	* who_subset %>% ggplot(aes(y = country, x = log10(cases_2016))) + geom_point()  
* Can reorder the point charts, applied directly in the aesthetic  
	* who_subset %>% mutate(logFoldChange = log2(cases_2016/cases_2006)) %>% ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) + geom_point()  
  
Tuning Charts:  
  
* Can make the default charts more efficient and attractive  
	* busy_bars <- who_disease %>% filter(region == 'EMR', disease == 'measles', year == 2015) %>% ggplot(aes(x = country, y = cases)) + geom_col()  # Base plot  
* Flipping bar charts can make labels easier to read  
	* busy_bars + coord_flip() # swap x and y axes!  
* Can also get rid of excess grid lines along the categorical axis  
	* # get rid of vertical grid lines  
    * plot + theme( panel.grid.major.x = element_blank() )  
* Lighter backgrounds can make for better contrasts in a point chart  
	* who_subset %>% ggplot(aes(y = reorder(country, cases_2016), x = log10(cases_2016))) + geom_point(size = 2) + theme_minimal()  
  
Example code includes:  
```{r}

who_disease %>% 
    # filter to india in 1980
    filter(country=="India", year==1980) %>% 
    # map x aesthetic to disease and y to cases
    ggplot(aes(x=disease, y=cases)) +
    # use geom_col to draw
    geom_col()


who_disease %>%
    # filter data to observations of greater than 1,000 cases
    filter(cases > 1000) %>%
    # map the x-axis to the region column
    ggplot(aes(x=region)) +
    # add a geom_bar call
    geom_bar()


interestingCountries <- c('NGA', 'SDN', 'FRA', 'NPL', 'MYS', 'TZA', 'YEM', 'UKR', 'BGD', 'VNM')
who_subset <- who_disease %>% 
    filter(countryCode %in% interestingCountries, disease == 'measles', year %in% c(1992, 2002)) %>% 
    mutate(year = paste0('cases_', year)) %>%
    spread(year, cases)

 
# Reorder y axis and change the cases year to 1992
ggplot(who_subset, aes(x = log10(cases_1992), y = reorder(country, cases_1992))) +
    geom_point()


who_subset %>% 
    # calculate the log fold change between 2016 and 2006
    mutate(logFoldChange = log2(cases_2002/cases_1992)) %>% 
    # set y axis as country ordered with respect to logFoldChange
    ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) +
    geom_point() +
    # add a visual anchor at x = 0
    geom_vline(xintercept=0)


who_subset %>% 
    mutate(logFoldChange = log2(cases_2002/cases_1992)) %>% 
    ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) +
    geom_point() +
    geom_vline(xintercept = 0) +
    xlim(-6,6) +
    # add facet_grid arranged in the column direction by region and free_y scales
    facet_grid(region ~ ., scale="free_y")


amr_pertussis <- who_disease %>% 
    filter(region == 'AMR', year == 1980, disease == 'pertussis')


# Set x axis as country ordered with respect to cases. 
ggplot(amr_pertussis, aes(x = reorder(country, cases), y = cases)) +
    geom_col() +
    # flip axes
    coord_flip()


amr_pertussis %>% 
    # filter to countries that had > 0 cases. 
    filter(cases > 0) %>%
    ggplot(aes(x = reorder(country, cases), y = cases)) +
    geom_col() +
    coord_flip() +
    theme(panel.grid.major.y = element_blank())


amr_pertussis %>% filter(cases > 0) %>% 
    ggplot(aes(x = reorder(country, cases), y = cases)) + 
    # switch geometry to points and set point size = 2
    geom_point(size=2) + 
    # change y-axis to log10. 
    scale_y_log10() +
    # add theme_minimal()
    theme_minimal() +
    coord_flip()

```
  
  
  
***
  
Chapter 3 - Single Distributions  
  
Importance of Distributions:  
  
* Distribution data includes multiple observations from the same population  
* Plotting distribution is valuable in many ways  
	* Identifying errors or outliers  
    * Identifying multiple peaks; potential segmenting variable  
    * Distributions are more accurate and truthful to the data than a summary statistic  
* Histograms are a common approach to showing distributions  
* Box plots are a common approach for comparing multiple distributions  
* Will examing the md_speeding dataset from Montgomery County, MD  
	* md_speeding %>% filter(vehicle_color == 'BLUE') %>% ggplot(aes(x = speed)) + geom_histogram()  
  
Histogram Nuances:  
  
* Histograms have the advantage of being intuitive to readers and easier to interpret; higher bars mean more frequent  
* Histograms need care in setting the number of bins for better interpretation; binning can significantly change the plot  
	* If you have 150+ points, set the bins to 100 and call it a day  
  
Kernel Density Estimates:  
  
* The KDE (kernel density estimator) is a common approach to managing the binning and other challenges associated with histograms  
	* Kernel function is created on top of every point, then summed across the distribution  
    * Typically, a Gaussian kernel is used, though there are other options  
    * sample_n(md_speeding, 100) %>% ggplot(aes(x = percentage_over_limit)) + geom_density( fill = 'steelblue', bw = 8 )  
* For the KDE, need to choose the width of the kernel, which is controlled by the bw parameter (analogous to sd for Gaussians)  
* Can also show all of the data using the rug plot  
	* p <-sample_n(md_speeding, 100) %>% ggplot(aes(x = percentage_over_limit)) + geom_density( fill = 'steelblue', # fill in curve with color bw = 8 # standard deviation of kernel )  
    * p + geom_rug(alpha = 0.4)  
  
Example code includes:  
```{r cache=TRUE}

colKeep <- c('work_zone', 'vehicle_type', 'vehicle_year', 'vehicle_color', 'race', 'gender', 
             'driver_state', 'speed_limit', 'speed', 'day_of_week', 'day_of_month', 'month', 
             'hour_of_day', 'speed_over', 'percentage_over_limit'
             )
colRead <- c("Work Zone", "VehicleType", "Year", "Color", "Race", "Gender", 
             "DL State", "Description", "Date Of Stop", "Time Of Stop")
regFind <- ".*EXCEEDING MAXIMUM SPEED: ([0-9]+) MPH .* POSTED ([0-9]+) MPH .*"

md_speeding <- readr::read_csv("./RInputFiles/MD_Traffic/Traffic_violations.csv", n_max=200000) %>%
    select(colRead) %>%
    filter(grepl("EXCEEDING MAXIMUM SPEED: ", Description)) %>%
    rename(work_zone="Work Zone", vehicle_type=VehicleType, vehicle_year=Year, 
           vehicle_color=Color, race=Race, gender=Gender, driver_state="DL State", 
           stopDate="Date Of Stop", stopTime="Time Of Stop") %>%
    mutate(speed_limit=as.integer(gsub(regFind, "\\2", Description)), 
           speed=as.integer(gsub(regFind, "\\1", Description)), 
           speed_over=speed - speed_limit, 
           percentage_over_limit=100 * speed_over / speed_limit, 
           stopDate=as.Date(stopDate, format="%m/%d/%Y"), 
           day_of_week=lubridate::wday(stopDate), 
           day_of_month=lubridate::day(stopDate), 
           month=lubridate::month(stopDate), 
           hour_of_day=lubridate::hour(stopTime)
           )

# Print data to console
glimpse(md_speeding)

# Change filter to red cars
md_speeding %>% 
    filter(vehicle_color == 'RED') %>% 
    # switch x mapping to speed_over column
    ggplot(aes(x = speed_over)) +
    geom_histogram() +
    # give plot a title
    ggtitle('MPH over speed limit | Red cars')


ggplot(md_speeding) + 
    # Add the histogram geometry with x mapped to speed_over
    geom_histogram(aes(x=speed_over), alpha=0.7) +
    # Add minimal theme
    theme_minimal()


ggplot(md_speeding) +
    geom_histogram(aes(x=hour_of_day, y=stat(density)), alpha=0.8)


# Load md_speeding into ggplot
ggplot(md_speeding) +
  # add a geom_histogram with x mapped to percentage_over_limit
    geom_histogram(
        aes(x=percentage_over_limit), 
        bins=40,     # set bin number to 40
        alpha=0.8)    # reduce alpha to 0.8


ggplot(md_speeding) +
    geom_histogram(
        aes(x = percentage_over_limit),
        bins = 100 ,         # switch to 100 bins
        fill="steelblue",   # set the fill of the bars to 'steelblue'
        alpha = 0.8 )


ggplot(md_speeding, aes(x = hour_of_day)) +
    geom_histogram(
        binwidth=1,  # set binwidth to 1
        center=0.5,  # Center bins at the half (0.5) hour
    ) +
    scale_x_continuous(breaks = 0:24)


# filter data to just heavy duty trucks
truck_speeding <- md_speeding %>% 
    filter(vehicle_type == "06 - Heavy Duty Truck")
 
ggplot(truck_speeding, aes(x = hour_of_day)) +
    # switch to density with bin width of 1.5, keep fill 
    geom_density(fill = 'steelblue', bw=1.5) +
    # add a subtitle stating binwidth
    labs(title = 'Citations by hour', subtitle="Gaussian kernel SD = 1.5")


ggplot(truck_speeding, aes(x = hour_of_day)) +
    # Adjust opacity to see gridlines with alpha = 0.7
    geom_density(bw = 1.5, fill = 'steelblue', alpha=0.7) +
    # add a rug plot using geom_rug to see individual datapoints, set alpha to 0.5.
    geom_rug(alpha=0.5) +
    labs(title = 'Citations by hour', subtitle = "Gaussian kernel SD = 1.5")


ggplot(md_speeding, aes(x = percentage_over_limit)) +
    # Increase bin width to 2.5
    geom_density(fill = 'steelblue', bw = 2.5,  alpha = 0.7) + 
    # lower rugplot alpha to 0.05
    geom_rug(alpha = 0.05) + 
    labs(
        title = 'Distribution of % over speed limit', 
        # modify subtitle to reflect change in kernel width
        subtitle = "Gaussian kernel SD = 2.5"
    )

```
  
  
  
***
  
Chapter 4 - Comparing Distributions  
  
Introduction to Comparing Distributions:  
  
* Box plots can be helpful for comparing across key covariates, especially since multiple histograms are space-inefficient  
	* Include median, IQR, 1.5*IQR, Outliers  
    * Can be helpful to see the actual data rather than just the boxplot  
    * md_speeding %>%  
    *   filter(vehicle_color == 'BLUE') %>%  
    *   ggplot(aes(x = gender, y = speed)) +
    *   geom_jitter(alpha = 0.3, color = 'steelblue') +  
    *   geom_boxplot(alpha = 0) + # make transparent  
    *   labs(title = 'Distribution of speed for blue cars by gender')  
  
Bee Swarms and Violins:  
  
* Jittering is helpful to see quantities of data, but bee swarms or violin plots can help summarize data  
* The bee swarm is a smart version of a jitter plot, with denser points stacking in an area (shape shows quantity of data)  
	* library(ggbeeswarm)  
    * ggplot(data, aes(y = y, x = group)) + geom_beeswarm(color = 'steelblue')  
* Violin plots are KDE that are symmetric around the categorical axis  
	* ggplot(data, aes(y = y, x = group)) + geom_violin(fill = 'steelblue')  
  
Comparing Spatially Related Distributions:  
  
* Bee swarm and violin plots are not ideal when data are spatially connected (ordering to the categorical axis, such as longitudinal time series data)  
* Ridge-line plots can be great for seeing how the KDE evolves over time; good for seeing individual KDE as well as evolutions over time  
	* library(ggridges) # gives us geom_density_ridges()  
    * ggplot(md_speeding, aes(x=speed_over, y=month)) + geom_density_ridges(bandwidth = 2) + xlim(1, 35)  
  
Wrap Up:  
  
* Subtle things can take a visualization fro good to great  
* Proportions - pie charts can be OK, as can waffle charts, stacked bars, etc.  
* Point Data - bar charts are good for stackable data, while points are good for non-stockable data  
	* Remove grid lines where needed and order by value unless categories having inherne tvalue  
* Single distributions - histograms, KDE, rug charts, etc.  
* Comparing distributions - box plots, jitter plots, violin plots, beeswarm plots, etc.  
* Further exploration can include  
	* Flowing data blog  
    * Datawrapper blog  
    * Twitter #datavis  
    * "Data Visualization" by Andy Kirk, "Functional Art and Truthful Art" by Alberto Cairo  
  
Example code includes:  
```{r cache=TRUE}

md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    # Map x and y to gender and speed columns respectively
    ggplot(aes(x=gender, y=speed)) + 
    # add a boxplot geometry
    geom_boxplot() +
    # give plot supplied title
    labs(title = 'Speed of red cars by gender of driver')


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    # add jittered points with alpha of 0.3 and color 'steelblue'
    geom_jitter(alpha=0.3, color="steelblue") +
    # make boxplot transparent with alpha = 0
    geom_boxplot(alpha=0) +
    labs(title = 'Speed of red cars by gender of driver')


# remove color filter
md_speeding %>%
    ggplot(aes(x = gender, y = speed)) + 
    geom_jitter(alpha = 0.3, color = 'steelblue') +
    geom_boxplot(alpha = 0) +
    # add a facet_wrap by vehicle_color
    facet_wrap(~ vehicle_color) +
    # change title to reflect new faceting
    labs(title = 'Speed of different car colors, separated by gender of driver')


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    # change point size to 0.5 and alpha to 0.8
    ggbeeswarm::geom_beeswarm(cex=0.5, alpha=0.8) +
    # add a transparent boxplot on top of points
    geom_boxplot(alpha=0)


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    # Replace beeswarm geometry with a violin geometry with kernel width of 2.5
    geom_violin(bw = 2.5) +
    # add individual points on top of violins
    geom_point(alpha=0.3, size=0.5)


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    geom_violin(bw = 2.5) +
    # add a transparent boxplot and shrink its width to 0.3
    geom_boxplot(alpha=0, width=0.3) +
    # Reset point size to default and set point shape to 95
    geom_point(alpha = 0.3, shape = 95) +
    # Supply a subtitle detailing the kernel width
    labs(subtitle = 'Gaussian kernel SD = 2.5')


md_speeding %>% 
    ggplot(aes(x = gender, y = speed)) + 
    # replace with violin plot with kernel width of 2.5, change color argument to fill 
    geom_violin(bw = 2.5, fill = 'steelblue') +
    # reduce width to 0.3
    geom_boxplot(alpha = 0, width=0.3) +
    facet_wrap(~vehicle_color) +
    labs(
        title = 'Speed of different car colors, separated by gender of driver',
        # add a subtitle w/ kernel width
        subtitle = "Gaussian kernel width: 2.5"
    )


md_speeding %>% 
    mutate(day_of_week = factor(day_of_week, levels=c(2, 3, 4, 5, 6, 7, 1), 
                                labels = c("Mon","Tues","Wed","Thu","Fri","Sat","Sun")
                                )
           ) %>% 
    ggplot(aes( x = percentage_over_limit, y = day_of_week)) + 
    # Set bandwidth to 3.5
    ggridges::geom_density_ridges(bandwidth=3.5) +
    # add limits of 0 to 150 to x-scale
    scale_x_continuous(limits=c(0, 150)) + 
    # provide subtitle with bandwidth
    labs(subtitle='Gaussian kernel SD = 3.5')


md_speeding %>% 
    mutate(day_of_week = factor(day_of_week, levels=c(2, 3, 4, 5, 6, 7, 1), 
                                labels = c("Mon","Tues","Wed","Thu","Fri","Sat","Sun")
                                )
           ) %>% 
    ggplot(aes( x = percentage_over_limit, y = day_of_week)) + 
    # make ridgeline densities a bit see-through with alpha = 0.7
    ggridges::geom_density_ridges(bandwidth = 3.5, alpha=0.7) +
    # set expand values to c(0,0)
    scale_x_continuous(limits = c(0,150), expand=c(0, 0)) +
    labs(subtitle = 'Guassian kernel SD = 3.5') +
    # remove y axis ticks
    theme(axis.ticks.y=element_blank())


md_speeding %>% 
    mutate(day_of_week = factor(day_of_week, levels=c(2, 3, 4, 5, 6, 7, 1), 
                                labels = c("Mon","Tues","Wed","Thu","Fri","Sat","Sun")
                                )
           ) %>% 
    ggplot(aes( x = percentage_over_limit, y = day_of_week)) + 
    geom_point(
        # make semi-transparent with alpha = 0.2
        alpha=0.2, 
        # turn points to vertical lines with shape = '|'
        shape="|", 
        # nudge the points downward by 0.05
        position=position_nudge(y=-0.05)
    ) +
    ggridges::geom_density_ridges(bandwidth = 3.5, alpha = 0.7) +
    scale_x_continuous(limits = c(0,150), expand  = c(0,0)) +
    labs(subtitle = 'Guassian kernel SD = 3.5') +
    theme( axis.ticks.y = element_blank() )

```
  
  
  
***
  
###_Linear Algebra for Data Science in R_  
  
Chapter 1 - Introduction to Linear Algebra  
  
Motivations:  
  
* Vectors, matrixs, tensors, and associated operations on and between them  
* Vectors are the most basic, non-trivial element for storing data  
	* Generally shown with an arrow above the variable name  
    * Can be transposed between row vectors and column vectors  
* Can create vectors in R in many ways  
	* rep()  
    * seq()  
    * c()  
    * z[n] <- a  # replace element n of z with a  
* Matrices are tables of data with rows and columns; data frames can be considered a form of matrix  
* Can create matrices in R in many ways  
	* matrix(data, nrow, ncol, byrow=FALSE)  # set byrow=TRUE to fill the matrix by row  
    * A[a, b] <- c  # sets the value of row a and column b of matrix A to c  
  
Matrix-Vector Operations:  
  
* May want to convert or run mathematical operations on vectors and matrices  
	* axb %*% bxc will form an axc  
* The asterisk between percents is matrix multiplication (%*%)  
* Vector-vector multiplication is the dot-product  
* Matrix-vector multiplication requires that the vector have the same number of elements as the matrix has columns  
  
Matrix-Matrix Calculations:  
  
* Matrix-matrix calculations can be useful for predictions (e.g., neural networks)  
	* (mxn) %*% (nxr) = (mxr)  
    * Order matters - A%*%B is not the same as B%*%A  
    * The asterisk will give component-wise multiplication; A*B is not the same as A%*%B  
* The identity matrix I is a diagnonal matrix with 1 on the diagonal and 0 elsewhere  
* Additional important types of matrices  
	* Square matrices are a special matrix where columns and rows are the same  
    * Inverse matrices can be multiplied to create the identity matrix  
    * Singular matrices do not have an inverse  
    * Diagonal and triangular matrices, which are more computationally efficient  
  
Example code includes:  
```{r}

# Creating three 3's and four 4's, respectively
rep(3, 3)
rep(4, 4)

# Creating a vector with the first three even numbers and the first three odd numbers
seq(2, 6, by = 2)
seq(1, 5, by = 2)

# Re-creating the previous four vectors using the 'c' command
c(3, 3, 3)
c(4, 4, 4, 4)

c(2, 4, 6)
c(1, 3, 5)


x <- 1:7
y <- 2*x
z <- c(1, 1, 2)

# Add x to y and print
print(x + y)

# Multiply z by 2 and print
print(2 * z)

# Multiply x and y by each other and print
print(x * y)

# Add x to z, if possible, and print
print(x + z)  # should throw a warning for the recycling problem


A <- matrix(1, 2, 2)

# Create a matrix of all 1's and all 2's that are 2 by 3 and 3 by 2, respectively
matrix(1, 2, 3)

print(matrix(2, 3, 2))

# Create a matrix and changing the byrow designation.
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = FALSE)
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)

# Add A to the previously-created matrix
A + matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)


A <- matrix(data=c(4, 0, 0, 1), nrow=2, ncol=2, byrow=FALSE)
b <- c(1, 1)
B <- matrix(data=c(1, 0, 0, 2/3), nrow=2, ncol=2, byrow=FALSE)


# Multiply A by b on the left
A %*% b

# Multiply B by b on the left
B %*% b


b <- c(2, 1)
A <- matrix(data=c(-1, 0, 0, 1), nrow=2, ncol=2, byrow=FALSE)
B <- matrix(data=c(1, 0, 0, -1), nrow=2, ncol=2, byrow=FALSE)
C1 <- matrix(data=c(-4, 0, 0, -2), nrow=2, ncol=2, byrow=FALSE)

# Multiply A by b on the left
A%*%b

# Multiplby B by b on the left
B%*%b

# Multiply C by b on the left
C1%*%b


A <- matrix(data=sqrt(2)*c(1, 1, -1, 1), nrow=2, ncol=2, byrow=FALSE)
B <- matrix(data=c(1, 0, 0, -1), nrow=2, ncol=2, byrow=FALSE)
b <- c(1, 1)

# Multply A by B on the left
A%*%B

# Multiply A by B on the right
B%*%A

# Multiply b by B then A (on the left)
A%*%B%*%b

# Multiply b by A then B (on the left)
B%*%A%*%b


A <- matrix(data=c(1, -1, 2, 2), nrow=2, ncol=2, byrow=FALSE)

# Take the inverse of the 2 by 2 identity matrix
solve(diag(2))

# Take the inverse of the matrix A
Ainv <- solve(A)

# Multiply A by its inverse on the left
Ainv%*%A

# Multiply A by its inverse on the right
A%*%Ainv

```
  
  
  
***
  
Chapter 2 - Matrix-Vector Equations  
  
Motivation for Solving Matrix-Vector Equations:  
  
* Question is often whether a build from atromic objects is possible and unique  
	* Assuming that this question is linear, it can be solved using matrices and vectors  
* Vectors times constants is called a linear combination  
	* The question is often whether vector b can be produced as linear combinations of vector x  
    * Ax = b  
* Can consider a matrix of performances in an n-team league as follows - Massey matrix  
	* Diagonals are each number of games that team has played (n-1 in a round-robin)  
    * Off-diagonals are all negative the number of games between the two teams (-1 in a round-robin)  
    * Matrix is nxn  
* Attempt is then to multiple the Massey matrix by an unknown vector R to get the actual net point differential of the teams  
  
Matrix-Vector Equations Theory:  
  
* Need to know the circumatsnaces under which a matrix can be uniquely solved  
	* Inconsistent systems cannot be solved  
    * Consistently systems can be solved, but there may be infinitely-many solutions  
* When a matrix-vector has a solution but not infinite solutions, then it will have only one solution  
* If A is an nxn matrix, then the following conditions are equivalent and imply a solution to Ax = b  
	* The matrix A must have an inverse - solve() will not produce an error  
    * The determinant of A must be non-zero - det() will return a non-zero answer  
    * The rows and columns of A form a basis for the set of all vectors with n elements  
  
Solving Matrix-Vector Equations:  
  
* Solving equations in linear algebra is similar to solving equations in regular algebra  
	* Ainv %*% A gives I and I %*% x gives x  
    * x <- solve(A)%*%b  
* There is an additional matrix solution condition related to the zero vector  
	* The homogenous equation Ax = 0 must have only the trivial solution x=0  
  
Other Considerations for Matrix-Vector Equations:  
  
* With more equations than unknowns, then one or more of the equations must be redundant for a solution to exist  
* With fewer equations than unknowns, it is difficult to have a unique solution (cannot rule out the extra solutions)  
	* However, solutions can exist or fail to exist depending on the redundancy of the columns  
* Options for non-square matrices include  
	* Row Reduction  
    * Least Squares  
    * Singular Value Decomposition  
    * Generalized or Pseudo-Inverses  
* Can calculate the Moore-Penrose Generalized Inverse  
	* MASS::ginv(A)  
    * ginv(A)%*%A  # gives the identity matrix  
    * A%*%ginv(A)  # does not give the identity matrix  
    * x <- ginv(A)%*%b  # gives one of the solutions to Ax = b  
  
Example code includes:  
```{r}

M <- readr::read_csv("./RInputFiles/WNBA_Data_2017_M.csv")
glimpse(M)

names(M) <- stringr::str_replace(names(M), " ", ".")
M <- M %>%
    select(-WNBA) %>%
    slice(-n())
    
M <- as.data.frame(M)
row.names(M) <- names(M)


f <- readr::read_csv("./RInputFiles/WNBA_Data_2017_f.csv")
glimpse(f)

f <- slice(f, -n())

f <- as.data.frame(f)
row.names(f) <- names(M)


# Print the Massey Matrix M Here
print(M)

# Print the vector of point differentials f here
print(f)

# Find the sum of the first column 
sum(M[, 1])

# Find the sum of the vector f
sum(f)


M <- as.matrix(M)

# Add a row of 1's
M <- rbind(M, rep(1, 12))

# Add a column of -1's 
M <- cbind(M, rep(-1, 13))

# Change the element in the lower-right corner of the matrix M
M[13, 13] <- 1

# Print M
print(M)


#Find the inverse of M
solve(M)


f <- as.matrix(f)
f <- rbind(f, 0)

# Solve for r and rename column
r <- solve(M)%*%f
colnames(r) <- "Rating"

# Print r
print(r)


# Find the rating vector using ginv
r <- MASS::ginv(M)%*%f
colnames(r) <- "Rating"
print(r)

```
  
  
  
***
  
Chapter 3 - Eigenvalues and Eigenvectors  
  
Introduction to Eigenvalues and Eigenvectors:  
  
* Eigenvalues and eigenvectors take collections of multi-dimensional objects and select a subset of vectors that closely approximate the originals  
* Matrix-vector multiplication can have many impacts on a vector - reverse, reflect, dilate, contract, project, extract, permutations of these, etc.  
* Eigenvalue-eigenvector analysis allows for summing of the component vectors  
	* Scalar c can be multiplied by vector x to create cx - note that cIx = cx  
* Goal is to decompose a matrix in to a few matrices that can be treated similar to scalar multiplication  
  
Definition of Eigenvalues and Eigenvectors:  
  
* For a matrix A, scalar lambda is considered an eigenvalue of A with associate eigenvector v (v not equal to 0) such that A%*%v = lambda*v  
	* The matrix need not be a diagonal matrix  
    * The eigenvalue-eigenvector are often called an eigenpair  
    * The eigenvectors stay fixed as the matrix is applied; need not be the simple x and y axes  
    * If a eigenpair exists, it can be multiplied by any non-zero scalar; eigenvectors are purely about direction, not about magnitude  
  
Computing Eigenvalues and Eigenvectors in R:  
  
* An nxn matrix has, up to multiplicity, n eigenvalues  
* The eigenvalues need not be real; some or all may be complex, though complex eigenvalues come in conjugate pairs of a + b*i and a-b*i  
* Can get the eigenvalues and eigenvectors using eigen()  
	* eigen(A)  
    * E <- eigen(A)  
    * E$values[1]  # the first eigenvalue  
    * E$vectors[, 1]  # the first eigenvector associated to the first eigenvalue  
* Example of matrix with complex eigenvalues  
	* A <- matrix(data=c(1, -2, 2, -1), nrow=2, ncol=2, byrow=FALSE)  
    * eigen(A)  # both the eigenvalues and the eigenvectors will be complex  
  
Some more on Eigenvalues and Eigenvectors:  
  
* If the eigenvalues lambda(1-to-n) of A are distinct with associated set of eigenvectors v(1-to-n), then this set of vectors forms a basis for the set of all n-dimensional vectors  
	* Every n-dimensional vector can be expressed as a linear combination of these eigenvectors  
    * Basically, eigenpair turn matrix multiplication in to a linear combination of scalar multiplications  
* If the matrix multiplication is iterated, then the lambdas become raise to the power of the number of iterations  
* The leading eigenvector, when normalized to probability 1, is called the stationary distribution of the Markov matrix model  
  
Example code includes:  
```{r}

A <- matrix(data=c(1, 0, 0, 2/3), nrow=2, ncol=2, byrow=FALSE)

# A is loaded for you
print(A%*%rep(1, 2))


A <- matrix(data=c(-1, 0, 0, 2, 7, 0, 4, 12, -4), nrow=3, ncol=3, byrow=FALSE)

# Show that 7 is an eigenvalue for A
A%*%c(0.2425356, 0.9701425, 0) - 7*c(0.2425356, 0.9701425, 0)

# Show that -4 is an eigenvalue for A
A%*%c(-0.3789810, -0.6821657, 0.6253186) - (-4)*c(-0.3789810, -0.6821657, 0.6253186)

# Show that -1 is an eigenvalue for A
A%*%c(1, 0, 0) - (-1)*c(1, 0, 0)


# Show the double of the eigenvector
A%*%((2)*c(0.2425356, 0.9701425, 0)) - 7*(2)*c(0.2425356, 0.9701425, 0)

# Show half of the eigenvector
A%*%((0.5)*c(0.2425356, 0.9701425, 0)) - 7*(0.5)*c(0.2425356, 0.9701425, 0)


A <- matrix(data=c(1, 1, 2, 1), nrow=2, ncol=2, byrow=FALSE)

# Compute the eigenvalues of A and store in Lambda
Lambda <- eigen(A)

# Print eigenvalues
print(Lambda$values[1])
print(Lambda$values[2])

# Verify that these numbers satisfy the conditions of being an eigenvalue
det(Lambda$values[1]*diag(2) - A)
det(Lambda$values[2]*diag(2) - A)


# Find the eigenvectors of A and store them in Lambda
Lambda <- eigen(A)

# Print eigenvectors
print(Lambda$vectors[, 1])
print(Lambda$vectors[, 2])

# Verify that these eigenvectors & their associated eigenvalues satisfy Av - lambda v = 0
Lambda$values[1]*Lambda$vectors[, 1] - A%*%Lambda$vectors[, 1]
Lambda$values[2]*Lambda$vectors[, 2] - A%*%Lambda$vectors[, 2]


Mtemp <- matrix(data=c(0.98, 0.005, 0.005, 0.01, 0.005, 0.98, 0.01, 0.005, 0.005, 0.01, 0.98, 0.005, 0.01, 0.005, 0.005, 0.98), nrow=4, ncol=4, byrow=FALSE)
Mtemp

# This code iterates mutation 100 times
x <- c(1, 0, 0, 0)
for (j in 1:1000) {x <- Mtemp%*%x}

# Print x
print(x)

# Print and scale the first eigenvector of M
Lambda <- eigen(M)
v1 <- Lambda$vectors[, 1]/sum(Lambda$vectors[, 1])

print(v1)

```
  
  
  
***
  
Chapter 4 - Principal Component Analysis  
  
Introduction to the Idea of PCA (Principal Component Analysis):  
  
* PCA is a common dimension-reduction technique in machine learning and data science  
* Real-world data often has many rows (observations) and also many columns (features)  
	* More rows is almost always better, but more columns can be problematic (especially if they are correlated)  
* PCA is a useful applied method for linear algebra  
	* Non-parametric manner of extracting information from confusing data sets  
    * Uncovers hidden, low-dimensional structures that underlie data  
    * Often easier to visualize and interpret  
  
Linear Algebra Behind PCA:  
  
* The matrix t(A) is made by interchanging the rows and columns of A  
* Suppose that the matrix A has n rows and also has had each column mean-adjusted to 0  
	* Then, t(A)%*%A / (n-1) is the covariance matrix where the value in cell (I, j) is the covariance between columns I and j of matrix A  
    * As such, cell(I, i) - the diagonal - contains the variance of the i-column  
    * Note that t(A)%*%A is always a square matrix  
* Suppose that A <- matrix(data=c(1:5, 2*(1:5)), nrow=5, ncol=2, byrow=FALSE)  
	* A[, 1] <- A[, 1] - mean(A[, 1])  
    * A[, 2] <- A[, 2] - mean(A[, 2])  
    * t(A)%*%A/(nrow(A) - 1)  
* The total variance of the dataset is the sum of the eigenvalues of t(A) %*% A / (n-1)  
	* The associated eigenvector are called the principal components of the data  
  
Performing PCA in R:  
  
* Can run the PCA using prcomp()  
	* prcomp(A)  # prints the standard deviations, which are just the square roots of the variances from the previous example  
    * summary(prcomp(A))  
* Can extract the principal components and then apply them to the data  
	* head(prcomp(A)$x[, 1:2])  
    * head(cbind(combine[, 1:4], prcomp(A)$x[, 1:2]))  
* Can run many checks after PCA  
	* Data wrangling and quality control - perhaps there is a main grouping variable such as position  
    * Data visualization may be much easier in 2-dimensions, though PCA is generally better for clustering  
    * Can use PCA as an input for supervised learning, since by definition there is no redundancy remaining after PCA  
  
Wrap Up:  
  
* Vectors and matrices and their interactions  
* Matrix-vector equations and their solutions  
* Eigenvalues and eigenvectors  
* PCA and applications to multivariate datasets  
  
Example code inludes:  
```{r}

combine <- readr::read_csv("./RInputFiles/DataCampCombine.csv")
glimpse(combine)


# Print the first few observations of the dataset
head(combine)

# Find the correlation between variables forty and three_cone
cor(combine$forty, combine$three_cone)

# Find the correlation between variables vertical and broad_jump
cor(combine$vertical, combine$broad_jump)


# Extract columns 5-12 of combine
A <- combine[, 5:12]

# Take the matrix of A
A <- as.matrix(A)

# Subtract the mean of all columns
A[, 1] <- A[, 1] - mean(A[, 1])
A[, 2] <- A[, 2] - mean(A[, 2])
A[, 3] <- A[, 3] - mean(A[, 3])
A[, 4] <- A[, 4] - mean(A[, 4])
A[, 5] <- A[, 5] - mean(A[, 5])
A[, 6] <- A[, 6] - mean(A[, 6])
A[, 7] <- A[, 7] - mean(A[, 7])
A[, 8] <- A[, 8] - mean(A[, 8])


# Create matrix B from equation in instructions
B <- t(A)%*%A/(nrow(A) - 1)

# Compare 1st element of B to 1st column of variance of A
B[1,1]
var(A[, 1])

# Compare 1st element of 2nd column and row element of B to 1st and 2nd columns of A 
B[1, 2]
B[2, 1]
cov(A[, 1], A[, 2])


# Find eigenvalues of B
V <- eigen(B)

# Print eigenvalues
V$values


# Scale columns 5-12 of combine
B <- scale(combine[, 5:12])

# Print the first few rows of the data
head(B)

# Summarize the principal component analysis
summary(prcomp(B))


# Subset combine only to "WR"
combine_WR <- subset(combine, position == "WR")

# Scale columns 5-12 of combine
B <- scale(combine_WR[, 5:12])

# Print the first few rows of the data
head(B)

# Summarize the principal component analysis
summary(prcomp(B))

```
  
  
  
***
  
###_HR Analytics in R: Predicting Employee Churn_  
  
Chapter 1 - Introduction  
  
Turnover:  
  
* Employee turnover is about an employee exiting the company (churn, turnover, attrition are roughly equivalent)  
	* Nearly 25% of US workers leave their jobs every year  
    * Replacing an employee can cost ~50% of the employee's first-year salary  
* Turnover may be voluntary (resignation) or involuntary (firing, end of contract, etc.)  
	* Stated and actual drivers of turnover may not always be the same  
  
Exploring the data:  
  
* Can begin with data exploration of the organizational data  
	* emp_id is the employee ID  
    * status is "active" or "inactive"  
    * turnover is 1 if they left, 0 otherwise  
    * cutoff_date is the study end date  
* Turnover rate is the percentage of employees who left the organization in a given period of time  
	* org %>% count(status)  
    * org %>% summarize(turnover_rate = mean(turnover))  
    * df_level <- org %>% group_by(level) %>% summarize(turnover_level = mean(turnover))  
    * ggplot(df_level, aes(x = level, y = turnover_level)) + geom_col()  
  
HR data architecture:  
  
* May want to identify segments of the data for more meaningful insights and interventions  
	* org2 <- org %>% filter(level %in% c("Analyst", "Specialist"))  
* There are many data sources that may need to be integrated for employee analysis  
	* df3 <- left_join(df1, df2, by = "emp_id")  
  
Example code includes:  
```{r eval=FALSE}

# Load the readr and dplyr packages
library(readr)
library(dplyr)

# Import the org data
org <- read_csv("org.csv")

# Check the structure of org dataset, the dplyr way
glimpse(org)


# Count Active and Inactive employees
org %>% 
  count(status)

# Calculate turnover rate
org %>% 
  summarize(avg_turnover_rate = mean(turnover))


# Calculate level wise turnover rate
df_level <- org %>% 
  group_by(level) %>% 
  summarize(turnover_level = mean(turnover))

# Check the results
df_level

# Visualize the results
ggplot(df_level, aes(x = level, y = turnover_level)) + 
  geom_col()


# Calculate location wise turnover rate
df_location <- org %>% 
  group_by(location) %>% 
  summarize(turnover_location = mean(turnover))

# Check the results
df_location

# Visualize the results
ggplot(df_location, aes(x = location, y = turnover_location)) +
  geom_col()


# Count the number of employees across levels
org %>% 
  count(level)

# Select the employees at Analyst and Specialist level
org2 <- org %>%
  filter(level %in% c("Analyst", "Specialist")) 

# Validate the results
org2 %>% 
  count(level)


# View the structure of rating dataset
glimpse(rating)

# Complete the code to join rating to org2 dataset
org3 <- left_join(org2, rating, by = "emp_id")

# Calculate rating wise turnover rate
df_rating <- org3 %>% 
  group_by(rating) %>% 
  summarize(turnover_rating = mean(turnover))

# Check the result
df_rating


# View the structure of survey dataset
glimpse(survey)

# Complete the code to join survey to org3 dataset
org_final <- left_join(org3, survey, by="mgr_id")

# Compare manager effectiveness scores
ggplot(org_final, aes(x = status, y = mgr_effectiveness)) +
  geom_boxplot()


# View the structure of the dataset
glimpse(org_final)

# Number of variables in the dataset
variables <- ncol(org_final)

# Compare the travel distance of Active and Inactive employees
ggplot(org_final, aes(x = status, y = distance_from_home)) +
  geom_boxplot()

```
  
  
  
***
  
Chapter 2 - Feature Engineering  
  
Feature engineering:  
  
* Can create additional variables from the variables already available in the dataset  
* Job-hopping is described as frequently switching jobs; often high turnover  
* Can computer tenure and timespan  
	* org_final %>% mutate(date_of_joining = dmy(date_of_joining), cutoff_date = dmy(cutoff_date), last_working_date = dmy(last_working_date))  
    * date_1 <- ymd("2000-01-01")  
    * date_2 <- ymd("2014-08-09")  
    * time_length(interval(date_1, date_2), "years")  
  
Compensation:  
  
* Compensation is a key factor driving turnover, and there are many drivers  
	* ggplot(emp_tenure, aes(x = compensation)) + geom_histogram()  
    * ggplot(emp_tenure, aes(x = level, y = compensation)) + geom_boxplot()  
* The compa-ratio is the actual compensation divided by the median compensation  
	* emp_compa_ratio <- emp_tenure %>% group_by(level) %>% mutate(median_compensation = median(compensation), compa_ratio = (compensation / median_compensation))  
    * emp_compa_ratio %>% distinct(level, median_compensation)  
  
Information value:  
  
* Can examine the "information value" for each of the independent variables on the dependent variables  
	* Measure of the predictive power of the independent variable on the response  
    * IV <- Information::create_infotables(data = emp_final, y = "turnover")  # y is the target (dependent) variable  
    * IV$Summary  
* Generally, information value >0.4 is strong, <0.15 is weak, and 0.15 < x < 0.4 is moderate  
  
Example code includes:  
```{r eval=FALSE}

# Add age_diff
emp_age_diff <- org_final %>%
  mutate(age_diff = mgr_age - emp_age)

# Plot the distribution of age difference
ggplot(emp_age_diff, aes(x = status, y = age_diff)) + 
  geom_boxplot()


# Add job_hop_index
emp_jhi <- emp_age_diff %>% 
  mutate(job_hop_index = ifelse(no_previous_companies_worked != 0,  total_experience / no_previous_companies_worked, 0))

# Compare job hopping index of Active and Inactive employees
ggplot(emp_jhi, aes(x = status, y = job_hop_index)) + 
  geom_boxplot()


# Add tenure
emp_tenure <- emp_jhi %>%
  mutate(tenure = ifelse(status == "Active", 
                         time_length(interval(date_of_joining, cutoff_date), 
                                     "years"), 
                         time_length(interval(date_of_joining, last_working_date), 
                                     "years")))

# Compare tenure of active and inactive employees
ggplot(emp_tenure, aes(x = status, y = tenure)) + 
  geom_boxplot()


# Plot the distribution of compensation
ggplot(emp_tenure, aes(x = compensation)) + 
  geom_histogram()

# Plot the distribution of compensation across levels
ggplot(emp_tenure, 
       aes(x = level, y = compensation)) +
  geom_boxplot()

# Compare compensation of Active and Inactive employees across levels
ggplot(emp_tenure, 
       aes(x = level, y = compensation, fill = status)) + 
  geom_boxplot()


# Add median_compensation and compa_ratio
emp_compa_ratio <- emp_tenure %>%  
  group_by(level) %>%   
  mutate(median_compensation = median(compensation), 
         compa_ratio = compensation / median_compensation)

# Look at the median compensation for each level           
emp_compa_ratio %>% 
  distinct(level, median_compensation)


# Add compa_level
emp_final <- emp_compa_ratio %>%  
  mutate(compa_level = ifelse(compa_ratio > 1, "Above", "Below"))

# Compare compa_level for Active & Inactive employees
ggplot(emp_final, aes(x = status, fill = compa_level)) + 
  geom_bar(position = "fill")


# Load Information package
library(Information)

# Compute Information Value 
IV <- create_infotables(data = emp_final, y = "turnover")

# Print Information Value 
IV$Summary

```
  
  
  
***
  
Chapter 3 - Predicting Turnover  
  
Data Splitting:  
  
* Can split the data in to test and train, then build the model on the train data and confirm out-of-sample data on the (previously unseen) test data  
	* index_train <- caret::createDataPartition(emp_final$turnover, p = 0.5, list = FALSE)  
    * train_set <- emp_final[index_train, ]  
    * test_set <- emp_final[-index_train, ]  
  
Introduction to Logistic Regression:  
  
* Can use logistic regression to predict the probability of employee turnover  
	* Categorize data based on the independent variable  
    * Target class for this case is turnover  
    * simple_log <- glm(turnover ~ emp_age, family = "binomial", data = train_set)  
    * summary(simple_log)  
* May want to remove some of the independent variables that are non-relevant  
	* train_set_multi <- train_set %>% select(-c(emp_id, mgr_id, date_of_joining, last_working_date, cutoff_date, mgr_age, emp_age, median_compensation, department, status))  
    * multi_log <- glm(turnover ~ ., family = "binomial", data = train_set_multi)  
  
Multicollinearity:  
  
* Strongly related variables do not provide new, linear information  
* Correlation is the measure of linear association between two variables, between -1 and +1  
	* cor(train_set$emp_age, train_set$compensation)  
* Multicollinearity is where a variable is correlated with 2+ of the other variable  
* The VIF (variance inflation factor) can be assessed using car::vif() - considered to be highly correlated for VIF >= 5  
	* multi_log <- glm(turnover ~ ., family = "binomial", data = train_set_multi)  
    * car::vif(multi_log)  
* Generally, remove the highest VIF variable (assuming greater than 5) model, then re-run, the repeat  
	* new_model <- glm(dependent_variable ~ . - variable_to_remove, family = "binomial", data = dataset)  
  
Building final model:  
  
* Can then build the final model based on the relevant, independent variables  
* Can use the model to calculate probabilities  
    * prediction_train <- predict(final_log, newdata = train_set_final, type = "response")  # type="response" gives a vector of probabilities  
* Generally a best practice to explore the range of the probabilities  
	* hist(prediction_train)  
    * prediction_test <- predict(final_log, newdata = test_set, type = "response")  
  
Example code includes:  
```{r eval=FALSE}

# Load caret
library(caret)

# Set seed of 567
set.seed(567)

# Store row numbers for training dataset: index_train
index_train <- createDataPartition(emp_final$turnover, p = 0.7, list = FALSE)

# Create training dataset: train_set
train_set <- emp_final[index_train, ]

# Create testing dataset: test_set
test_set <- emp_final[-index_train, ]


# Calculate turnover proportion in train_set
train_set %>% 
  count(status) %>% 
  mutate(prop = n / sum(n))

# Calculate turnover proportion in test_set
test_set %>% 
  count(status) %>% 
  mutate(prop = n / sum(n))


# Build a simple logistic regression model
simple_log <- glm(turnover ~ percent_hike, 
                  family = "binomial", data = train_set_multi)

# Print summary
summary(simple_log)


# Build a multiple logistic regression model
multi_log <- glm(turnover ~ ., family = "binomial", 
                 data = train_set_multi)

# Print summary
summary(multi_log)


# Load the car package
library(car)

# Model you built in a previous exercise
multi_log <- glm(turnover ~ ., family = "binomial", data = train_set_multi)

# Check for multicollinearity
vif(multi_log)

# Which variable has the highest VIF?
highest <- "level"


# Remove level
model_1 <- glm(turnover ~ . - level, family = "binomial", 
               data = train_set_multi)

# Check multicollinearity again
vif(model_1)

# Which variable has the highest VIF value?
highest <- "compensation"

# Remove level & compensation
model_2 <- glm(turnover ~ . - level - compensation, family = "binomial", 
               data = train_set_multi)

# Check multicollinearity again
vif(model_2)

# Does any variable have a VIF greater than 5?
highest <- FALSE


# Build the final logistic regression model
final_log <- glm(turnover ~ ., family = "binomial", data=train_set_final)

# Print summary 
summary(final_log)


# Make predictions for training dataset
prediction_train <- predict(final_log, newdata = train_set, type = "response")

# Look at the prediction range
hist(prediction_train)

# Make predictions for testing dataset
prediction_test <- predict(final_log, newdata = test_set, type = "response")

# Look at the prediction range
hist(prediction_test)

# Print the probaility of turnover
prediction_test[c(150, 200)]

```
  
  
  
***
  
Chapter 4 - Model Validation, HR Interventions, and ROI  
  
Validating logistic regression results:  
  
* Need to use a cutoff to convert probabilities to binary decisions  
	* pred_cutoff_50_test <- ifelse(predictions_test > 0.5, 1, 0)  
* The confusion matrix will assess the number of correct predictions made by the model  
	* TP - true positives  
    * TN - true positives  
    * FP - predicted positive, actually negative  
    * FN - predicted negative, actually positive  
* Can look at metrics for the confusion matrix  
	* Accuracy = (TP + TN) / (TP + TN + FP + FN)  
    * conf_matrix_50 <- confusionMatrix(table(test_set$turnover, pred_cutoff_50_test))  
  
Designing retention strategy:  
  
* May want to design retention strategies based on the predicted likelihoods of leaving  
	* Calculate probabilitiy of turnover only for the active employees  
    * emp_risk <- emp_final %>% filter(status=="Active") %>% tidypredict::tidypredict_to_column(final_log)  
    * emp_risk %>% select(emp_id, fit) %>% top_n(5, wt = fit)  
* May want to then bucket the risk of leaving for each employee  
	* emp_risk_bucket <- emp_risk %>% mutate(risk_bucket = cut(fit, breaks = c(0, 0.5, 0.6, 0.8, 1), labels = c("no-risk", "low-risk", "medium-risk", "high-risk")))  
* Can then prioritize retention strategies for employees in the higher turnover risk buckets  
  
Return on investment:  
  
* Can calculate ROI based on the retention strategies employed (costs and outcomes)  
  
Wrap up:  
  
* Basics of turnover analysis  
* Integrating data  
* Modeling data  
* Making conclusions  
  
Example code includes:  
```{r eval=FALSE}

# Classify predictions using a cut-off of 0.5
prediction_categories <- ifelse(prediction_test > 0.5, 1, 0)

# Construct a confusion matrix
conf_matrix <- table(prediction_categories, test_set$turnover)
conf_matrix


# Load caret
library(caret)

# Call confusionMatrix
confusionMatrix(conf_matrix)

# What is the accuracy?
accuracy <- round(unname(confusionMatrix(conf_matrix)$overall["Accuracy"]), 3)


# Load tidypredict 
library(tidypredict)

# Calculate probability of turnover
emp_risk <- emp_final %>%
  filter(status == "Active") %>%
  tidypredict_to_column(final_log)

# Run the code
emp_risk %>% 
  select(emp_id, fit) %>% 
  top_n(2)


# Create turnover risk buckets
emp_risk_bucket <- emp_risk %>% 
  mutate(risk_bucket = cut(fit, breaks = c(0, 0.5, 0.6, 0.8, 1), 
                           labels = c("no-risk", "low-risk", 
                                      "medium-risk", "high-risk")))

# Count employees in each risk bucket
emp_risk_bucket %>% 
  count(risk_bucket)


# Plot histogram of percent hike
ggplot(emp_final, aes(x = percent_hike)) +
  geom_histogram(binwidth = 3)

# Create salary hike_range of Analyst level employees
emp_hike_range <- emp_final %>% 
  filter(level == "Analyst") %>% 
  mutate(hike_range = cut(percent_hike, breaks = c(0, 10, 15, 20),
                          include.lowest = TRUE, 
                          labels = c("0 to 10", "11 to 15", "16 to 20")
                          )
        )


# Calculate the turnover rate for each salary hike range
df_hike <- emp_hike_range %>% 
  group_by(hike_range) %>% 
  summarize(turnover_rate_hike = mean(turnover))

# Check the results
df_hike

# Visualize the results
ggplot(df_hike, aes(x = hike_range, y = turnover_rate_hike)) + 
  geom_col()


# Compute extra cost
extra_cost <- median_salary_analyst * (0.05)

# Compute savings
savings <-  turnover_cost * 0.15

# Calculate ROI
ROI <- (savings / extra_cost) * 100

# Print ROI
cat(paste0("The return on investment is ", round(ROI), "%!"))

```
  
  
  
***
  
###_Dealing with Missing Data in R_  
  
Chapter 1 - Why Care About Missing Data?  
  
Introduction to Missing Data:  
  
* Need to be able to work with missing data; common in real-world applications  
* Imputation needs to be done carefully; just plugging in the mean or median may lead to quirky results  
* Missing values are values that should have been recorded but were not  
	* naniar::any_na(x)  
    * naniar::are_na(x)  
    * naniar::n_miss(x)  
    * naniar::prop_miss(x)  
* Working with missing data  
	* NA + <anything> = NA  
* Can also have NaN, which is interpreted by R as a missing number  
	* naniar::any_na(NaN)  # TRUE  
* NULL is an empty value that is not missing  
	* naniar::any_na(NULL)  # FALSE  
* Be careful about boolean operations  
	* NA | TRUE  # TRUE  
    * NA | FALSE  # FALSE  
  
Why care about missing values?  
  
* Basic summaries of missingness return a single number - n_miss() or n_complete() for example  
* The naniar library has a series of functions at various granularity that all start with miss_*()  
	* miss_var_summary(airquality)  
    * miss_case_summary(airquality)  
    * miss_var_table(airquality)  
    * miss_case_table(airquality)  
* Can look at data over a specific span or run  
	* miss_var_span(pedestrian, var = hourly_counts, span_every = 4000)  # each span of 4000 is treated as a group, with statistics reported  
    * miss_var_run(pedestrian, hourly_counts)  # streaks of missingness; length of each run (repeating patterns)  
    * airquality %>% group_by(Month) %>% miss_var_summary()  
  
How to visualize missing values?  
  
* Visualizing the missingness of the data can help highlight issues  
* Can get a bird's-eye view of the data, including spans and groups  
	* vis_miss(airquality)  
    * vis_miss(airquality, cluster = TRUE)  
    * gg_miss_var(airquality)  
    * gg_miss_case(airquality)  
    * gg_miss_var(airquality, facet = Month)  
    * gg_miss_upset(airquality)  # co-occurrence of missing data  
    * gg_miss_fct(x = airquality, fct = Month)  # plots by factor  
    * gg_miss_span(pedestrian, hourly_counts, span_every = 3000)  
  
Example code includes:  
```{r}

# Create x, a vector, with values NA, NaN, Inf, ".", and "missing"
x <- c(NA, NaN, Inf, ".", "missing")

# Use any_na() and are_na() on to explore the missings
naniar::any_na(x)
naniar::are_na(x)


dat_hw <- data.frame(weight=c(95.16, NA, 102.82, 80.98, 112.91, 94, 105.43, 77.79, NA, 98.93, 68.26, 94.16, 105.32, 61.4, 72.89, 85.67, NA, 63.3, 98.98, 72.17, NA, 103.63, 87.52, 89.78, 103.03, 97.26, 82.77, 68.27, 92.93, 74.55, 61.55, 86.09, 80.04, 88.78, 76.25, 80.44, 99.37, 84.21, NA, 88.5, 97.34, 95.35, 91.91, 78.76, NA, 101.57, 68.33, 89.75, 90.96, 87.17, 104.96, NA, 72.18, 74.09, NA, 92.65, 79.61, 110.09, 77.67, 87.46, 66.91, 76.59, 84.96, 80.21, NA, 64.15, 55.14, NA, 84.47, 100.97, NA, 83.26, 42.15, 89.25, 92.04, NA, 72.76, 69.67, 80.37, NA, 58.38, 84.34, 62.84, NA, 94.23, 83.48, 75.54, 79.93, 79.66, NA, 97.61, 77.11, 83.92, 104.56, 105.94, 107.15, 45.75, 76.61, 88.29, 93.05), height=c(1.95, 2.35, 1.64, 2.47, 1.92, 1.9, 0.83, 2.7, 1.98, 1.83, 0.24, NA, 1.67, NA, 2.03, 2.78, 0.59, 1.99, 2.34, 1.99, -0.05, 0.36, NA, 0.88, NA, 1.37, 2.62, 0.71, 0.52, -0.12, 2.25, 1.06, 1.99, 0.94, -1.11, 1.23, 1.31, 2, 1.1, 0.55, 1.84, 2.14, NA, NA, 1.94, 0.66, 0.47, 2.37, 3.4, 1.4, 2.52, 0.15, 2.42, 0.47, NA, 1.08, 1.89, 2.92, 2.71, NA, 2.72, NA, NA, 1.76, 0.73, 1.84, -0.09, 3.62, 2.34, 0.61, 2.15, 0.39, 0.92, NA, 1.41, 0, 3.51, NA, 0.18, 1.31, 1.19, 2.81, 3.32, 0.06, 3.44, NA, 1.32, NA, 2.46, 3.09, 0.13, 0.92, 0.16, 0.88, 1.38, 0.28, 2.51, NA, 1.05, 3.16))

# Use n_miss() to count the total number of missing values in dat_hw
naniar::n_miss(dat_hw)

# Use n_miss() on dat_hw$weight to count the total number of missing values
naniar::n_miss(dat_hw$weight)

# Use n_complete() on dat_hw to count the total number of complete values
naniar::n_complete(dat_hw)

# Use n_complete() on dat_hw$weight to count the total number of complete values
naniar::n_complete(dat_hw$weight)

# Use prop_miss() and prop_complete() on dat_hw to count the total number of missing values in each of the variables
naniar::prop_miss(dat_hw)
naniar::prop_complete(dat_hw)


data(airquality)
str(airquality)

# Summarise missingness in each variable of the `airquality` dataset
naniar::miss_var_summary(airquality)

# Summarise missingness in each case of the `airquality` dataset
naniar::miss_case_summary(airquality)

# Return the summary of missingness in each variable, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_var_summary()

# Return the summary of missingness in each case, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_case_summary()


# Tabulate missingness in each variable and case of the `airquality` dataset
naniar::miss_var_table(airquality)
naniar::miss_case_table(airquality)

# Tabulate the missingness in each variable, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_var_table()

# Tabulate of missingness in each case, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_case_table()


data(pedestrian, package="naniar")
str(pedestrian)

library(naniar)


# need to add so that the RLE can be converted to data.frame in naniar::miss_var_run
as.data.frame.rle <- function(x, ...) do.call(data.frame, x)


# Calculate the summaries for each run of missingness for the variable `hourly_counts`
naniar::miss_var_run(pedestrian, var = hourly_counts)

# Calculate the summaries for each span of missingness, for a span of 4000, for the variable `hourly_counts`
naniar::miss_var_span(pedestrian, var = "hourly_counts", span_every = 4000)

# For each `month` variable, calculate the run of missingness for `hourly_counts`
pedestrian %>% 
    group_by(month) %>% 
    naniar::miss_var_run(var = "hourly_counts")

# For each `month` variable, calculate the span of missingness of a span of 2000
pedestrian %>% 
    group_by(month) %>% 
    naniar::miss_var_span(var = "hourly_counts", span_every = 2000)


data(riskfactors, package="naniar")
str(riskfactors)

# Visualize all of the missingness in the `riskfactors`  dataset
naniar::vis_miss(riskfactors)

# Visualize and cluster all of the missingness in the `riskfactors` dataset
naniar::vis_miss(riskfactors, cluster = TRUE)

# visualise and sort the columns by missingness in the `riskfactors` dataset
naniar::vis_miss(riskfactors, sort_miss = TRUE)


# Visualize the number of missings in cases using `gg_miss_case()`
naniar::gg_miss_case(riskfactors)

# Explore the number of missings in cases using `gg_miss_case()` and facet by the variable `education`
naniar::gg_miss_case(riskfactors, facet = education)

# Visualize the number of missings in variables using `gg_miss_var()`
naniar::gg_miss_var(riskfactors)

# Explore the number of missings in variables using `gg_miss_var()` and facet by the variable `education`
naniar::gg_miss_var(riskfactors, facet = education)


# Using the `airquality` dataset, explore the missingness pattern using `gg_miss_upset()`
naniar::gg_miss_upset(airquality)

# With the `riskfactors` dataset, explore how the missingness changes across the `marital` using `gg_miss_fct()`
naniar::gg_miss_fct(x = riskfactors, fct = marital)

# Using the `pedestrian` dataset Explore how the missingness changes over a span of 3000 
naniar::gg_miss_span(pedestrian, var = hourly_counts, span_every = 3000)

# Using the `pedestrian` dataset: Explore the impact of `month` by facetting by `month`
# and explore how missingness changes for a span of 1000
naniar::gg_miss_span(pedestrian, var = hourly_counts, span_every = 1000, facet = month)

```
  
  
  
***
  
Chapter 2 - Wrangling and Tidying Missing Values  
  
Search for and replace missing values:  
  
* May find that not all missing values are coded as NA - N/A or "missing" or the like  
* Can use the "chaos" dataset  
	* chaos %>% miss_scan_count(search = list("N/A"))  
    * chaos %>% miss_scan_count(search = list("N/A", "N/a"))  
* Can replace the other specifications of NA  
	* chaos %>% replace_with_na(replace = list(grade = c("N/A", "N/a")))  
* Can use the "scoped variance" features similar to dplyr  
	* chaos %>% replace_with_na_all(condition = ~.x == -99)  # ~ is for function, and .x is the reference to a variable  
    * chaos %>% replace_with_na_all(condition = ~.x %in% c("N/A", "missing", "na"))  
  
Filling down missing values:  
  
* May want to manage implied missing values - "missing missing values"  
* Can spread the data to make missing values more obvious; may help to "un-tidy" the data  
	* tetris %>% tidyr::complete(name, time)  # will add the missing combinations, keeping the data in tidy format  
* May want to fill a value down, for example when name is added only the first time the record is shown  
	* tetris %>% tidyr::fill(name)  # default LOCF  
  
Missing data dependence:  
  
* Missing data dependence has theory associate with at  
	* MCAR - missing compleetly at random  
    * MAR - missing at random  
    * MNAR - missing not at random  
* MCAR is where the data has no association with other observed or unobserved data (e.g., missing workers on vacation)  
	* Implications are that imputing is OK; deleting might be OK, but may lose too much data (delete only if ~5% loss or better)  
* MAR is where missingness depends on data observed but not on data unobserved (e.g., missing workers more likely with depression)  
	* Do not delete and be careful with imputing  
* MNAR is where missingness is related to an unobserved variable of interest  
* Can use visualizations to see the potential structures of various types of missing data  
	* vis_miss(mt_cars, cluster = TRUE)  # noisy patterns suggest MCAR  
    * oceanbuoys %>% arrange(year) %>% vis_miss()  # clustering of missingness suggest MAR  
    * vis_miss(ocean, cluster = TRUE)  # blocks of data may suggest MNAR, but can be challenging to find  
  
Example code includes:  
```{r}

pacman <- tibble(year=c('2004', '1991', 'na', '1992', '1988', '2007', '2016', '2011', '2018', '2012', '1983', '1988', '1981', '1990', '1989', '1995', 'missing', 'missing', '2003', '2000', '2012', '2008', '2007', '1987', '2009', '1987', '2016', '2011', '2008', '1984', '2003', '1988', '2001', '1990', '2018', '1985', '2010', '1986', '1980', '1982', '2009', '1998', '1991', '1987', '1982', '1998', '2004', '2007', '2000', '2014', '1980', '1983', '2011', '2003', '2013', '2018', '2006', '2005', '1994', '2009', '2004', '1991', 'na', '2004', '1993', '1989', '2004', '2011', '1990', '1985', '2017', '1992', '1999', '2014', '1996', '2007', '2008', '1998', '1996', '1998', '2017', '1998', '2016', '1983', '2009', 'missing', '1993', '1989', '1994', '1980', '1983', '2004', 'missing', '1997', '1994', '2008', 'missing', '2007', '2016', '1992', '2000', '2002', '2004', '2007', '2013', '1983', '2005', '1999', '1990', '1998', '1982', '2002', 'na', '1998', '2006', '2004', '2012', '1981', '2000', '2014', '1999', '1997', '2003', '1993', '1982', '1992', '2008', '1985', '2016', '1990', '1991', '1980', '2000', 'na', '2018', 'na', '2014', '1988', 'missing', '2002', '2012', '2017', '1987', '1998', '1999', '1985', '1989', '2017', '1982', '1994', '2003', 'na', '2011', 'missing', 'missing', '1986', '2007', '2006', 'missing', '2010', '1982', '2008', '1983', '2018', '1987', '1983', 'missing', 'missing', '1998', '1988', '2010', '1981', 'na', '2016', 'na', '1992', '2001', '1995', '1999', '2009', 'na', 'na', '2003', '2017', 'na', '1982', '2005', '2013', '1990', '2004', '2004', '2006', '2009', '1984', '2007', '1987', 'na', '2001', '1983', '2012'), 
                 month=c('6', '11', 'na', '11', '9', '12', '9', '1', '4', '9', '4', '11', '6', '7', '10', '8', 'missing', 'missing', '5', '5', '8', '1', '10', '11', '6', '7', '10', '8', '7', '1', '9', '10', '11', '7', '1', '5', '10', '6', '8', '11', '11', '8', '10', '8', '1', '9', '9', '7', '11', '11', '10', '7', '12', '9', '12', '8', '11', '4', '11', '1', '1', '9', 'na', '7', '10', '10', '3', '3', '9', '5', '8', '1', '5', '12', '6', '3', '7', '9', '12', '2', '5', '8', '4', '6', '1', 'missing', '8', '1', '12', '2', '5', '8', 'missing', '7', '2', '7', 'missing', '4', '11', '6', '5', '11', '12', '3', '3', '5', '1', '6', '12', '1', '11', '7', 'na', '9', '11', '7', '9', '10', '10', '11', '5', '11', '6', '6', '4', '10', '10', '1', '4', '6', '8', '12', '11', 'na', '11', 'na', '10', '9', 'missing', '5', '1', '5', '4', '3', '2', '11', '2', '9', '3', '3', '5', 'na', '12', 'missing', 'missing', '4', '1', '2', 'missing', '7', '2', '1', '4', '9', '4', '3', 'missing', 'missing', '1', '12', '2', '4', 'na', '3', 'na', '11', '7', '2', '5', '1', 'na', 'na', '1', '4', 'na', '10', '5', '4', '8', '2', '9', '11', '7', '10', '2', '9', 'na', '4', '6', '10'), 
                 day=c('1', '22', 'na', '16', '16', '4', '5', '25', '14', '25', '1', '8', '17', '14', '15', '18', 'missing', 'missing', '21', '18', '21', '2', '15', '18', '22', '26', '25', '3', '24', '6', '18', '6', '4', '1', '23', '3', '10', '23', '11', '4', '11', '17', '23', '10', '8', '19', '10', '6', '24', '9', '25', '18', '7', '25', '24', '23', '17', '8', '10', '17', '8', '8', 'na', '24', '25', '7', '6', '19', '10', '13', '24', '13', '26', '4', '5', '21', '28', '15', '22', '10', '11', '15', '20', '23', '6', 'missing', '3', '2', '24', '11', '21', '21', 'missing', '24', '13', '6', 'missing', '14', '13', '17', '11', '18', '24', '9', '6', '1', '11', '21', '3', '12', '23', '27', 'na', '1', '13', '7', '17', '11', '13', '11', '20', '7', '2', '10', '9', '24', '21', '12', '25', '17', '14', '24', '18', 'na', '7', 'na', '22', '8', 'missing', '12', '14', '15', '21', '21', '1', '11', '18', '9', '18', '15', '1', 'na', '9', 'missing', 'missing', '28', '13', '12', 'missing', '3', '16', '5', '3', '19', '14', '7', 'missing', 'missing', '4', '25', '15', '4', 'na', '21', 'na', '5', '11', '10', '11', '15', 'na', 'na', '2', '10', 'na', '4', '16', '12', '11', '19', '5', '23', '24', '22', '27', '11', 'na', '23', '21', '27'), 
                 initial=c('XGB', 'VGP', 'UAW', 'MXL', 'ZPM', 'ESF', 'YKM', 'ABS', 'NDT', 'GAS', 'IFA', 'OUH', 'PZB', 'EKR', 'TXO', 'NCV', 'XSL', 'ATM', 'LEN', 'QNE', 'CBV', 'DLU', 'LTW', 'TCV', 'BVC', 'GSP', 'LVJ', 'YQD', 'HSX', 'KNX', 'PYK', 'PVD', 'OAB', 'GHB', 'LCI', 'HMU', 'VRQ', 'WAJ', 'AIK', 'YPJ', 'BMO', 'YEH', 'YHK', 'YIA', 'TDA', 'XYF', 'LMH', 'JTO', 'ZFD', 'SXE', 'QYC', 'MPI', 'TSI', 'IVR', 'ILM', 'CME', 'FVU', 'HFJ', 'DEF', 'TCX', 'BGA', 'PBK', 'TIB', 'FYX', 'OJA', 'GEH', 'LJB', 'IHF', 'NMS', 'WSC', 'WTO', 'JBV', 'JQI', 'TCP', 'MLU', 'NBM', 'QMY', 'DLV', 'UHP', 'BGE', 'WCR', 'DNC', 'KZS', 'DBM', 'IUC', 'LRG', 'ONT', 'VKF', 'GFU', 'EQI', 'CUR', 'SAZ', 'CFU', 'SOH', 'QTM', 'CZV', 'QNR', 'LMG', 'SGR', 'DXC', 'BKI', 'CMP', 'VDR', 'CIA', 'QYW', 'CJR', 'HJQ', 'NTE', 'EGA', 'ZUY', 'AMT', 'LKP', 'HFW', 'PZQ', 'PJI', 'QJB', 'LAU', 'XYO', 'OJV', 'OBZ', 'QPV', 'LAH', 'UHW', 'XIT', 'UMB', 'OPM', 'GSC', 'PFU', 'OEC', 'ERU', 'ZWA', 'CJA', 'IGE', 'ZBQ', 'XVO', 'BWF', 'VAW', 'WDQ', 'JWT', 'QCT', 'JAH', 'WAQ', 'RCS', 'JPL', 'KCF', 'NXE', 'OPW', 'WYP', 'RMS', 'LND', 'YVO', 'XIR', 'AUW', 'OLA', 'ORF', 'ZAU', 'FXE', 'ACE', 'FQW', 'BND', 'SKA', 'BZX', 'JKY', 'IOZ', 'IYG', 'YZK', 'FOU', 'ZJT', 'XLA', 'TEZ', 'YKB', 'CYS', 'UBJ', 'DKO', 'EWZ', 'PBU', 'GEU', 'LVW', 'YWO', 'WBH', 'GXH', 'NPY', 'UIW', 'EXP', 'QAX', 'RCH', 'ZFM', 'SML', 'FNC', 'HQI', 'NQO', 'QLM', 'EGI', 'CIQ', 'ORU', 'AGP', 'MPY', 'EFL', 'VXR', 'QYE'), 
                 score=c('892369', '2412494', '1874449', '1583331', '3159043', '2755582', '804088', '2392395', '431430', '1482088', '3099396', '810873', '2410285', '1602619', 'N/A', '1547264', '1086746', '885575', '2464437', '333868', '2991881', '1207552', '332352', '115716', 'N/A', '2551711', '679715', '3033343', '275723', '1677698', '1031285', '3251416', '1812998', '1767317', '2457197', '2194699', '1258734', '535437', '3202731', '899729', '1099688', '2125942', '2407498', '1785754', '2181741', '1058088', '1630900', '1629161', '2378243', '3211114', '65436', '2006229', '2068916', '1653110', '2589346', '1520554', '374610', 'N/A', '2841676', '1001739', '438268', '2476918', '2584965', '702929', '189630', 'N/A', '410549', '1269273', '2658430', '1760979', 'N/A', '2705304', '1560004', '826721', '3291811', '2366950', '832279', '426785', '2898752', '1369821', '2712315', '2123280', '2513951', '1004901', '645429', '846193', '313628', '1791507', '2612127', '836682', '1955459', '1866444', '75834', '532534', '3267355', '235734', '2279669', '2976729', '2297788', '1166581', '15715', '890432', '1670356', '1463904', '2867923', '1761345', '2667484', '2357424', '3053758', '2077402', '1052647', '1661650', '123930', '3171836', '1910536', '2100782', '679137', '1424599', '2194459', '1263044', '1948854', 'N/A', '3092624', '2077243', '1010777', '3289300', '3172553', '891045', '1592747', '728752', 'N/A', '24667', 'N/A', '827488', '1643701', '2844488', '539713', '3160321', '762261', '2505569', '271322', '1479487', '1217212', '2960042', '1825455', '1287888', '2105751', '450550', '894755', '3115431', '781721', '3220718', '767717', '3204211', '1666549', '3128098', '2445271', '1571440', '2088915', '645360', '2321491', '1135310', '1736847', '2378391', '3097570', '1220994', '165122', '2007635', '876910', '1551229', '1357429', '2168680', '1411345', '3290465', '1860365', '3181429', '2872190', '2780599', '2160057', '60716', '2222480', '22113', '2815280', 'N/A', '2517561', '500742', '3077608', '1481553', '1349499', '2539062', '2057675', '2869686', '863857', '2609949', '2337505', '76444', '3062706', '3031438', '759570', '1741154'), 
                 country=c(' ', 'US', ' ', ' ', ' ', 'US', 'NZ', 'CA', 'GB', 'CN', 'ES', 'US', 'NZ', 'AU', 'CN', 'US', 'CA', 'US', 'US', 'CN', 'AU', 'ES', 'NZ', 'CA', 'CN', 'ES', 'NZ', 'NZ', 'CN', 'GB', 'CN', 'US', 'ES', 'CN', 'US', 'CN', 'AU', 'GB', 'ES', 'AT', ' ', 'US', 'NZ', 'AU', ' ', 'US', 'US', 'US', 'ES', 'NZ', 'AT', 'NZ', 'JP', 'ES', 'NZ', 'NZ', 'GB', 'CN', 'AU', 'GB', 'ES', 'GB', 'AT', 'NZ', 'CN', 'US', 'AU', 'GB', 'US', 'JP', 'CA', 'AT', 'AT', 'CN', 'AU', 'JP', 'CA', 'GB', 'AT', 'AU', 'GB', 'CN', 'AU', 'GB', 'AT', 'NZ', 'JP', 'GB', ' ', 'CN', 'US', 'JP', 'CN', 'GB', 'GB', 'GB', 'AT', 'US', 'GB', 'GB', 'JP', 'CN', 'AU', 'AU', 'AT', 'JP', 'US', 'JP', 'NZ', 'JP', 'AT', 'NZ', 'CA', 'CA', 'GB', 'ES', 'ES', 'GB', 'ES', 'GB', 'AU', 'GB', 'AT', 'CN', 'AT', 'ES', ' ', 'CA', 'CA', 'GB', 'AU', 'CN', 'ES', 'NZ', 'CA', 'JP', 'JP', 'NZ', 'GB', 'CA', 'ES', 'AT', 'AU', 'CA', 'CN', 'US', 'JP', 'AT', 'CA', 'JP', ' ', 'GB', 'GB', 'NZ', 'AU', 'JP', 'US', 'US', 'AU', 'US', 'AT', 'GB', 'GB', 'GB', 'AT', 'CN', 'ES', 'US', 'JP', 'GB', 'AT', 'JP', 'AU', 'NZ', 'GB', 'GB', 'ES', 'ES', 'AT', 'GB', 'CN', ' ', 'US', 'JP', 'AT', 'US', 'CA', 'AT', 'US', 'GB', 'US', 'ES', 'US', ' ', 'ES', 'JP', 'CA', 'AU', 'CA', 'US')                 )
pacman


# Explore all of the strange missing values, "N/A", "missing", " ", "na"
naniar::miss_scan_count(data = pacman, search = list("N/A", "missing", " ", "na"))


# Print the top of the pacman data using `head()`
head(pacman)

# Replace the strange missing values "N/A" and "missing" with `NA`
pacman_clean <- naniar::replace_with_na(pacman, replace = list(year = c("N/A", "na", "missing"),
                                                               score = c("N/A", "na", "missing")
                                                               )
                                        )
                                        
# Test if `pacman_clean` still has these values in it?
naniar::miss_scan_count(pacman_clean, search = list("N/A", "na", "missing"))


# Use `replace_with_na_at()` to replace with NA
naniar::replace_with_na_at(pacman, .vars = c("year", "month", "day"),  
                           ~.x %in% c("N/A", "missing", "na", " ")
                           )

# Use `replace_with_na_if()` to replace with NA
naniar::replace_with_na_if(pacman, .predicate = is.character, 
                           ~.x %in% c("N/A", "missing", "na")
                           )

# Use `replace_with_na_all()` to replace with NA
naniar::replace_with_na_all(pacman, ~.x %in% c("N/A", "missing", "na"))


frogger <- tibble(name=factor(c('jesse', 'jesse', 'jesse', 'jesse', 'andy', 'andy', 'andy', 'nic', 'nic', 'dan', 'dan', 'alex', 'alex', 'alex', 'alex')), 
                  time=factor(c('morning', 'afternoon', 'evening', 'late_night', 'morning', 'afternoon', 'late_night', 'afternoon', 'late_night', 'morning', 'evening', 'morning', 'afternoon', 'evening', 'late_night')), 
                  value=as.integer(c(6678, 800060, 475528, 143533, 425115, 587468, 111000, 588532, 915533, 388148, 180912, 552670, 98355, 266055, 121056))
                  )
str(frogger)


# Use `complete()` on the `time` variable to make implicit missing values explicit
frogger
frogger_tidy <- frogger %>% 
    complete(name, time)
frogger_tidy



# Use `fill()` to fill down the name variable in the frogger dataset
frogger
frogger %>% 
    fill(name)


# Correctly fill() and complete() missing values so that our dataset becomes sensible
frogger
frogger %>% 
    fill(name) %>%
    complete(name, time)


data("oceanbuoys", package="naniar")
str(oceanbuoys)

# Arrange by year
oceanbuoys %>% 
    arrange(year) %>% 
    naniar::vis_miss()

# Arrange by latitude
oceanbuoys %>% 
    arrange(latitude) %>% 
    naniar::vis_miss()

# Arrange by wind_ew (wind east west)
oceanbuoys %>% 
    arrange(wind_ew) %>% 
    naniar::vis_miss()

```
  
  
  
***
  
Chapter 3 - Testing Missing Relationships  
  
Tools to explore missing data dependence:  
  
* Example of census data containing income and education  
	* Faceting by education will show differences in income by education  
* Can consider a shadow matrix where 1 means missing and 0 means present  
	* The shadow matrix variables have the same names as the regular data, with _NA as a suffix  
    * Clear values are given; they are either NA or !NA  
    * Can also bind the shadow matrix to the original data ("nabular" data)  
    * bind_shadow(airquality)  
    * airquality %>% bind_shadow() %>% group_by(Ozone_NA) %>% summarise(mean = mean(Wind))  
  
Visualizing missingness across one variable:  
  
* Can explore conditional plots based on the missingness status of the data  
	* ggplot(airquality, aes(x = Temp)) + geom_density()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, color = Ozone_NA)) + geom_density()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Ozone_NA, y = Temp)) + geom_boxplot()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp)) + geom_density() + facet_wrap(~Ozone_NA)  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, y = Wind)) + geom_point() + facet_wrap(~Ozone_NA)  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, y = Wind, color = Ozone_NA)) + geom_point()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, color = Ozone_NA)) + geom_density() + facet_wrap(~Solar.R_NA)  
  
Visualizing misingness across two variables:  
  
* Missing values are frequently ignored in a scatterplot, but it may be helpful to still consider them  
	* ggplot(airquality, aes(x = Ozone, y = Solar.R)) + geom_miss_point()  # imputes to 10% below the minimum for the missing dimension  
    * ggplot(airquality, aes(x = Wind, y = Ozone)) + geom_miss_point() + facet_wrap(~Month)  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Wind, y = Ozone)) + geom_miss_point() + facet_wrap(~Solar.R_NA)  
  
Example code includes:  
```{r}

# Create shadow matrix data with `as_shadow()`
naniar::as_shadow(oceanbuoys)

# Create nabular data by binding the shadow to the data with `bind_shadow()`
naniar::bind_shadow(oceanbuoys)

# Bind only the variables with missing values by using bind_shadow(only_miss = TRUE)
naniar::bind_shadow(oceanbuoys, only_miss = TRUE)


# `bind_shadow()` and `group_by()` humidity missingness (`humidity_NA`)
oceanbuoys %>%
    naniar::bind_shadow() %>%
    group_by(humidity_NA) %>% 
    summarise(wind_ew_mean = mean(wind_ew), 
              wind_ew_sd = sd(wind_ew)
              ) 

# Repeat this, but calculating summaries for wind north south (`wind_ns`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    group_by(humidity_NA) %>% 
    summarise(wind_ns_mean = mean(wind_ns), 
              wind_ns_sd = sd(wind_ns)
              )


# Summarise wind_ew by the missingness of `air_temp_c_NA`
oceanbuoys %>% 
    naniar::bind_shadow() %>%
    group_by(air_temp_c_NA) %>%
    summarise(wind_ew_mean = mean(wind_ew),
              wind_ew_sd = sd(wind_ew),
              n_obs = n()
              )


# Summarise wind_ew by missingness of `air_temp_c_NA` and `humidity_NA`
oceanbuoys %>% 
    naniar::bind_shadow() %>%
    group_by(air_temp_c_NA, humidity_NA) %>%
    summarise(wind_ew_mean = mean(wind_ew),
              wind_ew_sd = sd(wind_ew),
              n_obs = n()
              )


# First explore the missingness structure of `oceanbuoys` using `vis_miss()`
naniar::vis_miss(oceanbuoys)

# Explore the distribution of `wind_ew` for the missingness of `air_temp_c_NA` using  `geom_density()`
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = wind_ew, color = air_temp_c_NA)) + 
    geom_density()

# Explore the distribution of sea temperature for the missingness of humidity (humidity_NA) using  `geom_density()`
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = sea_temp_c, color = humidity_NA)) + 
    geom_density()


# Explore the distribution of wind east west (`wind_ew`) for the missingness of air temperature using  `geom_density()` and facetting by the missingness of air temperature (`air_temp_c_NA`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = wind_ew)) + 
    geom_density() + 
    facet_wrap(~air_temp_c_NA)

# Build upon this visualisation by coloring by the missingness of humidity (`humidity_NA`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = wind_ew, color = humidity_NA)) + 
    geom_density() + 
    facet_wrap(~air_temp_c_NA)


# Explore the distribution of wind east west (`wind_ew`) for the missingness of air temperature using  `geom_boxplot()`
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = air_temp_c_NA, y = wind_ew)) + 
    geom_boxplot()

# Build upon this visualisation by facetting by the missingness of humidity (`humidity_NA`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = air_temp_c_NA, y = wind_ew)) + 
    geom_boxplot() + 
    facet_wrap(~humidity_NA)


# Explore the missingness in wind and air temperature, and display the missingness using `geom_miss_point()`
ggplot(oceanbuoys, aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point()

# Explore the missingness in humidity and air temperature, and display the missingness using `geom_miss_point()`
ggplot(oceanbuoys, aes(x = humidity, y = air_temp_c)) + 
    naniar::geom_miss_point()


# Explore the missingness in wind and air temperature, and display the missingness using `geom_miss_point()`. Facet by year to explore this further.
ggplot(oceanbuoys, aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_wrap(~year)

# Explore the missingness in humidity and air temperature, and display the missingness using `geom_miss_point()` Facet by year to explore this further.
ggplot(oceanbuoys, aes(x=humidity, y=air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_wrap(~year)


# Use geom_miss_point() and facet_wrap to explore how the missingness in wind_ew and air_temp_c is different for missingness of humidity
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_wrap(~humidity_NA)

# Use geom_miss_point() and facet_grid to explore how the missingness in wind_ew and air_temp_c is different for missingness of humidity AND by year - by using `facet_grid(humidity_NA ~ year)`
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_grid(humidity_NA~year)

```
  
  
  
***
  
Chapter 4 - Imputation  
  
Filling in the blanks:  
  
* Imputation can help with understanding data structure, as well as visualizing and analyzing based on imputed data  
	* impute_below(c(5,6,7,NA,9,10))  # imputes the NA to be lower than anything in the data at hand  
    * impute_below_if(data, is.numeric)  # run only for numeric  
    * impute_below_at(data, vars(var1,var2))  # select variables  
    * impute_below_all(data)  # all variables  
* Can also use bind_shadow() to maintain a history of which data points were initially missing and then imputed  
	* aq_imp <- airquality %>% bind_shadow() %>% impute_below_all()  
    * ggplot(aq_imp, aes(x = Ozone, fill = Ozone_NA)) + geom_histogram()  
    * ggplot(aq_imp, aes(x = Ozone, fill = Ozone_NA)) + geom_histogram() + facet_wrap(~Month)  
* Can add labels for whether any of the data are missing  
	* aq_imp <- airquality %>% bind_shadow() %>% add_label_missings() %>% impute_below_all()  
    * ggplot(aq_imp, aes(x = Ozone, y = Solar.R, colour = any_missing)) + geom_point()  
  
What makes a good imputation?  
  
* Imputation should be done with care such that the resulting dataset are still reasonable given the rest of the data and the real world  
* Bad imputations come in many forms  
	* Mean imputation - calculate the mean from the non-missing data (often ignores the underlying structure of the data)  
    * aq_impute_mean <- airquality %>% bind_shadow(only_miss = TRUE) %>% impute_mean_all() %>% add_label_shadow()  
* Can explore imputations using the boxplot, scatterplot, long shadow format histograms, etc.  
	* ggplot(aq_impute_mean, aes(x = Ozone_NA, y = Ozone)) + geom_boxplot()  
    * ggplot(aq_impute_mean, aes(x = Ozone, y = Solar.R, colour = any_missing)) + geom_point()  
    * aq_imp <- airquality %>% bind_shadow() %>% impute_mean_all()  
    * aq_imp_long <- shadow_long(aq_imp, Ozone, Solar.R)  
    * ggplot(aq_imp_long, aes(x = value, fill = value_NA)) + geom_histogram() + facet_wrap(~variable) 
  
Performing imputations:  
  
* Can use linear regression as a tool for imputation - can use the package "simputation"  
	* df %>% bind_shadow(only_miss = TRUE) %>% add_label_shadow() %>% simputation::impute_lm(y ~ x1 + x2)  
    * aq_imp_lm <- airquality %>% bind_shadow() %>% add_label_shadow() %>% simputation::impute_lm(Solar.R ~ Wind + Temp + Month) %>% simputation::impute_lm(Ozone ~ Wind + Temp + Month)  
    * ggplot(aq_imp_lm, aes(x = Solar.R, y = Ozone, colour = any_missing)) + geom_point()  
* Can compare multiple attempts at imputation as well  
	* aq_imp_small <- airquality %>% bind_shadow() %>% impute_lm(Ozone ~ Wind + Temp) %>% simputation::impute_lm(Solar.R ~ Wind + Temp) %>% add_label_shadow()  
    * aq_imp_large <- airquality %>% bind_shadow() %>% impute_lm(Ozone ~ Wind + Temp + Month + Day) %>% simputation::impute_lm(Solar.R ~ Wind + Temp + Month + Day) %>% add_label_shadow()  
    * bound_models <- bind_rows(small = aq_imp_small, large = aq_imp_large, .id = "imp_model")  
    * ggplot(bound_models, aes(x = Ozone, y = Solar.R, colour = any_missing)) + geom_point() + facet_wrap(~imp_model)  
    * bound_models_gather <- bound_models %>% select(Ozone, Solar.R, any_missing, imp_model) %>% gather(key = "variable", value = "value", -any_missing, -imp_model)  
    * ggplot(bound_models_gather, aes(x = imp_model, y = value)) + geom_boxplot() + facet_wrap(~key)
bound_models_gather %>% filter(any_missing == "Missing") %>% ggplot(aes(x = imp_model, y = value)) + geom_boxplot() + facet_wrap(~key)  
  
Evaluating imputations and models:  
  
* Can run a standard linear regression, or a regression only with complete.cases  
	* aq_cc <- airquality %>% na.omit() %>% bind_shadow() %>% add_label_shadow()  
    * aq_imp_lm <- bind_shadow(airquality) %>% add_label_shadow() %>% impute_lm(Ozone ~ Temp + Wind + Month + Day) %>% impute_lm(Solar.R ~ Temp + Wind + Month + Day)  
    * bound_models <- bind_rows(cc = aq_cc, imp_lm = aq_imp_lm, .id = "imp_model")  
    * model_summary <- bound_models %>% group_by(imp_model) %>% nest() %>% mutate(mod = map(data, ~lm(Temp ~ Ozone + Solar.R + Wind + Temp + Days + Month data = .)), res = map(mod, residuals), pred = map(mod, predict), tidy = map(mod, broom::tidy))  
* Can then examine the impacts of the various imputation techniques  
	* model_summary %>% select(imp_model, tidy) %>% unnest()  
    * model_summary %>% select(imp_model, res) %>% unnest() %>% ggplot(aes(x = res, fill = imp_model)) + geom_histogram(position = "dodge")  
    * model_summary %>% select(imp_model, pred) %>% unnest() %>% ggplot(aes(x = pred, fill = imp_model)) + geom_histogram(position = "dodge")  
  
Example code includes:  
```{r}

# Impute the oceanbuoys data below the range using `impute_below`.
ocean_imp <- naniar::impute_below_all(oceanbuoys)

# Visualise the new missing values
ggplot(ocean_imp, aes(x = wind_ew, y = air_temp_c)) +  
  geom_point()

# Impute and track data with `bind_shadow`, `impute_below_all`, and `add_label_shadow`
ocean_imp_track <- naniar::bind_shadow(oceanbuoys) %>% 
  naniar::impute_below_all() %>%
  naniar::add_label_shadow()

# Look at the imputed values
ocean_imp_track
ggplot(ocean_imp_track, aes(x=wind_ew, y=air_temp_c, colour=any_missing)) + 
  geom_point()

# Visualise the missingness in wind and air temperature, coloring missing air temp values with air_temp_c_NA
ggplot(ocean_imp_track, aes(x = wind_ew, y = air_temp_c, color = air_temp_c_NA)) + 
    geom_point()

# Visualise humidity and air temp, coloring any missing cases using the variable any_missing
ggplot(ocean_imp_track, aes(x = humidity, y = air_temp_c, color = any_missing)) + 
    geom_point()


# Explore the values of air_temp_c, visualising the amount of missings with `air_temp_c_NA`.
p <- ggplot(ocean_imp_track, aes(x = air_temp_c, fill = air_temp_c_NA)) + 
    geom_histogram()

# Expore the missings in humidity using humidity_NA
p2 <- ggplot(ocean_imp_track,  aes(x = humidity, fill = humidity_NA)) + 
    geom_histogram()

# Explore the missings in air_temp_c according to year, using `facet_wrap(~year)`.
p + facet_wrap(~year)

# Explore the missings in humidity according to year, using `facet_wrap(~year)`.
p2 + facet_wrap(~year)


# Impute the mean value and track the imputations 
ocean_imp_mean <- naniar::bind_shadow(oceanbuoys) %>% 
  naniar::impute_mean_all() %>% 
  naniar::add_label_shadow()

# Explore the mean values in humidity in the imputed dataset
ggplot(ocean_imp_mean, aes(x = humidity_NA, y = humidity)) + 
    geom_boxplot()

# Explore the values in air temperature in the imputed dataset
ggplot(ocean_imp_mean, aes(x = air_temp_c_NA, y = air_temp_c)) + 
    geom_boxplot()

# Explore imputations in air temperature and humidity, coloring by the variable, any_missing
ggplot(ocean_imp_mean, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point()

# Explore imputations in air temperature and humidity, coloring by the variable, any_missing, and faceting by year
ggplot(ocean_imp_mean, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point() + 
    facet_wrap(~year)


# Gather the imputed data 
ocean_imp_mean_gather <- naniar::shadow_long(ocean_imp_mean, humidity, air_temp_c)

# Inspect the data
ocean_imp_mean_gather

# Explore the imputations in a histogram 
ggplot(ocean_imp_mean_gather, aes(x = as.numeric(value), fill = value_NA)) + 
    geom_histogram() + 
    facet_wrap(~variable)


# Impute humidity and air temperature using wind_ew and wind_ns, and track missing values
ocean_imp_lm_wind <- oceanbuoys %>% 
    naniar::bind_shadow() %>%
    simputation::impute_lm(air_temp_c ~ wind_ew + wind_ns) %>% 
    simputation::impute_lm(humidity ~ wind_ew + wind_ns) %>%
    naniar::add_label_shadow()
    
# Plot the imputed values for air_temp_c and humidity, colored by missingness
ggplot(ocean_imp_lm_wind, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point()


# Bind the models together 
bound_models <- bind_rows(mean = ocean_imp_mean,
                          lm_wind = ocean_imp_lm_wind,
                          .id = "imp_model")

# Inspect the values of air_temp and humidity as a scatterplot
ggplot(bound_models, aes(x = air_temp_c, y = humidity, color = any_missing)) +
    geom_point() + 
    facet_wrap(~imp_model)


# Build a model adding year to the outcome
ocean_imp_lm_wind_year <- bind_shadow(oceanbuoys) %>%
    simputation::impute_lm(air_temp_c ~ wind_ew + wind_ns + year) %>%
    simputation::impute_lm(humidity ~ wind_ew + wind_ns + year) %>%
    naniar::add_label_shadow()

# Bind the mean, lm_wind, and lm_wind_year models together
bound_models <- bind_rows(mean = ocean_imp_mean,
                          lm_wind = ocean_imp_lm_wind,
                          lm_wind_year = ocean_imp_lm_wind_year,
                          .id = "imp_model"
                          )

# Explore air_temp and humidity, coloring by any missings, and faceting by imputation model
ggplot(bound_models, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point() + 
    facet_wrap(~imp_model)


# Gather the data and inspect the distributions of the values
bound_models_gather <- bound_models %>%
    select(air_temp_c, humidity, any_missing, imp_model) %>%
    gather(key = "key", value = "value", -any_missing, -imp_model)

# Inspect the distribution for each variable, for each model
ggplot(bound_models_gather, aes(x = imp_model, y = value, color = imp_model)) +
    geom_boxplot() + 
    facet_wrap(~key, scales = "free_y")

# Inspect the imputed values
bound_models_gather %>%
    filter(any_missing == "Missing") %>%
    ggplot(aes(x = imp_model, y = value, color = imp_model)) +
    geom_boxplot() + 
    facet_wrap(~key, scales = "free_y")


# Create an imputed dataset using a linear models
ocean_imp_lm_all <- naniar::bind_shadow(oceanbuoys) %>%
    naniar::add_label_shadow() %>%
    simputation::impute_lm(sea_temp_c ~ wind_ew + wind_ns + year + latitude + longitude) %>%
    simputation::impute_lm(air_temp_c ~ wind_ew + wind_ns + year + latitude + longitude) %>%
    simputation::impute_lm(humidity ~ wind_ew + wind_ns + year + latitude + longitude)

# Bind the datasets
bound_models <- bind_rows(imp_lm_wind_year = ocean_imp_lm_wind_year,
                          imp_lm_wind = ocean_imp_lm_wind,
                          imp_lm_all = ocean_imp_lm_all,
                          .id = "imp_model"
                          )
# Look at the models
bound_models


# Create the model summary for each dataset
model_summary <- bound_models %>% 
    group_by(imp_model) %>%
    nest() %>%
    mutate(mod = map(data, ~lm(sea_temp_c ~ air_temp_c + humidity + year, data = .)), 
           res = map(mod, residuals), pred = map(mod, predict), tidy = map(mod, broom::tidy)
           )

# Explore the coefficients in the model
model_summary %>% 
    select(imp_model, tidy) %>%
    unnest()

best_model <- "imp_lm_all"

```
  
  
  
***
  
###_Analyzing Election and Polling Data in R_  
  
Chapter 1 - Presidential Job Approval Polls  
  
Introduction:  
  
* Basic tools for organizing, analyzing, and visualizing polling data in R  
	* Data wrangling  
    * Prediction of election winners  
    * Mapping and regression at the county level  
    * Overall ensemble of exercises  
* Presidential approval polls are surveys of the public  
	* Tends to have a strong relationship with election outcomes  
    * Gallup dataset of approval polls since 1946  
* Can select and filter approval data in R  
	* library(tidyverse)  
    * data.slim <- data %>% select(variable_1, variable_2, ...)  
    * data.slim %>% filter(variable_1 == "observation)  
    * gallup %>% select(President, Date, Approve) %>% filter(President == "Trump")  
  
Averaging Job Approval by President:  
  
* Can group by variables and then take appropriate summaries  
	* data %>% group_by(variable)  
    * Gallup %>% group_by(President) %>% summarise(MeanApproval = mean(Approve))  
  
Visualizing Trump's Approval Over Time:  
  
* Can create averages of available polling and visualize over time  
	* Example of the RCP aggregate approval data poll  
* Can convert dates using lubridate  
	* library(lubridate)  
    * date <- ymd("2018-01-01")  
    * month(date) # Equal to 1  
    * month(date,label = T) # Equal to "Jan"  
* Creating a moving average is a simple but powerful technique - can use zoo and the rollmean() function, followed by ggplot2  
	* TrumpApproval %>% mutate(AvgApprove = rollmean(Approve, 10, na.pad=TRUE, align = "right"))  
  
Example code includes:  
```{r}

approval_polls <- readr::read_csv("./RInputFiles/gallup_approval_polls.csv")
glimpse(approval_polls)


# Select President, Date, and Approve from approval_polls
approval_polls %>% 
    select(President, Date, Approve) %>%
    head()


# Select the President, Date, and Approve columns and filter to observations where President is equal to "Trump"
approval_polls %>% 
    select(President, Date, Approve) %>%
    filter(President == "Trump")


# Group the approval_polls dataset by president and summarise a mean of the Approve column 
approval_polls %>%
    group_by(President) %>%
    summarise(Approve = mean(Approve))


# Extract, or "pull," the Approve column as a vector and save it to the object "TrumpApproval"
TrumpApproval <- approval_polls %>% 
    select(President, Date, Approve) %>%
    filter(President == "Trump") %>%
    pull(Approve)

# Take a mean of the TrumpApproval vector
mean(TrumpApproval)


# Select the relevant columns from the approval_polls dataset and filter them for the Trump presidency
TrumpPolls <- approval_polls %>% 
    select(President, Date, Approve) %>%
    filter(President == "Trump")
  
# Use the months() and mdy() function to get the month of the day each poll was taken
# Group the dataset by month and summarize a mean of Trump's job approval by month
TrumpPolls %>%
    mutate(Month = months(lubridate::mdy(Date))) %>%
    group_by(Month) %>%
    summarise(Approve = mean(Approve))


# Save Donald Trump's approval polling to a separate object 
TrumpApproval <- approval_polls %>% 
    filter(President == "Trump") %>%
    mutate(Date = lubridate::mdy(Date)) %>%
    arrange(Date) 


# use the rollmean() function from the zoo package to get a moving average of the last 10 polls
TrumpApproval <- TrumpApproval %>%
    mutate(AvgApprove = zoo::rollmean(Approve, 10, na.pad=TRUE, align = "right"))


# Use ggplot to graph Trump's average approval over time
ggplot(data = TrumpApproval, aes(x=Date, y=AvgApprove)) + 
    geom_line()


# Create an moving average of each president's approval rating
AllApproval <- approval_polls %>%
    group_by(President) %>%
    mutate(AvgApprove = zoo::rollmean(Approve, 10, na.pad=TRUE, align = "right"))


# Graph an moving average of each president's approval rating
ggplot(data = AllApproval, aes(x=Days, y=AvgApprove, col=President)) + 
    geom_line()

```
  
  
  
***
  
Chapter 2 - US House and Senate Polling  
  
Elections and Polling Parties:  
  
* Can use polling data to predict elections - "generic ballot"for which party is supported in general  
	* Data are a mix of publicly available data from pollingreport.com and RCP  
    * head(generic_ballot)  
    * ggplot(generic_ballot,aes(x=mdy(Date),y=Democrats)) + geom_point()  
* Can explore and wrangle the "generic ballot" dataset  
  
73 Years of "Generic Ballot" Polls:  
  
* Can analyze the data over time (D-R margin)  
	* data %>% group_by(year, month)  
    * data %>% group_by(year, month) %>% summarise(support = mean(support))  
    * ggplot(data,aes(x=month,y=support)) + geom_point() + geom_smooth(span=0.2)  
  
Calculating and Visualizing Error in Polls:  
  
* Polls have errors - domain knowledge is important in analyzing the estimates and methodologies revealed in polls  
	* Sometimes polls are systemically biased, particularly if similar methodologies are used by all companies  
* Can compare polling data and elections data  
	* poll_error <- generic_ballot %>% mutate(Democrats_Poll_Margin = Democrats - Republicans, Democrats_Vote_Margin = Democrats_vote - Republicans_vote)  
    * poll_error <- poll_error %>% group_by(Year) %>% summarise(Democrats_Poll_Margin = mean(Democrats_Poll_Margin), Democrats_Vote_Margin = mean(Democrats_Vote_Margin))  
    * poll_error %>% mutate(error = Dem.Poll.Margin - Dem.Vote.Margin)  
    * rmse <- sqrt(mean(poll_error$error^2))  
    * CI <- rmse * 1.96  
* May be helpful to visualize the errors over time  
	* ggplot(by_year) + geom_point(aes(x=ElecYear,y=Dem.Poll.Margin,col="Poll")) + geom_point(aes(x=ElecYear,y=Dem.Vote.Margin,col="Vote")) + geom_errorbar(aes(x=ElecYear,ymin=lower, ymax=upper))  
  
Predicting Winners with Linear Regression:  
  
* Can use linear regression to predict seats in Congress based on polls  
	* model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin, by_year)  
  
Example code includes:  
```{r}

generic_ballot <- readr::read_csv("./RInputFiles/generic_ballot.csv")
glimpse(generic_ballot)


# Look at the header and first few rows of the data
head(generic_ballot)

# Filter the election year to 2016 and select the Date, Democrats, and Republicans columns
generic_ballot %>%
    filter(ElecYear == 2016) %>%
    select(Date, Democrats, Republicans)


# Mutate a new variable called "Democratic.Margin" equal to the difference between Democrats' vote share and Republicans'
democratic_lead <- generic_ballot %>%
    mutate(Democratic.Margin = Democrats - Republicans)

# Take a look at that new variable!
democratic_lead %>%
    select(Democratic.Margin)


# Group the generic ballot dataset by year and summarise an average of the Democratic.Margin variable
over_time <- democratic_lead %>% 
    group_by(ElecYear) %>%
    summarize(Democratic.Margin = mean(Democratic.Margin))

# Explore the data.frame
head(over_time)


# Create a month and year variable for averaging polls by approximate date
timeseries <- democratic_lead %>%
    mutate(Date = lubridate::mdy(Date), month = lubridate::month(Date), yr = lubridate::year(Date))

# Now group the polls by their month and year, then summarise
timeseries <- timeseries %>%
    group_by(yr, month) %>%
    summarise(Democratic.Margin = mean(Democratic.Margin))


# Mutate a new variable to use a date summary for the monthly average
timeseries_plot <- timeseries %>%
    mutate(time = sprintf("%s-%s-%s", yr, month, "01"))

# Plot the line over time
ggplot(timeseries_plot, aes(x=lubridate::ymd(time), y=Democratic.Margin)) +
    geom_line()


# Make a ggplot with points for monthly polling averages and one trend line running through the entire time series
ggplot(timeseries_plot, aes(x=lubridate::ymd(time), y=Democratic.Margin)) +
    geom_point() + 
    geom_smooth(span=0.2)


# Mutate two variables for the Democrats' margin in polls and election day votes
poll_error <- generic_ballot %>%
    mutate(Dem.Poll.Margin = Democrats - Republicans,
           Dem.Vote.Margin = DemVote - RepVote
           )

# Average those two variables per year and mutate the "error" variable
poll_error <- poll_error %>%
    group_by(ElecYear) %>%
    summarise(Dem.Poll.Margin = mean(Dem.Poll.Margin), Dem.Vote.Margin = mean(Dem.Vote.Margin)) %>%
    mutate(error = Dem.Poll.Margin - Dem.Vote.Margin)

# Calculate the room-mean-square error of the error variable
rmse <- sqrt(mean(poll_error$error^2))

# Multiply the RMSE by 1.96 to get the 95% confidence interval, or "margin of error"
CI <- rmse * 1.96

# Add variables to our dataset for the upper and lower bound of the `Dem.Poll.Margin` variable
by_year <- poll_error %>%
    mutate(upper = Dem.Poll.Margin + CI, lower = Dem.Poll.Margin - CI)


# Plot estimates for Dem.Poll.Margin and Dem.Vote.Margin on the y axis for each year on the x axis with geom_point
ggplot(by_year) + 
    geom_point(aes(x=ElecYear, y=Dem.Poll.Margin, col="Poll")) +
    geom_point(aes(x=ElecYear, y=Dem.Vote.Margin, col="Vote")) +
    geom_errorbar(aes(x=ElecYear, ymin=lower, ymax=upper))


# Fit a model predicting Democratic vote margin with Democratic poll margin
model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin, data=by_year)
  
# Evaluate the model
summary(model)


# Make a new data.frame that has our prediction variable and value
predictdata <- data.frame("Dem.Poll.Margin" = 5)

# Make the prediction with the coefficients from our model
predict(model, predictdata)


```
  
  
  
***
  
Chapter 3 - Election Results and Political Demography  
  
2016 Presidential Election:  
  
* County-level results are made available by secretaries of state  
	* Can be combined with census data to draw findings - for example, is there a correlation between race and vote share by county?  
* Data are available in the chloroplethr package  
	* county_merged <- left_join(df_county_demographics, uspres_county, by = "county.fips")  
    * ggplot(county_merged, aes(x=percent_white,y=Dem.pct)) + geom_point()  
    * ggplot(county_merged, aes(x=percent_white,y=Dem.pct)) + geom_point() + geom_smooth(method="lm")  
  
Making County-Level Maps in R:  
  
* Mapping can be helpful for identifying trends and areas of interest in the data - a few options include  
	* choroplethr  
    * geom_sf()  
    * leaflet  
* The choroplethr package allows for easy creation of maps with minimal pre-processing  
	* library(choroplethr)  
    * county_map <- county_merged %>% dplyr::rename("region" = county.fips, "value" = Dem.pct)  # names need to be region and value  
    * county_choropleth(county_map)  
  
Analyzing Results with Linear Regression:  
  
* Can further analyze findings using linear regression; both understanding past results and predicting future  
	* fit <- lm(Dem.pct ~ percent_white, data=county_merged)  
    * summary(fit)  
  
2016 Brexit Referendum:  
  
* Data wrangling, modeling, and visualization of 2016 Brexit vote  
* Can either run a short-term average, or a LOESS with various windows  
	* head(brexit_polls)  
    * ggplot(brexit_polls, aes(x = mdy(Date), y = Remain - Leave)) + geom_point() + geom_smooth(method = 'loess')  
  
Example code includes:  
```{r}

uspres_results <- readr::read_csv("./RInputFiles/us_pres_2016_by_county.csv")
glimpse(uspres_results)


# Deselect the is.national.winner, national.count, and national.party.percent variables
uspres_results.slim <- uspres_results %>%
    select(-c(is.national.winner, national.count, national.party.percent))


# Spread party and votes to their own columns
uspres_county <- uspres_results.slim %>%
    tidyr::spread(key=party,value=vote.count)

# Add a variable to the uspres_county dataset to store the Democrat's percentage of votes
uspres_county <- uspres_county %>%
    mutate(Dem.pct = D/county.total.count)


# Load the county demographic data
data(df_county_demographics, package="choroplethr")

# Look at the demographic data
head(df_county_demographics)


# Rename the 'region' variable in df_county_demographics to "county.fips"
df_county_demographics <- df_county_demographics %>%
    rename("county.fips" = region)

# Join county demographic with vote share data via its FIPS code
county_merged <- left_join(df_county_demographics, uspres_county, by = "county.fips")
head(county_merged)


# plot percent_white and Dem.pct on the x and y axes. add points and a trend line
ggplot(county_merged, aes(x=percent_white, y=Dem.pct)) +
    geom_point() +
    geom_smooth(method="lm")


# Rename the county.fips and Dem.pct variables from our dataset to "region" and "value"
county_map <- county_merged %>%
    rename("region" = county.fips, "value" = Dem.pct)

# Create the map with choroplethrMaps's county_choropleth()
democratic_map <- choroplethr::county_choropleth(county_map)

# Print the map
democratic_map


# Rename variables from our dataset
county_map <- county_merged %>%
    rename("region" = county.fips, "value" = percent_white)

# Create the map with choroplethr's county_choropleth()
white_map <- choroplethr::county_choropleth(county_map)

# Graph the two maps (democratic_map and white_map) from the previous exercises side-by-side
gridExtra::grid.arrange(democratic_map, white_map)


# Fit a linear model to predict Dem.pct dependent on percent_white in each county
fit <- lm(Dem.pct ~ percent_white, data=county_merged)

# Evaluate the model
summary(fit)


# Fit a linear model to predict Dem.pct dependent on percent_white and per_capita_income in each county
fit <- lm(Dem.pct ~ percent_white + per_capita_income, data=county_merged)

# Evaluate the model
summary(fit)


brexit_polls <- readr::read_csv("./RInputFiles/brexit_polls.csv")
glimpse(brexit_polls)

brexit_results <- readr::read_csv("./RInputFiles/brexit_results.csv")
glimpse(brexit_results)


# Filter the dataset to polls only released after June 16th, 2016, and mutate a variable for the Remain campaign's lead
brexit_average <- brexit_polls %>%
    filter(lubridate::mdy(Date)>lubridate::ymd("2016-06-16") )%>%
    mutate(RemainLead = Remain - Leave)  

# Average the last seven days of polling
mean(brexit_average$RemainLead)

# Summarise the Remain lead from the entire month of the referendum 
ggplot(brexit_polls, aes(x=lubridate::mdy(Date), y=Remain-Leave)) +
    geom_point() + 
    geom_smooth(method='loess')


# Familiarize yourself with the data using the head() function
head(brexit_results)

# Chart the counstituency-by-constituency relationship between voting for the Labour Party and voting to leave the EU
ggplot(brexit_results,aes(x=lab_2015, y=leave_share)) + 
  geom_point()


# Show the relationship between UKIP and Leave vote share with points and a line representing the linear relationship between the variables
ggplot(brexit_results,aes(x=ukip_2015, y=leave_share)) + 
    geom_point() +
    geom_smooth(method = "lm")


# predict leave's share with the percentage of a constituency that holds a college degree and its 2015 UKIP vote share
model.multivar <- lm(leave_share ~ ukip_2015 + degree, brexit_results)
summary(model.multivar)

```
  
  
  
***
  
Chapter 4 - Predicting the Future of Politics  
  
US House 2018:  
  
* Can make predictions based on publicly available polling data and also make predictions using past data to evaluate general error rates  
	* polls_2018 %>% filter(date > "2018-06-01")  
    * polls_2018 %>% mutate(Dem.Margin = Dem - Rep)  
    * polls_2018 %>% pull(Dem.Margin)  
    * mean(polls_2018$Dem.Margin)  
* Can also extend with group_by() function  
  
Training a Model to Predict Future with Polls:  
  
* Can start with the base model  
	* lm(Dem.Vote.Margin ~ Dem.Poll.Margin)  
* There may be additional variables however; perhaps party in office systemically leads to over/under performance of polls  
	* ggplot(generic_ballot,aes(x=Dem.Poll.Margin,y=Dem.Vote.Margin, col=party_in_power) + geom_text(aes(label=ElecYear)) + geom_smooth(method='lm')  
    * model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin + party_in_power, data=polls_predict)  
    * predict(model, data.frame(Dem.Poll.Margin = 8, party_in_power=-1))  
* May want to extend with the predictive margin of error  
	* sqrt(mean(c(model$fitted.values - data$actual_results)^2)) * 1.96  
    * sqrt(mean(c(model$fitted.values - polls_predict$Dem.Vote.Margin)^2)) *1.96  
  
Presidency in 2020:  
  
* Presidential elections have many moving parts and with a small sample size of data  
	* Popular vote - nationwide tally  
    * Electoral vote - tally by states (weighted by size of state)  
* Popular vote can be modeled based on approval rating, economic growth, and length of party incumbency  
	* lm(vote_share ~ pres_approve + q2_gdp + two_plus_terms, pres_elecs)  
    * ggplot(pres_elecs,aes(x=predict,y=vote_share,label=Year)) + geom_abline() + geom_text()  
    * sqrt(mean(c(pres_elecs$predict-pres_elecs$vote_share)^2)) * 1.96  
  
Wrap-up:  
  
* Approval polls - wrangling and visualizing data  
* Polls and linear regression  
* Mapping election results and running multi-regressions  
* Prediction and applied examples  
  
Example code includes:  
```{r}

polls_2018 <- tibble::tibble(Democrat=c(45, 47, 49, 44, 41, 48, 45, 44, 45, 51, 42, 52, 46, 44, 41, 42, 44, 42, 44, 44, 42, 41, 51, 47, 49, 45, 44, 45, 48, 42, 42, 47, 42, 44, 47, 43, 50, 43, 43, 41, 45, 44, 44, 42, 45, 50, 48, 48, 43, 45, 48, 46, 48, 44, 43, 44, 49, 42, 39, 42, 44, 43, 40, 42, 42, 38, 43, 44, 39, 42, 47, 42, 43, 43, 48, 49, 46, 43, 43, 45, 44, 43, 44, 44, 47, 44, 38, 42, 43, 43, 41, 41, 42, 42, 50, 50, 44, 46, 44, 40, 42, 43, 38, 43, 49, 43, 38, 50, 44, 40, 37, 41, 47, 54, 46, 43, 38, 42, 39, 38, 49, 49, 43, 38, 42, 45, 47, 42, 37, 41, 38, 43, 42, 51, 51, 42, 37, 41, 53, 46, 44, 40, 44, 42, 38, 44, 44, 39, 44, 56, 51, 44, 51, 37, 41, 50, 42, 37, 40, 41, 36, 42, 37, 42, 43, 43, 42, 38, 44, 51, 40, 38, 38, 51, 51, 39, 40, 43, 44, 50, 40, 36, 42, 41, 42, 54, 40, 43, 39, 41, 40, 48, 42, 49, 39, 43, 40, 40, 39, 43, 40, 40, 39, 49, 41, 46, 41, 40, 47, 39, 51, 43, 39, 44, 40, 40, 40, 50, 42, 39, 43, 37, 43, 47, 41, 48, 42, 38, 43, 38, 42, 50, 41, 42, 39, 43, 38, 41, 40, 42, 49, 42, 40, 42, 38, 41, 47, 39, 50, 40, 47, 47, 38, 40, 45, 40, 43, 41, 48, 47, 46, 46, 49, 45, 48), 
                             Republican=c(39, 34, 38, 38, 37, 43, 36, 40, 36, 42, 38, 41, 39, 41, 39, 36, 40, 36, 37, 38, 37, 38, 39, 40, 37, 37, 35, 37, 40, 38, 37, 34, 38, 34, 37, 40, 41, 35, 38, 38, 36, 37, 37, 36, 39, 42, 43, 41, 37, 35, 43, 40, 39, 38, 32, 37, 42, 39, 37, 37, 37, 38, 37, 36, 37, 37, 38, 35, 38, 35, 44, 39, 37, 34, 43, 41, 38, 34, 38, 34, 35, 38, 34, 39, 43, 36, 30, 36, 36, 34, 37, 37, 35, 35, 39, 44, 39, 41, 38, 37, 38, 38, 30, 36, 40, 37, 31, 41, 37, 38, 31, 39, 32, 38, 39, 35, 30, 38, 30, 39, 41, 38, 37, 31, 38, 39, 45, 37, 31, 37, 30, 37, 37, 39, 41, 36, 32, 38, 39, 40, 37, 31, 36, 36, 31, 36, 35, 27, 34, 38, 40, 36, 36, 29, 36, 37, 36, 31, 38, 35, 30, 36, 28, 33, 36, 40, 34, 31, 36, 36, 33, 30, 39, 40, 40, 36, 31, 38, 37, 35, 33, 28, 36, 33, 37, 38, 33, 37, 32, 39, 34, 37, 38, 43, 33, 37, 31, 37, 33, 38, 32, 36, 33, 35, 36, 40, 33, 38, 40, 34, 42, 36, 34, 37, 34, 40, 34, 40, 40, 32, 40, 35, 39, 41, 35, 38, 40, 35, 37, 36, 39, 40, 37, 39, 33, 39, 36, 37, 33, 35, 38, 37, 35, 36, 35, 41, 42, 34, 41, 44, 42, 41, 32, 40, 38, 37, 40, 39, 43, 38, 41, 43, 41, 42, 40), 
                             end_date=as.Date(c('2018-08-28', '2018-08-28', '2018-08-21', '2018-08-21', '2018-08-21', '2018-08-19', '2018-08-18', '2018-08-14', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-12', '2018-08-12', '2018-08-07', '2018-08-07', '2018-08-06', '2018-07-31', '2018-07-31', '2018-07-30', '2018-07-24', '2018-07-24', '2018-07-23', '2018-07-23', '2018-07-22', '2018-07-22', '2018-07-17', '2018-07-17', '2018-07-14', '2018-07-11', '2018-07-10', '2018-07-10', '2018-07-10', '2018-07-03', '2018-07-02', '2018-07-02', '2018-07-01', '2018-07-01', '2018-06-29', '2018-06-26', '2018-06-26', '2018-06-25', '2018-06-24', '2018-06-19', '2018-06-19', '2018-06-18', '2018-06-17', '2018-06-17', '2018-06-13', '2018-06-12', '2018-06-12', '2018-06-12', '2018-06-10', '2018-06-06', '2018-06-05', '2018-06-05', '2018-06-04', '2018-05-30', '2018-05-29', '2018-05-29', '2018-05-29', '2018-05-22', '2018-05-22', '2018-05-22', '2018-05-19', '2018-05-15', '2018-05-15', '2018-05-14', '2018-05-08', '2018-05-08', '2018-05-07', '2018-05-05', '2018-05-01', '2018-05-01', '2018-05-01', '2018-05-01', '2018-04-30', '2018-04-30', '2018-04-24', '2018-04-24', '2018-04-24', '2018-04-23', '2018-04-17', '2018-04-17', '2018-04-13', '2018-04-11', '2018-04-10', '2018-04-10', '2018-04-07', '2018-04-03', '2018-04-03', '2018-04-01', '2018-03-27', '2018-03-27', '2018-03-27', '2018-03-25', '2018-03-25', '2018-03-21', '2018-03-21', '2018-03-20', '2018-03-20', '2018-03-19', '2018-03-13', '2018-03-13', '2018-03-12', '2018-03-08', '2018-03-06', '2018-03-06', '2018-03-05', '2018-03-05', '2018-02-27', '2018-02-27', '2018-02-26', '2018-02-24', '2018-02-23', '2018-02-21', '2018-02-20', '2018-02-20', '2018-02-13', '2018-02-13', '2018-02-12', '2018-02-11', '2018-02-07', '2018-02-06', '2018-02-06', '2018-02-04', '2018-02-01', '2018-01-30', '2018-01-30', '2018-01-30', '2018-01-23', '2018-01-23', '2018-01-21', '2018-01-20', '2018-01-18', '2018-01-18', '2018-01-16', '2018-01-16', '2018-01-16', '2018-01-15', '2018-01-10', '2018-01-09', '2018-01-09', '2018-01-05', '2018-01-02', '2018-01-02', '2017-12-26', '2017-12-19', '2017-12-19', '2017-12-18', '2017-12-17', '2017-12-12', '2017-12-12', '2017-12-12', '2017-12-12', '2017-12-11', '2017-12-07', '2017-12-05', '2017-12-05', '2017-12-03', '2017-11-28', '2017-11-28', '2017-11-25', '2017-11-22', '2017-11-21', '2017-11-19', '2017-11-15', '2017-11-14', '2017-11-14', '2017-11-11', '2017-11-09', '2017-11-07', '2017-11-07', '2017-11-06', '2017-11-05', '2017-11-01', '2017-10-31', '2017-10-31', '2017-10-30', '2017-10-30', '2017-10-24', '2017-10-24', '2017-10-24', '2017-10-23', '2017-10-16', '2017-10-16', '2017-10-15', '2017-10-10', '2017-10-09', '2017-10-03', '2017-10-01', '2017-09-26', '2017-09-25', '2017-09-24', '2017-09-20', '2017-09-19', '2017-09-17', '2017-09-12', '2017-09-11', '2017-09-05', '2017-09-03', '2017-08-29', '2017-08-28', '2017-08-22', '2017-08-21', '2017-08-19', '2017-08-17', '2017-08-15', '2017-08-14', '2017-08-12', '2017-08-08', '2017-08-06', '2017-08-06', '2017-08-01', '2017-07-29', '2017-07-25', '2017-07-24', '2017-07-18', '2017-07-17', '2017-07-15', '2017-07-11', '2017-07-09', '2017-07-04', '2017-06-30', '2017-06-27', '2017-06-27', '2017-06-25', '2017-06-24', '2017-06-20', '2017-06-19', '2017-06-13', '2017-06-12', '2017-06-11', '2017-06-06', '2017-06-02', '2017-05-30', '2017-05-30', '2017-05-23', '2017-05-22', '2017-05-16', '2017-05-14', '2017-05-14', '2017-05-11', '2017-05-09', '2017-05-06', '2017-05-02', '2017-04-30', '2017-04-25', '2017-04-25', '2017-04-25', '2017-04-24', '2017-04-20', '2017-04-18', '2017-04-18', '2017-04-15', '2017-04-12', '2017-04-11', '2017-04-09', '2017-04-01', '2017-03-28', '2017-03-27', '2017-03-12', '2017-02-22', '2017-02-08', '2017-01-31', '2017-01-24'))
                             )

# average all of the generic ballot polls that have been taken since June
polls_2018 %>% 
    filter(lubridate::month(end_date) > 6, lubridate::year(end_date)==2018) %>% 
    mutate(Dem.Margin = Democrat - Republican) %>%
    pull(Dem.Margin) %>% 
    mean()


# Filter the dataset to include polls from August and September
# Mutate a variable for the Democratic vote margin in that year
polls_predict <- generic_ballot %>%
    filter(lubridate::month(lubridate::mdy(Date)) %in% c(8, 9), ElecYear >= 1980) %>%
    mutate(Dem.Poll.Margin = Democrats - Republicans,
           Dem.Vote.Margin = DemVote - RepVote
           ) %>%
    group_by(ElecYear) %>%
    summarise(Dem.Poll.Margin = mean(Dem.Poll.Margin),
              Dem.Vote.Margin = mean(Dem.Vote.Margin)
              ) %>%
    arrange(ElecYear) %>%
    mutate(error=Dem.Poll.Margin - Dem.Vote.Margin, 
           party_in_power=c(-1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1)
           )


# Fit a model to predict Democrats' November vote margin with the Democratic poll margin and party in power variable
model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin + party_in_power, data=polls_predict)

# Evaluate the model
summary(model)


# Make a prediction for November if Democrats are up 7.5 points in the generic ballot and the party_in_power is the Republicans (-1)
predict(model, data.frame(Dem.Poll.Margin = 7.5, party_in_power=-1))

# Multiply the root-mean-square error by 1.96
sqrt(mean(c(model$fitted.values - polls_predict$Dem.Vote.Margin)^2)) * 1.96


pres_elecs <- tibble::tibble(Year=c(2016, 2012, 2008, 2004, 2000, 1996, 1992, 1988, 1984, 1980, 1976, 1972, 1968, 1964, 1960, 1956, 1952, 1948), 
                             q2_gdp=c(2.3, 1.3, 1.3, 2.6, 8, 7.1, 4.3, 5.2, 7.1, -7.9, 3, 9.8, 7, 4.7, -1.9, 3.2, 0.4, 7.5), 
                             pres_approve=c(7, -0.8, -37, -0.5, 19.5, 15.5, -18, 10, 20, -21.7, 5, 26, -5, 60.3, 37, 53.5, -27, -6), 
                             two_plus_terms=c(1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1), 
                             vote_share=c(51.1, 51.96, 46.3, 51.2, 50.3, 54.7, 46.5, 53.9, 59.2, 44.7, 48.9, 61.8, 49.6, 61.3, 49.9, 57.8, 44.5, 52.4)
                             )
pres_elecs


#  Make a plot with points representing a year's presidential approval and vote share and a line running through them to show the linear relationship
ggplot(pres_elecs, aes(x=pres_approve, y=vote_share, label=Year)) + 
    geom_text() + 
    geom_smooth(method='lm')


# Make a model that predict the vote_share variable with pres_approve, q2_gdp, and two_plus_terms
fit <- lm(vote_share ~ pres_approve + q2_gdp + two_plus_terms, pres_elecs)

# Evaluate the model
summary(fit)


# Save the predicted vote shares to a variable called predict
pres_elecs$predict <- predict(fit, pres_elecs)

# Graph the predictions and vote shares with a label for each election year
ggplot(pres_elecs,aes(x=predict, y=vote_share, label=Year)) + 
    geom_abline() +
    geom_text()

# Calculate the model's root-mean-square error
sqrt(mean(c(pres_elecs$predict - pres_elecs$vote_share)^2)) * 1.96

# Make a prediction for hypothetical data
predict(fit, data.frame(pres_approve=-15, q2_gdp=2, two_plus_terms=0))

```
  
  
  
***
  
###_Analyzing US Census Data in R_  
  
Chapter 1 - Census Data in R with tidycensus  
  
Overview:  
  
* Acquiring census and ACS data through tidycensus, wrangling with tidyverse, acquiring boundary data with tigris, and visualizing with ggplot2  
* The dicennial (every 10 years) census is a full count of the population  
	* ACS is a detailed study of 3 million households per year  
* Need to acquire a census API key and provide to the tidycensus function  
	* state_pop <- get_decennial(geography = "state", variables = "P001001")  # geography is the level of aggregation and variables are the desired variables  
    * state_income <- get_acs(geography = "state", variables = "B19013_001")  # default dataset is the 5-year 2012-2016 data; returns both an estimate and an extra column "moe" (margin of error)  
  
Basic tidycensus functionality:  
  
* Can acquire census data by legal entities (county, state) or statistical entities (census tracks)  
	* geography = "county"  
    * geography = "tract"  
    * county_income <- get_acs(geography = "county", variables = "B19013_001")  
* Can request the variable(s) of interest  
* Can further subset the data to only include states or counties of interest  
	* texas_income <- get_acs(geography = "county", variables = c(hhincome = "B19013_001"), state = "TX")  
* By default, census data are returned in tidy format, though wide (spread) data may be desirable at times  
	* get_acs(geography = "county", variables = c(hhincome = "B19013_001", medage = "B01002_001"), state = "TX", output = "wide")  
  
Searching for data with tidycensus:  
  
* There are 1000s of variables across ACS and the census, so it can be challenging to find variables of interest  
	* https://censusreporter.org/  
* Builit-in search capability in tidycensus  
	* v16 <- load_variables(year = 2016, dataset = "acs5", cache = TRUE)   # year is the end year, so 2012-2016; cache=TRUE will store the dataset locally for future browsing  
* The ACS variable structure, as shown using B19001_002E (income < 10k dollars)  
	* B means "base", while C means "collapse", P means "profile", and S means "subject"  
    * The 19001 is the table ID (household income)  
    * The 002 is a specific variable  
    * The E means that it is an estimate (tidycensus by default returns both error and margin of error)  
  
Visualizing census data with ggplot2:  
  
* Can plot census data using ggplot2  
	* ne_income <- get_acs(geography = "state", variables = "B19013_001", survey = "acs1", state = c("ME", "NH", "VT", "MA", "RI", "CT", "NY"))  
    * ggplot(ne_income, aes(x = estimate, y = NAME)) + geom_point()  
* Can improve plots using sorting and formatting  
	* ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + geom_point(color = "navy", size = 4) + scale_x_continuous(labels = scales::dollar) + theme_minimal(base_size = 14) + labs(x = "2016 ACS estimate", y = "", title = "Median household income by state")  
  
Example code includes:  
```{r eval=FALSE}

# Load the tidycensus package into your R session
library(tidycensus)

# Define your Census API key and set it with census_api_key()
api_key <- "INSERT HERE"
census_api_key(api_key)

# Check your API key
Sys.getenv("CENSUS_API_KEY")


# Obtain and view state populations from the 2010 US Census
state_pop <- get_decennial(geography = "state", variables = "P001001")

head(state_pop)

# Obtain and view state median household income from the 2012-2016 American Community Survey
state_income <- get_acs(geography = "state", variables = "B19013_001")

head(state_income)


# Get an ACS dataset for Census tracts in Texas by setting the state
tx_income <- get_acs(geography = "tract",
                     variables = "B19013_001",
                     state = "TX")

# Inspect the dataset
head(tx_income)

# Get an ACS dataset for Census tracts in Travis County, TX
travis_income <- get_acs(geography = "tract",
                         variables = "B19013_001", 
                         state = "TX",
                         county = "Travis")

# Inspect the dataset
head(travis_income)

# Supply custom variable names
travis_income2 <- get_acs(geography = "tract", 
                          variables = c(hhincome = "B19013_001"), 
                          state = "TX",
                          county = "Travis")

# Inspect the dataset
head(travis_income2)


# Return county data in wide format
or_wide <- get_acs(geography = "county", state = "OR",
                   variables = c(hhincome = "B19013_001", medage = "B01002_001"), 
                   output = "wide"
                   )

# Compare output to the tidy format from previous exercises
head(or_wide)

# Create a scatterplot
plot(or_wide$hhincomeE, or_wide$medageE)


# Load variables from the 2012-2016 ACS
v16 <- load_variables(year = 2016, dataset = "acs5", cache = TRUE)

# Get variables from the ACS Data Profile
v16p <- load_variables(year = 2016, dataset = "acs5/profile", cache = TRUE)

# Set year and dataset to get variables from the 2000 Census SF3
v00 <- load_variables(year = 2000, dataset = "sf3", cache = TRUE)


# Filter for table B19001
filter(v16, str_detect(name, "B19001"))

# Use public transportation to search for related variables
filter(v16p, str_detect(label, fixed("public transportation", ignore_case = TRUE)))


# Access the 1-year ACS  with the survey parameter
ne_income <- get_acs(geography = "state",
                     variables = "B19013_001", 
                     survey = "acs1", 
                     state = c("ME", "NH", "VT", "MA", 
                               "RI", "CT", "NY"))

# Create a dot plot
ggplot(ne_income, aes(x = estimate, y = NAME)) + 
  geom_point()

# Reorder the states in descending order of estimates
ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + 
  geom_point()


# Set dot color and size
g_color <- ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + 
  geom_point(color = "navy", size = 4)

# Format the x-axis labels
g_scale <- g_color + 
  scale_x_continuous(labels = scales::dollar) + 
  theme_minimal(base_size = 18) 

# Label your x-axis, y-axis, and title your chart
g_label <- g_scale + 
  labs(x = "2016 ACS estimate", y = "", title = "Median household income by state")
  
g_label

```
  
  
  
***
  
Chapter 2 - Wrangling US Census Data  
  
Tables and summary variables in tidycensus:  
  
* Can grab all of the variables from a table at once using the table= option  
	* wa_income <- get_acs(geography = "county", state = "WA", table = "B19001")  
* Can normalize based on denominators  
	* race_vars <- c(White = "B03002_003", Black = "B03002_004", Native = "B03002_005", Asian = "B03002_006", HIPI = "B03002_007", Hispanic = "B03002_012")  
    * tx_race <- get_acs(geography = "county", state = "TX", variables = race_vars, summary_var = "B03002_001")  
    * tx_race_pct <- tx_race %>% mutate(pct = 100 * (estimate / summary_est)) %>% select(NAME, variable, pct)  
  
Census data wrangling with tidy tools:  
  
* Can use the tidyverse to interact with data that has been retrieved from ACS and the census  
* The split-apply-combine approach to analysis is commonly used  
	* tx_largest <- tx_race %>% group_by(GEOID) %>% filter(estimate == max(estimate)) %>% select(NAME, variable, estimate)  
    * tx_largest %>% group_by(variable) %>% tally()  
* Can recode variables for group-wise analysis using case_when()  
	* wa_grouped <- wa_income %>% filter(variable != "B19001_001") %>% mutate(incgroup = case_when( variable < "B19001_008" ~ "below35k", variable < "B19001_013" ~ "35kto75k", TRUE ~ "above75k")) %>% group_by(NAME, incgroup) %>% summarize(group_est = sum(estimate))  
* Can use purrr to gather data from multiple years and then combine  
	* mi_cities <- map_df(2012:2016, function(x) { get_acs(geography = "place", variables = c(totalpop = "B01003_001"), state = "MI", survey = "acs1", year = x) %>% mutate(year = x) })  
  
Working with margins of error in tidycensus:  
  
* Margins of error exist in the ACS data since it is an annual survey of ~3 million; the MOE is the value needed to obtain the 95% CI  
	* get_acs(geography = "county", variables = c(median_age = "B01002_001"), state = "OR")  
* With sparse data (small geographies or niche queries), moe can be a meaningful percentage of the estimate, even exceeding it at times  
	* vt_eldpov <- get_acs(geography = "tract", variables = c(eldpovm = "B17001_016", eldpovf = "B17001_030"), state = "VT")  
* There are multiple tidycensus functions for calculating margin of error  
	* moe_sum(): MOE for a derived sum  
    * moe_product(): MOE for a derived product  
    * moe_ratio(): MOE for a derived ratio  
    * moe_prop(): MOE for a derived proportion  
    * vt_eldpov2 <- vt_eldpov %>% group_by(GEOID) %>% summarize( estmf = sum(estimate), moemf = moe_sum(moe = moe, estimate = estimate) )  
  
Visualizing margins of error from ACS:  
  
* Can use an error-bar plot to show the range of uncertainty - geom_errorbar() and geom_errorbarh()  
	* wyoming_age <- get_acs(geography = "county", variables = c(medianage = "B01002_001"), state = "WY")  
    * ggplot(wyoming_age, aes(x = estimate, y = NAME)) + geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + geom_point()  
* Can clean up visualizations - remove redundancies in names and order values  
	* wyoming_age2 <- wyoming_age %>% mutate(NAME = str_replace(NAME, " County, Wyoming", ""))  
    * ggplot(wyoming_age2, aes(x = estimate, y = reorder(NAME, estimate))) + geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + geom_point(size = 3, color = "darkgreen") + theme_grey(base_size = 14) + labs(title = "Median age, counties in Wyoming", subtitle = "2012-2016 American Community Survey", x = "ACS estimate (bars represent margins of error)", y = "")  
  
Example code includes:  
```{r eval=FALSE}

library(tidycensus)

# Download table "B19001"
wa_income <- get_acs(geography = "county", 
                 state = "WA", 
                 table = "B19001")

# Check out the first few rows of wa_income
head(wa_income)


# Assign Census variables vector to race_vars
race_vars <- c(White = "B03002_003", Black = "B03002_004", Native = "B03002_005", 
               Asian = "B03002_006", HIPI = "B03002_007", Hispanic = "B03002_012"
               )

# Request a summary variable from the ACS
ca_race <- get_acs(geography = "county", 
                   state = "CA",
                   variables = race_vars, 
                   summary_var = "B03002_001")

# Calculate a new percentage column and check the result
ca_race_pct <- ca_race %>%
  mutate(pct = 100 * (estimate / summary_est))

head(ca_race_pct)


# Group the dataset and filter the estimate
ca_largest <- ca_race %>%
  group_by(GEOID) %>%
  filter(estimate == max(estimate)) 

head(ca_largest)

# Group the dataset and get a breakdown of the results
ca_largest %>% 
  group_by(variable) %>%
  tally()


# Use a tidy workflow to wrangle ACS data
wa_grouped <- wa_income %>%
  filter(variable != "B19001_001") %>%
  mutate(incgroup = case_when(
    variable < "B19001_008" ~ "below35k", 
    variable < "B19001_013" ~ "35kto75k", 
    TRUE ~ "above75k"
  )) %>%
  group_by(NAME, incgroup) %>%
  summarize(group_est = sum(estimate))

wa_grouped


# Map through ACS1 estimates to see how they change through the years
mi_cities <- map_df(2012:2016, function(x) {
  get_acs(geography = "place", 
          variables = c(totalpop = "B01003_001"), 
          state = "MI", 
          survey = "acs1", 
          year = x) %>%
    mutate(year = x)
})

mi_cities %>% arrange(NAME, year)


# Get data on elderly poverty by Census tract in Vermont
vt_eldpov <- get_acs(geography = "tract", 
                     variables = c(eldpovm = "B17001_016", 
                                   eldpovf = "B17001_030"), 
                     state = "VT")

vt_eldpov

# Identify rows with greater margins of error than their estimates
moe_check <- filter(vt_eldpov, moe > estimate)

# Check proportion of rows where the margin of error exceeds the estimate
nrow(moe_check) / nrow(vt_eldpov)


# Calculate a margin of error for a sum
moe_sum(moe = c(55, 33, 44, 12, 4))

# Calculate a margin of error for a product
moe_product(est1 = 55, est2 = 33, moe1 = 12, moe2 = 9)

# Calculate a margin of error for a ratio
moe_ratio(num = 1000, denom = 950, moe_num = 200, moe_denom = 177)

# Calculate a margin of error for a proportion
moe_prop(num = 374, denom = 1200, moe_num = 122, moe_denom = 333)


# Group the dataset and calculate a derived margin of error
vt_eldpov2 <- vt_eldpov %>%
  group_by(GEOID) %>%
  summarize(
    estmf = sum(estimate), 
    moemf = moe_sum(moe = moe, estimate = estimate)
  )

# Filter rows where newly-derived margin of error exceeds newly-derived estimate
moe_check2 <- filter(vt_eldpov2, moemf > estmf)

# Check proportion of rows where margin of error exceeds estimate
nrow(moe_check2) / nrow(vt_eldpov2)


# Request median household income data
maine_inc <- get_acs(geography = "county", 
                     variables = c(hhincome = "B19013_001"), 
                     state = "ME") 

# Generate horizontal error bars with dots
ggplot(maine_inc, aes(x = estimate, y = NAME)) + 
  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + 
  geom_point()


# Remove unnecessary content from the county's name
maine_inc2 <- maine_inc %>%
  mutate(NAME = str_replace(NAME, " County, Maine", ""))

# Build a margin of error plot incorporating your modifications
ggplot(maine_inc2, aes(x = estimate, y = reorder(NAME, estimate))) + 
  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + 
  geom_point(size = 3, color = "darkgreen") + 
  theme_grey(base_size = 14) + 
  labs(title = "Median household income", 
       subtitle = "Counties in Maine", 
       x = "ACS estimate (bars represent margins of error)", 
       y = "") + 
  scale_x_continuous(labels = scales::dollar)

```
  
  
  
***
  
Chapter 3 - US Census Geographic Data in R  
  
Understanding census geography and tigris basics:  
  
* The TIGER line shape files are made publicly available by the US Census Bureau  
* The tigris package simplifies the process of downloading and mapping with TIGER shapes  
	* library(tigris)  
    * az_counties <- counties(state = "AZ")  
    * plot(az_counties)  
    * nh_roads <- primary_secondary_roads(state = "NH")  
    * plot(nh_roads)  
* By default, tigris returns objects in Spatial DF format - slots encode characteristics of the data  
  
Customizing tigris options:  
  
* The tigris package allows for customization of plotting options  
	* ri_tiger <- counties("RI")  
    * ri_cb <- counties("RI", cb = TRUE)  
    * par(mfrow = c(1, 2))  
    * plot(ri_tiger, main = "TIGER/Line")  
    * plot(ri_cb, main = "Cartographic boundary")  
* Can use the sf package for further plotting of tigris data  
	* options(tigris_class = "sf")  
    * az_sf <- counties("AZ", cb = TRUE)  
    * class(az_sf)  
* Can cache tigris shape files on a local computer  
	* options(tigris_use_cache = TRUE)  
* The defaults in tigris are to the most recent year of data, though this can be overridden by argument  
	* williamson90 <- tracts(state = "TX", county = "Williamson", cb = TRUE, year = 1990)  
    * williamson16 <- tracts(state = "TX", county = "Williamson", cb = TRUE, year = 2016)  
  
Combining and joining census geographic districts:  
  
* Can combine data obtained from multiple tigris data pulls; for example, studying Kansas City MO/KS  
	* missouri <- tracts("MO", cb = TRUE)  
    * kansas <- tracts("KS", cb = TRUE)  
    * kansas_missouri <- rbind_tigris(kansas, missouri)  
    * plot(kansas_missouri$geometry)  
* Can also create a geography such as New England using iteration  
	* new_england <- c("ME", "NH", "VT", "MA")  
    * ne_tracts <- map(new_england, function(x) { tracts(state = x, cb = TRUE) }) %>% rbind_tigris()  
* Can use the standard sf joining tool provided that the data are available in sf format  
	* tx_house <- state_legislative_districts(state = "TX", house = "lower", cb = TRUE)  
    * tx_joined <- left_join(tx_house, tx_members, by = c("NAME" = "District"))  
  
Plotting data with tigris and ggplot2:  
  
* Can create maps of legislative districts by political party currently holding the office  
	* ggplot(tx_joined) + geom_sf()  
    * ggplot(tx_joined, aes(fill = Party)) + geom_sf()  
    * ggplot(tx_joined, aes(fill = Party)) + geom_sf() + scale_fill_manual(values = c("R" = "red", "D" = "blue"))  
    * ggplot(tx_joined, aes(fill = Party)) + geom_sf() + coord_sf(crs = 3083, datum = NA) + scale_fill_manual(values = c("R" = "red", "D" = "blue")) + theme_minimal() + labs(title = "State House Districts in Texas")  
  
Example code includes:  
```{r eval=FALSE}

library(tigris)

# Get a counties dataset for Colorado and plot it
co_counties <- counties(state = "CO")
plot(co_counties)


# Get a Census tracts dataset for Denver County, Colorado and plot it
denver_tracts <- tracts(state = "CO", county = "Denver")
plot(denver_tracts)


# Plot area water features for Lane County, Oregon
lane_water <- area_water(state = "OR", county = "Lane")
plot(lane_water)

# Plot primary & secondary roads for the state of New Hampshire
nh_roads <- primary_secondary_roads(state = "NH")
plot(nh_roads)


# Check the class of the data
class(co_counties)

# Take a look at the information in the data slot
head(co_counties@data)

# Check the coordinate system of the data
co_counties@proj4string


# Get a counties dataset for Michigan
mi_tiger <- counties("MI")

# Get the equivalent cartographic boundary shapefile
mi_cb <- counties("MI", cb = TRUE)

# Overlay the two on a plot to make a comparison
plot(mi_tiger)
plot(mi_cb, add = TRUE, border = "red")


# Get data from tigris as simple features
options(tigris_class = "sf")

# Get countries from Colorado and view the first few rows
colorado_sf <- counties("CO")
head(colorado_sf)

# Plot its geometry column
plot(colorado_sf$geometry)


# DO NOT ADD CACHE FOR NOW
# Set the cache directory
# tigris_cache_dir("Your preferred cache directory path would go here")

# Set the tigris_use_cache option
# options(tigris_use_cache = TRUE)

# Check to see that you've modified the option correctly
# getOption("tigris_use_cache")


# Get a historic Census tract shapefile from 1990 for Williamson County, Texas
williamson90 <- tracts(state = "TX", county = "Williamson", 
                       cb = TRUE, year = 1990)

# Compare with a current dataset for 2016
williamson16 <- tracts(state = "TX", county = "Williamson", 
                       cb = TRUE, year = 2016)

# Plot the geometry to compare the results                       
par(mfrow = c(1, 2))
plot(williamson90$geometry)
plot(williamson16$geometry)


# Get Census tract boundaries for Oregon and Washington
or_tracts <- tracts("OR", cb = TRUE)
wa_tracts <- tracts("WA", cb = TRUE)

# Check the tigris attributes of each object
attr(or_tracts, "tigris")
attr(wa_tracts, "tigris")

# Combine the datasets then plot the result
or_wa_tracts <- rbind_tigris(or_tracts, wa_tracts)
plot(or_wa_tracts$geometry)


# Generate a vector of state codes and assign to new_england
new_england <- c("ME", "NH", "VT", "MA")

# Iterate through the states and request tract data for state
ne_tracts <- map(new_england, function(x) {
  tracts(state = x, cb = TRUE)
}) %>%
  rbind_tigris()

plot(ne_tracts$geometry)


# Get boundaries for Texas and set the house parameter
tx_house <- state_legislative_districts(state = "TX", house = "lower", cb = TRUE)

# Merge data on legislators to their corresponding boundaries
tx_joined <- left_join(tx_house, tx_members, by = c("NAME" = "District"))

head(tx_joined)


# Plot the legislative district boundaries
ggplot(tx_joined) + 
  geom_sf()

# Set fill aesthetic to map areas represented by Republicans and Democrats
ggplot(tx_joined, aes(fill = Party)) + 
  geom_sf()

# Set values so that Republican areas are red and Democratic areas are blue
ggplot(tx_joined, aes(fill = Party)) + 
  geom_sf() + 
  scale_fill_manual(values = c("R" = "red", "D" = "blue"))


# Draw a ggplot without gridlines and with an informative title
ggplot(tx_joined, aes(fill = Party)) + 
  geom_sf() + 
  coord_sf(crs = 3083, datum = NA) + 
  scale_fill_manual(values = c("R" = "red", "D" = "blue")) + 
  theme_minimal(base_size = 16) + 
  labs(title = "State House Districts in Texas")

```
  
  
  
***
  
Chapter 4 - Mapping US Census Data  
  
Simple feature geometry and tidycensus:  
  
* The tidycensus package can wrap the tigris package for some of its less complex data requests  
	* geometry = TRUE is available for the following geographies: state, county, tract, 'block group', block, zcta  
    * cook_value <- get_acs(geography = "tract", state = "IL", county = "Cook", variables = "B25077_001", geometry = TRUE)  
* For geographies that are not available for grabbing geography by default, can use join instead  
	* idaho_income <- get_acs(geography = "school district (unified)", variables = "B19013_001", state = "ID")  
    * idaho_school <- school_districts(state = "ID", type = "unified", class = "sf")  
    * id_school_joined <- left_join(idaho_school, idaho_income, by = "GEOID")  
* Maps may move and rescale AK and HI - set shift_geo=TRUE to implement this  
	* state_value <- get_acs(geography = "state", variables = "B25077_001", survey = "acs1", geometry = TRUE, shift_geo = TRUE)  
  
Mapping demographic data with ggplot2:  
  
* Can use fills to create instructive maps, and can edit for improved clarity  
	* ggplot(cook_value, aes(fill = estimate)) + geom_sf()  
    * ggplot(cook_value, aes(fill = estimate, color = estimate)) + geom_sf() + scale_fill_viridis_c() + scale_color_viridis_c()  
    * ggplot(cook_value, aes(fill = estimate, color = estimate)) + geom_sf() + scale_fill_viridis_c(labels = scales::dollar) + scale_color_viridis_c(guide = FALSE) + theme_minimal() + coord_sf(crs = 26916, datum = NA) + labs(title = "Median home value by Census tract", subtitle = "Cook County, Illinois", caption = "Data source: 2012-2016 ACS.\nData acquired with the R tidycensus package.", fill = "ACS estimate")  
  
Advance demographic mapping:  
  
* There are many visual variables in cartography - position, size, shape, value, hue, orientation, texture  
* The graduated symbol map is a popular plotting method - shapes are differentially sized based on underlying variable values  
	* centers <- st_centroid(state_value)  
    * ggplot() + geom_sf(data = state_value, fill = "white") + geom_sf(data = centers, aes(size = estimate), shape = 21, fill = "lightblue", alpha = 0.7, show.legend = "point") + scale_size_continuous(range = c(1, 20))  
* The "small multiples" plot is another common plot type - achieved with faceting  
	* ggplot(dc_race, aes(fill = percent, color = percent)) + geom_sf() + coord_sf(datum = NA) + facet_wrap(~variable)  
* Interactive visualizations are continually more prevalent, and many of the libraries are wrapped for use in R - leaflet, plotly, htmlwidgets  
	* library(mapview)  
    * mapview(cook_value, zcol = "estimate", legend = TRUE)  
  
Cartographic workflows with tigris and tidycensus:  
  
* Can create maps using a dot-density distribution; dots are scattered by a sub-unit (such as census tract), proportional to the size (such as population), colored optionally by a third attriubute (such as race)  
	* dc_dots <- map(c("White", "Black", "Hispanic", "Asian"), function(group) { dc_race %>% filter(variable == group) %>% st_sample(., size=.$value / 100) %>% st_sf() %>% mutate(group = group) }) %>% reduce(rbind)  
    * dc_dots <- dc_dots %>% group_by(group) %>% summarize()  # for faster plotting  
    * dc_dots_shuffle <- sample_frac(dc_dots, size = 1)  # ensures random dots rather than clumped dots by group  
    * plot(dc_dots_shuffle, key.pos = 1)  
* Can be valuable to supplement maps with roadways and bodies of water  
	* options(tigris_class = "sf")  
    * dc_roads <- roads("DC", "District of Columbia") %>%  filter(RTTYP %in% c("I", "S", "U"))  
    * dc_water <- area_water("DC", "District of Columbia")  
    * dc_boundary <- counties("DC", cb = TRUE)  
    * plot(dc_water$geometry, col = "lightblue")  
    * ggplot() + geom_sf(data = dc_boundary, color = NA, fill = "white") + geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + geom_sf(data = dc_water, color = "lightblue", fill = "lightblue") + geom_sf(data = dc_roads, color = "grey") + coord_sf(crs = 26918, datum = NA) + scale_color_brewer(palette = "Set1", guide = FALSE) + scale_fill_brewer(palette = "Set1") + labs(title = "The racial geography of Washington, DC", subtitle = "2010 decennial U.S. Census", fill = "", caption = "1 dot = approximately 100 people.\nData acquired with the R tidycensus and tigris packages.")  
* Note that ggplot2 plots the layers in order, meaning that they tend to over-write lower layers  
  
Next steps for working with demographic data in R:  
  
* Additional R packages for deomgraphic data include censusapi, ipumsr (MN data), cancensus (demographic and census data from Canadian census)  
  
Example code includes:  
```{r eval=FALSE}

library(sf)

# Get dataset with geometry set to TRUE
orange_value <- get_acs(geography = "tract", state = "CA", county = "Orange", 
                        variables = "B25077_001", geometry = TRUE
                        )

# Plot the estimate to view a map of the data
plot(orange_value["estimate"])


# Get an income dataset for Idaho by school district
idaho_income <- get_acs(geography = "school district (unified)", 
                        variables = "B19013_001", 
                        state = "ID")

# Get a school district dataset for Idaho
idaho_school <- school_districts(state = "ID", type = "unified", class = "sf")

# Join the income dataset to the boundaries dataset
id_school_joined <- left_join(idaho_school, idaho_income, by = "GEOID")

plot(id_school_joined["estimate"])


# Get a dataset of median home values from the 1-year ACS
state_value <- get_acs(geography = "state", 
                       variables = "B25077_001", 
                       survey = "acs1", 
                       geometry = TRUE, 
                       shift_geo = TRUE)

# Plot the dataset to view the shifted geometry
plot(state_value["estimate"])


# Create a choropleth map with ggplot
ggplot(marin_value, aes(fill = estimate)) + 
  geom_sf()


# Set continuous viridis palettes for your map
ggplot(marin_value, aes(fill = estimate, color = estimate)) + 
  geom_sf() + 
  scale_fill_viridis_c() +  
  scale_color_viridis_c()


# Set the color guide to FALSE and add a subtitle and caption to your map
ggplot(marin_value, aes(fill = estimate, color = estimate)) + 
  geom_sf() + 
  scale_fill_viridis_c(labels = scales::dollar) +  
  scale_color_viridis_c(guide = FALSE) + 
  theme_minimal() + 
  coord_sf(crs = 26911, datum = NA) + 
  labs(title = "Median owner-occupied housing value by Census tract", 
       subtitle = "Marin County, California", 
       caption = "Data source: 2012-2016 ACS.\nData acquired with the R tidycensus package.", 
       fill = "ACS estimate")


# Generate point centers
centers <- st_centroid(state_value)

# Set size parameter and the size range
ggplot() + 
  geom_sf(data = state_value, fill = "white") + 
  geom_sf(data = centers, aes(size = estimate), shape = 21, 
          fill = "lightblue", alpha = 0.7, show.legend = "point") + 
  scale_size_continuous(range = c(1, 20))


# Check the first few rows of the loaded dataset dc_race
head(dc_race)

# Remove the gridlines and generate faceted maps
ggplot(dc_race, aes(fill = percent, color = percent)) + 
  geom_sf() + 
  coord_sf(datum = NA) + 
  facet_wrap(~variable)


# Map the orange_value dataset interactively
m <- mapview(orange_value)
m@map

# Map your data by the estimate column
m <- mapview(orange_value, zcol = "estimate")
m@map

# Add a legend to your map
m <- mapview(orange_value, zcol = "estimate", legend=TRUE)

m@map


# Generate dots, create a group column, and group by group column
dc_dots <- map(c("White", "Black", "Hispanic", "Asian"), function(group) {
  dc_race %>%
    filter(variable == group) %>%
    st_sample(., size = .$value / 100) %>%
    st_sf() %>%
    mutate(group = group) 
}) %>%
  reduce(rbind) %>%
  group_by(group) %>%
  summarize()


# Filter the DC roads object for major roads only
dc_roads <- roads("DC", "District of Columbia") %>%
  filter(RTTYP %in% c("I", "S", "U"))

# Get an area water dataset for DC
dc_water <- area_water("DC", "District of Columbia")

# Get the boundary of DC
dc_boundary <- counties("DC", cb = TRUE)


# Plot your datasets and give your map an informative caption
ggplot() + 
  geom_sf(data = dc_boundary, color = NA, fill = "white") + 
  geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + 
  geom_sf(data = dc_water, color = "lightblue", fill = "lightblue") + 
  geom_sf(data = dc_roads, color = "grey") + 
  coord_sf(crs = 26918, datum = NA) + 
  scale_color_brewer(palette = "Set1", guide = FALSE) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "The racial geography of Washington, DC", 
       subtitle = "2010 decennial U.S. Census", 
       fill = "", 
       caption = "1 dot = approximately 100 people.\nData acquired with the R tidycensus and tigris packages.")

```
  
  
  
***
  
###_Multivariate Probability Distributions in R_  
  
Chapter 1 - Reading and Plotting Mutivariate Data  
  
Reading multivariate data:  
  
* Multivariate distributions describe 2+ variables (particularly when correlated) at the same time  
* Generally, multivariate data are in the tidy format - rows are observations, columns are variables/attributes  
	* iris_url <- "http://mlg.eng.cam.ac.uk/teaching/3f3/1011/iris.data"  
    * iris_raw <- read.table(iris_url, sep ="", header = FALSE)  
    * head(iris_raw, n = 4)  
    * colnames(iris_raw) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species" )  
    * bwt <- read.csv("birthweight.csv", row.names = 1)  
* Can access specific portions of the data and change data types as needed  
	* iris_raw$species <- as.factor(iris_raw$species)  
    * iris_raw$Species <- recode(iris_raw$Species, " 1 ='setosa'; 2 = 'versicolor'; 3 = 'virginica'")  
  
Mean vector and variance-covariance matrix:  
  
* Can use summary statistics to explore the dataset  
* The mean vector is the vector containing the means of each of the dimensions, while the variance-covariance matrix shows the variance and angles of the data  
	* colMeans(iris_raw[, 1:4])  
    * by(data = iris[,1:4], INDICES = iris$Species, FUN = colMeans)  
    * aggregate(. ~ Species, iris_raw, mean)  
* The variance-covariance matrix is produced using var() and cor()  
	* var(iris_raw[, 1:4])  
    * cor(iris_raw[, 1:4])  
    * corrplot(cor(iris_raw[, 1:4]), method = "ellipse")  
  
Plotting mutivariate data:  
  
* Can look at multiple bivariate plots or mutivariate data in many manners  
	* pairs(iris_raw[, 1:4])  # pair plot  
    * pairs(iris_raw[, 1:4], col = iris_raw$Species)  
    * lattice::splom(~iris_raw[, 1:4], col = iris_raw$Species, pch = 16)  
    * Ggally::ggpairs(data = iris_raw, columns = 1:4, mapping = aes(color = Species))  
    * scatterplot3d::scatterplot3d(iris_raw[, c(1, 3, 4)], color = as.numeric(iris_raw$Species))  
  
Example code includes:  
```{r cache=TRUE}

# Read in the wine dataset
wine <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep = ",")

# Print the first four entries
head(wine, n=4)

# Find the dimensions of the data
dim(wine)

# Check the names of the wine dataset 
names(wine)


# Assign new names
names(wine) <- c('Type', 'Alcohol', 'Malic', 'Ash', 'Alcalinity', 'Magnesium', 'Phenols', 'Flavanoids', 'Nonflavanoids','Proanthocyanins', 'Color', 'Hue', 'Dilution', 'Proline')
                      
# Check the new column names
names(wine)

# Check data type of each variable
str(wine)

# Change the Type variable data type
wine$Type <- as.factor(wine$Type)

# Check data type again 
str(wine)


# Calculate the mean of the Alcohol, Malic, Ash, and Alcalinity variables 
colMeans(wine[, 2:5])

# Calculate the mean of the variables by wine type
by(wine[, 2:5], wine$Type, FUN=colMeans)


# Calculate the variance-covariance matrix of the variables Alcohol, Malic, Ash, Alcalinity
var.wine <- var(wine[, 2:5])

# Round the matrix values to two decimal places 
round(var.wine, 2)


# Calculate the covariance matrix 
cor.wine <- cor(wine[, 2:5])

# Round the matrix to two decimal places 
round(cor.wine, 2)

# Plot the correlations 
corrplot::corrplot(cor.wine, method = "ellipse")


# Scatter plot matrix with base R 
pairs(wine[, 2:5])

# Scatter plot matrix with lattice  
lattice::splom(~wine[, 2:5])

# Scatter plot matrix colored by groups
lattice::splom( ~ wine[2:5], pch = 16, col=wine$Type)


# Produce a matrix of plots for the first four variables 
wine.gg <- GGally::ggpairs(wine[, 2:5])
wine.gg

# Produce a matrix of plots for the first four variables 
wine.gg <- GGally::ggpairs(wine, columns=2:5)
wine.gg

# Color the points by wine type 
wine.gg <- GGally::ggpairs(data = wine, columns = 2:5, aes(color=Type))
wine.gg


# Plot the three variables 
scatterplot3d::scatterplot3d(wine[, c("Alcohol", "Malic", "Alcalinity")], color=wine$Type)

```
  
  
  
***
  
Chapter 2 - Multivariate Normal Distribution  
  
Multivariate Normal Distribution:  
  
* Important probability distribution - generalization of the univariate normal, but with a variance-covariance matrix  
	* Elliptical, with center at the joint mean  
    * Shape of the ellipse depends on the variance-covariance matrix  
    * If the covariance is 0 and the variances are equal, then this is a circle  
    * If the covariance is high, then this will become like a line  
* Can use mvtnorm::rmvnorm() and the related functions  
  
Density of a multivariate normal distribution:  
  
* Can use the dnorm() function to get the height of the density curve at any given location  
* Can use densities to estimate the likelihood that a data point is generated from a given distribution  
	* dmvnorm(x, mean, sigma)  
    * mu1 <- c(1, 2)  
    * sigma1 <- matrix(c(1, .5, .5, 2), 2)  
    * dmvnorm(x = c(0, 0), mean = mu1, sigma = sigma1)  
    * x <- rbind(c(0, 0), c(1, 1), c(0, 1)); x  
    * dmvnorm(x = x, mean = mu, sigma = sigma)  
* Can create a perspective plot using the persp() function  
	* d <- expand.grid(seq(-3, 6, length.out = 50 ), seq(-3, 6, length.out = 50))  
    * dens1 <- dmvnorm(as.matrix(d), mean=c(1,2), sigma=matrix(c(1, .5, .5, 2), 2))  
    * dens1 <- matrix(dens1, nrow = 50 )  
    * persp(dens1, theta = 80, phi = 30, expand = 0.6, shade = 0.2, col = "lightblue", xlab = "x", ylab = "y", zlab = "dens")  
  
Cumulative distribution and inverse CDF:  
  
* Can be valuable to calculate the CDF and the inverse CDF for statistical purposes  
	* mu1 <- c(1, 2)  
    * sigma1 <- matrix(c(1, 0.5, 0.5, 2), 2)  
    * pmvnorm(upper = c(2, 4), mean = mu1, sigma = sigma1)  
    * pmvnorm(lower = c(1, 2), upper = c(2, 4), mean = mu1, sigma = sigma1)  
* Suppose that you want to find the smallest ellipse that contains 95% of the density  
	* sigma1 <- diag(2)  # force the contours to be circular  
    * qmvnorm(p = 0.95, sigma = sigma1, tail = "both")  
  
Checking normality of multivariate data:  
  
* Normality is a convenient assumption for simplifying many common statistical tests  
	* qqnorm(iris_raw[, 1])  
    * qqline(iris_raw[, 1])  
    * uniPlot(iris_raw[, 1:4])  
* Can use several tests for normality of the multivariate distributions  
	* mardiaTest(iris_raw[, 1:4])  
    * mardiaTest(iris_raw[, 1:4], qqplot = TRUE)  
    * hzTest(iris_raw[,1:4])  
    * mardiaTest(iris[iris_raw$Species == "setosa", 1:4])  
  
Example code includes:  
```{r}

mu.sim <- c(2, -2)
sigma.sim <- matrix(data=c(9, 5, 5, 4), nrow=2, byrow=FALSE)

mu.sim
sigma.sim


# Generate 100 bivariate normal samples
multnorm.sample <- mvtnorm::rmvnorm(100, mean=mu.sim, sigma=sigma.sim)

# View the first 6 samples
head(multnorm.sample)

# Scatterplot of the bivariate samples 
plot(multnorm.sample)


# Calculate density
multnorm.dens <- mvtnorm::dmvnorm(multnorm.sample, mean = mu.sim, sigma = sigma.sim)

# Create scatter plot of density heights 
scatterplot3d::scatterplot3d(cbind(multnorm.sample, multnorm.dens), color="blue", pch="", 
                             type = "h", xlab = "x", ylab = "y", zlab = "density"
                             )


mvals <- expand.grid(seq(-5, 10, length.out = 40), seq(-8, 4, length.out = 40))
str(mvals)


# Calculate density over the specified grid
mvds <- mvtnorm::dmvnorm(mvals, mean=mu.sim, sigma=sigma.sim)
matrix_mvds <-  matrix(mvds, nrow = 40)

# Create a perspective plot
persp(matrix_mvds, theta = 80, phi = 30, expand = 0.6, shade = 0.2, 
      col = "lightblue", xlab = "x", ylab = "y", zlab = "dens"
      )


# Volume under a bivariate standard normal
mvtnorm::pmvnorm(lower = c(-1, -1), upper = c(1, 1))

# Volume under specified mean and variance-covariance matrix
mvtnorm::pmvnorm(lower = c(-5, -5), upper = c(5, 5), mean = mu.sim, sigma = sigma.sim)


# Probability contours for a standard bivariate normal
mvtnorm::qmvnorm(p=0.9, tail = "both", sigma = diag(2))

# Probability contours for a bivariate normal 
mvtnorm::qmvnorm(p=0.95, tail = "both", mean=mu.sim, sigma=sigma.sim)


# Test sample normality 
qqnorm(multnorm.sample[, 1])
qqline(multnorm.sample[, 1])


# requires RJAGS 4+
# Create qqnorm plot (no longer exported from MVN)
# MVN::uniPlot(wine[, c("Alcohol", "Malic", "Ash", "Alcalinity")], type = "qqplot")
# MVN::mvn(wine[, c("Alcohol", "Malic", "Ash", "Alcalinity")], univariatePlot = "qq")

# requires RJAGS 4+
# mardiaTest qqplot 
# wine.mvntest <- MVN::mardiaTest(wine[, 2:5])  # 'MVN::mardiaTest' is deprecated.\nUse 'mvn' instead.\nSee help(\"Deprecated\")
# wine.mvntest <- MVN::mvn(wine[, 2:5])
# wine.mvntest


# requires RJAGS 4+
# Use mardiaTest
# MVN::mvn(multnorm.sample)

# requires RJAGS 4+
# Use hzTest
# MVN::hzTest(wine[, 2:5])  # 'MVN::hzTest' is deprecated.  Use 'mvn' instead.
# MVN::mvn(wine[, 2:5], mvnTest="hz")

```
  
  
  
***
  
Chapter 3 - Other Multivariate Distributions  
  
Other common multivariate distributions:  
  
* Not all multivariate distributions are normal - may be skewed or follow a different distribution  
	* Normal has mean, sigma  
    * t has delta, sigma with DF  
    * Skew-normal has xi, Omega  
    * Skew-t has xi, Omega with DF  
* For the t-distribution, tails are fatter than they would be for the normal (which behaves like a t-distribution with oo degrees of freedom)  
	* rmvt(n, delta, sigma, df)  
    * dmvt(x, delta, sigma, df)  
    * qmvt(p, delta, sigma, df)  
    * pmvt(upper, lower, delta, sigma, df)  
  
Density and cumulative density for mutlivariate-T:  
  
* Individual stocks are often modeled by a univariate t-distribution  
* Portfolios are often valued using a multivariate t-distribution  
	* dmvt(x, delta = rep(0, p), sigma = diag(p), log = TRUE)  
    * x <- seq(-3, 6, by = 1); y <- seq(-3, 6, by = 1)  
    * d <- expand.grid(x = x, y = x)  
    * del1 <- c(1, 2); sig1 <- matrix(c(1, .5, .5, 2), 2)  
    * dens <- dmvt(as.matrix(d), delta = del1, sigma = sig1, df = 10, log = FALSE)  
    * scatterplot3d(cbind(d, dens), type = "h", zlab = "density")  
* Can claculate the pmvt()  
	* pmvt(lower = -Inf, upper = Inf, delta, sigma, df, ...)  
    * pmvt(lower = c(-1, -2), upper = c(2, 2), delta = c(1, 2), sigma = diag(2), df = 6)  
    * qmvt(p, interval, tail, delta, sigma, df)  
  
Multivariate skewed distributions:  
  
* Normal and t distributions both model symmetric (non-skewed) data, though real-world data are frequently skewed  
* The univariate skew-normal includes psi (location) and omega (variance-covariance) and alpha (skew)  
* The sn library offers skew-normal as a distribution  
	* dmsn(x, xi, Omega, alpha)  
    * pmsn(x, xi, Omega, alpha)  
    * rmsn(n, xi, Omega, alpha)  
    * dmst(x, xi, Omega, alpha, nu)  # nu is df  
    * pmst(x, xi, Omega, alpha, nu)  # nu is df  
    * rmst(n, xi, Omega, alpha, nu )  # nu is df  
* Example of creating random data  
	* xi1 <- c(1, 2, -5)  
    * Omega1 <- matrix(c(1, 1, 0, 1, 2, 0, 0, 0, 5), 3, 3)  
    * alpha1 <- c(4, 30, -5)  
    * skew.sample <- rmsn(n = 2000, xi = xi1, Omega = Omega1, alpha = alpha1)  
    * skewt.sample <- rmst(n = 2000, xi = xi1, Omega = Omega1, alpha = alpha1, nu = 4)  
* Can estimate the parameters from the data  
	* msn.mle(y = skew.sample, opt.method = "BFGS")  
  
Example code includes:  
```{r}

# Generate the t-samples 
multt.sample <- mvtnorm::rmvt(200, delta=mu.sim, sigma=sigma.sim, df=5)

# Print the first 6 samples
head(multt.sample)

# Requires RJAGS 4+
# Check multivariate normality
# MVN::mvn(multt.sample, univariatePlot="qq", mvnTest="mardia")


# Calculate densities
multt.dens <- mvtnorm::dmvt(multt.sample, delta=mu.sim, sigma=sigma.sim, df=5) 

# Plot 3D heights of densities
scatterplot3d::scatterplot3d(cbind(multt.sample, multt.dens), color = "blue", pch = "", 
                             type = "h", xlab = "x", ylab = "y", zlab = "density"
                             )

# Calculate the volume under the specified t-distribution
mvtnorm::pmvt(lower = c(-5, -5), upper = c(5, 5), delta=mu.sim, sigma=sigma.sim, df=5)

# Calculate the equal probability contour
mvtnorm::qmvt(p=0.9, tail="both", delta=0, sigma=diag(2), df=5)


# Generate the skew-normal samples 
skewnorm.sample <- sn::rmsn(n=100, xi=mu.sim, Omega=sigma.sim, alpha=c(4, -4)) 

# Print first six samples
head(skewnorm.sample)

# Generate the skew-t samples 
skewt.sample <- sn::rmst(n = 100, xi = mu.sim, Omega = sigma.sim, alpha = c(4, -4), nu=5)

# Print first six samples
head(skewt.sample)


skewnorm.sampleDF <- data.frame(x=skewnorm.sample[, 1], y=skewnorm.sample[, 2])
str(skewnorm.sampleDF)


# Contour plot for skew-normal sample
ggplot(skewnorm.sampleDF, aes(x=x, y=y)) + 
    geom_point() + 
    geom_density_2d()

# Requires RJAGS 4+
# Normality test for skew-normal sample
# skewnorm.Test <- MVN::mvn(skewnorm.sample, mvnTest="mardia", univariatePlot="qq")

# Requires RJAGS 4+
# Normality test for skew-t sample
# skewt.Test <- MVN::mvn(skewt.sample, mvnTest="mardia", univariatePlot="qq") 


ais.female <- data.frame(Ht=c(195.9, 189.7, 177.8, 185, 184.6, 174, 186.2, 173.8, 171.4, 179.9, 193.4, 188.7, 169.1, 177.9, 177.5, 179.6, 181.3, 179.7, 185.2, 177.3, 179.3, 175.3, 174, 183.3, 184.7, 180.2, 180.2, 176, 156, 179.7, 180.9, 179.5, 178.9, 182.1, 186.3, 176.8, 172.6, 176, 169.9, 183, 178.2, 177.3, 174.1, 173.6, 173.7, 178.7, 183.3, 174.4, 173.3, 168.6, 174, 176, 172.2, 182.7, 180.5, 179.8, 179.6, 171.7, 170, 170, 180.5, 173.3, 173.5, 181, 175, 170.3, 165, 169.8, 174.1, 175, 171.1, 172.7, 175.6, 171.6, 172.3, 171.4, 178, 162, 167.3, 162, 170.8, 163, 166.1, 176, 163.9, 173, 177, 168, 172, 167.9, 177.5, 162.5, 172.5, 166.7, 175, 157.9, 158.9, 156.9, 148.9, 149), 
                         Wt=c(78.9, 74.4, 69.1, 74.9, 64.6, 63.7, 75.2, 62.3, 66.5, 62.9, 96.3, 75.5, 63, 80.5, 71.3, 70.5, 73.2, 68.7, 80.5, 72.9, 74.5, 75.4, 69.5, 66.4, 79.7, 73.6, 78.7, 75, 49.8, 67.2, 66, 74.3, 78.1, 79.5, 78.5, 59.9, 63, 66.3, 60.7, 72.9, 67.9, 67.5, 74.1, 68.2, 68.8, 75.3, 67.4, 70, 74, 51.9, 74.1, 74.3, 77.8, 66.9, 83.8, 82.9, 64.1, 68.85, 64.8, 59, 72.1, 75.6, 71.4, 69.7, 63.9, 55.1, 60, 58, 64.7, 87.5, 78.9, 83.9, 82.8, 74.4, 94.8, 49.2, 61.9, 53.6, 63.7, 52.8, 65.2, 50.9, 57.3, 60, 60.1, 52.5, 59.7, 57.3, 59.6, 71.5, 69.7, 56.1, 61.1, 47.4, 56, 45.8, 47.8, 43.8, 37.8, 45.1)
                         )
str(ais.female)


# Fit skew-normal parameters
fit.ais <- sn::msn.mle(y = cbind(ais.female$Ht, ais.female$Wt), opt.method = "BFGS")

# Print the skewness parameters
fit.ais$dp$alpha

# Fit skew-normal parameters
fit.ais <- sn::msn.mle(y = ais.female[, c("Ht", "Wt")])

# Print the skewness parameters
fit.ais$dp$alpha

```
  
  
  
***
  
Chapter 4 - Principal Component Analysis and Multidimensional Scaling  
  
Principal Component Analysis:  
  
* Creation of uncorrelated (orthogonal) components that can be linearly combined to create the original dataset  
	* PC1 explains the maximum variance  
    * PC2 is orthogonal to PC1 and explains the maximum remaining variance  
    * PC3 is orthogonal to PC1/PC2 and explains the maximum remaning variance  
    * princomp(x, cor = FALSE, scores = TRUE)  # if cor=TRUE, use the correlation/covariance matrix to create PCA rather than the original data provided  
* Can run an example using the mtcars dataset  
	* mtcars.sub <- mtcars[ , -c(8,9)]  
    * cars.pca <- princomp(mtcars.sub, cor = TRUE, scores = TRUE)  
  
Choosing the number of components:  
  
* Several methods for assessing the ideal number of PC to use  
	* screeplot(cars.pca, type = "lines")  # pick an elbow point as the cutoff  
    * summary(cars.pca)  # pick the number of components that explain X% of the variance, with X specified prior to analysis  
  
Interpreting PCA attributes:  
  
* There are many attributes of the principal components object, especially when scores=TRUE is set  
	* cars.pca <- princomp(mtcars.sub, cor = TRUE, scores = TRUE)  
    * cars.pca$loadings # or loadings(cars.pca)  
    * biplot(cars.pca, col = c("gray","steelblue"), cex = c(0.5, 1.3))  
* PCA scores are the projection of the original dataset on the principal components  
	* head(cars.pca$scores)  
    * head(cars.pca$scores[, 1:2])  
    * biplot(cars.pca, col = c("steelblue", "white"), cex = c(0.8, 0.01))  
    * scores <- data.frame(cars.pca$scores)  
    * ggplot(data = scores, aes(x = Comp.1, y = Comp.2, label = rownames(scores))) + geom_text(size = 4, col = "steelblue")  
    * cylinder <- factor(mtcars$cyl)  
    * ggplot(data = scores, aes(x = Comp.1, y = Comp.2, label = rownames(scores), color = cylinder)) + geom_text(size = 4)  
* Can use functions from the factoextra library  
	* fviz_pca_biplot(cars.pca)  
    * fviz_pca_ind(cars.pca)  
    * fviz_pca_var(cars.pca)  
  
Multi-dimensional scaling:  
  
* Placing objects conceptually in a 2D or 3D space such that distances match as closesly as possible - MDS (multi-dimensional scaling)  
	* cmdscale(d, k = 2, ...)  # default k=2  
    * usloc <- cmdscale(UScitiesD)  
    * ggplot(data = data.frame(usloc), aes(x = X1, y = X2, label = rownames(usloc))) + geom_text()  
* Can apply MDS using the mtcars dataset  
	* cars.dist <- dist(mtcars)  
    * cars.mds <- cmdscale(cars.dist, k = 2)  
    * cars.mds <- data.frame(cars.mds)  
    * ggplot(data = cars.mds, aes(x = X1, y = X2, label = rownames(cars.mds))) + geom_text()  
* Can run in higher dimensions by changing the k= parameter  
	* cars.dist <- dist(mtcars)  
    * cmds3 <- data.frame(cmdscale(cars.dist, k = 3))  
    * scatterplot3d(cmds3, type = "h", pch = 19, lty.hplot = 2)  
  
Wrap-Up:  
  
* Reading and reformatting data  
* Summary statistics - Mean Vector, Variance-covariance matrix, Correlation matrix  
* Plotting in 2D and 3D  
* Multivariate probability distributions: Normal, T, Skew-normal, Skew-t  
* Dimension reduction: PCA, MDS  
  
Example code includes:  
```{r}

data(state)
str(state.x77)


par(mfrow=c(1, 1))
par(mfcol=c(1, 1))


# Calculate PCs
pca.state <- princomp(state.x77, cor=TRUE, scores=TRUE) 

# Plot the PCA object  
plot(pca.state) 

# Print the summary of the PCs
summary(pca.state) 


# Variance explained by each PC
pca.var <- pca.state$sdev^2  

# Proportion of variance explained by each PC
pca.pvar <- pca.var/sum(pca.var) 


# Proportion of variance explained by each principal component
pca.pvar

# Cumulative variance explained plot
plot(cumsum(pca.pvar), xlab = "Principal component", 
     ylab = "Cumulative Proportion of variance explained", ylim = c(0,1), type = 'b')
grid()

# Add a horizontal line
abline(h=0.95, col="blue")


# Draw screeplot
screeplot(pca.state, type = "l")
grid()


# Create dataframe of scores
scores.state <- data.frame(pca.state$scores)

# Plot of scores labeled by state name
ggplot(data = scores.state, aes(x = Comp.1, y = Comp.2, label = rownames(scores.state))) + 
    geom_text( alpha = 0.8, size = 3) + 
    ggtitle("PCA of states data")


# Create dataframe of scores
scores.state <- data.frame(pca.state$scores)

# Plot of scores colored by region
ggplot(data=scores.state, aes(x=Comp.1, y=Comp.2, label=rownames(scores.state), color=state.region)) + 
    geom_text(alpha = 0.8, size = 3) + 
    ggtitle("PCA of states data colored by region")


# Plot the scores
factoextra::fviz_pca_ind(pca.state)

# Plot the PC loadings
factoextra::fviz_pca_var(pca.state)

# Create a biplot
factoextra::fviz_pca_biplot(pca.state)


# Calculate distance 
state.dist <- dist(state.x77)

# Perform multidimensional scaling 
mds.state <- cmdscale(state.dist) 
mds.state_df <- data.frame(mds.state)

# Plot the representation of the data in two dimensions 
ggplot(data = mds.state_df, aes(x = X1, y = X2, label = rownames(mds.state), color = state.region)) + 
    geom_text(alpha = 0.8, size = 3)


# Calculate distance 
wine.dist <- dist(wine[, -1])

# Perform multidimensional scaling 
mds.wine <- cmdscale(wine.dist, k=3) 
mds.wine_df <- data.frame(mds.wine)

# Plot the representation of the data in three dimensions 
scatterplot3d::scatterplot3d(mds.wine_df, color = wine$Type, pch = 19, type = "h", lty.hplot = 2)

```
  
  
  
***
  
###_Intermediate Functional Programming with purrr_  
  
Chapter 1 - Programming with purrr  
  
Refresher of purrr Basics:  
  
* The map() function is one of the most basic purrr calls  
	* map(.x, .f, .)  # for each element of .x do .f  
* OpenData files available from French city St Malo  
    * JSON format; nested list  
* The map() function will always return a list by default  
	* res <- map(visit_2015, sum)  # returns a list  
* Can override to other preferred outputs, such as map_dbl()  
	* res <- map_dbl(visit_2015, sum)  # returns a numeric  
* Can also extend to map2(.x, .y, .f, .) which resolves to do .f(.x, .y, .)  
	* res <- map2(visit_2015, visit_2016, sum)  
    * res <- map2_dbl(visit_2015, visit_2016, sum)  
* Can use pmap() to run operations on 3+ items, though these need to be passed in as a list  
	* l <- list(visit_2014, visit_2015, visit_2016)  
    * res <- pmap(l, sum)  
    * res <- pmap_dbl(l, sum)  
  
Introduction to mappers:  
  
* The .f is the action element - function applied to every element, number n to extract the nth element, character vector of named elements to extract  
* The functions can either be regular functions or lambda (anonymous) functions  
	* map_dbl(visit_2014, function(x) { round(mean(x)) })   
* The anonymous function with a one-sided formula can be written in any of several ways  
	* map_dbl(visits2017, ~ round(mean(.x)))  # typically the default  
    * map_dbl(visits2017, ~ round(mean(.)))  
    * map_dbl(visits2017, ~ round(mean(..1)))  
    * map2(visits2016, visits2017, ~ .x + .y)  
    * map2(visits2016, visits2017, ~ ..1 + ..2)  
* Can extend to data with more than 2 parameters  
	* pmap(list, ~ ..1 + ..2 + ..3)  
* Can use as_mapper to create mapper objects from lambda functions  
	* round_mean <- function(x){ round(mean(x)) }  
    * round_mean <- as_mapper(~ round(mean(.x))))  
* Mappers have several benefits  
	* More concise  
    * Easier to read than functions  
    * Reusable  
  
Using Mappers to Clean Data:  
  
* Can use set_names from purrr to set the names of a list  
	* visits2016 <- set_names(visits2016, month.abb)  
    * all_visits <- list(visits2015, visits2016, visits2017)  
    * named_all_visits <- map(all_visits, ~ set_names(.x, month.abb))  
* The keep() function extracts elements that satisfy a condition  
	* over_30000 <- keep(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * over_mapper <- keep(visits2016, limit)  
* The discard() function removes elements that satisfy a condition  
	* under_30000 <- discard(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * under_mapper <- discard(visits2016, limit)  
    * names(under_mapper)  
* Can use keep() and discard() with map() to clean up lists  
	* df_list <- list(iris, airquality) %>% map(head)  
    * map(df_list, ~ keep(.x, is.factor))  
  
Predicates:  
  
* Predicates return either TRUE or FALSE - example of is.numeric()  
* Predicate functionals take an element and a predicate, and then use the predicate on the element  
	* keep(airquality, is.numeric)  # keep all elements that return TRUE when run against the predicate  
* There are also extensions of every() and some()  
	* every(visits2016, is.numeric)  
    * every(visits2016, ~ mean(.x) > 1000)  
    * some(visits2016, ~ mean(.x) > 1000)  
* The detect_index() returns the first and last element that satisfies a condition  
	* detect_index(visits2016, ~ mean(.x) > 1000)  # index of first element that satisfies  
    * detect_index(visits2016, ~ mean(.x) > 1000, .right = TRUE)  # index of last element that satisfies  
* The detect() returns the value rather than the index  
	* detect(visits2016, ~ mean(.x) > 1000, .right = TRUE)  
* The has_element() tests whether an object contains an item  
	* visits2016_mean <- map(visits2016, mean)  
    * has_element(visits2016_mean,981)  
  
Example code includes:  
```{r}

visit_a <- c(117, 147, 131, 73, 81, 134, 121)
visit_b <- c(180, 193, 116, 166, 131, 153, 146)
visit_c <- c(57, 110, 68, 72, 87, 141, 67)

# Create the to_day function
to_day <- function(x) {
    x*24
}

# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)

# Convert to daily number of visits: all_visits_day
all_visits_day <- map(all_visits, to_day)

# Map the mean() function and output a numeric vector 
map_dbl(all_visits_day, mean)


# You'll test out both map() and walk() for plotting
# Both return the "side effects," that is to say, the changes in the environment (drawing plots, downloading a file, changing the working directory...), but walk() won't print anything to the console.

# Create all_tests list  and modify with to_day() function
all_tests <- list(visit_a, visit_b, visit_c)
all_tests_day <- map(all_tests, to_day)

# Plot all_tests_day with map
map(all_tests_day, barplot)

# Plot all_tests_day
walk(all_tests_day, barplot)

# Get sum of all visits and class of sum_all
sum_all <- pmap(all_tests_day, sum)
class(sum_all)


# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) { x*24 })

# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)

# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)

# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
map(visit_c, to_day)


# Round visit_a to the nearest tenth with a mapper
map_dbl(visit_a, ~ round(.x, -1))

# Create to_ten, a mapper that rounds to the nearest tenth
to_ten <- as_mapper(~ round(.x, -1))

# Map to_ten on visit_b
map_dbl(visit_b, to_ten)

# Map to_ten on visit_c
map_dbl(visit_c, to_ten)


# Create a mapper that test if .x is more than 100 
is_more_than_hundred <- as_mapper(~ .x > 100)

# Run this mapper on the all_visits object
map(all_visits, ~ keep(.x, is_more_than_hundred) )

# Use the  day vector to set names to all_list
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
full_visits_named <- map(all_visits, ~ set_names(.x, day))

# Use this mapper with keep() 
map(full_visits_named, ~ keep(.x, is_more_than_hundred))


# Set the name of each subvector
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
all_visits_named <- map(all_visits, ~ set_names(.x, day))

# Create a mapper that will test if .x is over 100 
threshold <- as_mapper(~.x > 100)

# Run this mapper on the all_visits object: group_over
group_over <- map(all_visits, ~ keep(.x, threshold) )

# Run this mapper on the all_visits object: group_under
group_under <-  map(all_visits, ~ discard(.x, threshold) )


# Create a threshold variable, set it to 160
threshold <- 160

# Create a mapper that tests if .x is over the defined threshold
over_threshold <- as_mapper(~ .x > threshold)

# Are all elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ every(.x, over_threshold))

# Are some elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ some(.x, over_threshold))

```
  
  
  
***
  
Chapter 2 - Functional Programming from Theory to Practice  
  
Functional Programming in R:  
  
* Everything that exists is an object and everything that happens is a function call  
	* This means that a function is an object and can be treated as such  
    * Every action in R is performed by a function  
    * Functions are first-class citizens, and behave like any other object  
    * Functions can be manipulated, stored as variables, lambda (anonymous), stored in a list, arguments of a function, returned by a function  
    * R is a functional programming language  
* In a "pure function", output depends only on input, and there are no side-effects (no changes to the environment)  
	* Sys.Date() depends on the enviornment and is thus not pure  
    * write.csv() is called solely for the side effect (writing a file) and is thus not pure  
  
Tools for Functional Programming in purrr:  
  
* A high order function can take functions as input and return functions as output  
	* nop_na <- function(fun){  
    *     function(...){ fun(..., na.rm = TRUE) }  
    * }  
    * sd_no_na <- nop_na(sd)  
    * sd_no_na( c(NA, 1, 2, NA) )  
* There are three types of high-order functions  
	* Functionals take another function and return a vector - like map()  
    * Function factories take a vector and create a function  
    * Function operators take functions and return functions - considered to be "adverbs"  
* Two of the most common adverbs in purrr are safely() and possibly()  
	* The safely() call returns a function that will return $result and $error when run; helpful for diagnosing issues with code rather than losing the information  
    * safe_log <- safely(log)  
    * safe_log("a")  # there will be $result of NULL and $error with the error code  
    * map( list(2, "a"), safely(log) )  
  
Using possibly():  
  
* The possibly() function is an adverb that returns either the value of the function OR the value specified in the otherwise element  
	* possible_sum <- possibly(sum, otherwise = "nop")  
    * possible_sum("a")  # result will be "nop"  
* Note that possibly() cannot be made to run a function; it will just return a pre-specified value  
  
Handling adverb results:  
  
* Can use transpose() to change the output (converts the list to inside out)  
	* Transpose turn a list of n elements a and b to a list of a and b, with each n elements  
* The compact() function will remove the NULL elements  
	* l <- list(1,2,3,"a")  
    * possible_log <- possibly(log, otherwise = NULL)  
    * map(l, possible_log) %>% compact()  
* Can use the httr package specifically for http requests  
	* httr::GET(url) will return the value from attempting to connect to url - 200 is good, 404 is unavailable, etc.  
  
Example code includes:  
```{r cache=TRUE}

# `$` is a function call, of a special type called 'infix operator', as they are put between two elements, and can be used without parenthesis.

# Launch Sys.time(), Sys.sleep(1), & Sys.time()
Sys.time()
Sys.sleep(1)
Sys.time()


data(iris)
str(iris)


# Launch nrow(iris), Sys.sleep(1), & nrow(iris)
nrow(iris)
Sys.sleep(1)
nrow(iris)


# Launch ls(), create an object, then rerun the ls() function
# ls()
# this <- 12
# ls()

# Create a plot of the iris dataset
plot(iris)


urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')


# Create a safe version of read_lines()
safe_read <- safely(readr::read_lines)

# Map it on the urls vector
res <- map(urls, safe_read)

# Set the name of the results to `urls`
named_res <-  set_names(res, urls)

# Extract only the "error" part of each sublist
map(named_res, "error")


# Code a function that discard() the NULL from safe_read()
safe_read_discard <- function(url){
    safe_read(url) %>%
        discard(is.null)
}

# Map this function on the url list
res <- map(urls, safe_read_discard)


# Create a possibly() version of read_lines()
possible_read <- possibly(readr::read_lines, otherwise = 404)

# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)

# Paste each element of the list 
res_pasted <- map(res, paste, collapse=" ")

# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)


url_tester <- function(url_list){
    url_list %>%
        # Map a version of read_lines() that otherwise returns 404
        map( possibly(readr::read_lines, otherwise = 404) ) %>%
        # Set the names of the result
        set_names( urls ) %>% 
        # paste() and collapse each element
        map(paste, collapse =" ") %>%
        # Remove the 404 
        discard(~.x==404) %>%
        names() # Will return the names of the good ones
}

# Try this function on the urls object
url_tester(urls)


url_tester <- function(url_list, type = c("result", "error")){
    res <- url_list %>%
        # Create a safely() version of read_lines() 
        map( safely(readr::read_lines) ) %>%
        set_names( url_list ) %>%
        # Transpose into a list of $result and $error
        transpose() 
    # Complete this if statement
    if (type == "result") return( res$result ) 
    if (type == "error") return( res$error ) 
}

# Try this function on the urls object
url_tester(urls, type = "error") 


url_tester <- function(url_list){
    url_list %>%
        # Map a version of GET() that would otherwise return NULL 
        map( possibly(httr::GET, otherwise=NULL) ) %>%
        # Set the names of the result
        set_names( urls ) %>%
        # Remove the NULL
        compact() %>%
        # Extract all the "status_code" elements
        map("status_code")
}

# Try this function on the urls object
url_tester(urls)

```
  
  
  
***
  
Chapter 3 - Better Code with purrr  
  
Rationale for cleaner code:  
  
* Cleaner code is easier to debug (spot typos), easier to interpret, and easier to modify  
	* tidy_iris_lm <- compose( as_mapper(~ filter(.x, p.value < 0.05)), tidy, partial(lm, data=iris, na.action = na.fail) )  
    * list( Petal.Length ~ Petal.Width, Petal.Width ~ Sepal.Width, Sepal.Width ~ Sepal.Length ) %>% map(tidy_iris_lm)  
* Clean code characteristics  
	* Light - no unnecessary code  
    * Readable - less repition makes for easier reading (one piece of code for one task)  
    * Interpretable  
    * Maintainable  
* The compose() function is used to compose a function from two or more functions  
	* rounded_mean <- compose(round, mean)  
  
Building functions with compose() and negate():  
  
* There is a limitless amount of functions that can be included in compose()  
	* clean_aov <- compose(tidy, anova, lm)  
* Can use negate() to flip the predicate - TRUE becomes FALSE and FALSE becomes TRUE  
	* is_not_na <- negate(is.na)  
    * under_hundred <- as_mapper(~ mean(.x) < 100)  
    * not_under_hundred <- negate(under_hundred)  
    * map_lgl(98:102, under_hundred)  
    * map_lgl(98:102, not_under_hundred)  
* The "good" status return codes from GET() are in the low-200s  
	* good_status <- c(200, 201, 202, 203)  
    * status %in% good_status  
  
Prefilling functions:  
  
* The partial() allows for pre-filling a function  
	* mean_na_rm <- partial(mean, na.rm = TRUE)  
    * lm_iris <- partial(lm, data = iris)  
* Can also combine partial() and compose()  
	* rounded_mean <- compose( partial(round, digits = 2), partial(mean, na.rm = TRUE) )  
* Can use functions from rvest for web scraping  
	* read_html()  
    * html_nodes()  
    * html_text()  
    * html_attr()  
  
List columns:  
  
* A list column is part of a nested data frame - one or more of the data frame columns is itself a list (requires use of tibble rather than data.frame)  
	* df <- tibble( classic = c("a", "b","c"), list = list( c("a", "b","c"), c("a", "b","c", "d"), c("a", "b","c", "d", "e") ) )  
    * a_node <- partial(html_nodes, css = "a")  
    * href <- partial(html_attr, name = "href")  
    * get_links <- compose( href, a_node,  read_html )  
    * urls_df <- tibble( urls = c("https://thinkr.fr", "https://colinfay.me", "https://datacamp.com", "http://cran.r-project.org/") )  
    * urls_df %>% mutate(links = map(urls, get_links))  
* Can also unnest the data from the list columns  
	* urls_df %>% mutate(links = map(urls, get_links)) %>% unnest()  
* Can also nest() a standard data.frame  
	* iris_n <- iris %>% group_by(Species) %>% tidyr::nest()  
* Since the list column is a list, the purrr functions can be run on them  
	* iris_n %>% mutate(lm = map(data, ~ lm(Sepal.Length ~ Sepal.Width, data = .x)))  
    * summary_lm <- compose(summary, lm)  
    * iris %>% group_by(Species) %>% nest() %>% mutate(data = map(data, ~ summary_lm(Sepal.Length ~ Sepal.Width, data = .x)), data = map(data, "r.squared")) %>% unnest()  
  
Example code includes:  
```{r cache=TRUE}

urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')


# Compose a status extractor (compose is also an igraph function)
status_extract <- purrr::compose(httr::status_code, httr::GET)

# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")

# Map it on the urls vector, return a vector of numbers
oldUrls <- urls
urls <- oldUrls[c(1, 2, 4, 5)]
map_dbl(urls, status_extract)


# Negate the %in% function 
`%not_in%` <- negate(`%in%`)

# Compose a status extractor 
status_extract <- purrr::compose(httr::status_code, httr::GET)

# Complete the function
strict_code <- function(url){
    code <- status_extract(url)
    if (code %not_in% c(200:203)){ return(NA) } else { return(code) } 
}


# Map the strict_code function on the urls vector
res <- map(urls, strict_code)

# Set the names of the results using the urls vector
res_named <- set_names(res, urls)

# Negate the is.na function
is_not_na <- negate(is.na)

# Run is_not_na on the results
is_not_na(res_named)


# Prefill html_nodes() with the css param set to h2
get_h2 <- partial(rvest::html_nodes, css="h2")

# Combine the html_text, get_h2 and read_html functions
get_content <- purrr::compose(rvest::html_text, get_h2, xml2::read_html)

# Map get_content to the urls list
res <- map(urls, get_content) %>%
    set_names(urls)

# Print the results to the console
res


# Create a partial version of html_nodes(), with the css param set to "a"
a_node <- partial(rvest::html_nodes, css="a")

# Create href(), a partial version of html_attr()
href <- partial(rvest::html_attr, name = "href")

# Combine href(), a_node(), and read_html()
get_links <- purrr::compose(href, a_node, xml2::read_html)

# Map get_links() to the urls list
res <- map(urls, get_links) %>%
    set_names(urls)


df <- tibble::tibble(urls=urls)
df


# Create a "links" columns, by mapping get_links() on urls
df2 <- df %>%
    mutate(links = map(urls, get_links)) 

# Print df2 to see what it looks like
df2

# unnest() df2 to have a tidy dataframe
df2 %>%
    tidyr::unnest()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Discovering the Dataset:  
  
* The dataset is available from https://github.com/ThinkR-open/datasets  
	* rstudioconf: a list of 5055 tweets  
    * length(rstudioconf)  
    * length(rstudioconf[[1]])  
    * purrr::vec_depth(rstudioconf)  
* JSON is a standard data format for the web, and typically consists of key-value pairs which are read as nested lists by R  
* Refresher of keep() and discard() usage  
	* keep(1:10, ~ .x < 5)  
    * discard(1:10, ~ .x < 5)  
  
Extracting Information from the Dataset:  
  
* Can manipulate functions for list cleaning using high-order functions - includes partial() and compose()  
	* sum_no_na <- partial(sum, na.rm = TRUE)  
    * map_dbl(airquality, sum_no_na)  
    * rounded_sum <- compose(round, sum_no_na)  
    * map_dbl(airquality, rounded_sum)  
* Can also clean lists using compact() to remove NULL and flatten() to remove one level from a nested list  
	* l <- list(NULL, 1, 2, 3, NULL)  
    * compact(l)  
    * my_list <- list( list(a = 1), list(b = 2) )  
    * flatten(my_list)  
  
Manipulating URL:  
  
* Can use the mapper functions to create a re-usable function  
	* mult <- as_mapper(~ .x * 2)  
* Can use str_detect inside the mapper function  
	* lyrics <- c("Is this the real life?", "Is this just fantasy?", "Caught in a landslide", "No escape from reality")  
    * stringr::str_detect(a, "life")  
  
Identifying Influencers:  
  
* Can use the map_at() function to run a function at a specific portion of the list  
	* my_list <- list( a = 1:10, b = 1:100, c = 12 )  
    * map_at(.x = my_list, .at = "b", .f = sum)  
* Can also use negate() to reverse the actio of a predicate  
	* not_character <- negate(is.character)  
    * my_list <- list( a = 1:10, b = "a", c = iris )  
    * map(my_list, not_character)  
  
Wrap up:  
  
* Lambda functions and reusable mappers  
	* map(1:5, ~ .x*10)  
    * ten_times <- as_mapper(~ .x * 10)  
    * map(1:5, ten_times)  
* Function manipulation using functionals (functions that take functions as inputs and return vectors)  
	* map() & friends  
    * keep() & discard()  
    * some() & every()  
* Function operators take functions and return (modified) functions  
	* safely() & possibly()  
    * partial()  
    * compose()  
    * negate()  
* Cleaner code is easier to read, understand, and maintain  
	* rounded_mean <- compose( partial(round, digits = 1), partial(mean, trim = 2, na.rm = TRUE) )  
    * map( list(airquality, mtcars), ~ map_dbl(.x, rounded_mean) )  
  
Example code includes:  
```{r}

rstudioconfDF <- readRDS("./RInputFiles/#RStudioConf.RDS")
dim(rstudioconfDF)
rstudioconf <- as.list(as.data.frame(t(rstudioconfDF)))
length(rstudioconf)
length(rstudioconf[[1]])


# Print the first element of the list to the console 
rstudioconf[[1]]

# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")

# Extract the favorite count element of each non_rt sublist
fav_count <- map_dbl(non_rt, "favorite_count")

# Get the median of favorite_count for non_rt
median(fav_count)


# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
    map("user_id") %>% 
    unique()

# Remove the RT, extract the user id, remove the duplicate
non_rt <- discard(rstudioconf, "is_retweet") %>%
    map("user_id") %>% 
    unique()

# Determine the total number of users
union(rt, non_rt) %>% length()

# Determine the number of users who has just retweeted
setdiff(rt, non_rt) %>% length()


# Prefill mean() with na.rm, and round() with digits = 1
mean_na_rm <- partial(mean, na.rm=TRUE)
round_one <- partial(round, digits=1)

# Compose a rounded_mean function
rounded_mean <- purrr::compose(round_one, mean_na_rm)

# Extract the non retweet  
non_rt <- discard(rstudioconf, "is_retweet")

# Extract "favorite_count", and pass it to rounded_mean()
map_dbl(non_rt, "favorite_count") %>%
    rounded_mean()


# Combine as_vector(), compact(), and flatten()
flatten_to_vector <- purrr::compose(as_vector, compact, flatten)

# Complete the fonction
extractor <- function(list, what = "mentions_screen_name"){
    map(list, what) %>%
        flatten_to_vector()
}

# Create six_most, with tail(), sort(), and table()
six_most <- purrr::compose(tail, sort, table)

# Run extractor() on rstudioconf
extractor(rstudioconf) %>% 
    six_most()


# Extract the "urls_url" elements, and flatten() the result
urls_clean <- map(rstudioconf, "urls_url") %>%
    flatten()

# Remove the NULL
compact_urls <- compact(urls_clean)
compact_urls <- discard(urls_clean, is.na)  # Due to creation of the list above, NULL became NA

# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x, "github"))

# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
    sum()


# Complete the function
ratio_pattern <- function(vec, pattern){
    n_pattern <- str_detect(vec, pattern) %>% sum()
    n_pattern / length(vec)
}

# Create flatten_and_compact()
extraDiscard <- function(x) { discard(x, is.na) }  # address same NA issue as above
flatten_and_compact <- purrr::compose(compact, extraDiscard, flatten)

# Complete the pipe to get the ratio of URLs with "github"
map(rstudioconf, "urls_url") %>%
    flatten_and_compact() %>% 
    ratio_pattern("github")


# Create mean_above, a mapper that tests if .x is over 3.3
mean_above <- as_mapper(~ .x > 3.3)

# Prefil map_at() with "retweet_count", mean_above for above, 
# and mean_above negation for below
above <- partial(map_at, .at = "retweet_count", .f = mean_above )
below <- partial(map_at, .at = "retweet_count", .f = negate(mean_above) )

# Map above() and below() on non_rt, keep the "retweet_count"
# ab <- map(non_rt, above) %>% keep("retweet_count")
# bl <- map(non_rt, below) %>% keep("retweet_count")

# Compare the size of both elements
# length(ab)
# length(bl)


# Get the max() of "retweet_count" 
max_rt <- map_dbl(non_rt, "retweet_count") %>% 
    max()

# Prefill map_at() with a mapper testing if .x equal max_rt
# max_rt_calc <- partial(map_at, .at = "retweet_count", .f = ~.x==max_rt )

# Map max_rt_calc on non_rt, keep the retweet_count & flatten
# res <- map(non_rt, max_rt_calc) %>% 
#     keep("retweet_count") %>% 
#     flatten()

# Print the "screen_name" and "text" of the result
# res$screen_name
# res$text

```
  
  
  
***
  
###_Foundations of Functional Programming with purrr_  
  
Chapter 1 - Simplifying Iteration and Lists with purrr  
  
The power of iteration:  
  
* Iteration is the process of repeating commands over elements of a dataset  
	* d <- list()  
    * for(i in 1:10){ d[[i]] <- read_csv(files[i]) }  
* Typos are a risk when using for loops; purrr simplifies this by reducing everything to the commands map_*()  
	* map(.x=object, .f=function)  
    * d <- map(files, read_csv)  
* Can use the bird_counts data as an example  
	* bird_sum <- map(bird_counts, sum)  
  
Subsetting lists:  
  
* Lists can store multiple data types - data frames, models, numbers, text, whatever  
* Indexing a data frame  
	* mtcars[1, "wt"]  
    * mtcars$wt  
* List indexing differs from data frames and vectors  
	* lo[[2]]  # pull the second element  
    * lo[["model"]]  # pull by name  
* Calculate something on each element with purrr  
	* map(survey_data, ~nrow(.x))  # The ~ symbolizes that this is a user-written function  
  
Multiple flavors of map():  
  
* May want to output data in a different class, and the map_*() help with this  
	* map_dbl(survey_data, ~nrow(.x))  # return as a vector of doubles  
    * map_lgl(survey_data, ~nrow(.x)==14)  # return as logical vector (boolean)  
    * map_chr(species_names, ~.x)  # return as character vector 
    * survey_rows <- data.frame(names = names(survey_data), rows = NA) 
    * survey_rows$rows <- map_dbl(survey_data, ~nrow(.x)) 
  
Example code includes:  
```{r}

files <- list('data_from_1990.csv', 'data_from_1991.csv', 'data_from_1992.csv', 'data_from_1993.csv', 'data_from_1994.csv', 'data_from_1995.csv', 'data_from_1996.csv', 'data_from_1997.csv', 'data_from_1998.csv', 'data_from_1999.csv', 'data_from_2000.csv', 'data_from_2001.csv', 'data_from_2002.csv', 'data_from_2003.csv', 'data_from_2004.csv', 'data_from_2005.csv'
              )
files <- map(files, function(x) { paste0("./RInputFiles/", x) })


# Initialize list
all_files <- list()

# For loop to read files into a list
for(i in seq_along(files)){
  all_files[[i]] <- readr::read_csv(file = files[[i]])
}

# Output size of list object
length(all_files)


# Use map to iterate
all_files_purrr <- purrr::map(files, read_csv) 

# Output size of list object
length(all_files_purrr)


temp <- c("1", "2", "3", "4")
list_of_df <- list(temp, temp, temp, temp, temp, temp, temp, temp, temp, temp)


# Check the class type of the first element
class(list_of_df[[1]])

# Change each element from a character to a number
for(i in seq_along(list_of_df)){
    list_of_df[[i]] <- as.numeric(list_of_df[[i]])
}

# Check the class type of the first element
class(list_of_df[[1]])

# Print out the list
list_of_df


temp <- c("1", "2", "3", "4")
list_of_df <- list(temp, temp, temp, temp, temp, temp, temp, temp, temp, temp)


# Check the class type of the first element
class(list_of_df[[1]])  

# Change each character element to a number
list_of_df <- map(list_of_df, as.numeric)

# Check the class type of the first element again
class(list_of_df[[1]])

# Print out the list
list_of_df


# Load wesanderson dataset
data(wesanderson, package="repurrrsive")

# Get structure of first element in wesanderson
str(wesanderson[[1]])

# Get structure of GrandBudapest element in wesanderson
str(wesanderson$GrandBudapest)


# Third element of the first wesanderson vector
wesanderson[[1]][3]

# Fourth element of the GrandBudapest wesanderson vector
wesanderson$GrandBudapest[4]


data(sw_films, package="repurrrsive")


# Subset the first element of the sw_films data
sw_films[[1]]

# Subset the first element of the sw_films data, title column 
sw_films[[1]]$title


# Map over wesanderson to get the length of each element
map(wesanderson, length)

# Map over wesanderson, and determine the length of each element
map(wesanderson, ~length(.x))


# Map over wesanderson and determine the length of each element
map(wesanderson, length)

# Create a numcolors column and fill with length of each wesanderson element
data.frame(numcolors = map_dbl(wesanderson, ~length(.x)))

```
  
  
  
***
  
Chapter 2 - More Complex Iterations  
  
Working with unnamed lists:  
  
* The purrr package works well with the piping (%>%) operator  
	* names(survey_data)  
    * survey_data %>% names()  
    * sw_films <- sw_films %>% set_names(map_chr(sw_films, "title"))  # make the sw_films list in to a named list based on the title of each movie in the last (sw_films is Star Wars films from repurrrsive)  
    * map(waterfowl_data, ~.x %>% sum() %>% log())  
  
More map():  
  
* Can use map() to simulate data, run models, test models, etc.  
	* list_of_df <- map(list_of_means, ~data.frame(a=rnorm(mean = .x, n = 200, sd = (5/2))))  
    * models <- education_data %>% map(~ lm(income ~ education_level, data=.x)) %>% map(summary)  
    * map(livingthings, ~.x[["species"]])  
* There are many flavors of map_*() which can be applied  
	* map_lgl - logical vector output  
    * map_dbl - double vector output  
    * map_int - integer vector output  
    * map_chr - character vector output  
    * map_dbl(bird_measurements, ~.x[["wing length"]])  
* Can use map_df to create a data frame, and using the data_frame() function  
	* bird_measurements %>% map_df(~ data_frame(weight=.x[["weight"]], wing_length = .x[["wing length"]]))  
  
map2() and pmap():  
  
* The map2() function allows for pulling out information from two inputs - .x is list 1 and .y is list 2  
	* simdata <- map2(list_of_means, list_of_sd, ~data.frame(a = rnorm(mean=.x, n=200, sd=.y), b = rnorm(mean=200, n=200, sd=15)))  
* Can also use the pmap() function for as many lists as we want to use (takes a list of lists as an input)  
	* input_list <- list( means = list_of_means, sd = list_of_sd, samplesize = list_of_samplesize)  
    * simdata <- pmap(inputs_list, function(means, sd, samplesize) data.frame(a = rnorm(mean=means, n=samplesize, sd=sd)))  
  
Example code includes:  
```{r}

# Use pipes to check for names in sw_films
sw_films %>%
    names()


# Set names so each element of the list is named for the film title
sw_films_named <- sw_films %>% 
  set_names(map_chr(., "title"))

# Check to see if the names worked/are correct
names(sw_films_named)


# Create a list of values from 1 through 10
numlist <- list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

# Iterate over the numlist 
map(numlist, ~.x %>% sqrt() %>% sin())


# List of sites north, east, and west
sites <- list("north", "east", "west")

# Create a list of dataframes, each with a years, a, and b column 
list_of_df <-  map(sites,  
  ~data.frame(years = .x,
       a = rnorm(mean = 5, n = 200, sd = 5/2),
       b = rnorm(mean = 200, n = 200, sd = 15)))

map(list_of_df, head)


# Map over the models to look at the relationship of a vs b
list_of_df %>%
    map(~ lm(a ~ b, data = .)) %>%
    map(~summary(.))


# Pull out the director element of sw_films in a list and character vector
map(sw_films, ~.x[["director"]])
map_chr(sw_films, ~.x[["director"]])

# Compare outputs when checking if director is George Lucas
map(sw_films, ~.x[["director"]] == "George Lucas")
map_lgl(sw_films, ~.x[["director"]] == "George Lucas")


# Pull out episode_id element as list
map(sw_films, ~.x[["episode_id"]])

# Pull out episode_id element as double vector
map_dbl(sw_films, ~.x[["episode_id"]])

# Pull out episode_id element as list
map(sw_films, ~.x[["episode_id"]])

# Pull out episode_id element as integer vector
map_int(sw_films, ~.x[["episode_id"]])


# List of 1 through 3
means <- list(1, 2, 3)

# Create sites list
sites <- list("north", "west", "east")

# Map over two arguments: years and mu
list_of_files_map2 <- map2(sites, means, ~data.frame(sites = .x,
                           a = rnorm(mean = .y, n = 200, sd = (5/2))))

map(list_of_files_map2, head)


means <- list(1, 2, 3)
sigma <- list(1, 2, 3)
means2 <- list(0.5, 1, 1.5)
sigma2 <- list(0.5, 1, 1.5)


# Create a master list, a list of lists
pmapinputs <- list(sites = sites,  means1 = means, sigma1 = sigma, 
                   means2 = means2, sigma2 = sigma2)

# Map over the master list
list_of_files_pmap <- pmap(pmapinputs, 
                           function(sites, means1, sigma1, means2, sigma2) {
                               data.frame(years = sites, 
                                          a = rnorm(mean = means1, n = 200, sd = sigma1), 
                                          b = rnorm(mean = means2, n = 200, sd = sigma2)
                                          )
                               }
                           )
                           
map(list_of_files_pmap, head)

```
  
  
  
***
  
Chapter 3 - Troubleshooting Lists with purrr  
  
How to purrr safely():  
  
* Can be QA/QC challenges if elements of the large list are not as expected  
* The map_*() will only work if the list is as expected  
* Can use the safely() command to find out where the issues are occurring, keep running, and putting a default output using the otherwise= element  
	* a <- list("unknown", 10) %>% map(safely(function(x) x * 10, otherwise = NA_real_))  
    * Each element has both an output element and an error-flagging element  
* Can pipe the transpose function to make it easier to investigate the error messages  
	* a <- list("unknown",10) %>% map(safely(function(x) x * 10, otherwise = NA_real_)) %>% transpose()  
  
Another way to possibly() purrr:  
  
* The possibly() function helps to get past the errors in the input list  
* Typical workflow is to find the issues using safely(), then address them using possibly()  
	* a <- list(-10, "unknown", 10) %>% map(safely(function(x) x * 10, otherwise = NA_real_))  
    * a <- list(-10, "unknown", 10) %>% map(possibly(function(x) x * 10, otherwise = NA_real_))  
  
purr is a walk() in the park:  
  
* The walk() function helps create human-readable results in a compact manner  
	* short_list <- list(-10, 1, 10)  
    * walk(short_list, print)  
* The walk() function has both a .x and a .f element  
	* walk(plist, print)  # no output created for the console  
  
Example code includes:  
```{r}

# Map safely over log
a <- list(-10, 1, 10, 0) %>% 
    map(safely(log, otherwise = NA_real_)) %>%
    # Transpose the result
    transpose()

# Print the list
a

# Print the result element in the list
a[["result"]]

# Print the error element in the list
a[["error"]]


# Load sw_people data
data(sw_people, package="repurrrsive")


# Map over sw_people and pull out the height element
height_cm <- map(sw_people, "height") %>%
    map(function(x) { ifelse(x == "unknown", NA, as.numeric(x)) })


# Map over sw_people and pull out the height element
height_ft <- map(sw_people , "height") %>% 
    map(safely(function(x){ ifelse(x == "unknown", NA, as.numeric(x) * 0.0328084) }, quiet = FALSE)) %>%
    transpose()

# Print your list, the result element, and the error element
walk(height_ft, function(x) { print(x[1:10]) })
height_ft[["result"]][1:10]
height_ft[["error"]][1:10]


# Take the log of each element in the list
a <- list(-10, 1, 10, 0) %>% 
    map(possibly(function(x){ log(x) }, otherwise=NA_real_))


# Create a piped workflow that returns double vectors
height_cm %>%  
    map_dbl(possibly(function(x){ x * 0.0328084 }, otherwise=NA_real_)) 


films <- map_chr(sw_films, "url")
people <- map(sw_films, "characters")

people_by_film <- tibble::tibble(films = rep(films, times=map_int(people, length)), 
                                 film_url = unlist(people)
                                 )

# Print with walk
walk(people_by_film, print)


data(gapminder, package="gapminder")
str(gapminder)

gap_split <- split(gapminder, gapminder$country)
gap_split[[1]]


# Map over the first 10 elements of gap_split
plots <- map2(gap_split[1:10], names(gap_split[1:10]), 
              ~ ggplot(.x, aes(year, lifeExp)) + geom_line() + labs(title = .y)
              )

# Object name, then function name
walk(plots, print)

```
  
  
  
***
  
Chapter 4 - Problem Solving with purrr  
  
Using purrr in your workflow:  
  
* Can set the names for a list using the purrr approach  
	* sw_films <- sw_films %>% set_names(map_chr(sw_films, "title"))  
    * map_chr(sw_films, ~.x[["episode_id"]]) %>% set_names(map_chr(sw_films, "title")) %>% sort()  
  
Even more complex problems:  
  
* Values may be buried inside lists inside lists inside etc.  
	* forks <- gh_repos %>% map( ~map(.x, "forks"))  
    * bird_measurements %>% map_df(~ data_frame( weight = .x[["weight"]], wing_length = .x[["wing length"]], taxa = "bird")) %>% select_if(is.numeric) %>% summary(.x)  
  
Graphs in purrr:  
  
* Can use ggplot2 to plot the elements using purrr  
  
Wrap up:  
  
* Iteration, data stored in lists, easy to read/write code  
  
Example code includes:  
```{r}

# Load the data
data(gh_users, package="repurrrsive")

# Check if data has names
names(gh_users)

# Map over name element of list
map(gh_users, ~.x[["name"]])

# Name gh_users with the names of the users
gh_users <- gh_users %>% 
    set_names(map_chr(gh_users, "name"))


# Check gh_repos structure
data(gh_repos, package="repurrrsive")
# str(gh_repos)  # List is much too long for str() printing

# Name gh_repos with the names of the repo owner 
gh_repos_named <- gh_repos %>% 
    map_chr(~map_chr(.x, ~.x$owner$login)[1]) %>% 
    set_names(gh_repos, .)


# Determine who joined github first
map_chr(gh_users, ~.x[["created_at"]]) %>%
    set_names(map_chr(gh_users, "name")) %>%
    sort()

# Determine user versus organization
map_lgl(gh_users, ~.x[["type"]] == "User") %>%
    sum() == length(gh_users)

# Determine who has the most public repositories
map_int(gh_users, ~.x[["public_repos"]]) %>%
    set_names(map_chr(gh_users, "name")) %>%
    sort()


# Set names of gh_repos with name subelement
gh_repos <- gh_repos %>% 
    map_chr(~map_chr(.x, ~.x$owner$login)[1]) %>% 
    set_names(gh_repos, .)

# Check to make sure list has the right names
names(gh_repos)


# Map over gh_repos to generate numeric output
map(gh_repos, 
    ~map_dbl(.x, ~.x[["size"]])) %>%
    # Grab the largest element
    map(~max(.x))


gh_users_df <- tibble::tibble(public_repos=map_int(gh_users, ~.x[["public_repos"]]), 
                              followers=map_int(gh_users, "followers")
                              )

# Scatter plot of public repos and followers
ggplot(data = gh_users_df, aes(x = public_repos, y = followers)) + 
    geom_point()


map(gh_repos_named, "followers")

# Histogram of followers        
gh_users_df %>%
    ggplot(aes(x = followers)) + 
    geom_histogram()


# Create a dataframe with four columns
map_df(gh_users, `[`, c("login", "name", "followers", "public_repos")) %>%
    # Plot followers by public_repos
    ggplot(., aes(x = followers, y = public_repos)) + 
    # Create scatter plots
    geom_point()


# Turn data into correct dataframe format
film_by_character <- tibble(filmtitle = map_chr(sw_films, "title")) %>%
    transmute(filmtitle, characters = map(sw_films, "characters")) %>%
    unnest()

# Pull out elements from sw_people
sw_characters <- map_df(sw_people, `[`, c("height", "mass", "name", "url"))

# Join the two new objects
inner_join(film_by_character, sw_characters, by = c("characters" = "url")) %>%
    # Make sure the columns are numbers
    mutate(height1 = ifelse(height=="unknown", NA, as.numeric(height)), 
           mass1 = ifelse(mass=="unknown", NA, as.numeric(stringr::str_replace(mass, ",", "")))
           ) %>% 
    filter(!is.na(height)) %>%
    ggplot(aes(x = height)) +
    geom_bar(stat="count") + 
    # geom_histogram(stat = "count") + 
    facet_wrap(~filmtitle)

```
  
  
  
***
  
###_Joining Data in R with data.table_  
  
Chapter 1 - Joining Multiple data.tables  
  
Introduction:  
  
* Combining data from multiple data set can be valuable for analysis  
* Need to identfy the table keys for a successful join  
	* demographics <- data.table(name = c("Trey", "Matthew", "Angela"), gender = c(NA, "M", "F"), age = c(54, 43, 39))  
    * shipping <- data.table(name = c("Matthew", "Trey", "Angela"), address = c("7 Mill road", "12 High street", "33 Pacific boulevard"))  
    * tables()  # will show all the data.table in the session along with rows, columns, space, and keys
str(demographics)  
  
Merge function:  
  
* Can run all of inner, outer, left, and right joins using merge()  
	* merge(x = demographics, y = shipping, by.x = "name", by.y = "name")  # default is an inner merge  
    * merge(x = demographics, y = shipping, by = "name")  # if the tables have the same key name  
    * merge(x = demographics, y = shipping, by = "name", all = TRUE)  # full outer join  
  
Left and right joins:  
  
* Can run left joins or right joins using all.x and all.y  
	* merge(x = demographics, y = shipping, by = "name", all.x = TRUE)  
    * merge(x = demographics, y = shipping, by = "name", all.y = TRUE)  
    * merge(x = demographics, y = shipping, by = "name", all.y = TRUE) is the same as merge(x = shipping, y = demographics, by = "name", all.x = TRUE)  
  
Example code includes:  
```{r}

library(data.table)

netflix <- fread("./RInputFiles/netflix_2017.csv", sep=",")
imdb <- fread("./RInputFiles/imdb_ratings.csv", sep=",")


# What data.tables are in my R session?
tables()

# View the first six rows 
head(netflix)
head(imdb)

# Print the structure
str(netflix)
str(imdb)


# Print the data.tables in your R session
netflix
imdb


# Inner join netflix and imdb
merge(netflix, imdb, by = "title")

# Full join netflix and imdb
merge(netflix, imdb, by = "title", all=TRUE)

# Full join imdb and netflix
merge(imdb, netflix, by = "title", all = TRUE)

# Left join imdb to netflix
merge(netflix, imdb, by="title", all.x=TRUE)

# Right join imdb to netflix
merge(netflix, imdb, by="title", all.y=TRUE)

# Compare to a left join of netflix to imdb
merge(imdb, netflix, by="title", all.x=TRUE)


australia_area <- fread("./RInputFiles/australia_area.csv", sep=",")
australia_capitals <- fread("./RInputFiles/australia_capitals.csv", sep=",")
australia_cities_top20 <- fread("./RInputFiles/australia_cities_top20.csv", sep=",")


# Identify the key for joining capitals and population
capitals_population_key <- "city"

# Left join population to capitals
capital_pop <- merge(australia_capitals, 
                     australia_cities_top20[, c("city", "population")], 
                     by=capitals_population_key, all.x=TRUE
                     )
capital_pop


# Identify the key for joining capital_pop and area
capital_pop_area_key <- "state"

# Inner join area to capital pop
australia_stats <- merge(capital_pop, australia_area[, c("state", "area_km2")], by=capital_pop_area_key)

# Print the final result
australia_stats

```
  
  
  
***
  
Chapter 2 - Joins Using data.table Syntax  
  
Joins using data.table syntax:  
  
* The general form of data.table syntax includes  
	* DT[i, j, by]  # grouped by "by", action j is taken on rows i  
    * DT[i, on]  # this will merge data.table "i" to data.table "DT" by key "on"  
    * demographics[shipping, on = .(name)]  
* Variables inside list() or .() are looked up in the column names of both data.tables  
	* shipping[demographics, on = list(name)]  # all records in demographics will be kept, plus records in shipping that have a match by on= to demographics  
    * shipping[demographics, on = .(name)]  
    * join_key <- c("name")  
    * shipping[demographics, on = join_key]  
* For an inner join, supply nomatch=0L; full joins are not possible using the data.table syntax, so it is necessary to use merge() instead  
	* shipping[demographics, on = .(name), nomatch = 0L]  
* The anti-join is possible using the negation operator  
	* demographics[!shipping, on = .(name)]  # all records in demographics that are NOT in shipping  
  
Setting and viewing data.table keys:  
  
* With keys set, the on= argument is no longer needed for joins on that key  
	* Setting a key also sorts the data.table by that key in memory (makes merge operations faster)  
    * setkey(DT, ...)  
    * setkey(DT, key1, key2, key3)  
    * setkey(DT, "key1", "key2", "key3")  
* The setkeyv() function allows for passing in a character vector for the key column names  
	* keys <- c("key1", "key2", "key3")  
    * setkeyv(dt, keys)  
* Can check for keys and find their names if they exist  
	* haskey(dt1)  
    * key(dt1)  
  
Incorporating joins in the data.table workflow:  
  
* Joins can be included in the data.table workflows, enabling rapid analysis  
	* DT1[DT2, on][i, j, by]  # join, followed by standard data.table operations  
    * customers[purchases, on = .(name)][sales > 1, j = .(avg_spent = sum(spent) / sum(sales)), by = .(gender)]  
* Can also incorporate calculations and new column creations with joins as follows  
	* DT1[DT2, on, j]  
    * customers[purchases, on = .(name), return_customer := sales > 1]  
    * DT1[DT2, on, j, by = .EACHI]  # will have a groupby for each match in DT1  
    * shipping[customers, on = .(name), j = .("# of shipping addresses" = .N), by = .EACHI]  
    * customers[shipping, on = .(name), .(avg_age = mean(age)), by = .(gender)]  
  
Example code includes:  
```{r}

# Right join population to capitals using data.table syntax
australia_capitals[australia_cities_top20, on = "city"]

# Right join using merge
merge(australia_capitals, australia_cities_top20, by = "city", all.y = TRUE)


# Inner join with the data.table syntax
australia_capitals[australia_cities_top20, on="city", nomatch=0L]


# Anti-join capitals to population
australia_cities_top20[!australia_capitals, on="city"]

# Anti-join capitals to area
australia_area[!australia_capitals, on="state"]


# Set the keys
setkey(netflix, "title")
setkey(imdb, "title")

# Inner join
netflix[imdb, nomatch=0L]


# Check for keys
haskey(netflix)
haskey(imdb)

# Find the key
the_key <- "title"

# Set the key for the other data.table
setkeyv(imdb, the_key)


# Inner join capitals to population
australia_cities_top20[australia_capitals, on="city", nomatch=0L]

# Join and sum
australia_cities_top20[australia_capitals, on = .(city), nomatch = 0, j = sum(percentage)]


continents <- fread("./RInputFiles/continents.csv", sep=",")
life_exp <- fread("./RInputFiles/gapminder_life_expectancy_2010.csv", sep=",")
life_exp <- life_exp %>% rename(years = life_expectancy)
str(continents)
str(life_exp)


# What countries are listed in multiple continents?
continents[life_exp, on = .(country), .N, by = .EACHI][N > 1]

# Calculate average life expectancy per continent:
avg_life_expectancy <- continents[life_exp, on = .(country), nomatch=0L][, j = mean(years), by = continent]
avg_life_expectancy

```
  
  
  
***
  
Chapter 3 - Diagnosing and Fixing Common Join Problems  
  
Complex keys:  
  
* A misspecified join is when an incorrect join key has been used  
* A malformed join is when they keys have no values in common (stacked join)  
* Best practice is to study the data in each column prior to attempting a join  
* When the keys have different names, can manage the merge with code  
	* merge(customers, web_visits, by.x = "name", by.y = "person")  
    * customers[web_visits, on = .(name = person)]  
    * customers[web_visits, on = c("name" = "person")]  
* Can also merge with multiple keys  
	* merge(purchases, web_visits, by = c("name", "date"))  
    * merge(purchases, web_visits, by.x = c("name", "date"), by.y = c("person", "date")  # matches in the order that the appear  
    * purchases[web_visits, on = c("name" = "person", "date")]  
  
Tricky columns:  
  
* Tables sometimes share column names that are not intended as join keys  
* With the data.table syntax, duplicate names from the i= area have i. as a prefix  
	* By contrast, with merge() duplicate names may have a .x and .y suffix  
* Can be helpful instead to rename columns prior to the join using setnames()  
	* setnames(parents, c("parent", "parent.gender", "parent.age"))  
* May want to join a data.frame and data.table, but rownames is the key for the data.table  
	* parents <- as.data.table(parents, keep.rownames = "parent")  
  
Duplicate matches:  
  
* May want to join columns that have many-many on the keys  
	* This will throw a long error message, and requires setting parameters to show this is intentional  
    * Note that NA will match all other NA; behavior can be over-ridden by filtering out the NA  
    * site2_ecology <- site2_ecology[!is.na(genus)]  
* Can keep only the first match or only the last match  
	* site1_ecology[site2_ecology, on = .(genus), mult = "first"]  
    * children[parents, on = .(parent = name), mult = "last"]  
* Can find the duplicated values  
	* duplicated(site1_ecology)  # boolean  
    * duplicated(site1_ecology, by = "genus")  # boolean, lookin just at column genus  
    * unique(site1_ecology, by = "genus")  # removes the duplicates  
    * duplicated(site1_ecology, by = "genus", fromLast = TRUE)  
    * unique(site1_ecology, by = "genus", fromLast = TRUE)  
  
Example code includes:  
```{r}

guardians <- fread("./RInputFiles/school_db_guardians.tsv")
locations <- fread("./RInputFiles/school_db_locations.tsv")
students <- fread("./RInputFiles/school_db_students.tsv")
subjects <- fread("./RInputFiles/school_db_subjects.tsv")
teachers <- fread("./RInputFiles/school_db_teachers.tsv")


# Full join
merge(students, guardians, by="name", all=TRUE)
students[guardians, on="name"]

# Change the code to an inner join
students[guardians, on = .(name), nomatch=0L]

# What are the correct join key columns?
students[guardians, on = c("guardian"="name"), nomatch = 0L]


# Intentionally errors out due to type mismatch
# subjects[locations, on=c("class", "semester")]

# Structure 
str(subjects)
str(locations)

# Does semester have the same class? 
same_class <- FALSE

# Fix the column class
locations[, semester := as.integer(semester)]

# Right join
subjects[locations, on=c("class", "semester")]


# Identify and set the keys
join_key <- c("subject"="class")

# Right join
teachers[locations, on=join_key]


# Inner join 1
capital_pop <- merge(australia_capitals, australia_cities_top20, by="city", nomatch=0L)

# Inner join 2
merge(capital_pop, australia_area, by="state", suffixes=c(".pop", ".area"), nomatch=0L)


netflixOrig <- fread("./RInputFiles/netflix_2017.csv", sep=",")
imdb <- fread("./RInputFiles/imdb_ratings.csv", sep=",")
netflix <- as.data.frame(netflixOrig)[, c("episodes", "release_date")]
rownames(netflix) <- netflixOrig$title

# Convert netflix to a data.table
netflix <- as.data.table(netflix, keep.rownames="series")

# Rename "title" to "series" in imdb
setnames(imdb, c("series", "rating"))

# Right join
imdb[netflix, on="series"]


cardio <- fread("./RInputFiles/affymetrix_chd_genes.csv")
framingham <- fread("./RInputFiles/framingham_chd_genes.csv")
heart <- fread("./RInputFiles/illumina_chd_genes.csv")


# Try an inner join
merge(heart, cardio, by=c("gene"), allow.cartesian=TRUE)

# Filter missing values
heart_2 <- heart[!is.na(gene)]
cardio_2 <- cardio[!is.na(gene)]

# Repeat the inner join
merge(heart_2, cardio_2, by=c("gene"), allow.cartesian=TRUE)


# Keep only the last probe for each gene
heart_3 <- unique(heart_2, by="gene", fromLast=TRUE)
cardio_3 <- unique(cardio_2, by="gene", fromLast=TRUE)

# Inner join
reproducible <- merge(heart_3, cardio_3, by="gene", suffixes=c(".heart", ".cardio"))
reproducible


# Right join taking the first match
heart_2[framingham, on="gene", mult="first"]

# Anti-join
reproducible[!framingham, on="gene"]

```
  
  
  
***
  
Chapter 4 - Concatenating and Reshaping data.table  
  
Concatenating two or more data.table:  
  
* Can concatenate tables that have rows split across multiple tables (e.g., multiple file reads)  
	* rbind(sales_2015, sales_2016)  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year")  # new column year will use the names for the tables  
    * rbind(sales_2015, sales_2016, idcol = TRUE)  # column gets the default name .id  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year", fill = TRUE)  # allows for misaligned columns, filled with NA as appropriate  
* Can also rbindlist() for tables that are stored as lists  
	* table_files <- c("sales_2015.csv", "sales_2016.csv")  
    * list_of_tables <- lapply(table_files, fread)  
    * rbindlist(list_of_tables)  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year", use.names = TRUE)  # use.names matches columns by name  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year", use.names = FALSE)  # matches columns by ORDER rather than by name (name mismatch explicitly allowed)  
  
Set operations:  
  
* Can run set operations with data.tables()  
	* fintersect(): what rows do these two data.tables share in common?  Duplicate rows are ignored by default, but can set all=TRUE to get all the copies printed  
    * fsetdiff(): what rows are unique to this data.table?  Will return the rows in the first table that are not in the second table; can set all=TRUE to get ALL the rows from the first table  
    * funion(): what is the unique set of rows across these two data.tables?  Returns all the unique rows found in either table, with duplicates ignired by default; set all=TRUE to get everything, which is equivalent to rbind  
  
Melting data.tables:  
  
* May want to melt a data.table from wide format to long format  
	* melt(sales_wide, measure.vars = c("2015", "2016"))  # the default for the new column names are "variable" and "value"  
    * melt(sales_wide, id.vars = "quarter", variable.name = "year", value.name = "amount")  # give the columns names that differ from the defaults; keeps only the columns specified, dropping all the others (?)  
  
Casting data.tables:  
  
* Can cast a long data.table to a wide format  
	* sales_wide <- dcast(sales_long, quarter ~ year, value.var = "amount")  # the value.var is the name of the column that will be split, while a ~ b means a will be the new rows and b is the column to throw in to the new columns  
    * dcast(profit_long, quarter ~ year, value.var = c("revenue", "profit"))  
    * dcast(sales_long, quarter ~ department + year, value.var = "amount")  # by default, department and year will be combined with an "_"  
    * sales_wide <- dcast(sales_long, season ~ year, value.var = "amount")  
    * mat <- as.matrix(sales_wide, rownames = "season")  
  
Example code includes:  
```{r}

ebola_W50 <- fread("./RInputFiles/ebola_2014_W50.csv")
ebola_W51 <- fread("./RInputFiles/ebola_2014_W51.csv")
ebola_W52 <- fread("./RInputFiles/ebola_2014_W52.csv")


# Concatenate case numbers from weeks 50 and 51
rbind(ebola_W50, ebola_W51)

# Intentionally throws an error
# Concatenate case numbers from all three weeks
# rbind(ebola_W50, ebola_W51, ebola_W52)

# Modify the code
rbind(ebola_W50, ebola_W51, ebola_W52, fill=TRUE)


gdp_africa <- fread("./RInputFiles/gdp_africa_2000.csv")
gdp_asia <- fread("./RInputFiles/gdp_asia_2000.csv")
gdp_europe <- fread("./RInputFiles/gdp_europe_2000.csv")
gdp_north_america <- fread("./RInputFiles/gdp_north_america_2000.csv")
gdp_oceania <- fread("./RInputFiles/gdp_oceania_2000.csv")
gdp_south_america <- fread("./RInputFiles/gdp_south_america_2000.csv")

gdp <- list(africa=gdp_africa, asia=gdp_asia, europe=gdp_europe, 
            north_america=gdp_north_america, oceania=gdp_oceania, south_america=gdp_south_america
            )


# Concatenate its data.tables
gdp_all_1 <- rbindlist(gdp)

# Concatenate its data.tables
gdp_all_2 <- rbindlist(gdp, idcol="continent")
str(gdp_all_2)
gdp_all_2[95:105]

# Fix the problem
gdp_all_3 <- rbindlist(gdp, idcol = "continent", use.names=TRUE)
gdp_all_3


# Obtain countries in both Asia and Europe
fintersect(gdp$europe, gdp$asia)

# Concatenate all data tables
gdp_all <- rbindlist(gdp, use.names=TRUE)

# Find all countries that span multiple continents
gdp_all[duplicated(gdp_all)]


# Get all countries in either Asia or Europe
funion(gdp$europe, gdp$asia)

# Concatenate all data tables
gdp_all <- rbindlist(gdp, use.names=TRUE)

# Print all unique countries
unique(gdp_all)


gdp_middle_east <- fread("./RInputFiles/gdp_middle_east_2000.csv")

# Which countries are in Africa but not considered part of the middle east?
fsetdiff(gdp$africa, gdp_middle_east)

# Which countries are in Asia but not considered part of the middle east?
fsetdiff(gdp$asia, gdp_middle_east)

# Which countries are in Europe but not considered part of the middle east?
fsetdiff(gdp$europe, gdp_middle_east)


gdp_per_capita_wrong <- fread("./RInputFiles/gdp_per_capita_oceania.csv")

colNames <- gdp_per_capita_wrong$V1
colNames[1] <- "year"
numData <- t(gdp_per_capita_wrong[, -1])

gdp_per_capita <- as.data.table(numData)
colnames(gdp_per_capita) <- colNames


# Print gdp_per_capita
gdp_per_capita

# Reshape gdp_per_capita to the long format
melt(gdp_per_capita, id.vars="year")

# Rename the new columns
melt(gdp_per_capita, id.vars = "year", variable.name="country", value.name="gdp_pc")


# Print ebola_wide
ebola_wide <- rbind(ebola_W50, ebola_W51) %>% 
    mutate(Week_50=ifelse(period_code=="2014-W50", Confirmed, NA), 
           Week_51=ifelse(period_code=="2014-W51", Confirmed, NA)
           ) %>%
    select(Location, period_start, period_end, Week_50, Week_51) %>%
    arrange(Location, period_start)
ebola_wide


# Stack Week_50 and Week_51
melt(ebola_wide, measure.vars=c("Week_50", "Week_51"), variable.name="period", value.name="cases")

# Modify the code
melt(ebola_wide, measure.vars = c("Week_50", "Week_51"), 
     variable.name = "period", value.name = "cases", id.vars="Location"
     )


gdp_oceania <- fread("./RInputFiles/gdp_and_pop_oceania.csv")
gdp_oceania$continent <- "Oceania"


# Split the population column by year
dcast(gdp_oceania, formula = country ~ year, value.var = "population")

# Split the gdp column by country
dcast(gdp_oceania, formula = year ~ country, value.var = "gdp")


# Reshape from wide to long format
wide <- dcast(gdp_oceania, formula = country ~ year, value.var = c("gdp", "population"))

# convert to a matrix
as.matrix(wide, rownames="country")

# Modify your previous code
dcast(gdp_oceania, formula = continent + country ~ year, value.var = c("gdp", "population"))


gdp_by_industry_oceania <- fread("./RInputFiles/gdp_by_industry_oceania.tsv")


# Split gdp by industry and year
gdp_by_industry_oceania
dcast(gdp_by_industry_oceania, formula = country ~ industry + year, value.var=c("gdp"))

```
  
  
  
***
  
###_Fraud Detection in R_  
  
Chapter 1 - Introduction and Motivation  
  
Introduction and Motivation:  
  
* Fraud is an uncommon, well-considered, imperceptibly concealed, time-evolving and often carefully organized crime which appears in many types and forms  
* Fraud is rare, but the cost of not detecting fraud is very large  
* Fraud detection models need to have multiple charcteristics  
	* Statistical accuracy  
    * Interpretability  
    * Regulatory compliance  
    * Economical impact - total cost of ownership and ROI  
    * Complement expert-based findings  
* Multiple challenges with fraud detection  
	* Imbalanced samples - credit card fraud is typically less than 0.5% ("needle in a haystack" problem)  
    * Operational efficiency (e.g., credit card decisions need to be made in ~8 seconds)  
    * Avoid harassing good customers over legitimate transactions  
* Confusion matrices are commonly used - true positive, true negative, false positive, false negative  
	* caret::confusionMatrix(data = predictions, reference = fraud_label)  
    * Accuracy is not a good measure when there is an imbalanced sample  
  
Time features:  
  
* Time is an important aspect of fraud detection; transactions tend to occur at similar hours  
	* There is no perfect natural ordering to time though, since time is cricular; arithmetic mean does not accout for the periodicity  
* Can create a circular histogram based off the time series data  
	* ts <- as.numeric(lubridate::hms(timestamps)) / 3600  
    * clock <- ggplot(data.frame(ts), aes(x = ts)) + geom_histogram(breaks = seq(0, 24), colour = "blue", fill = "lightblue") + coord_polar()  
    * arithmetic_mean <- mean(ts)  
    * clock + geom_vline(xintercept = arithmetic_mean, linetype = 2, color = "red", size = 2)  
* Can instead use the von Mises distribution for modeling time (variable wrapped around a circle)  
	* mu - periodic mean  
    * kappa - 1/k is the periodic variance (k is a measure of concentration)  
    * library(circular)  
    * ts <- circular(ts, units = "hours", template = "clock24")  
    * estimates <- mle.vonmises(ts)  
    * p_mean <- estimates$mu %% 24  
    * concentration <- estimates$kappa  
* Can then obtain a binary feature as to whether a given timestamp falls inside the interval  
	* densities <- dvonmises(ts, mu = p_mean, kappa = concentration)  
    * alpha <- 0.90  
    * quantile <- qvonmises((1 - alpha)/2, mu = p_mean, kappa = concentration) %% 24  
    * cutoff <- dvonmises(quantile, mu = p_mean, kappa = concentration)  
    * time_feature <- densities >= cutoff  
* Can also run confidence intervals with rolling windows  
	* time_feature = c(NA, NA)  
    * for (i in 3:length(ts)) {  
    *     ts_history <- ts[1:(i-1)]  
    *     estimates <- mle.vonmises(ts_history)  
    *     p_mean <- estimates$mu %% 24  
    *     concentration <- estimates$kappa  
    *     dens_i <- dvonmises(ts[i], mu = p_mean, kappa = concentration)  
    *     alpha <- 0.90  
    *     quantile <- qvonmises((1-alpha)/2, mu=p_mean, kappa=concentration) %% 24  
    *     cutoff <- dvonmises(quantile, mu = p_mean, kappa = concentration)  
    *     time_feature[i] <- dens_i >= cutoff  
    * }  
  
Frequency features:  
  
* May need to add features for a good-performing fraud detection algorithm  
	* trans %>% select(fraud_flag, orig_account_id, benef_country, authentication_cd, channel_cd, amount)  
* For example, may want to look at the authentication methods by person and associations to fraud  
	* trans <- trans %>% arrange(timestamp)  
    * trans_Alice <- trans %>% filter(account_name == "Alice")  
    * frequency_fun <- function(steps, auth_method) {  
    *     n <- length(steps)  
    *     frequency <- sum(auth_method[1:n] == auth_method[n + 1])  
    *     return(frequency)  
    * }  
    * freq_auth <- zoo::rollapply(trans_Alice$transfer_id, width=list(-1:-length(trans_Alice$transfer_id)),  partial = TRUE, FUN = frequency_fun, trans_Alice$authentication_cd)  
    * freq_auth <- c(0, freq_auth)  
* Can run a similar process for multiple accounts  
	* trans %>% group_by(account_name)  
    * trans <- trans %>% group_by(account_name) %>% mutate(freq_auth = c(0, zoo::rollapplyr(transfer_id, width = list(-1:-length(transfer_id)), partial = TRUE, FUN = count_fun, authentication_cd) ) )  
  
Recency features:  
  
* Recency features capture the dimension of time in a manner designed to highlight potential fraud  
	* Example of an authentication method that has not recently been used  
    * Recency=0 means not used recently (or never used before), while recency=1 means used recently  
    * recency=exp(-gamma*time)  # gamma is a tuning parameter, typically a small number  
    * Gamma is often chosen such that recency will be 0.01 after 180 days  
* Can create the recency feature in R  
	* recency_fun <- function(t, gamma, auth_cd, freq_auth) {  
    *     n_t <- length(t)  
    *     if (freq_auth[n_t] == 0) { recency <- 0 } else {  
    *         time_diff <- t[1] - max(t[2:n_t][auth_cd[(n_t-1):1] == auth_cd[n_t]])
    *         recency <- exp(-gamma * time_diff)  
    *     }
    *     return(recency)  
    * }  
* Can then calculate gamma and apply to each individual account  
	* gamma <- -log(0.01)/180 # = 0.0256  
    * trans <- trans %>% 
    *     group_by(account_name) %>%   
    *     mutate(rec_auth = zoo::rollapply(timestamp, width = list(0:-length(transfer_id)), partial = TRUE, FUN = recency_fun, gamma, authentication_cd, freq_auth))  
  
Example code includes:  
```{r}

load("./RInputFiles/transfers02_v2.RData")  # data.frame transfers is 628x12

# Print the first 6 rows of the dataset
head(transfers)

# Display the structure of the dataset
str(transfers)

# Determine fraction of legitimate and fraudulent cases
class_distribution <- prop.table(table(transfers$fraud_flag))
print(class_distribution)

# Make pie chart of column fraud_flag
df <- data.frame(class = c("no fraud", "fraud"), pct = as.numeric(class_distribution)) %>%
    mutate(class = factor(class, levels = c("no fraud", "fraud")),
           cumulative = cumsum(pct), midpoint = cumulative - pct / 2,
           label = paste0(class, " ", round(pct*100, 2), "%")
           )

ggplot(df, aes(x = 1, weight = pct, fill = class)) +
    scale_fill_manual(values = c("dodgerblue", "red")) +
    geom_bar(width = 1, position = "stack") +
    coord_polar(theta = "y") +
    geom_text(aes(x = 1.3, y = midpoint, label = label)) +
    ggmap::theme_nothing()


# Create vector predictions containing 0 for every transfer
predictions <- factor(rep(0, nrow(transfers)), levels = c(0, 1))

# Compute confusion matrix
caret::confusionMatrix(data = predictions, reference = transfers$fraud_flag)

# Compute cost of not detecting fraud
cost <- sum(transfers$amount[transfers$fraud_flag == 1])
print(cost)


# load("./RInputFiles/timestamps_circular.RData")  # 'circular' num[1:25] ts
load("./RInputFiles/timestamps_digital.RData")  # chr[1:25] timestamps

# Convert the plain text to hours
ts <- as.numeric(lubridate::hms(timestamps)) / 3600

# Convert the data to class circular
ts <- circular::circular(ts, units = "hours", template = "clock24")

# Plot a circular histogram
clock <- ggplot(data.frame(ts), aes(x = ts)) +
    geom_histogram(breaks = seq(0, 24), colour = "blue", fill = "lightblue") +
    coord_polar() + 
    scale_x_continuous("", limits = c(0, 24), breaks = seq(0, 24))
plot(clock)

# Create the von Mises distribution estimates
estimates <- circular::mle.vonmises(ts)

# Extract the periodic mean from the estimates
p_mean <- estimates$mu %% 24

# Add the periodic mean to the circular histogram
clock <- ggplot(data.frame(ts), aes(x = ts)) +
    geom_histogram(breaks = seq(0, 24), colour = "blue", fill = "lightblue") +
    coord_polar() + 
    scale_x_continuous("", limits = c(0, 24), breaks = seq(0, 24)) +
    geom_vline(xintercept = as.numeric(p_mean), color = "red", linetype = 2, size = 1.5)
plot(clock)


# Estimate the periodic mean and concentration on the first 24 timestamps
alpha <- 0.95
p_mean <- estimates$mu %% 24
concentration <- estimates$kappa

# Estimate densities of all 25 timestamps
densities <- circular::dvonmises(ts, mu = p_mean, kappa = concentration)

# Check if the densities are larger than the cutoff of 95%-CI
cutoff <- circular::dvonmises(circular::qvonmises((1 - alpha)/2, mu = p_mean, kappa = concentration), 
                              mu = p_mean, kappa = concentration
                              )

# Define the variable time_feature
time_feature <- densities >= cutoff
print(cbind.data.frame(ts, time_feature))


load("./RInputFiles/transfers_Bob.RData")  # data.frame trans_Bob 17x12

# Frequency feature based on channel_cd
frequency_fun <- function(steps, channel) {
    n <- length(steps)
    frequency <- sum(channel[1:n] == channel[n+1])
    return(frequency)
}

# Create freq_channel feature
freq_channel <- zoo::rollapply(trans_Bob$transfer_id, width = list(-1:-length(trans_Bob$transfer_id)),
                               partial = TRUE, FUN = frequency_fun, trans_Bob$channel_cd
                               )

# Print the features channel_cd, freq_channel and fraud_flag next to each other
freq_channel <- c(0, freq_channel)
cbind.data.frame(trans_Bob$channel_cd, freq_channel, trans_Bob$fraud_flag)


# load("./RInputFiles/transfers_AliceBob.RData")  # data.frame trans 40x12
load("./RInputFiles/transfers_AliceBob_freq.RData")  # data.frame trans 40x14

# Group the data
trans <- trans %>% group_by(account_name) %>%
    # Mutate the data to add a new feature
    mutate(freq_channel = c(0, zoo::rollapply(transfer_id, width = list(-1:-length(transfer_id)),
                                              partial = TRUE, FUN = frequency_fun, channel_cd
                                              )
                            )
           )

# Print the features as columns next to each other
as.data.frame(trans %>% select(account_name, channel_cd, freq_channel, fraud_flag))


# Create the recency function
recency_fun <- function(t, gamma, channel_cd, freq_channel) {
    n_t <- length(t)
    # If the channel has never been used, return 0 else, return the exponent
    if (freq_channel[n_t] == 0) { 
        return(0) 
    } else {
        time_diff <- t[1] - max(t[2:n_t][channel_cd[(n_t-1):1] == channel_cd[n_t]])
        exponent <- -gamma * time_diff
        return(exp(exponent))
    }
}

# Group, mutate and rollapply
gamma <- -log(0.01)/90
trans <- trans %>% 
    group_by(account_name) %>%
    mutate(rec_channel = zoo::rollapply(timestamp, width = list(0:-length(transfer_id)), 
                                        partial = TRUE, FUN = recency_fun, 
                                        gamma, channel_cd, freq_channel
                                        )
           )

# Print a new dataframe
as.data.frame(trans %>% 
                  select(account_name, channel_cd, timestamp, rec_channel, fraud_flag)
              )


load("./RInputFiles/transfers_chap1_L4.RData")  # data.frame transfers 222x16

# Statistics of frequency & recency features of legitimate transactions:
summary(transfers %>% 
            filter(fraud_flag==0) %>% 
            select(freq_channel, freq_auth, rec_channel, rec_auth)
        )

# Statistics of frequency & recency features of fraudulent transactions:
summary(transfers %>% 
            filter(fraud_flag==1) %>% 
            select(freq_channel, freq_auth, rec_channel, rec_auth)
        )

```
  
  
  
***
  
Chapter 2 - Social Network Analytics  
  
Social network analytics:  
  
* Social networks include both nodes and edges  
* Edges could include money transfers, and may be weighted based on frequency, amount, intensity, etc.  
	* Negative weights are rare, but can be used to show animosity  
    * Incoming and outgoing edges can be reflected using a directed network  
* Connectivity matrices can be valuable; for example, square matrix of 1s and 0s  
* Adjacency lists can also be used as a list of form (Node1, Node2, weight)  
* Can move from a transactional database to a network  
	* network <- igraph::graph_from_data_frame(transactions, directed = FALSE)  
    * plot(network)  
    * E(network)  
    * V(network)  
    * V(network)$name  
    * plot(net)  
    * E(net)$width <- count.multiple(net)  
    * edge_attr(net)  
    * E(net)$curved <- FALSE  
    * plot(net)  
  
Fraud and social network analytics:  
  
* Social network analytics can help improve fraud detection  
* Fraudsters tend to cluster together; same activities, resources, victims, etc.  
* Homophily is the concept that fraudsters are more likely to be connected to fraudsters  
* Social networks can be helpful for finding identity theft  
	* Legitimate customers tend to call their frequent contacts  
    * Fraudsters tend to call different people  
* Money mules transfer money illegally; can potentially be identified by nodes  
	* Assign colors to node by status, then add information to the network  
  
Social network based inference:  
  
* Can attempt to predict the behavior of a node based on the nodes elsewhere in the network  
	* Non-relational models - only use local information (logit, decision trees)  
    * Relational models - network links included (relational neighbor classifiers)  
* Assuming that there is homophily, then relational neighbor classifiers may be appropriate  
	* Can be as simple as proportion of known links belonging to each class (may be weighted by the strength of the edge)  
    * subnetwork <- subgraph(network, v = c("?", "B", "D"))  
    * prob_fraud <- strength(subnetwork, v = "?") / strength(network, v = "?")  
    * prob_fraud  
  
Social network metrics:  
  
* The geodesic is the shortest path between two nodes (may include weights in the calculation)  
* Generally, the closer to a fraudulent node, the greater the potential impact  
* The maximum degree possible for a network with N nodes is N-1 (normalizing means dividing the degree by N-1)  
* Closeness is the inverse of the sum of the distance of a node to all other nodes in the network  
	* Closeness is always at most 1 / (N-1) so normalizing is to multiply by (N-1)  
* Betweenness is the number of times that a node is in the geodesic for two other nodes  
	* betweenness(network)  
    * betweenness(network, normalized = TRUE)  
  
Example code includes:  
```{r}

load("./RInputFiles/network_data.RData")  # data.frame transfers 60x6 and data.frame account_info 38x2


library(igraph)

# Have a look at the data
head(transfers)

# Create an undirected network from the dataset
net <- graph_from_data_frame(transfers, directed = FALSE)

# Plot the network with the vertex labels in bold and black
plot(net, vertex.label.color = "black", vertex.label.font = 2)


load("./RInputFiles/network_data_simple.RData")  # data.frame edges 16x2

# Create a network from the data frame
net <- graph_from_data_frame(edges, directed = FALSE)

# Plot the network with the multiple edges
plot(net, layout = layout_in_circle)

# Specify new edge attributes width and curved
E(net)$width <- count.multiple(net)
E(net)$curved <- FALSE

# Check the new edge attributes and plot the network with overlapping edges
edge_attr(net)
plot(net, layout = layout_in_circle)


# Create an undirected network from the dataset
net <- graph_from_data_frame(transfers, directed = FALSE)

# Add account_type as an attribute to the nodes of the network
V(net)$account_type <- account_info$type

# Have a look at the vertex attributes
print(vertex_attr(net))

# Check for homophily based on account_type
assortativity_nominal(net, types = V(net)$account_type, directed = FALSE)


# Each account type is assigned a color
vertex_colors <- c("grey", "lightblue", "darkorange")

# Add attribute color to V(net) which holds the color of each node depending on its account_type
V(net)$color <- vertex_colors[V(net)$account_type]

# Plot the network
plot(net)


load("./RInputFiles/network_data_v2.RData")  # data.frame transfers 60x6 and data.frame account_info 38x3

# From data frame to graph
net <- graph_from_data_frame(transfers, directed = FALSE)

# Plot the network; color nodes according to isMoneyMule-variable
V(net)$color <- ifelse(account_info$isMoneyMule, "darkorange", "slateblue1")
plot(net, vertex.label.color = "black", vertex.label.font = 2, vertex.size = 18)

# Find the id of the money mule accounts
print(account_info$id[account_info$isMoneyMule])

# Create subgraph containing node "I41" and all money mules nodes
subnet <- induced_subgraph(net, v = c("I41", "I47", "I87", "I20"))

# Compute the money mule probability of node "I41" based on the neighbors
strength(subnet, v="I41") / strength(net, v="I41")


load("./RInputFiles/kite.RData")  # list[1:10] kite

# Plot network kite
plot(kite)

# Find the degree of each node
degree(kite)

# Which node has the largest degree?
which.max(degree(kite))

# Plot kite with vertex.size proportional to the degree of each node
plot(kite, vertex.size = 6 * degree(kite))

# Find the closeness of each node
closeness(kite)

# Which node has the largest closeness?
which.max(closeness(kite))

# Plot kite with vertex.size proportional to the closeness of each node
plot(kite, vertex.size = 500 * closeness(kite))

# Find the betweenness of each node
betweenness(kite)

# Which node has the largest betweenness?
which.max(betweenness(kite))

# Plot kite with vertex.size proportional to the betweenness of each node
plot(kite, vertex.size = 5 * betweenness(kite))


# Plot network and print account info
plot(net)
legend("bottomleft", legend = c("known money mule", "legit account"), 
       fill = c("darkorange", "lightblue"), bty = "n"
       )
print(account_info)

# Degree
account_info$degree <- degree(net, normalized = TRUE)

# Closeness
account_info$closeness <- closeness(net, normalized = TRUE)

# Betweenness
account_info$betweenness <- betweenness(net, normalized = TRUE)

print(account_info)

```
  
  
  
***
  
Chapter 3 - Imbalanced Class Distributions  
  
Dealing with imbalanced datasets:  
  
* Key challenge is labelling events as fraud or not (classification or anomaly detection)  
* Meaningful risk of a model that sees a very large class and classifies everything to that large class  
* Preference for a balanced distribution; tends to drive improved model performance  
	* Increase (over-sample) the number of fraud (minority) cases  
    * Decrease (under-sample) the number of normal (majority) cases  
* Example of random over-sampling of the dataset  
	* Increase the minority (fraud) cases in the training sample through randomly over-sampling the fraud data  
    * ROSE package: Random Over-Sampling Examples  
    * ovun.sample() for random over-sampling, under-sampling or combination!  
    * n_legit <- 24108  
    * new_frac_legit <- 0.50  
    * new_n_total <- n_legit/new_frac_legit # = 21408/0.50 = 42816  
    * library(ROSE)  
    * oversampling_result <- ovun.sample(Class ~ ., data = creditcard, method = "over", N = new_n_total, seed = 2018)  
    * oversampled_credit <- oversampling_result$data  
    * table(oversampled_credit$Class)  
  
Random under-sampling:  
  
* Can instead change the class ditribution through random under-sampling  
	* Remove some of the non-fraud cases rather than double-counting some of the fraud cases  
* Can again use ovun.sample for this task  
	* new_n_total <- n_fraud/new_frac_fraud # = 492/0.50 = 984  
    * undersampling_result <- ovun.sample(Class ~ ., data = creditcard, method = "under", N = new_n_total, seed = 2018)  
* Can also combine both over-sampling and under-sampling  
	* n_new <- nrow(creditcard) # = 24600  
    * fraction_fraud_new <- 0.50  
    * sampling_result <- ovun.sample(Class ~ ., data = creditcard, method = "both", N = n_new, p = fraction_fraud_new, seed = 2018)  
    * sampled_credit <- sampling_result$data  
  
Synthetic minority over-sampling:  
  
* SMOTE - Synthetic Minority Oversampling Technique  
	* Find the nearest k fraudulent neighbors of a minority class data point  
    * Randomly choose one of the k neighbors  
    * Choose a random number between 0 and 1  
    * Linear combination of the minority class data point and the randomly chosen neighbor, using the random number above as the weighting  
    * Repeat the process "dup_size" number of times  
* Can run the process using the "smotefamily" library  
	* library(smotefamily)  
    * smote_output = SMOTE(X = transfer_data[, -1], target = transfer_data$isFraud, K = 4, dup_size = 10)  
    * oversampled_data = smote_output$data  
  
From dataset to detection model:  
  
* General SMOTE modeling process  
	* Divide data in to train and test  
    * Run SMOTE on the training data only  
    * Train the model on the SMOTE-balanced dataset  
    * Test performance on the test dataset  
* Example of running using SMOTE  
	* smote_result = SMOTE(X = train[, -17], target = train$Class, K = 5, dup_size = 10)  
    * train_oversampled = smote_result$data  
    * colnames(train_oversampled)[17] = "Class"  
    * model2 = rpart::rpart(Class ~ ., data = train_oversampled)  
    * scores2 = predict(model2, newdata = test, type = "prob")[, 2]  
    * predicted_class2 = factor(ifelse(scores2 > 0.5, 1, 0))  
    * CM2 = caret::confusionMatrix(data = predicted_class2, reference = test$Class)  
    * auc(roc(response = test$Class, predictor = scores2))  
* Modeling can be based on costs - associated with fraud, and associated with wrongly flagging fraud  
	* cost_model <- function(predicted.classes, true.classes, amounts, fixedcost) {  
    *     cost <- sum(true.classes * (1 - predicted.classes) * amounts + predicted.classes * fixedcost)
    *     return(cost)  
    * }  
    * cost_model(predicted_class1, test$Class, test$Amount, fixedcost = 10)  
    * cost_model(predicted_class2, test$Class, test$Amount, fixedcost = 10)  
  
Example code includes:  
```{r}

load("./RInputFiles/transfers02_v2.RData")  # data.frame transfers is 628x12

# Make a scatter plot
ggplot(transfers, aes(x = amount, y = orig_balance_before)) +
    geom_point(aes(color = fraud_flag, shape = fraud_flag)) +
    scale_color_manual(values = c('dodgerblue', 'red'))


load("./RInputFiles/creditcard5.RData")  # data.frame creditcard is 9840x32

# Calculate the required number of cases in the over-sampled dataset
n_new <- sum(creditcard$Class==0) / (1-0.3333)

# Over-sample
oversampling_result <- ROSE::ovun.sample(formula = Class ~ ., data = creditcard, 
                                         method = "over", N = n_new, seed = 2018
                                         )

# Verify the Class-balance of the over-sampled dataset
oversampled_credit <- oversampling_result$data
prop.table(table(oversampled_credit$Class))


# Calculate the required number of cases in the over-sampled dataset
n_new <- sum(creditcard$Class == 1) / (0.4)

# Under-sample
undersampling_result <- ROSE::ovun.sample(formula = Class ~ ., data = creditcard,
                                          method = "under", N = n_new, seed = 2018
                                          )

# Verify the Class-balance of the under-sampled dataset
undersampled_credit <- undersampling_result$data
prop.table(table(undersampled_credit$Class))


# Specify the desired number of cases in the balanced dataset and the fraction of fraud cases
n_new <- 10000
fraud_fraction <- 0.3

# Combine ROS & RUS!
sampling_result <- ROSE::ovun.sample(formula = Class ~ ., data = creditcard, method = "both", 
                                     N = n_new,  p = fraud_fraction, seed = 2018
                                     )

# Verify the Class-balance of the re-balanced dataset
sampled_credit <- sampling_result$data
prop.table(table(sampled_credit$Class))


# Set the number of fraud and legitimate cases, and the desired percentage of legitimate cases
n0 <- sum(creditcard$Class==0)
n1 <- sum(creditcard$Class==1)
r0 <- 0.6

# Calculate the value for the dup_size parameter of SMOTE
ntimes <- ((1 - r0) / r0) * (n0 / n1) - 1

# Create synthetic fraud cases with SMOTE
smote_output <- smotefamily::SMOTE(X = creditcard[ , -c(1, 31, 32)], target = creditcard$Class, 
                                   K = 5, dup_size = ntimes
                                   )

# Make a scatter plot of the original and over-sampled dataset
credit_smote <- smote_output$data
colnames(credit_smote)[30] <- "Class"
prop.table(table(credit_smote$Class))

ggplot(creditcard, aes(x = V1, y = V2, color = Class)) +
    geom_point() +
    scale_color_manual(values = c('dodgerblue2', 'red'))

ggplot(credit_smote, aes(x = V1, y = V2, color = Class)) +
    geom_point() +
    scale_color_manual(values = c('dodgerblue2', 'red'))


set.seed(1903172344)
testIdx <- sort(sample(1:nrow(creditcard), round(0.5*nrow(creditcard)), replace=FALSE))
test <- creditcard[testIdx, ]
train_original <- creditcard[-testIdx, ]

n_new <- 7380
fraud_fraction <- 0.3
train_oversampled <- ROSE::ovun.sample(formula = Class ~ ., data = train_original, 
                                       method = "over", N = n_new, seed = 2018
                                       )$data

# Train the rpart algorithm on the original training set and the SMOTE-rebalanced training set
model_orig <- rpart::rpart(Class ~ ., data = train_original)
model_smote <- rpart::rpart(Class ~ ., data = train_oversampled)

# Predict the fraud probabilities of the test cases
scores_orig <- predict(model_orig, newdata = test, type = "prob")[, 2]
scores_smote <- predict(model_smote, newdata = test, type = "prob")[, 2]

# Convert the probabilities to classes (0 or 1) using a cutoff value
predicted_class_orig <- factor(ifelse(scores_orig > 0.5, 1, 0))
predicted_class_smote <- factor(ifelse(scores_smote > 0.5, 1, 0))

# Determine the confusion matrices and the model's accuracy
CM_orig <- caret::confusionMatrix(data = predicted_class_orig, reference = test$Class)
CM_smote <- caret::confusionMatrix(data = predicted_class_smote, reference = test$Class)
print(CM_orig$table)
print(CM_orig$overall[1])
print(CM_smote$table)
print(CM_smote$overall[1])


cost_model <- function(predicted.classes, true.classes, amounts, fixedcost) {
    predicted.classes <- hmeasure::relabel(predicted.classes)
    true.classes <- hmeasure::relabel(true.classes)
    cost <- sum(true.classes * (1 - predicted.classes) * amounts + predicted.classes * fixedcost)
    return(cost)
}

# Calculate the total cost of deploying the original model
cost_model(predicted_class_orig, test$Class, test$Amount, fixedcost=10)

# Calculate the total cost of deploying the model using SMOTE
cost_model(predicted_class_smote, test$Class, test$Amount, fixedcost=10)

```
  
  
  
***
  
Chapter 4 - Digit Analysis and Robust Statistics  
  
Digit analysis using Benford's Law:  
  
* Benford's Law considers the left-most digit in a number  
	* The distribution will often have 30% 1s with only 4.6% 9s  
    * Generally, frequencies will be proportional to log10(1 + 1/d) where d is the digit  
    * Pinkham discovered that Benford's Law is scale invariant (e.g., Euros to pesos should preserve the distribution)  
* Example of Benford's Law in code  
	* benlaw <- function(d) log10(1 + 1 / d)  
    * df <- data.frame(digit = 1:9, probability = benlaw(1:9))  
    * ggplot(df, aes(x = digit, y = probability)) + geom_bar(stat = "identity", fill = "dodgerblue") + xlab("First digit") + ylab("Expected frequency") + scale_x_continuous(breaks = 1:9, labels = 1:9) + ylim(0, 0.33) + theme(text = element_text(size = 25))  
    * n <- 1000  
    * fibnum <- numeric(len)  
    * fibnum[1] <- 1  
    * fibnum[2] <- 1  
    * for (i in 3:n) { fibnum[i] <- fibnum[i-1]+fibnum[i-2] }  
    * pow2 <- 2^(1:n)  
    * library(benford.analysis)  
    * bfd.fib <- benford(fibnum, number.of.digits = 1)  
    * plot(bfd.fib)  
    * bfd.pow2 <- benford(pow2, number.of.digits = 1)  
    * plot(bfd.pow2)  
  
Benford's Law for fraud detection:  
  
* Many datasets follow Benford's law  
	* data where numbers represent sizes of facts or events  
    * data in which numbers have no relationship to each other  
    * data sets that grow exponentially or arise from multiplicative fluctuations  
    * mixtures of different data sets  
    * Some well-known infinite integer sequences  
* Fraud is typically committed by changing numbers, and changes often fail to conform with Benford's Law  
* Caution that many types of data can never be expected to comply with Benford's Law  
	* If there is lower and/or upper bound or data is concentrated in narrow interval, e.g. hourly wage rate, height of people  
    * If numbers are used as identification numbers or labels, e.g. social security number, flight numbers, car license plate numbers, phone numbers  
    * Additive fluctuations instead of multiplicative fluctuations, e.g. heartbeats on a given day  
* Benford's Law can be applied for the first two digits of a series of numbers  
	* P(d1d2) = log10(1 + 1/(d1d2))  # note that this is NOT multiplicative; d1d2 for d1=1 and d2=2 is 12, not 2  
    * bfd.cen <- benford(census.2009$pop.2009,number.of.digits = 2)  
    * plot(bfd.cen)  
  
Detecting univariate outliers:  
  
* Can use global statistics to detect outliers; not all outliers are fraudulent, so follow-up and validation are needed  
* One popular tool for outlier detection is the z-score - (x - mu) / sigma  
	* One challenge is that the sample mean and sample standard deviation are artificially driven by an extreme outlier  
* "Robust statistics" attempt to flag outliers better than the classical z-scores  
	* Classical statistical methods rely on (normality) assumptions, but even single outlier can influence conclusions significantly and may lead to misleading results  
    * Robust statistics produce also reliable results when data contains outliers and yield automatic outlier detection tools  
    * It is perfect to use both classical and robust methods routinely, and only worry when they differ enough to matter... But when they differ, you should think hard. J.W. Tukey (1979)  
* The median is more robust than the mean, while the median-absolute-deviation (MAD) and IQR are more robust than the standard deviation  
* The Boxplot is a common way to identify outliers  
	* ggplot(data.frame(los), aes(x = "", y = los)) + geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 3, fill = "lightblue", width = 0.5) + xlab("") + ylab("Length Of Stay (LOS)") + theme(text = element_text(size = 25))  
* The boxplot has assumptions of normality, even though in reality data may not be normally distributed  
	* At asymmetric distributions, boxplot may flag many regular points as outliers  
    * The skewness-adjusted boxplot corrects for this by using a robust measure of skewness in determining the fence  
    * library(robustbase)  
    * adjbox_stats <- adjboxStats(los)$stats  
    * ggplot(data.frame(los), aes(x = "", y = los)) +  
    *     stat_boxplot(geom = "errorbar", width = 0.2, coef = 1.5*exp(3*mc(los))) +  
    *     geom_boxplot(ymin = adjbox_stats[1], ymax = adjbox_stats[5], middle = adjbox_stats[3], upper = adjbox_stats[4], lower = adjbox_stats[2], outlier.shape = NA, fill = "lightblue", width = 0.5) +  
    *     geom_point(data=subset(data.frame(los), los < adjbox_stats[1] | los > adjbox_stats[5]), col = "red", size = 3, shape = 16) + xlab("") +  
    *     ylab("Length Of Stay (LOS)") +  
    *     theme(text = element_text(size = 25))  
    * adjbox(los,col="lightblue", ylab="LOS data")$out  
  
Detecting multivariate outliers:  
  
* Multivariate outliers have unusual combinations of data across multiple dimensions  
* Example of animals data - brain vs. body  
	* X <- data.frame(body = log(Animals$body), brain = log(Animals$brain))  
    * fig <- ggplot(X, aes(x = body, y = brain)) + geom_point(size = 5) + xlab("log(body)") + ylab("log(brain)") + ylim(-5, 15) + scale_x_continuous(limits = c(-10, 16), breaks = seq(-15, 15, 5)))  
* Mahalanobis distance is a multidimensional distance (distance on each axis is scaled based on length of ellipse in that direction  
	* Classical Mahalanobis distances : sample mean as estimate for location and sample covariance matrix as estimate for scatter  
    * To detect multivariate outliers the mahalanobis distance is compared with a cut-off value, which is derived from the chisquare distribution  
    * In two dimensions we can construct corresponding 97.5% tolerance ellipsoid, which is defined by those observations whose Mahalanobis distance does not exceed the cut-off value  
* Extending the Mahalanobis distance to the animal data  
	* animals.clcenter <- colMeans(X)  
    * animals.clcov <- cov(X)  
    * rad <- sqrt(qchisq(0.975, df = ncol(X)))  
    * library(car)  
    * ellipse.cl <- data.frame(ellipse(center = animals.clcenter, shape = animals.clcov,radius = rad, segments = 100, draw = FALSE))  
    * colnames(ellipse.cl) <- colnames(X)  
    * fig <- fig + geom_polygon(data=ellipse.cl, color = "dodgerblue", fill = "dodgerblue", alpha = 0.2) + geom_point(aes(x = animals.clcenter[1], y = animals.clcenter[2]), color = "blue", size = 6)  
    * fig  
* Can improve the Mahalanobis distance with a more robust estimate of location and scatter  
	* Minimum Covariance Determinant (MCD) estimator of Rousseeuw is a popular robust estimator of multivariate location and scatter  
    * MCD looks for those hh observations whose classical covariance matrix has the lowest possible determinant  
    * MCD estimate of location is then mean of these hh observations  
    * MCD estimate of scatter is then sample covariance matrix of these hh points (multiplied by consistency factor)  
    * Reweighting step is applied to improve efficiency at normal data  
    * Computation of MCD is difficult, but several fast algorithms are proposed  
* Examples of using robust statistics on the animals data  
	* library(robustbase)  
    * animals.mcd <- covMcd(X)  
    * animals.mcd$center  
    * animals.mcd$cov  
    * library(robustbase)  
    * animals.mcd <- covMcd(X)  
    * ellipse.mcd <- data.frame(ellipse(center = animals.mcd$center, shape = animals.mcd$cov, radius=rad, segments=100, draw=FALSE))  
    * colnames(ellipse.mcd) <- colnames(X)  
    * fig <- fig + geom_polygon(data=ellipse.mcd, color="red", fill="red", alpha=0.3) + geom_point(aes(x = animals.mcd$center[1], y = animals.mcd$center[2]), color = "red", size = 6)  
    * fig  
* The distance-distance plot is a common alternative when the number of dimensions is 4+  
	* When p>3 it is not possible to visualize the tolerance ellipsoid  
    * The distance-distance plot shows the robust distance of each observation versus its classical Mahalanobis distance, obtained immediately from MCD object  
    * plot(animals.mcd, which = "dd")  
  
Example code includes:  
```{r}

# Implement Benford's Law for first digit
benlaw <- function(d) log10(1 + 1 / d)

# Calculate expected frequency for d=5
benlaw(d=5)

# Create a dataframe of the 9 digits and their Benford's Law probabilities
df <- data.frame(digit = 1:9, probability = benlaw(1:9))

# Create barplot with expected frequencies
ggplot(df, aes(x = digit, y = probability)) + 
    geom_bar(stat = "identity", fill = "dodgerblue") + 
    xlab("First digit") + 
    ylab("Expected frequency") + 
    scale_x_continuous(breaks = 1:9, labels = 1:9) + 
    ylim(0, 0.33) + 
    theme(text = element_text(size = 25))


data(census.2009, package="benford.analysis")

# Check conformity
bfd.cen <- benford.analysis::benford(census.2009$pop.2009, number.of.digits = 1) 
plot(bfd.cen, except = c("second order", "summation", "mantissa", "chi squared", 
                         "abs diff", "ex summation", "Legend"), 
     multiple = F
     ) 

# Multiply the data by 3 and check conformity again
data <- census.2009$pop.2009 * 3
bfd.cen3 <- benford.analysis::benford(data, number.of.digits=1)
plot(bfd.cen3, except = c("second order", "summation", "mantissa", "chi squared", 
                          "abs diff", "ex summation", "Legend"), 
     multiple = F
     )


load("./RInputFiles/fireinsuranceclaims.RData")  # num[1:40000] fireinsuranceclaims

# Validate data against Benford's Law using first digit
bfd.ins <- benford.analysis::benford(fireinsuranceclaims, number.of.digits = 1) 
plot(bfd.ins, except=c("second order", "summation", "mantissa", "chi squared",
                       "abs diff", "ex summation", "Legend"), 
     multiple = F
     )

# Validate data against Benford's Law using first-two digits
bfd.ins2 <- benford.analysis::benford(fireinsuranceclaims, number.of.digits = 2)
plot(bfd.ins2, except=c("second order", "summation", "mantissa", "chi squared",
                        "abs diff", "ex summation", "Legend"), 
     multiple = F
     )


load("./RInputFiles/expensesCEO.RData")  # num[1:988] expensesCEO

# Validate data against Benford's Law using first digit
bfd.exp <- benford.analysis::benford(expensesCEO, number.of.digits = 1) 
plot(bfd.exp, except=c("second order", "summation", "mantissa", "chi squared",
                       "abs diff", "ex summation", "Legend"), 
     multiple = F
     )

# Validate data against Benford's Law using first-two digits
bfd.exp2 <- benford.analysis::benford(expensesCEO, number.of.digits = 2) 
plot(bfd.exp2, except=c("second order", "summation", "mantissa", "chi squared",
                        "abs diff", "ex summation", "Legend"), 
     multiple = F
     )


load("./RInputFiles/transfers_chap1_L4.RData")  # data.frame transfers 222x16

# Get observations identified as fraud
which(transfers$fraud_flag == 1)

# Compute median and mean absolute deviation for `amount`
m <- median(transfers$amount)
s <- mad(transfers$amount)

# Compute robust z-score for each observation
robzscore <- abs((transfers$amount - m) / (s))

# Get observations with robust z-score higher than 3 in absolute value
which(abs(robzscore) > 3)


thexp <- c(40517, 33541, 5182, 40385, 40302, 23189, 13503, 5110, 15754, 40763, 23061, 30839, 25206, 15891, 38821, 11766, 4934, 13754, 14142, 27813, 21005, 11511, 41750, 32855, 62043, 19415, 51815, 26961, 19185, 19704, 21831, 40768, 49079, 12766, 13030, 8841, 17943, 6214, 21114, 7898, 30707, 69698, 70155, 15032, 55858, 31747, 11562, 12390, 7016, 96396, 24614, 22735, 20483, 36907, 31822, 13619, 34401, 10281, 32165, 52226, 13941, 40850, 15270, 21143, 26029, 10209, 10950, 19745, 54153, 33668, 7562, 34231, 34219, 25784, 52952, 32959, 17459, 25611, 14998, 36229, 26485, 20563, 41865, 29821, 26792, 42406, 20083, 10205, 31353, 33674, 13523, 51835, 18136, 54736, 33499, 95389, 44967, 67707, 40879, 17729, 15643, 15648, 19150, 9789, 27978, 40469, 30696, 48195, 12817, 10527, 42946, 72281, 13773, 17189, 14340, 47962, 29063, 34477, 84354, 37943, 13584, 12184, 49563, 36263, 18313, 25399, 50235, 14230, 25617, 18226, 31542, 24262, 17617, 22068, 43534, 14574, 6471, 57500, 8535, 85065, 22749, 10481, 42094, 24436, 27975, 28347, 32929, 20106, 30992, 22202, 17005, 29900, 16871, 10790, 38355, 10315, 7782, 16084, 11788, 20005, 70859, 21706, 11929, 69816, 6351, 27217, 30178, 10597, 13715, 13687, 17116, 27426, 56579, 31655, 86577, 27051, 10477, 11178, 49785, 12626, 44817, 15758, 21396, 5590, 40538, 38834, 32693, 47330, 17823, 92957, 44439, 27188, 22972, 40020, 33067, 24562, 8408, 36088, 8823, 8022, 36395, 14523, 49188, 19744, 17536, 32456, 38400, 6451, 31766, 24727, 60013, 15664, 17356, 50482, 20752, 28048, 10932, 35337, 18755, 6572, 16065, 67257, 36303, 14846, 50468, 27237, 7165, 38067, 22040, 23794, 106032, 19303, 63934, 16818, 11621, 40566, 14921, 15188, 28087, 21026, 38907, 39727, 49794, 49112, 6886, 31674, 25053, 23835, 12160, 45640, 7898, 107065, 58206, 8270, 69529, 25776, 57742, 18456, 53523, 27514, 14089, 7291, 4727, 7319, 22650, 8462, 14980, 39085, 13627, 14998, 29686, 22750, 25487, 40127, 22844, 7597, 19265, 14869, 33010, 74958, 30320, 16602, 36376, 16467, 26946, 26870, 33433, 61134, 20121, 58389, 61594, 24150, 32594, 20177, 3133, 22330, 16905, 25053, 10837, 20807, 6647, 29696, 34457, 78286, 21551, 26533, 52237, 58745, 19607, 25062, 23823, 108756, 12067, 28813, 17803, 35795, 20860, 14307, 11991, 71409, 74111, 25916, 25914, 14092, 24780, 43417, 12767, 43919, 28205, 34075, 68173, 28509, 78760, 52329, 31858, 54088, 6670, 29371, 30066, 16554, 13866, 13806, 40504, 49841, 19729, 30881, 35484, 11373, 10624, 7544, 49465, 12499, 18316, 27963, 31601, 52243, 48927, 42339, 34707, 13034, 9452, 69461, 64335, 9964, 8993, 11217, 116262, 16693, 28622, 35251, 8587, 28414, 15552, 49460, 57721, 16398, 20597, 28592, 13795, 19534, 37065, 22847, 10679, 37552, 63611, 12023, 19659, 16040, 63519, 56897, 55381, 44055, 65130, 8251, 4857, 6225, 74197, 20986, 47412, 35014, 39263, 19486, 22648, 7826, 52697, 9196, 31890, 8836, 62723, 60814, 42018, 20671, 7444, 21584, 13213, 23959, 16394, 21742, 71587, 16866, 63788, 39375, 37087, 9370, 37440, 10666, 60304, 34391, 55988, 9921, 11709, 69198, 49491, 39201, 10065, 32063, 10380, 18818, 35576, 15695, 30336, 74993, 48089, 24255, 56063, 11932, 23251, 9537, 7757, 67473, 44949, 11842, 12681, 36156, 71957, 37576, 36120, 53232, 6743, 36142, 62565, 49525, 33161, 28272, 24179, 15227, 46151, 32764, 32374, 22462, 9593, 9442, 13202, 24634, 35284, 44800, 11044, 45655, 5502, 18124, 18007, 39303, 14012, 26558, 10926, 25389, 16447, 8720, 12977, 12942, 25020, 6430, 18919, 24916, 55667, 51081, 99834, 31840, 147498)
thexp <- c(thexp, 27913, 40044, 24278, 41614, 50038, 54307, 27598, 18022, 34789, 24503, 51856, 12160, 19107, 40845, 46171, 66460, 22241, 15245, 10925, 17671, 5666, 25305, 9394, 20357, 32948, 7942, 14555, 24012, 25235, 17292, 81646, 46737, 15012, 49861, 10012, 15076, 7693, 18513, 46293, 50770, 36122, 25598, 8633, 15568, 16954, 47036, 38076, 18458, 8092, 38576, 28692, 27211, 37485, 15162, 32968, 55021, 7060, 16714, 34581, 14939, 27056, 15090, 56905, 29528, 21282, 39487, 24239, 35466, 21982, 10334, 15133, 41591, 23260, 12882, 32149, 16219, 41605, 8346, 8549, 23818, 36217, 42766, 11239, 59532, 31806, 52218, 73118, 31701, 32761, 18745, 17949, 8017, 12833, 25583, 36468, 8706, 94587, 42900, 74298, 17201, 25618, 14888, 16308, 75043, 68056, 50797, 15956, 13820, 13985, 22742, 17692, 30214, 57582, 17273, 31885, 14307, 22597, 46389, 23366, 35128, 51769, 9251, 35663, 34474, 18748, 60091, 31137, 14366, 25347, 32175, 16065, 16672, 45192, 42039, 19665, 24933, 29570, 23400, 13517, 23993, 18140, 9545, 16042, 24425, 28400, 25035, 28316, 19001, 27203, 8016, 15199, 14069, 12037, 30455, 13877, 10696, 11010, 41384, 37241, 38328, 54434, 27174, 14015, 27354, 12944, 19718, 21558, 22239, 31076, 32940, 17810, 13462, 16122, 20417, 36205, 9871, 11892, 50737, 32511, 29767, 17032, 40276, 24005, 33884, 16278, 11326, 7187, 50434, 70436, 38353, 27723, 32811, 14833, 28465, 83158, 18866, 21823, 39125, 45372, 33933, 29469, 79147, 24190, 37007, 4259, 41346, 25087, 27216, 32780, 21190, 29067, 11316, 36103, 15389, 27257, 26051, 22853, 10551, 6661, 15878, 17131, 18220, 12045, 10573, 34645, 19517, 13933, 14452, 33694, 35605, 48376, 19567, 35762, 12931, 6286, 25321, 22167, 24243, 19433, 26852, 25802, 26647, 26423, 11537, 16011, 56547, 22690, 20391, 10487, 16994, 25690, 19260, 57525, 17802, 28135, 14365, 21640, 20817, 55771, 31693, 31859, 27496, 39715, 22775, 27933, 58875, 39133, 7604, 29409, 29296, 44377, 33107, 21235, 59129, 33427, 10164, 14595, 4744, 49674, 27827, 48830, 36196, 24979, 47800, 108752, 52300, 38343, 19381, 35881, 43688, 32938, 13341, 29297, 38603, 30202, 14797, 38529, 29055, 61303, 27109, 53496, 16665, 65132, 23903, 36096, 21247, 42292, 11176, 7542, 15210, 5289, 58444, 33295, 30456, 60595, 59624, 19642, 13317, 9262, 17611, 35079, 45469, 59510, 26852, 51484, 20195, 27751, 33555, 27692, 70407, 18102, 130773, 16637, 60463, 11653, 19275, 47114, 6117, 29645, 57846, 51033, 11790, 24970, 32391, 19278, 27778, 19596, 17761, 56884, 66230, 40617, 80495, 27704, 22815, 23390, 18092, 13037, 27954, 6979, 6942, 46155, 34240, 24484, 22375, 45916, 32788, 28017, 31922, 25357, 7314)

# Create boxplot
bp.thexp <- boxplot(thexp, col = "lightblue", main = "Standard boxplot", 
                    ylab = "Total household expenditure"
                    )

# Extract the outliers from the data
bp.thexp$out

# Create adjusted boxplot
adj.thexp <- robustbase::adjbox(thexp, col = "lightblue", main = "Adjusted boxplot", 
                                ylab = "Total household expenditure"
                                )


load("./RInputFiles/hailinsurance.RData")  # matrix num[1:100, 1:2] hailinsurance

# Create a scatterplot
plot(hailinsurance, xlab = "price house", ylab = "claim")

# Compute the sample mean and sample covariance matrix
clcenter <- colMeans(hailinsurance)
clcov <- cov(hailinsurance)

# Add 97.5% tolerance ellipsoid
rad <- sqrt(qchisq(0.975, df=ncol(hailinsurance)))
car::ellipse(center = clcenter, shape = clcov, radius = rad,col = "blue", lty = 2)


# Create a scatterplot of the data
plot(hailinsurance, xlab = "price house", ylab = "claim")

# Compute robust estimates for location and scatter
mcdresult <- robustbase::covMcd(hailinsurance)
robustcenter <- mcdresult$center
robustcov <- mcdresult$cov

# Add robust 97.5% tolerance ellipsoid
rad <- sqrt(qchisq(0.975, df=ncol(hailinsurance)))
car::ellipse(center = robustcenter, shape = robustcov, radius = rad, col = "red")

```
  
  
  
***
  
###_Dimensionality Reduction in R_  
  
Chapter 1 - Principal Component Analysis  
  
The curse of dimensionality:  
  
* The "curse of dimensionality" can make it challenging to understand the structure of a data set  
	* As the dimensionalities of the data grow, the feature space grows rapidly  
    * Big computational cost to handle high-dimensional data  
    * Estimation accuracy decreases  
    * Difficult interpretation of the data  
* Example of using the mtcars dataset (11 features)  
	* Most of the dimensions could probably be reduced due to a small set of latent dimensions, such as:  
    * the size of the car or  
    * the country of origin or  
    * the construction year  
* Need to convert any factor variables (e.g., cylinders) and then run correlations  
	* mtcars$cyl <- as.numeric(as.character(mtcars$cyl))  
    * mtcars_correl <- cor(mtcars, use = "complete.obs")  
    * library(ggcorrplot)  
    * ggcorrplot(mtcars_correl)  
* Several options for dealing with the curse of dimensionality  
	* Feature Engineering: Requires domain knowledge  
    * Remove redundancy  
  
Getting PCA to work with FactoMineR:  
  
* PCA removes noise introduced by correlation, changes the coordinate dimensions to best explain variability in the data, etc.  
	* Center and standardize  
    * Rotation and projection  
    * Projection and reduction  
* The screeplot can be helpful for finding elbow points for percentage of explained variance  
* Can implement using prcomp() or FactoMineR::PCA() in R  
	* mtcars_pca <- prcomp(mtcars)  
    * mtcars_pca <- FactoMineR::PCA(mtcars)  
* The eigenvalues and factor map can help with interpreting the outputs  
	* mtcars_pca$eig  
    * mtcars_pca$var$cos2  # squared cosine (closer to 1 is better)  
    * mtcars_pca$var$contrib  # contribution of each variable to the components  
    * dimdesc(mtcars_pca)  
  
Interpreting and visualizing PCA models:  
  
* Can plot the contributions of the individual principal components  
	* fviz_pca_var(mtcars_pca, col.var = "contrib", gradient.cols = c("#bb2e00", "#002bbb"), repel = TRUE)  
    * fviz_pca_var(mtcars_pca, select.var = list(contrib = 4), repel = TRUE)  
    * fviz_contrib(mtcars_pca, choice = "var", axes = 1, top = 5)  
    * fviz_pca_ind(mtcars_pca, col.ind="cos2", gradient.cols = c("#bb2e00", "#002bbb"), repel = TRUE)  
    * fviz_pca_ind(mtcars_pca, select.ind = list(cos2 = 0.8), gradient.cols = c("#bb2e00", "#002bbb"), repel = TRUE)  
    * fviz_cos2(mtcars_pca, choice = "ind", axes = 1, top = 10)  
    * fviz_pca_biplot(mtcars_pca)  
* Can add ellispoids to the data  
	* mtcars$cyl <- as.factor(mtcars$cyl)  
    * fviz_pca_ind(mtcars_pca, label="var", habillage=mtcars$cyl, addEllipses=TRUE)  
  
Example code includes:  
```{r}

cars <- as.data.frame(data.table::fread("./RInputFiles/04carsdata.csv"))

rowsDelete <- c(59, 65, 71, 83, 84, 85, 108, 109, 116, 119, 124, 127, 128, 138, 143, 146, 147, 148, 205, 239, 240, 244, 245, 247, 248, 256, 291, 293, 295, 296, 297, 304, 315, 321, 325, 355, 399, 400, 401, 414, 415)
cars$`Vehicle Name`[rowsDelete]

rowsMod <- c(182, 183, 252, 253, 255, 256)
cars$`Vehicle Name`[rowsMod]

rowNames <- cars$`Vehicle Name`
rowNames[182] <- paste0(rowNames[182], " RWD")
rowNames[183] <- paste0(rowNames[183], " AWD")
rowNames[252] <- paste0(rowNames[252], " RWD")
rowNames[253] <- paste0(rowNames[253], " AWD")
rowNames[255] <- paste0(rowNames[255], " RWD")
rowNames[256] <- paste0(rowNames[256], " AWD")

row.names(cars) <- rowNames
cars <- cars[-rowsDelete, ]

cars$`Vehicle Name` <- NULL
cars$type <- factor(c(3, 3, 5, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 4, 6, 3, 4, 4, 4, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 4, 4, 3, 3, 3, 3, 3, 5, 3, 3, 5, 3, 3, 3, 5, 3, 5, 4, 1, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 6, 3, 3, 5, 5, 5, 5, 1, 3, 3, 3, 3, 3, 4, 6, 3, 3, 3, 3, 3, 3, 1, 1, 5, 1, 5, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 3, 3, 3, 6, 3, 3, 1, 4, 4, 3, 6, 3, 4, 5, 1, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 1, 1, 5, 4, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 4, 3, 3, 6, 6, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 5, 5, 5, 3, 3, 3, 3, 6, 1, 5, 3, 3, 3, 5, 5, 5, 3, 3, 3, 5, 3, 3, 6, 3, 5, 5, 4, 5, 3, 3, 3, 3, 5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 6, 3, 5, 5, 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 1, 5, 6, 3, 3, 3, 3, 3, 4, 4, 5, 3, 4, 5, 5, 4, 4, 3, 3, 3, 3, 6, 5, 5, 1, 1, 3, 3, 3, 5, 3, 3, 1, 5, 3, 3, 3, 1, 1, 3, 3, 6, 4, 4, 4, 4, 4, 4, 5, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 3, 5, 3, 6, 6, 3, 4, 4, 3, 3, 6, 3, 3, 3, 3, 3, 6, 3, 3, 3, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 5, 5, 6, 4, 3, 5, 5, 1, 1, 3, 3, 6, 3, 3, 3, 3, 6, 3, 3, 6, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 5), levels=c(1, 2, 3, 4, 5, 6), labels=c('Minivan', 'Pickup', 'Small.Sporty..Compact.Large.Sedan', 'Sports.Car', 'SUV', 'Wagon'))
cars$wheeltype <- factor(c(1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1), levels=c(1, 2), labels=c("AWD", "RWD"))

colNames <- c('Small.Sporty..Compact.Large.Sedan', 'Sports.Car', 'SUV', 'Wagon', 'Minivan', 'Pickup', 'AWD', 'RWD', 'Retail.Price', 'Dealer.Cost', 'Engine.Size..l.', 'Cyl', 'HP', 'City.MPG', 'Hwy.MPG', 'Weight', 'Wheel.Base', 'Len', 'Width', 'type', 'wheeltype')
for (intCtr in seq_along(colNames)) {
    cat("Original Name: ", names(cars)[intCtr], " ---> New Name: ", colNames[intCtr], "\n")
}
names(cars) <- colNames

cars <- cars %>% mutate(City.MPG=as.integer(City.MPG), Hwy.MPG=as.integer(Hwy.MPG), 
                        Weight=as.integer(Weight), Wheel.Base=as.integer(Wheel.Base), 
                        Len=as.integer(Len), Width=as.integer(Width)
                        )
str(cars)


# Explore cars with summary()
summary(cars)

# Get the correlation matrix with cor()
correl <- cor(cars[,9:19], use = "complete.obs")

# Use ggcorrplot() to explore the correlation matrix
ggcorrplot::ggcorrplot(correl)

# Conduct hierarchical clustering on the correlation matrix
ggcorrplot_clustered <- ggcorrplot::ggcorrplot(correl, hc.order = TRUE, type = "lower")
ggcorrplot_clustered


# Run a PCA for the 10 non-binary numeric variables of cars
pca_output_ten_v <- FactoMineR::PCA(cars[,9:19], ncp = 4, graph = F)

# Get the summary of the first 100 cars
summary(pca_output_ten_v, nbelements = 100)

# Get the variance of the first 3 new dimensions
pca_output_ten_v$eig[,2][1:3]

# Get the cumulative variance
pca_output_ten_v$eig[,3][1:3]


# Run a PCA with active and supplementary variables
pca_output_all <- FactoMineR::PCA(cars, quanti.sup = 1:8, quali.sup = 20:21, graph = F)

# Get the most correlated variables
FactoMineR::dimdesc(pca_output_all, axes = 1:2)

# Run a PCA on the first 100 car categories
pca_output_hundred <- FactoMineR::PCA(cars, quanti.sup = 1:8, quali.sup = 20:21, 
                                      ind.sup = 1:100, graph = F
                                      )

# Trace variable contributions in pca_output_hundred
pca_output_hundred$var$contrib


# Run a PCA using the 10 non-binary numeric variables
cars_pca <- ade4::dudi.pca(cars[,9:19], scannf = FALSE, nf = 4)

# Explore the summary of cars_pca
summary(cars_pca)

# Explore the summary of pca_output_ten_v
summary(pca_output_ten_v)


# Create a factor map for the variables
factoextra::fviz_pca_var(pca_output_all, select.var = list(cos2 = 0.7), repel = TRUE)

# Modify the code to create a factor map for the individuals
factoextra::fviz_pca_ind(pca_output_all, select.ind = list(cos2 = 0.7), repel = TRUE)

# Create a barplot for the variables with the highest cos2 in the 1st PC
factoextra::fviz_cos2(pca_output_all, choice = "var", axes = 1, top = 10)

# Create a barplot for the variables with the highest cos2 in the 2nd PC
factoextra::fviz_cos2(pca_output_all, choice = "var", axes = 2, top = 10)


# Create a factor map for the top 5 variables with the highest contributions
factoextra::fviz_pca_var(pca_output_all, select.var = list(contrib = 5), repel = TRUE)

# Create a factor map for the top 5 individuals with the highest contributions
factoextra::fviz_pca_ind(pca_output_all, select.ind = list(contrib = 5), repel = TRUE)

# Create a barplot for the variables with the highest contributions to the 1st PC
factoextra::fviz_contrib(pca_output_all, choice = "var", axes = 1, top = 5)

# Create a barplot for the variables with the highest contributions to the 2nd PC
factoextra::fviz_contrib(pca_output_all, choice = "var", axes = 2, top = 5)


# Create a biplot with no labels for all individuals with the geom argument.
factoextra::fviz_pca_biplot(pca_output_all)

# Create ellipsoids for wheeltype columns respectively.
factoextra::fviz_pca_ind(pca_output_all, habillage = cars$wheeltype, addEllipses = TRUE)

# Create the biplot with ellipsoids
factoextra::fviz_pca_biplot(pca_output_all, habillage=cars$wheeltype, addEllipses=TRUE, alpha.var="cos2")

```
  
  
  
***
  
Chapter 2 - Advanced PCA and Non-Negative Matrix Factorization (NNMF)  
  
Determining the right number of PCs:  
  
* Desire to have stopping rules to determine the proper number of principal components  
* The screeplot can help to find an elbow, which is often a good stopping point  
	* mtcars_pca <- PCA(mtcars)  
    * fviz_screeplot(mtcars_pca, ncp=5)  
* The Kaiser-Guttman rule recommends keeping components with eigenvalues > 1  
	* mtcars_pca$eig  
    * get_eigenvalue(mtcars_pca)  
* Parallel analysis is superior to the previous tests - compares variance explained by PC to what would be expected with random data - select "above the line"  
	* library(paran)  
    * mtcars_pca_ret <- paran(mtcars_pca, graph = TRUE)  
  
Performing PCA on datasets with missing values:  
  
* Missing values can cause problems for PCA  
	* Skipping rows with missing values can lead to too little data while also biasing the results  
    * Mean imputation is quick and dirty, but distorts the distribution of the variable  
    * A preferred approach is to estimate each NA as a linear combination of the other parameters  
* The missMDA library can help with these estimations  
	* library(missMDA)  
    * nPCs <- estim_ncpPCA(as.matrix(sleep))  
    * completed_sleep <- imputePCA(sleep, ncp = nPCs$ncp, scale = TRUE)  
    * PCA(completed_sleep$completeObs)  
* Can also impute missing values in the pca() function  
	* library(pcaMethods)  
    * sleep_pca_methods <- pca(sleep, nPcs=2, method="ppca", center = TRUE)  
    * imp_air_pcamethods <- completeObs(sleep_pca_methods)  
  
NNMF and Topic Detection with nmf():  
  
* Non-negative matrix factorization is designed for cases where negative values should never be predicted  
	* Tries to decompose an mxn matrix in to mxr and rxn  
* Dimensionality reduction is critical for sparse data (e.g., text by document)  
* The NMF library implements nmf in R  
	* library(NMF)  
    * bbc_res <- nmf(bbc_tdm, 5)  # sets r=5  
    * W <- basis(bbc_res)  # terms as rows, topic as columns  
    * H <- coef(bbc_res)  # topics as rows, documents as columns  
    * colnames(W) <- c("topic1", "topic2", "topic3", "topic4", "topic5")  
    * W %>% rownames_to_column('words') %>% arrange(. , desc(topic1))%>% column_to_rownames('words')  
  
Example code includes:  
```{r}

data(airquality, package="datasets")
airquality <- airquality[complete.cases(airquality), ]

# Conduct a PCA on the airquality dataset
pca_air <- FactoMineR::PCA(airquality)

# Apply the Kaiser-Guttman rule
summary(pca_air, ncp = 4)

# Perform the screeplot test
factoextra::fviz_screeplot(pca_air, ncp = 5)


data(airquality, package="datasets")

# Conduct a parallel analysis with paran().
air_paran <- paran::paran(airquality[complete.cases(airquality), ])

# Check out air_paran's suggested number of PCs to retain.
air_paran$Retained

# Conduct a parallel analysis.
air_fa_parallel <- psych::fa.parallel(airquality)

# Check out air_fa_parallel's suggested number of PCs to retain.
air_fa_parallel$ncomp


# Check out the summary of airquality
summary(airquality)

# Check out the number of cells with missing values.
sum(is.na(airquality))

# Check out the number of rows with missing values.
nrow(airquality[!complete.cases(airquality), ])

# Estimate the optimal number of dimensions for imputation.
missMDA::estim_ncpPCA(airquality, ncp.max=5)


bbc_res <- readRDS("./RInputFiles/bbc_res.rds")

# Get a 5-rank approximation of corpus_tdm.
# bbc_res <- NMF::nmf(corpus_tdm, 5)

# Get the term-topic matrix W.
W <- NMF::basis(bbc_res)

# Check out the dimensions of W.
dim(W)

# Normalize W.
normal <- function(x) { x / sum(x) }
normal_W <- apply(W, 2, FUN=normal)


# Get the topic-text matrix H.
H <- coef(bbc_res)

# Check out the dimensions of H.
dim(H)

# Normalize H.
normal_H <- apply(H, 2, FUN=normal)


# Explore the nmf's algorithms.
alg <- NMF::nmfAlgorithm()

# Choose the algorithms implemented in R.
R_alg <- NMF::nmfAlgorithm(version="R")

# Use the two-version algorithms.
# bbc_double_opt <- NMF::nmf(x=corpus_tdm, rank=5, method=R_alg, .options="v")

```
  
  
  
***
  
Chapter 3 - Exploratory Factor Analysis  
  
Intro to EFA:  
  
* Variance and covariance are only partially explained by factors  
	* Latent constructs drive observed variables by way of loadings (weightings)  
    * Model accepts that not all variance can be explained by the latent constructs  
* Example of analyzing a single variable  
	* Check for data factorability  
    * Extract factors  
    * Choose the "right" number of factors to retain  
    * Rotate factors  
    * Interpret the results  
* Example of running the process on the bfi dataset from psych  
  
Intro to EFA: Data Factorability:  
  
* Need to check whether the dataset is factorable  
* Bartlett sphericity test - drawback is that it is always significant for large datasets  
	* H0: There is no significant difference between the correlation matrix and the identity matrix of the same dimensionality  
    * H1: There is significant difference betweeen them and, thus, we have strong evidence that there are underlying factors  
    * library(polycor)  
    * bfi_s <- bfi[1:200, 1:25]  
    * bfi_hetcor <- hetcor(bfi_s)  
    * bfi_c <- bfi_hetcor$correlations  
    * bfi_factorability <- cortest.bartlett(bfi_c)  
* Kaiser-Meyer-Olkin (KMO) is also known as the measure of sampling adequacy  
	* KMO(bfi_c)  
    * Should be in the 0.60s to be acceptable  
  
Extraction methods:  
  
* Can extract factors using fa(), with extraction methods including  
	* minres: minimum residual [default] (slightly modified methods: ols, wls, gls)  
    * mle: Maximum Likelihood Estimation (MLE)  
    * paf: Principal Axes Factor (PAF) extraction  
    * minchi: minimum sample size weighted chi square  
    * minrank: minimum rank  
    * alpha: alpha factoring  
* Example of using the minres extraction method  
	* library(psych)  
    * library(GPArotation)  
    * f_bfi_minres <- fa(bfi_c, nfactors = 3, rotate = "none")  
    * f_bfi_minres_common <- sort(f_bfi_minres$communality, decreasing = TRUE)  
    * data.frame(f_bfi_minres_common)  
    * f_bfi_mle <- fa(bfi_c, nfactors = 3, fm = "mle", rotate = "none")  
    * f_bfi_mle_common <- sort(f_bfi_mle$communality, decreasing = TRUE)  
    * data.frame(f_bfi_mle_common)  
  
Choosing the right number of factors:  
  
* Arriving at the right number of factors is not always an easy task  
	* fa.parallel(bfi_c, n.obs = 200, fa = "fa", fm = "minres")  
    * fa.parallel(bfi_c, n.obs = 200, fa = "fa", fm = "mle")  
  
Example code includes:  
```{r cache=TRUE}

hsq <- readr::read_delim("./RInputFiles/humor_dataset.csv", delim=";")
str(hsq, give.attr=FALSE)


# Check out the dimensionality of hsq.
dim(hsq)

# Explore the correlation object hsq_correl.
hsq_correl <- psych::mixedCor(hsq, c=NULL, p=1:32)
str(hsq_correl)

# Getting the correlation matrix of the dataset.
hsq_polychoric <- hsq_correl$poly$rho

# Explore the correlation structure of the dataset.
ggcorrplot::ggcorrplot(hsq_polychoric)


# Apply the Bartlett test on the correlation matrix.
psych::cortest.bartlett(hsq_polychoric)

# Check the KMO index.
psych::KMO(hsq_polychoric)


# EFA with four factors. 
f_hsq <- psych::fa(hsq_polychoric, nfactors=4)

# Inspect the resulting EFA object.
str(f_hsq)

# Use maximum likelihood for extracting factors.
psych::fa(hsq_polychoric, nfactors=4, fm="mle")


# Use PAF on hsq_polychoric.
hsq_correl_pa <- psych::fa(hsq_polychoric, nfactors=4, fm="pa")

# Sort the communalities of the f_hsq_pa.
f_hsq_pa_common <- sort(hsq_correl_pa$communality, decreasing = TRUE)

# Sort the uniqueness of the f_hsq_pa.
f_hsq_pa_unique <- sort(hsq_correl_pa$uniqueness, decreasing = TRUE)


# Check out the scree test and the Kaiser-Guttman criterion.
psych::scree(hsq_polychoric)

# Use parallel analysis for estimation with the minres extraction method.
psych::fa.parallel(hsq_polychoric, n.obs = 1069, fm = "minres", fa = "fa")

# Use parallel analysis for estimation with the mle extraction method.
psych::fa.parallel(hsq_polychoric, n.obs = 1069, fm = "mle", fa = "fa")

```
  
  
  
***
  
Chapter 4 - Advanced EFA  
  
Interpretation of EFA and factor rotation:  
  
* Factor rotation is a standard step for simplifying interpretation  
	* Orthogonal - uncorrelated factors with 90-degree angles (generally good for unrelated factors, and easier to interpret since they are fully orthogonal)  
    * Oblique - uncorrelated factors, but can have slight differences from 90-degree angles (generally good for related factors)  
* Example of the psych::bfi data analysis  
	* f_bfi_varimax <- fa(bfi_c, fm = "minres", nfactors = 5, rotate = "varimax")  
  
Interpretation of EFA and path diagrams:  
  
* Interpretation is an important component of the modeling process  
	* fa.diagram(f_bfi_varimax)  
    * print(f_bfi_varimax$loadings, cut=0)  # cut=0 will show the values between 0.0 and 0.1 (the default would be to exclude the 0.1 and under)  
  
EFA case study:  
  
* The "short dark triad" combines machiavellianism, narcissism, and psychopathy  
* A short version of the test is available at https://openpsychometrics.org/tests/SD3/  
	* sdt_test <- read.csv("SD3.csv", sep = "\t")  
    * dim(sdt_test)  
    * head(sdt_test)  
* General steps for EFA on the data will include  
	* Check for data factorability  
    * Extract factors  
    * Choose the "right" number of factors to retain  
    * Rotate factors  
    * Interpret the results  
  
Wrap up:  
  
* Biggest challenge is in handling large amounts of data - computation, interpretation, etc.  
* PCA and NNMF (positive entries only) are popular choices for many types of datasets  
* EFA for exploratory purposes  
* Common steps in dimensionality reduction  
    * Factor/Component/Dimension extraction  
    * Decision on the number of Factor/Component/Dimension to retain - parsimonious, minimum information loss, easy interpretation  
    * Use visual aid for interpretation (e.g. biplot)  
  
Example code includes:  
```{r}

# Check the default rotation method.
f_hsq$rotation

# Try Promax, another oblique rotation method.
f_hsq_promax <- psych::fa(hsq_polychoric, nfactors=4, rotate="promax")

# Try Varimax, an orthogonal method.
f_hsq_varimax <- psych::fa(hsq_polychoric, nfactors=4, rotate="varimax") 


# Check the factor loadings.
print(f_hsq$loadings, cut=0)

# Create the path diagram of the latent factors.
psych::fa.diagram(f_hsq)


SD3 <- readRDS("./RInputFiles/SD3.RDS")
# SD3_mod <- SD3 %>% mutate_all(factor, levels=1:5)
sdt_sub_correl <- polycor::hetcor(SD3)


# Explore sdt_sub_correl.
str(sdt_sub_correl)

# Get the correlation matrix of the sdt_sub_correl.
sdt_polychoric <- sdt_sub_correl$correlations

# Apply the Bartlett test on the correlation matrix.
psych::cortest.bartlett(sdt_polychoric)

# Check the KMO index.
psych::KMO(sdt_polychoric)


# Check out the scree test.
psych::scree(sdt_polychoric)

# Use parallel analysis for estimation with the minres extraction method.
psych::fa.parallel(sdt_polychoric, n.obs = 100, fa = "fa")

# Perform EFA with MLE. 
f_sdt <- psych::fa(sdt_polychoric, fm = "ml", nfactors = 4)


# Check the factor loadings.
print(f_sdt$loadings, cut=0)

# Create the path diagram of the latent factors.
psych::fa.diagram(f_sdt)

```
  
  
  
***
  
###_Anomaly Detection in R_  
  
Chapter 1 - Statistical Outlier Detection  
  
Meaning of anomalies:  
  
* Anomalies are data points that do not seem to follow the same patterns as the rest of the data  
	* Point anomaly - a single point that is unusual compared with the rest of the data  
    * boxplot(temperature, ylab = "Celsius")  
    * Collective anomaly - series of points that are unusual compared with the rest of the data  
  
Testing extremes with Grubbs' test:  
  
* Grubbs' test assumes data are normally distributed (should be checked prior to running the analysis)  
	* hist(temperature, breaks = 6)  
    * grubbs.test(temperature)  
* Can get the row number that was flagged based on whether Grubbs' test flagged the maximum or the mimimum value  
	* which.max(weights)  
    * which.min(weights)  
  
Anomalies in time series:  
  
* Can begin by visualizing a time series using plot  
	* plot(sales ~ month, data = msales, type = 'o')  
* The Seasonal Hybrid ESD algorithm usies the AnomalyDetection library  
	* library(AnomalyDetection)  
    * sales_ad <- AnomalyDetectionVec(x = msales$sales, period = 12, direction = 'both')  # direction can also be "small" or "large"  
    * sales_ad$anoms  
    * AnomalyDetectionVec(x = msales$sales, period = 12, direction = 'both', plot = T)  # plot the anomalies in blue  
  
Example code includes:  
```{r}

river <- data.frame(index=1:291, 
                    nitrate=c(1.581, 1.323, 1.14, 1.245, 1.072, 1.483, 1.162, 1.304, 1.14, 1.118, 1.342, 1.245, 1.204, 1.14, 1.204, 1.118, 1.025, 1.118, 1.285, 1.14, 0.949, 0.922, 0.949, 1.118, 1.265, 1.095, 1.183, 1.162, 1.118, 1.285, 1.049, 0.922, 0.775, 0.866, 0.922, 1.643, 1.323, 1.285, 1.095, 1.049, 1.095, 0.922, 0.866, 1.049, 0.922, 1.095, 1.183, 1.304, 1.162, 1.225, 1.285, 1.072, 1.533, 1.095, 1.396, 1.025, 0.922, 0.949, 1.118, 1.342, 1.36, 1.36, 1.204, 1.265, 1, 1.183, 1.025, 0.866, 1.072, 1.049, 1.049, 1.049, 1.095, 1.183, 1.095, 0.975, 1.118, 0.975, 1.049, 0.837, 0.922, 1.118, 1.072, 1.204, 0.975, 1.095, 1.049, 0.866, 0.922, 1.049, 1.127, 1.072, 0.975, 1.049, 1.183, 1.245, 1.225, 1.225, 1.265, 1.118, 1.14, 1.072, 1.095, 0.671, 1.183, 0.949, 1.162, 1.095, 1.323, 1.342, 1.277, 1.015, 1, 0.922, 0.894, 1, 1.049, 0.922, 1.517, 1.265, 1.414, 1.304, 1.14, 1.14, 1.049, 1.068, 0.906, 1.095, 0.883, 1.14, 1.025, 1.36, 1.183, 1.265, 1.304, 0.964, 0.975, 0.99, 0.877, 1.049, 0.975, 1, 1.183, 1.225, 1.265, 1.183, 1.049, 0.97, 0.894, 0.98, 0.964, 0.894, 0.922, 1.14, 1.183, 1.897, 1.095, 1.14, 1.414, 1.14, 1, 1.049, 0.889, 0.872, 1, 1.095, 0.671, 1.095, 1.14, 1.304, 1.025, 0.975, 1, 0.877, 0.949, 0.866, 1.058, 1.086, 1.118, 1.162, 1.221, 1.265, 1.122, 1.015, 1.162, 0.825, 0.906, 0.849, 0.985, 1.118, 1.077, 1.237, 1.237, 1.063, 1.01, 0.933, 0.922, 0.806, 0.748, 0.592, 0.911, 0.806, 0.98, 1.077, 1.212, 1.277, 0.954, 0.837, 0.917, 0.9, 1.068, 0.872, 0.99, 1.131, 1.068, 1.208, 1.319, 1.281, 0.905, 0.819, 0.826, 0.974, 0.888, 0.804, 0.996, 1.127, 1.17, 1.166, 1.261, 1.275, 1.179, 1.079, 0.951, 0.852, 0.872, 0.834, 0.859, 1.077, 1.095, 1.285, 1.323, 1.16, 1.125, 0.957, 0.948, 0.907, 0.89, 0.999, 0.999, 0.953, 0.9, 0.986, 1.187, 1.054, 1.079, 0.997, 0.851, 0.803, 0.971, 1.025, 1.086, 1.114, 1.068, 1.091, 1.034, 0.871, 0.781, 0.865, 0.7, 0.673, 0.881, 0.782, 0.97, 1.044, 1.17, 1.196, 1.091, 1.068, 0.967, 0.823, 0.73, 0.693, 0.788, 1.095, 1.183, 0.996, 1.105, 0.939, 0.914, 0.813, 0.775), 
                    month=factor(month.name[c(rep(1:12, times=24), 1:3)], levels=month.name)
                    )
str(river)


# Explore contents of dataset
head(river)

# Summary statistics of river nitrate concentrations
summary(river$nitrate)

# Plot the distribution of nitrate concentration
boxplot(river$nitrate)


# Plot a histogram of the nitrate column
hist(river$nitrate)

# Add a Nitrate concentration label 
hist(river$nitrate, xlab="Nitrate concentration")

# Separate the histogram into 40 bins 
hist(river$nitrate, xlab = "Nitrate concentration", breaks = 40)


# Apply Grubbs' test to the nitrate data
outliers::grubbs.test(river$nitrate)

# Use which.max to find row index of the max
which.max(river$nitrate)

# Runs Grubbs' test excluding row 156
outliers::grubbs.test(river$nitrate[-156])

# Print the value tested in the second Grubbs' test
min(river$nitrate[-156])


# View contents of dataset
head(river)

# Show the time series of nitrate concentrations with time
plot(nitrate ~ month, data = river, type = "o")

# Calculate the mean nitrate by month
monthly_mean <- tapply(river$nitrate, river$month, FUN = mean)
monthly_mean

# Plot the monthly means 
plot(monthly_mean, type = "o", xlab = "Month", ylab = "Monthly mean")

# Create a boxplot of nitrate against months
boxplot(nitrate ~ month, data=river)


# Package 'anomalyDetection' was removed from the CRAN repository.
# Formerly available versions can be obtained from the archive. 
# Archived on 2019-03-01 as check problems were not corrected in time. 

# Run Seasonal-Hybrid ESD for nitrate concentrations
# AnomalyDetectionVec(river$nitrate, period=12, direction = 'both', plot = T)

# Use Seasonal-Hybrid ESD for nitrate concentrations
# river_anomalies <- AnomalyDetectionVec(x = river$nitrate, period = 12, direction = 'both', plot = T)

# Print the anomalies
# river_anomalies$anoms

# Print the plot
# print(river_anomalies$plot)

```
  
  
  
***
  
Chapter 2 - Distance and Density Based Anomaly Detection  
  
k-Nearest-Neighbors Score:  
  
* Example dataset for heights and widths of furniture  
	* plot(Width ~ Height, data = furniture)  
* Anomalies are usually far away from their neighbors  
	* library(FNN)  
    * furniture_knn <- get.knn(data = furniture, k = 5)  
    * get.knn() returns two matrices - "nn.index" "nn.dist"  
    * head(furniture_knn$nn.dist, 3)  # distance matrix  
    * furniture_score <- rowMeans(furniture_knn$nn.dist)  # average distance to 5 nearest neighbors  
  
Visualizing kNN distance:  
  
* Can be important to standardize distances prior to kNN (unless it is desirable that a single variable dominate the scoring)  
	* furniture_scaled <- scale(furniture)  
    * furniture_scaled <- scale(furniture)  
    * furniture_knn <- get.knn(furniture_scaled, 5)  
    * furniture$score <- rowMeans(furniture_knn$nn.dist)  
    * plot(Width ~ Height, cex = sqrt(score), data = furniture, pch = 20)  
  
Local outlier factor (LOF):  
  
* LOF uses density rather than distance to set up a distance for points - score is a ratio that tends to be near 1  
	* kNN is helpful for finding global anomalies, while LOF is helpful for finding local anomalies  
    * library(dbscan)  
    * furniture_lof <- lof(scale(furniture), k = 5)  
* LOF is the density around the point relative to the density around the kNN  
	* LOF > 1 is more likely to be anomalous while LOF < 1 is much less likely to be anomalous  
    * furniture$score_lof <- furniture_lof  
    * plot(Width ~ Height, data = furniture, cex = score_lof, pch = 20)  
  
Example code includes:  
```{r}

wineOrig <- readr::read_csv("./RInputFiles/big_wine.csv")
str(wineOrig, give.attr=FALSE)
wine <- wineOrig %>% select(pH, alcohol)

# View the contents of the wine data
head(wine)

# Scatterplot of wine pH against alcohol
plot(pH ~ alcohol, data = wine)


# Calculate the 5 nearest neighbors distance
wine_nn <- FNN::get.knn(wine, k = 5)

# View the distance matrix
head(wine_nn$nn.dist)

# Distance from wine 5 to nearest neighbor
wine_nn$nn.dist[5, 1]

# Row index of wine 5's nearest neighbor 
wine_nn$nn.ind[5, 1]

# Return data for wine 5 and its nearest neighbor
wine[c(5, wine_nn$nn.ind[5, 1]), ]

# Create score by averaging distances
wine_nnd <- rowMeans(wine_nn$nn.dist)

# Print row index of the most anomalous point
which.max(wine_nnd)


# Observe differences in column scales 
summary(wine)

# Standardize the wine columns
wine_scaled <- scale(wine)

# Observe standardized column scales
summary(wine_scaled)


# Print the 5-nearest neighbor distance score
wine_nnd[1:5]

# Add the score as a new column 
wine$score <- wine_nnd


# Scatterplot showing pH, alcohol and kNN score
plot(pH ~ alcohol, data=wine, cex = sqrt(score), pch = 20)


# Calculate the LOF for wine data
wine$score <- NULL
wine_lof <- dbscan::lof(scale(wine), k=5)

# Append the LOF score as a new column
wine$score <- wine_lof


# Scatterplot showing pH, alcohol and LOF score
plot(pH ~ alcohol, data=wine, cex=score, pch=20)


# Calculate and append kNN distance as a new column
wine_nn <- FNN::get.knn(wine_scaled, k = 10)
wine$score_knn <- rowMeans(wine_nn$nn.dist)     

# Calculate and append LOF as a new column
wine$score_lof <- dbscan::lof(wine_scaled, k = 10)

# Find the row location of highest kNN
which.max(wine$score_knn)

# Find the row location of highest LOF
which.max(wine$score_lof)

```
  
  
  
***
  
Chapter 3 - Isolation Forest  
  
Isolation Trees:  
  
* Example of deer organizing in to a pack and predators trying to find isolated deer to attack  
	* Points that are more easily separated from other points are considered to be more anomalous  
    * Keep splitting data until every point lies within its own sub-region OR there are fewer than n points in each sub-region  
* Example of running the isolation forest  
	* library(isofor)  
    * furniture_tree <- iForest(data = furniture, nt = 1)  # nt is the number of trees to be grown  
    * furniture_score <- predict(furniture_tree, newdata = furniture)  # generates the isolation score  
* The isolation score is the average number of random splits needed to isolate a point  
	* The score returned by predict is normalized, with scores near 1 more likely to be anomalies (short path length, easy to split)  
  
Isolation Forest:  
  
* Can use sampling to build multiple trees (called an isolation forest, with score averaged over trees)  
	* furniture_tree <- iForest(data = furniture, nt = 1, phi = 100)  
    * furniture_forest <- iForest(data = furniture, nt = 100)  
* The forest of many trees is more robust and faster to grow  
	* The anomaly score should generally converge after "sufficient" trees have been run (100 is a typical default)  
    * Scores should largely be stable if more or less trees are added  
    * plot(trees_500 ~ trees_1000, data = furniture_scores)  
    * abline(a = 0, b = 1)  
  
Visualizing Isolation Scores:  
  
* Can visualize the isolation score using the contour plot  
	* h_seq <- seq(min(furniture$Height), max(furniture$Height), length.out = 20)  
    * w_seq <- seq(min(furniture$Width), max(furniture$Width), length.out = 20)  
    * furniture_grid <- expand.grid(Width = w_seq, Height = h_seq)  
    * furniture_grid$score <- predict(furniture_forest, furniture_grid)  
    * library(lattice)  
    * contourplot(score ~ Height + Width, data = furniture_grid, region = TRUE)  
  
Example code includes:  
```{r}

wine <- wine %>% select(pH, alcohol)
str(wine, give.attr=FALSE)


# CRAN - package 'isofor' is not available (for R version 3.5.1)

# Build an isolation tree 
# wine_tree <- iForest(wine, nt = 1)

# Create isolation score
# wine$tree_score <- predict(wine_tree, newdata = wine)

# Histogram plot of the scores
# hist(wine$tree_score, breaks=40)


# Fit isolation forest
# wine_forest <- iForest(wine, nt=100)

# Fit isolation forest
# wine_forest <- iForest(wine, nt = 100, phi = 200)

# Create isolation score from forest
# wine_score <- predict(wine_forest, newdata=wine)

# Append score to the wine data
# wine$score <- wine_score


# View the contents of the wine scores
# head(wine_scores)

# Score scatterplot 2000 vs 1000 trees 
# plot(trees_2000 ~ trees_1000, data = wine_scores)

# Add reference line of equality
# abline(a = 0, b = 1)


# Sequence of values for pH and alcohol
ph_seq <- seq(min(wine$pH), max(wine$pH), length.out = 25)
alcohol_seq <- seq(min(wine$alcohol), max(wine$alcohol) , length.out = 25)

# Create a data frame of grid coordinates
wine_grid <- expand.grid(pH = ph_seq, alcohol = alcohol_seq)

# Plot the grid
plot(pH ~ alcohol, data=wine_grid, pch = 20)


# Calculate isolation score at grid locations
# wine_grid$score <- predict(wine_forest, newdata=wine_grid)


# Contour plot of isolation scores
# contourplot(score ~ alcohol + pH, data=wine_grid, region = TRUE)

```
  
  
  
***
  
Chapter 4 - Comparing Performance  
  
Labeled Anomalies:  
  
* Sometimes, the anomalies are already labelled, allowing for supervised learning  
	* table(sat$label)  
    * plot(V2 ~ V3, data = sat, col = as.factor(label), pch = 20)  
    * sat_for <- iForest(sat[, -1], nt = 100)  
    * sat$score <- predict(sat_for, features)  
    * boxplot(score ~ label, data = sat, col = "olivedrab4")  
* Challenges with modeling can include  
	* Detecting rare cases  
    * Rapidly changing exploits (e.g., card fraud)  
  
Measuring Performance:  
  
* Decision threshholds are sometimes based off quantiles; flag a percentage of the data as anomalous  
	* high_score <- quantile(sat$score, probs = 0.99)  
    * sat$binary_score <- as.numeric(score >= high_score)  
    * table(sat$label, sat$binary_score)  
* Can calculate recall as correctly detected anomalies divided by total actual anomalies (1 means all anomalies detected)  
* Can calculate precision as correctly detected anomalies divided by total predicted anomalies (1 means no false detections)  
  
Working with Categorical Features:  
  
* Begin by checking for any non-numeric features - fct or chr classes  
	* sapply(X = sat, FUN = class)  
* The isolation forest can take factor variables (but not character variables directly) provided they have at most 32 unique levels  
	* sat$high_low <- as.factor(sat$high_low)  
    * class(sat$high_low)  
    * sat_for <- iForest(sat[, -1], nt = 100)  
* Can run LOF with factors using the Gower distance (all distances are standardized to be between 0 and 1) from the daisy package  
	* library(cluster)  
    * sat_dist <- daisy(sat[, -1], metric = "gower")  
    * sat_lof <- lof(sat_dist, k = 10)  
    * sat_distmat <- as.matrix(sat_dist)  
    * range(sat_distmat)  
  
Wrap Up:  
  
Example code includes:  
```{r}

thyroidOrig <- readr::read_csv("./RInputFiles/thyroid.csv")
str(thyroidOrig, give.attr=FALSE)
thyroid <- thyroidOrig


# View contents of thryoid data
head(thyroid)

# Tabulate the labels
table(thyroid$label)

# Proportion of thyroid cases
prop_disease <- mean(thyroid$label)


# Plot of TSH and T3
plot(TSH ~ T3, data=thyroid, pch=20)

# Plot of TSH, T3 and labels
plot(TSH ~ T3, data = thyroid, pch = 20, col = label + 1)

# Plot of TT4, TBG and labels
plot(TT4 ~ TBG, data = thyroid, pch = 20, col = label + 1)


# Package isofor not available on CRAN
# Fit isolation forest
# thyroid_forest <- isofor::iForest(thyroid[, -1], nt = 200)

# Anomaly score 
# thyroid$iso_score <- predict(thyroid_forest, thyroid[, -1])

# Boxplot of the anomaly score against labels
# boxplot(iso_score ~ label, data=thyroid, col = "olivedrab4")


# Create a LOF score for thyroid
lof_score <- dbscan::lof(scale(thyroid[, -1]), k = 10)
                 
# Calculate high threshold for lof_score
high_lof <- quantile(lof_score, probs = 0.98) 

# Append binary LOF score to thyroid data
thyroid$binary_lof <- as.numeric(lof_score >= high_lof)


iso_score <- c(394, 442, 408, 369, 431, 420, 398, 374, 384, 452, 478, 461, 356, 357, 405, 437, 357, 366, 488, 671, 395, 367, 346, 387, 354, 386, 411, 548, 423, 344, 355, 459, 413, 389, 373, 360, 520, 382, 690, 676, 388, 486, 530, 561, 423, 409, 352, 441, 395, 416, 367, 377, 426, 418, 378, 357, 422, 431, 526, 380, 450, 434, 462, 360, 529, 382, 390, 371, 385, 382, 367, 416, 384, 400, 377, 391, 380, 403, 361, 355, 418, 498, 649, 465, 413, 377, 383, 375, 422, 360, 353, 380, 569, 430, 377, 418, 374, 413, 369, 378, 456, 357, 559, 375, 370, 543, 410, 548, 380, 382, 362, 390, 460, 438, 392, 742, 665, 400, 393, 382, 382, 511, 375, 363, 422, 399, 358, 448, 399, 450, 392, 369, 435, 437, 375, 529, 370, 369, 429, 532, 485, 439, 429, 363, 366, 480, 408, 622, 358, 489, 520, 393, 388, 431, 378, 400, 400, 354, 405, 388, 416, 442, 382, 348, 347, 375, 366, 397, 467, 518, 387, 376, 353, 369, 442, 380, 391, 390, 358, 401, 409, 414, 452, 377, 362, 360, 380, 381, 412, 412, 418, 381, 432, 391, 448, 395, 418, 509, 525, 398, 432, 359, 499, 444, 383, 405, 467, 418, 721, 399, 421, 527, 481, 371, 364, 459, 398, 373, 388, 434, 428, 439, 381, 405, 352, 363, 352, 403, 362, 396, 367, 365, 432, 392, 396, 367, 404, 384, 381, 364, 366, 376, 369, 379, 379, 426, 401, 380, 404, 394, 368, 361, 393, 455, 396, 540, 368, 360, 466, 365, 377, 411, 442, 408, 373, 394, 344, 352, 345, 344, 346, 344, 378, 366, 401, 436, 366, 367, 382, 356, 362, 402, 405, 376, 368, 381, 371, 391, 359, 707, 367, 370, 387, 385, 373, 354, 354, 362, 358, 364, 353, 365, 374, 385, 395, 362, 461, 374, 362, 456, 405, 426, 385, 387, 387, 375, 503, 378, 370, 358, 377, 461, 357, 353, 346, 350, 393, 456, 425, 418, 371, 380, 477, 383, 382, 349, 360, 412, 395, 409, 441, 371, 420, 455, 358, 654, 365, 507, 508, 443, 364, 381, 468, 368, 362, 454, 381, 357, 432, 374, 379, 383, 389, 367, 393, 424, 378, 361, 512, 449, 522, 352, 354, 367, 359, 396, 486, 367, 409, 427, 351, 381, 357, 362, 369, 364, 356, 730, 353, 399, 383, 523, 429, 425, 420, 455, 414, 475, 433, 528, 425, 476, 352, 350, 413, 443, 435, 381, 472, 486, 376, 402, 361, 377, 391, 380, 355, 400, 394, 353, 379, 376, 469, 398, 464, 388, 378, 397, 396, 521, 417, 365, 420, 377, 350, 407, 364, 368, 426, 344, 351, 411, 412, 502, 381, 495, 350, 350, 344, 362, 389, 388, 370, 354, 394, 363, 564, 549, 387, 378, 411, 421, 427, 382, 385, 496, 372, 416, 365, 375, 406, 355, 362, 442, 410, 477, 361, 379, 386, 375, 351, 351, 360, 360, 412, 400, 409, 458, 351, 376, 400, 360, 499, 362, 476, 396, 407, 437, 358, 385, 373, 432, 353, 352, 369, 405, 376, 383, 462, 375, 361, 395, 426, 431, 418, 500, 585, 616, 372, 529, 418, 456, 360, 429, 397, 366, 384, 359, 515, 401, 389, 429, 371, 357, 398, 380, 371, 354, 403, 355, 356, 368, 363, 481, 545, 367, 350, 345, 344, 344, 426, 438, 464, 365, 460, 462, 419, 358, 428, 433, 352, 384, 416, 387) 
iso_score <- c(iso_score, 384, 366, 367, 487, 628, 638, 472, 349, 351, 351, 420, 422, 347, 347, 422, 396, 443, 419, 509, 507, 398, 375, 427, 492, 388, 387, 354, 390, 439, 358, 392, 379, 361, 392, 375, 407, 663, 442, 390, 437, 432, 420, 397, 413, 477, 494, 522, 354, 354, 357, 381, 384, 412, 406, 411, 448, 508, 411, 352, 345, 511, 386, 364, 396, 476, 389, 355, 464, 363, 380, 366, 423, 396, 407, 415, 504, 440, 406, 449, 394, 432, 397, 428, 434, 401, 363, 395, 404, 392, 454, 357, 380, 352, 382, 389, 389, 345, 348, 461, 390, 371, 345, 442, 402, 386, 375, 382, 382, 404, 373, 586, 426, 600, 368, 382, 358, 407, 379, 402, 367, 366, 385, 706, 352, 384, 363, 486, 366, 433, 373, 397, 434, 402, 378, 376, 376, 359, 380, 363, 351, 543, 435, 385, 503, 359, 353, 365, 405, 457, 345, 463, 445, 363, 353, 369, 370, 355, 681, 439, 360, 417, 383, 376, 416, 428, 386, 426, 420, 462, 370, 367, 398, 373, 354, 418, 364, 357, 420, 628, 442, 403, 478, 370, 367, 399, 413, 453, 423, 376, 385, 415, 447, 349, 551, 390, 438, 384, 401, 458, 526, 449, 480, 405, 388, 391, 361, 362, 387, 429, 391, 413, 391, 380, 511, 411, 376, 374, 436, 362, 434, 437, 517, 397, 406, 372, 345, 345, 345, 347, 423, 639, 373, 397, 358, 369, 399, 464, 453, 406, 358, 350, 395, 386, 454, 396, 373, 394, 444, 377, 376, 459, 393, 353, 349, 685, 382, 419, 394, 446, 346, 442, 426, 390, 422, 568, 365, 443, 353, 513, 364, 349, 373, 422, 389, 509, 411, 443, 375, 438, 556, 349, 445, 446, 413, 455, 419, 385, 358, 381, 375, 372, 497, 589, 386, 493, 539, 769, 351, 511, 456, 373, 378, 411, 523, 448, 400, 368, 428, 381, 444, 378, 402, 377, 411, 367, 446, 374, 435, 429, 409, 399, 349, 360, 468, 400, 366, 372, 446, 384, 524, 348, 384, 371, 381, 347, 357, 369, 359, 405, 390, 363, 419, 469, 410, 413, 365, 377, 482, 398, 347, 467, 446, 442, 399, 367, 502, 424, 452, 364, 372, 355, 386, 399, 399, 370, 409, 412, 409, 396, 380, 446, 470, 375, 386, 454, 350, 514, 396, 411, 402, 360, 458, 439, 349, 345, 398, 368, 378, 355, 528, 384, 397, 543, 410, 370, 389, 506, 412, 454, 442, 602, 383, 367, 377, 489, 371, 471, 361, 366, 355, 508, 368, 390, 368, 375, 406, 512, 374, 380, 378, 344, 381, 400, 544, 375, 527, 390, 398, 455, 393, 427, 435, 512, 379, 367, 380)
iso_score <- iso_score / 1000


# Calculate high threshold for iso_score
high_iso <- quantile(iso_score, probs=0.98)  

# Append binary isolation score to thyroid data
thyroid$binary_iso <- as.numeric(iso_score >= high_iso)         


# Tabulate agreement of label and binary isolation score 
table(thyroid$label, thyroid$binary_iso)

# Tabulate agreement of label and binary LOF score 
table(thyroid$label, thyroid$binary_lof)

# Proportion of binary_iso and label that agree
iso_prop <- mean(thyroid$label == thyroid$binary_iso)

# Proportion of binary_lof and label that agree
lof_prop <- mean(thyroid$label == thyroid$binary_lof)


table(thyroid$label, thyroid$binary_iso)
table(thyroid$label, thyroid$binary_lof)

# Precision for binary scores
precision_iso <- sum(thyroid$label == 1 & thyroid$binary_iso == 1) / sum(thyroid$binary_iso == 1)
precision_lof <- sum(thyroid$label == 1 & thyroid$binary_lof == 1) / sum(thyroid$binary_lof == 1)

# Recall for binary scores
recall_iso <- sum(thyroid$label == 1 & thyroid$binary_iso == 1) / sum(thyroid$label == 1)
recall_lof <- sum(thyroid$label == 1 & thyroid$binary_lof == 1) / sum(thyroid$label == 1)


age <- c('35-60', '0-35', '35-60', '60+', '0-35', '0-35', '0-35', '0-35', '35-60', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '60+', '35-60', '60+', '35-60', '35-60', '60+', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '60+', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '0-35', '0-35', '35-60', '0-35', '60+', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '35-60', '60+', '60+', '35-60', '35-60', '0-35', '0-35', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '60+', '0-35', '0-35', '60+', '35-60', '0-35', '35-60', '0-35', '0-35', '60+', '0-35', '0-35', '0-35', '35-60', '60+', '35-60', '35-60', '35-60', '60+', '0-35', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '35-60', '60+', '35-60', '60+', '60+', '0-35', '35-60', '60+', '60+', '0-35', '60+', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '0-35', '35-60', '35-60', '60+', '60+', '60+', '0-35', '60+', '0-35', '0-35', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '0-35', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '60+', '60+', '0-35', '0-35', '0-35', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '35-60', '60+', '0-35', '60+', '60+', '60+', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '35-60', '60+', '60+', '60+', '0-35', '0-35', '0-35', '0-35', '35-60', '60+', '35-60', '35-60', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '60+', '0-35', '60+', '60+', '60+', '0-35', '60+', '60+', '60+', '60+', '35-60', '0-35', '60+', '60+', '35-60', '60+', '0-35', '0-35', '60+', '60+', '60+', '0-35', '0-35', '35-60', '60+', '60+', '35-60', '35-60', '35-60', '60+', '0-35', '60+', '0-35', '60+', '35-60', '60+', '60+', '0-35', '35-60', '35-60', '0-35', '35-60', '60+', '0-35', '60+') 
age <- c(age, '60+', '60+', '0-35', '60+', '35-60', '0-35', '0-35', '60+', '35-60', '35-60', '0-35', '60+', '60+', '0-35', '60+', '35-60', '0-35', '35-60', '0-35', '35-60', '0-35', '60+', '0-35', '60+', '60+', '0-35', '60+', '60+', '60+', '60+', '60+', '0-35', '0-35', '60+', '0-35', '60+', '0-35', '60+', '60+', '35-60', '35-60', '60+', '60+', '60+', '60+', '60+', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '60+', '35-60', '60+', '0-35', '0-35', '0-35', '0-35', '35-60', '60+', '35-60', '0-35', '60+', '60+', '0-35', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '60+', '0-35', '0-35', '60+', '35-60', '60+', '0-35', '60+', '0-35', '60+', '35-60', '0-35', '35-60', '0-35', '60+', '60+', '0-35', '60+', '60+', '60+', '35-60', '35-60', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '60+', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '60+', '35-60', '35-60', '0-35', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '60+', '0-35', '60+', '0-35', '35-60', '0-35', '60+', '60+', '60+', '35-60', '60+', '60+', '35-60', '60+', '0-35', '0-35', '0-35', '60+', '60+', '60+', '35-60', '0-35', '0-35', '35-60', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '60+', '0-35', '35-60', '0-35', '0-35', '0-35', '0-35', '60+', '35-60', '60+', '35-60', '0-35', '60+', '35-60', '35-60', '60+', '35-60', '0-35', '60+', '60+', '35-60', '0-35', '60+', '35-60', '60+', '0-35', '60+', '0-35', '35-60', '0-35', '0-35', '35-60', '35-60', '0-35', '35-60', '60+', '35-60', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '0-35', '0-35', '60+', '0-35', '60+', '0-35', '60+', '60+', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '35-60', '35-60', '35-60', '0-35', '60+', '35-60', '60+', '60+', '60+', '0-35', '0-35', '35-60', '0-35', '0-35', '0-35', '60+', '60+', '60+', '0-35', '0-35', '0-35', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '60+', '0-35', '60+', '35-60', '0-35', '35-60', '60+', '35-60', '0-35', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '60+', '35-60', '60+', '35-60', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '60+', '60+', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '35-60', '60+', '0-35', '35-60', '35-60', '0-35', '35-60', '60+', '35-60', '35-60', '60+', '60+', '35-60', '0-35', '60+', '35-60', '0-35', '35-60', '35-60', '60+', '35-60', '60+', '60+', '60+', '60+', '0-35', '0-35', '60+', '35-60', '0-35', '60+', '35-60', '60+', '60+', '60+', '35-60', '0-35', '0-35', '60+', '0-35', '35-60', '35-60', '35-60', '60+', '0-35', '35-60', '60+', '35-60', '0-35', '60+', '0-35', '35-60')
age <- c(age, '60+', '35-60', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '35-60', '60+', '60+', '60+', '60+', '60+', '0-35', '60+', '60+', '35-60', '60+', '60+', '60+', '0-35', '60+', '35-60', '60+', '35-60', '35-60', '0-35', '35-60', '0-35', '35-60', '0-35', '35-60', '60+', '0-35', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '60+', '0-35', '0-35', '0-35', '35-60', '35-60', '60+', '35-60', '35-60', '35-60', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '35-60', '60+', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '0-35', '60+', '60+', '35-60', '0-35', '60+', '35-60', '60+', '60+', '0-35', '0-35', '35-60', '0-35', '35-60', '60+', '0-35', '0-35', '0-35', '60+', '35-60', '60+', '60+', '35-60', '35-60', '60+', '0-35', '60+', '60+', '60+', '35-60', '60+', '60+', '60+', '0-35', '60+', '60+', '0-35', '0-35', '0-35', '60+', '0-35', '0-35', '0-35', '60+', '60+', '60+', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '35-60', '60+', '60+', '35-60', '35-60', '0-35', '60+', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '0-35', '60+', '0-35', '0-35', '0-35', '0-35', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '0-35', '60+', '60+', '35-60', '35-60', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '0-35', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '60+', '60+', '0-35', '35-60', '35-60', '60+', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '60+', '0-35', '0-35', '60+', '0-35', '35-60', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '60+', '0-35', '0-35', '60+', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '60+', '60+', '60+', '35-60', '35-60', '0-35', '60+', '60+', '35-60', '60+', '60+', '60+', '35-60', '0-35', '35-60', '60+', '60+', '35-60', '0-35', '60+', '0-35', '35-60', '35-60', '60+', '60+', '35-60', '0-35', '60+', '60+', '35-60', '35-60', '0-35', '35-60', '35-60', '0-35', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '60+', '0-35', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '35-60', '35-60', '0-35', '0-35', '35-60', '35-60', '60+', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '35-60', '35-60', '60+', '60+', '35-60', '0-35', '0-35', '60+', '60+', '0-35', '0-35', '60+', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '0-35', '35-60', '60+', '0-35', '35-60', '60+', '60+', '35-60', '60+', '60+', '35-60', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '0-35', '60+', '35-60', '0-35', '0-35', '0-35', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '35-60', '60+', '0-35', '35-60', '60+', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '35-60', '60+', '0-35', '35-60', '0-35', '60+')

sex <- c('F', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F')
sex <- c(sex, 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F')


thyroid$age <- age
thyroid$sex <- sex

# Print the column classes in thyroid
sapply(X = thyroid, FUN = class)

# Convert column with character class to factor
thyroid$age <- as.factor(thyroid$age)
thyroid$sex <- as.factor(thyroid$sex)

# Check that all columns are factor or numeric
sapply(X = thyroid, FUN = class)


# Check the class of age column
class(thyroid$age)

# Check the class of sex column
class(thyroid$sex)

# Fit an isolation forest with 100 trees
# thyroid_for <- iForest(thyroid[, -1], nt=100)


# Calculate Gower's distance matrix
thyroid_dist <- cluster::daisy(thyroid[, -1], metric = "gower")

# Generate LOF scores for thyroid data
thyroid_lof <- dbscan::lof(thyroid_dist, k = 10)

# Range of values in the distance matrix
range(as.matrix(thyroid_dist))

```
  
  
  
***
  
###_GARCH Models in R_  
  
Chapter 1 - Standard GARCH Model as the Workhorse  
  
Analyzing volatility:  
  
* GARCH models in R help maximize returns while properly managing risk  
	* Relative gains are key; gain divided by starting point  
* Properties of daily returns include that average return is zero and variability changes over time  
	* sd(sp500ret)  # daily  
    * sqrt(252)*sd(sp500ret)  # annualized, assuming 252 trading days  
* Can use rolling windows to estimate how variability is changing over time - commonly, window is a multiple of 22 (trading days in a month)  
	* library(PerformanceAnalytics)  
    * chart.RollingPerformance(R = sp500ret , width = 22, FUN = "sd.annualized", scale = 252, main = "Rolling 1 month volatility")  
* The GARCH model helps drive greater precision and accuracy than the simple rolling window  
  
GARCH equation for volatility prediction:  
  
* GARCH models attempt to model forward-looking volatility  
	* Input is a time series of returns  
    * May want to predict a future return based on all current and previous returns - arithmeitc mean or ARMA  
    * Can also predict the future volatility based on previous returns and volatilities - weighted average (more recent weighted more highly) most recent prediction errors   
* ARCH is the overall model while GARCH(1, 1) is a generalized form of the model  
	* omega, alpha, beta should all be positive  
    * alpha plus beta must be less than 1 (meaning that variance always reverts to its long-run average)  
* Example of implementing GARCH variance using R (loop starting at 2 because of the lagged predictor)  
    * alpha <- 0.1  
    * beta <- 0.8  
    * omega <- var(sp500ret)*(1-alpha-beta)  # Then: var(sp500ret) = omega/(1-alpha-beta)  
    * e <- sp500ret - mean(sp500ret) # Constant mean  
    * e2 <- e^2  
    * nobs <- length(sp500ret)  
    * predvar <- rep(NA, nobs)  
    * predvar[1] <- var(sp500ret)  # Initialize the process at the sample variance  
    * for (t in 2:nobs){  
    *     predvar[t] <- omega + alpha * e2[t - 1] + beta * predvar[t-1]  # GARCH(1,1) equation  
    * }  
    * predvol <- sqrt(predvar)  # Volatility is sqrt of predicted variance  
    * predvol <- xts(predvol, order.by = time(sp500ret))  
    * uncvol <- sqrt(omega / (1 - alpha-beta))  # We compare with the unconditional volatility  
    * uncvol <- xts(rep(uncvol, nobs), order.by = time(sp500ret))  
  
rugarch package:  
  
* The normal GARCH(1, 1) model with a constant mean is a starting point  
	* Can use maximum likelihood to estimate mean, omega, alpha, and beta  
    * library(rugarch)  
    * garchspec <- ugarchspec( mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH"), distribution.model = "norm")  # first step is ugarchspec  
    * garchfit <- ugarchfit(data = sp500ret , spec = garchspec)  # ugarchfit is the second step  
    * garchforecast <- ugarchforecast(fitORspec = garchfit, n.ahead = 5)  # ugarchforecast is final step  
* Can access the results in a ugarchfit object  
	* The ugarchfit yields an object that contains all the results related to the estimation of the garch model  
    * Methods coef, uncvar, fitted and sigma:  
    * garchcoef <- coef(garchfit)  
    * garchuncvar <- uncvariance(garchfit)  
    * garchmean <- fitted(garchfit)  
    * garchvol <- sigma(garchfit)  
    * sigma(garchforecast)   
    * fitted(garchforecast)   
* Frequently, portfolios are set to a target volatility, with a portion of the portfolio held in cash to manage that  
  
Example code includes:  
```{r eval=FALSE}

library(xts)
library(PerformanceAnalytics)


load("./RInputFiles/sp500prices.RData")
str(sp500prices)


# Plot daily S&P 500 prices
plot(sp500prices)

# Compute daily returns
sp500ret <- CalculateReturns(sp500prices)

# Check the class of sp500ret
class(sp500ret)

# Plot daily returns
plot(sp500ret)


# Compute the daily standard deviation for the complete sample   
sd(sp500ret)

# Compute the annualized volatility for the complete sample
sd(sp500ret) * sqrt(252)

# Compute the annualized standard deviation for the year 2009 
sqrt(252) * sd(sp500ret["2009"])

# Compute the annualized standard deviation for the year 2017 
sqrt(252) * sd(sp500ret["2017"])


# Showing two plots onthe same figure
par(mfrow=c(2,1)) 

# Compute the rolling 1 month estimate of annualized volatility
chart.RollingPerformance(R = sp500ret["2000::2017"], width = 22,
     FUN = "sd.annualized", scale = 252, main = "One month rolling volatility")

# Compute the rolling 3 months estimate of annualized volatility
chart.RollingPerformance(R = sp500ret["2000::2017"], width = 66,
     FUN = "sd.annualized", scale = 252, main = "Three months rolling volatility")

par(mfrow=c(1,1)) 


sp500ret <- sp500ret[2:length(sp500ret), ]

# Compute the mean daily return
m <- mean(sp500ret)

# Define the series of prediction errors
e <- sp500ret - m

# Plot the absolute value of the prediction errors
par(mfrow = c(2,1), mar = c(3, 2, 2, 2))
plot(abs(e))

# Plot the acf of the absolute prediction errors
acf(abs(e))
par(mfrow = c(1,1), mar = c(5.1, 4.1, 4.1, 2.1))


nobs <- length(sp500ret)
predvar <- numeric(nobs)
omega <- 1.2086e-05
alpha <- 0.1
beta <- 0.8
e2 <- e**2

# Compute the predicted variances
predvar[1] <- var(sp500ret) 
for(t in 2:nobs){
   predvar[t] <- omega + alpha * e2[t-1] + beta * predvar[t-1]
}

# Create annualized predicted volatility
ann_predvol <- xts(sqrt(predvar) * sqrt(252), order.by = time(sp500ret))

# Plot the annual predicted volatility in 2008 and 2009
plot(ann_predvol["2008::2009"], main = "Ann. S&P 500 vol in 2008-2009")


# Specify a standard GARCH model with constant mean
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0, 0)),
                 variance.model = list(model = "sGARCH"), 
                 distribution.model = "norm")

# Estimate the model
garchfit <- rugarch::ugarchfit(data = sp500ret, spec = garchspec)

# Use the method sigma to retrieve the estimated volatilities 
garchvol <- rugarch::sigma(garchfit) 

# Plot the volatility for 2017
plot(garchvol["2017"])


# Compute unconditional volatility
sqrt(rugarch::uncvariance(garchfit))

# Print last 10 ones in garchvol
tail(garchvol, 10)

# Forecast volatility 5 days ahead and add 
garchforecast <- rugarch::ugarchforecast(fitORspec = garchfit, n.ahead = 5)

# Extract the predicted volatilities and print them
print(rugarch::sigma(garchforecast))


# Compute the annualized volatility
annualvol <- sqrt(252) * rugarch::sigma(garchfit)

# Compute the 5% vol target weights  
vt_weights <- 0.05 / annualvol

# Compare the annualized volatility to the portfolio weights in a plot
plot(merge(annualvol, vt_weights), multi.panel = TRUE)

```
  
  
  
***
  
Chapter 2 - Improvements of the Normal GARCH Model  
  
Non-normality of standardized returns:  
  
* Normal distributions are generally inconsistent with stock markets - "stairs up and elevators down"  
	* garchspec <- ugarchspec( mean.model=list(armaOrder=c(0,0)), variance.model=list(model="sGARCH"), distribution.model = "sstd")  
    * Caveat: The normality of the standardized returns follows from an assumption  
    * Let's compute the standardized returns and test whether the assumption is correct.  
    * stdret <- residuals(garchfit, standardize = TRUE)  
    * chart.Histogram(sp500ret, methods = c("add.normal", "add.density"), colorset=c("gray","red","blue"))  
* A more realistic distribution needs fat tails and skew  
	* The shape (nu) determines the fatness of the tails  
    * The skew parameter of 1 is symmetry while less than 1 is negative skew  
    * garchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH"), distribution.model = "sstd")  
  
Leverage effect:  
  
* Volatility predictions are unsigned, though the sign of the error matters (large negative returns drive more volatility than large positive returns)  
	* In the case of a negative surprise, it is common to scale-up the squared prediction error - alpha becomes (alpha + gamma) with gamma being positive  
    * GJR model proposed Glosten, Jagannathan and Runkle.  
    * garchspec <- ugarchspec( mean.model=list(armaOrder=c(0,0)), variance.model=list(model="gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * out <- newsimpact(garchfit)  
    * plot(out$zx, out$zy, xlab = "prediction error", ylab = "predicted variance")  
  
Mean model:  
  
* Higher returns come with greater risks; can quantify these using GARCH mean models  
	* The lambda parameter is typically a positive parameter that relates increases in volatility to increases in return  
    * garchspec <- ugarchspec( mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit( data = sp500ret , spec = garchspec)  
    * plot(fitted(garchfit))  
* The AR(1) model is a common approach used to drive correlation in returns - can be a negative or a positive parameter  
	* garchspec <- ugarchspec( mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = sp500ret, spec = garchspec)  
    * round(coef(garchfit)[1:2], 4)  
* The MA(1) and ARMA(1, 1) are also popular models  
	* garchspec <- ugarchspec( mean.model = list(armaOrder = c(0,1)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchspec <- ugarchspec( mean.model = list(armaOrder = c(1,1)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
  
Avoid unnecessary complexity:  
  
* Complexity has a price including loss of parsimony; avoid unneeded complexity  
* Can use setfixed() or setbounds() to set boundaries for the data  
	* setfixed(garchspec) <- list(alpha1 = 0.05, shape = 6)  
    * garchfit <- ugarchfit(data = EURUSDret, spec = garchspec)  
    * setbounds(garchspec) <- list("alpha1" = c(0.05,0.2), "beta1" = c(0.8,0.95))  
* Can also use variance targeting  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH", variance.targeting = TRUE), distribution.model = "std")  
    * garchfit <- ugarchfit(data = EURUSDret, spec = garchspec)  
  
Example code includes:  
```{r eval=FALSE}

load("./RInputFiles/ret.RData")
str(ret)


# Plot the return series
plot(ret)

# Specify the garch model to be used
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                 variance.model = list(model = "sGARCH"),
                                 distribution.model = "sstd"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data = ret, spec = garchspec)

# Inspect the coefficients
rugarch::coef(garchfit)


# Compute the standardized returns
stdret <- rugarch::residuals(garchfit, standardize = TRUE)

# Compute the standardized returns using fitted() and sigma()
stdret <- (ret - rugarch::fitted(garchfit)) / rugarch::sigma(garchfit)

# Load the package PerformanceAnalytics and make the histogram
chart.Histogram(stdret, methods = c("add.normal","add.density" ), colorset = c("gray","red","blue"))


load("./RInputFiles/msftret.RData")
str(msftret)


# Specify the GJR GARCH model
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                 variance.model = list(model = "gjrGARCH"),
                                 distribution.model = "sstd"
                                 )

# Estimate the model and compute volatility
gjrgarchfit <- rugarch::ugarchfit(data = msftret, spec = garchspec)
gjrgarchvol <- rugarch::sigma(gjrgarchfit)

# Compare volatility
plotvol <- plot(abs(msftret), col = "grey")
plotvol <- addSeries(gjrgarchvol, col = "red", on=1)
# plotvol <- addSeries(sgarchvol, col = "blue", on=1)
plotvol


# Specify AR(1)-GJR GARCH model
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1, 0)),
                                 variance.model = list(model = "gjrGARCH"),
                                 distribution.model = "sstd"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data=msftret, spec=garchspec)

# Print the first two coefficients
rugarch::coef(garchfit)[c(1:2)]


# GARCH-in-Mean specification and estimation
gim_garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),
                                     variance.model = list(model = "gjrGARCH"), 
                                     distribution.model = "sstd"
                                     )
gim_garchfit <- rugarch::ugarchfit(data = msftret , spec = gim_garchspec)

# Predicted mean returns and volatility of GARCH-in-mean
gim_mean <- rugarch::fitted(gim_garchfit)
gim_vol <- rugarch::sigma(gim_garchfit)


ar1_garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0), archm = TRUE, archpow = 2),
                                     variance.model = list(model = "sGARCH"), 
                                     distribution.model = "sstd"
                                     )
ar1_garchfit <- rugarch::ugarchfit(data = msftret , spec = ar1_garchspec)
ar1_mean <- rugarch::fitted(ar1_garchfit)
ar1_vol <- rugarch::sigma(ar1_garchfit)

cmu_garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),
                                     variance.model = list(model = "sGARCH"), 
                                     distribution.model = "sstd"
                                     )
cmu_garchfit <- rugarch::ugarchfit(data = msftret , spec = cmu_garchspec)
constmean_mean <- rugarch::fitted(cmu_garchfit)
constmean_vol <- rugarch::sigma(cmu_garchfit)

# Correlation between predicted return using AR(1) and GARCH-in-mean models
cor(ar1_mean, gim_mean)

# Correlation between predicted volatilities across mean.models
cor(merge(constmean_vol, ar1_vol, gim_vol))


load("./RInputFiles/EURUSDret.RData")
str(EURUSDret)


flexgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0), archm = FALSE),
                                     variance.model = list(model = "sGARCH"), 
                                     distribution.model = "sstd"
                                     )
flexgarchfit <- rugarch::ugarchfit(data = EURUSDret , spec = flexgarchspec)


# Print the flexible GARCH parameters
rugarch::coef(flexgarchfit)

# Restrict the flexible GARCH model by impose a fixed ar1 and skew parameter
rflexgarchspec <- flexgarchspec
rugarch::setfixed(rflexgarchspec) <- list(ar1 = 0, skew = 1)

# Estimate the restricted GARCH model
rflexgarchfit <- rugarch::ugarchfit(data = EURUSDret,  spec = rflexgarchspec)

# Compare the volatility of the unrestricted and restriced GARCH models
plotvol <- plot(abs(EURUSDret), col = "grey")
plotvol <- addSeries(rugarch::sigma(flexgarchfit), col = "black", lwd = 4, on=1 )
plotvol <- addSeries(rugarch::sigma(rflexgarchfit), col = "red", on=1)
plotvol


# Define bflexgarchspec as the bound constrained version
bflexgarchspec <- flexgarchspec
rugarch::setbounds(bflexgarchspec) <- list(alpha1 = c(0.05, 0.2), beta1 = c(0.8, 0.95))

# Estimate the bound constrained model
bflexgarchfit <- rugarch::ugarchfit(data = EURUSDret, spec = bflexgarchspec)

# Inspect coefficients
rugarch::coef(bflexgarchfit)

# Compare forecasts for the next ten days
cbind(rugarch::sigma(rugarch::ugarchforecast(flexgarchfit, n.ahead = 10)),
      rugarch::sigma(rugarch::ugarchforecast(bflexgarchfit, n.ahead = 10))
      )


# Complete the specification to do variance targeting
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                 variance.model = list(model = "sGARCH", variance.targeting = TRUE),
                                 distribution.model = "std"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data = EURUSDret, spec = garchspec)

# Print the GARCH model implied long run volatility
sqrt(rugarch::uncvariance(garchfit))

# Verify that it equals the standard deviation (after rounding)
all.equal(sqrt(rugarch::uncvariance(garchfit)), sd(EURUSDret), tol = 1e-4)

```
    
  
  
***
  
Chapter 3 - Performance Evaluation  
  
Statistical Significance:  
  
* The model should only use meaningful and significant variables  
	* Can simplify by removing non-significant parameters  
    * Since we do not know the true parameters, the zero parameters need to be estimated  
    * round(coef(flexgarchfit), 6)  
* Tests for statistical significance are available in the rgarch package - general rule of thumb is to keep is abs(t-value) >= 2  
	* flexgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * flexgarchfit <- ugarchfit(data = msftret, spec = flexgarchspec)  
    * round(flexgarchfit@fit$matcoef, 6)  
  
Goodness of Fit:  
  
* Evaluation criterion depend on the elements that you want to assess  
* Goodness of fit for mean predictions  
	* e <- residuals(tgarchfit)  
    * mean(e^2)  
* Goodness of fit for variance predictions  
	* e <- residuals(tgarchfit)  
    * d <- e^2 - sigma(tgarchfit)^2  
    * mean(d^2)  
* Goodness of fit for the distribution - higher density means more likely to see returns  
	* The higher the density, the more likely the return is under the estimated GARCH model  
    * The likelihood of the sample is based on the product of all these densities. It measures how likely it is to that the observed returns come from the estimated GARCH model  
    * The higher the likelihood, the better the model fits with your data  
    * likelihood(tgarchfit)  # compare with likelihood achieved using other models; higher likelihood is better  
* There is a risk of over-fitting since the in-sample data is used for the goodness of fit tests - should add penalties for non-parsimonious models  
	* information criteria = - likelihood + penalty(number of parameters)  
    * infocriteria(tgarchfit)  
  
Diagnosing Absolute Standardized Returns:  
  
* Can check the standardized returns - scaled for mean 0 and standard deviation 1  
	* Standardized returns should be constant over time, provided the model is making good estimates of mean and sigma over time  
    * Should also be no correlations (auto-correlations) in the standardized returns over time  
* Example application to the daily MSFT returns  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * stdmsftret <- residuals(garchfit, standardize = TRUE)  
    * acf(abs(msftret), 22)  
    * acf(abs(stdmsftret), 22)  
* Can check the auto-correlations using the Ljung-Box test - desire for p to be GREATER than 5% (no significant auto-correlations found)  
	* Box.test(abs(stdmsftret), 22, type = "Ljung-Box")  
  
Back-testing using ugarchroll:  
  
* Can use rolling estimation to avoid look-ahead bias  
	* tgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "sGARCH"), distribution.model = "std")  
    * garchroll <- ugarchroll(tgarchspec, data = EURUSDret, n.start = 2500, refit.window = "moving", refit.every = 500)  
* There are five arguments to specify  
	* GARCH specification used  
    * data : return data to use  
    * n.start: the size of the initial estimation sample  
    * refit.window: how to change that sample through time: "moving" or "expanding  
    * refit.every: how often to re-estimate the model  
* Can convert results to data frames  
	* preds <- as.data.frame(garchroll)  
    * preds$Mu: series of predicted mean values  
    * preds$Sigma: series of predicted volatility values  
    * garchvol <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))  
    * plot(garchvol)  
    * preds <- as.data.frame(garchroll)  
    * Evaluate accuracy of preds$Mu and preds$Sigma by comparing with preds$Realized  
    * e <- preds$Realized - preds$Mu  
    * mean(e^2)  
    * d <- e^2 - preds$Sigma^2  
    * mean(d^2)  
* Can run a comparison of two models  
	* tgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "sGARCH"), distribution.model = "std")  
    * garchroll <- ugarchroll(tgarchspec, data = EURUSDret, n.start = 2500, refit.window = "moving", refit.every = 500)  
    * gjrgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * gjrgarchroll <- ugarchroll(gjrgarchspec, data = EURUSDret, n.start = 2500, refit.window = "moving", refit.every = 500)  
  
Example code includes:  
```{r eval=FALSE}

# Specify model with AR(1) dynamics, GJR GARCH and skewed student t
flexgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)),
                                     variance.model = list(model = "gjrGARCH"),
                                     distribution.model = "sstd"
                                     )

# Estimate the model
flexgarchfit <- rugarch::ugarchfit(data = EURUSDret, spec = flexgarchspec)

# Complete and study the statistical significance of the estimated parameters  
round(flexgarchfit@fit$matcoef, 6)


# Specify model with constant mean, standard GARCH and student t
tgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1, 0)),
                                  variance.model = list(model = "sGARCH", variance.targeting = TRUE),
                                  distribution.model = "sstd"
                                  )

# Fix the mu parameter at zero
rugarch::setfixed(tgarchspec) <- list("mu" = 0)

# Estimate the model
tgarchfit <- rugarch::ugarchfit(data = EURUSDret, spec = tgarchspec)

# Verify that the differences in volatility are small
plot(rugarch::sigma(tgarchfit) - rugarch::sigma(flexgarchfit))


# Compute prediction errors
garcherrors <- rugarch::residuals(flexgarchfit)
gjrerrors  <- rugarch::residuals(tgarchfit)

# Compute MSE for variance prediction of garchfit model
mean((rugarch::sigma(flexgarchfit)**2 - garcherrors^2)**2)

# Compute MSE for variance prediction of gjrfit model
mean((rugarch::sigma(tgarchfit)**2 - gjrerrors^2)**2)


# Print the number of estimated parameters
length(rugarch::coef(flexgarchfit))
length(rugarch::coef(tgarchfit))

# Print likelihood of the two models
rugarch::likelihood(flexgarchfit)
rugarch::likelihood(tgarchfit)

# Print the information criteria of the two models
rugarch::infocriteria(flexgarchfit)
rugarch::infocriteria(tgarchfit)


# Compute the standardized returns
stdEURUSDret <- rugarch::residuals(tgarchfit, standardize = TRUE)

# Compute their sample mean and standard deviation
mean(stdEURUSDret)
sd(stdEURUSDret)

# Correlogram of the absolute (standardized) returns
par(mfrow = c(1, 2))
acf(abs(EURUSDret), 22)
acf(abs(stdEURUSDret), 22)
par(mfrow = c(1, 1))

# Ljung-Box test
Box.test(abs(stdEURUSDret), 22, type = "Ljung-Box")


# Estimate the model on the last 2500 observations
tgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                  variance.model = list(model = "sGARCH"),
                                  distribution.model = "std"
                                  )
tgarchfit <- rugarch::ugarchfit(data = tail(EURUSDret, 2500) , spec = tgarchspec)

# Compute standardized returns
stdEURUSDret <- rugarch::residuals(tgarchfit, standardize = TRUE)

# Do the Ljung-Box test on the absolute standardized returns
Box.test(abs(stdEURUSDret), 22, type = "Ljung-Box")


# Estimate the GARCH model using all the returns and compute the in-sample estimates of volatility
garchinsample <- rugarch::ugarchfit(data = sp500ret, spec = flexgarchspec)
garchvolinsample <- rugarch::sigma(garchinsample)

# Use ugarchroll for rolling estimation of the GARCH model 
garchroll <- rugarch::ugarchroll(flexgarchspec, data = sp500ret, 
                                 n.start = 2000, refit.window = "moving",  refit.every = 2500
                                 )

# Set preds to the data frame with rolling predictions
preds <- rugarch::as.data.frame(garchroll)

# Compare in-sample and rolling sample volatility in one plot
garchvolroll <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))
volplot <- plot(garchvolinsample, col = "darkgrey", lwd = 1.5, 
                main = "In-sample versus rolling vol forecasts"
                )
volplot <- addSeries(garchvolroll, col = "blue", on = 1)
plot(volplot)


# Inspect the first three rows of the dataframe with out of sample predictions
head(preds, 3)

# Compute prediction errors
e <- preds$Realized - preds$Mu  
d <- e^2 - preds$Sigma^2 

# Compute MSE for the garchroll variance prediction
garchMSE <- mean(d^2)

# Use ugarchroll for rolling estimation of the GARCH model 
gjrgarchroll <- rugarch::ugarchroll(tgarchspec, data = sp500ret, 
                                    n.start = 2000, refit.window = "moving",  refit.every = 2500
                                    )

# Compute MSE for gjrgarchroll
gjrgarchpreds <- rugarch::as.data.frame(gjrgarchroll)
e <- gjrgarchpreds$Realized - gjrgarchpreds$Mu  
d <- e^2 - gjrgarchpreds$Sigma^2 
gjrgarchMSE <- mean(d**2)

```
  
  
  
***
  
Chapter 4 - Applications  
  
Value at Risk:  
  
* A common metrics is the 5% VaR, which is the amount that would be lost in the "best case" of the bottom 5% of returns over a specified period of time (such as a year)  
	* Target is to predict the VaR forward rather than assess it backwards  
* Workflow for assessing VaR in R  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchroll <- ugarchroll(garchspec, data = sp500ret, n.start = 2500, refit.window = "moving", refit.every = 100)  
    * garchVaR <- quantile(garchroll, probs = 0.05)  
    * actual <- xts(as.data.frame(garchroll)$Realized, time(garchVaR))  
    * VaRplot(alpha = 0.05, actual = actual, VaR = garchVaR)  
* The VaR exceedence should be 5% (if the VaR is set at the best of the 5%) - called the "coverage" of the VaR  
  
Production and Simulation:  
  
* Data used for modeling will be different than data used during production  
	* Use ugarchfilter() for analyzing the recent dynamics in the mean and volatility  
    * Use ugarchforecast() applied to a ugarchspec object (instead of ugarchfit()) object for making the predictions about the future mean and volatility  
* Example of running the full process  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret["/2010-12"], spec = garchspec)  
    * progarchspec <- garchspec  
    * setfixed(progarchspec) <- as.list(coef(garchfit))  
    * garchfilter <- ugarchfilter(data = msftret, spec = progarchspec)  
    * plot(sigma(garchfilter))  
    * garchforecast <- ugarchforecast(data = msftret, fitORspec = progarchspec, n.ahead = 10)  
    * cbind(fitted(garchforecast), sigma(garchforecast))  
* Can use the model to simulate log returns - useful to assess randomness in future returns  
	* msftlogret <- diff(log(MSFTprice))[(-1)]  
    * garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftlogret, spec = garchspec)  
    * simgarchspec <- garchspec  
    * setfixed(simgarchspec) <- as.list(coef(garchfit))  
* Can run simulations using ugarchpath()  
	* spec : completely specified GARCH model  
    * m.sim : number of time series of simulated returns you want  
    * n.sim: number of observations in the simulated time series (e.g. 252)  
    * rseed : any number to fix seed used to generate the simulated series (needed for reproducibility)  
    * simgarch <- ugarchpath(spec = simgarchspec, m.sim = 4, n.sim = 10 * 252, rseed = 12345)  
    * simret <- fitted(simgarch)  
    * plot.zoo(simret)  
    * plot.zoo(sigma(simgarch))  
    * simprices <- exp(apply(simret, 2, "cumsum"))  
    * matplot(simprices, type = "l", lwd = 3)  
  
Model Risk:  
  
* Model averaging is often more valuable than a single point estimate from a single model  
	* Sensitivity analysis and outlier removal can be valuable  
    * variance.models <- c("sGARCH", "gjrGARCH")  
    * distribution.models <- c("norm", "std", "std")  
    * c <- 1  
    * for (variance.model in variance.models) {  
    *     for (distribution.model in distribution.models) {  
    *         garchspec <- ugarchspec(mean.model = list(armaOrder = c(0, 0)), variance.model = list(model = variance.model), distribution.model = distribution.model)  
    *         garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    *         if (c==1) { msigma <- sigma(garchfit) } else { msigma <- merge(msigma, sigma(garchfit)) }
    *         c <- c + 1  
    *     }
    * }  
    * avesigma <- xts(rowMeans(msigma), order.by = time(msigma))  
* Robustness to starting values - additional source of risk to the GARCH models  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = sp500ret, spec = garchspec)  
    * coef(garchfit)  
    * likelihood(garchfit)  
    * setstart(garchspec) <- list(alpha1 = 0.05, beta1 = 0.9, shape = 8)  
    * garchfit <- ugarchfit(data = sp500ret, spec = garchspec)  
* Outliers in the underlying return data are an additional source of potential error  
	* library(PerformanceAnalytics)  
    * clmsftret <- Return.clean(msftret, method = "boudt")  
    * plotret <- plot(msftret, col = "red")  
    * plotret <- addSeries(clmsftret, col = "blue", on = 1)  
    * garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * clgarchfit <- ugarchfit(data = clmsftret, spec = garchspec)  
    * plotvol <- plot(abs(msftret), col = "gray")  
    * plotvol <- addSeries(sigma(garchfit), col = "red", on = 1)  
    * plotvol <- addSeries(sigma(clgarchfit), col = "blue", on = 1)  
    * plotvol  
  
GARCH Covariance:  
  
* Covariances of asset returns may vary over time  
* GARCH covariance can be estimated in four steps  
	* Step 1: Use ugarchfit() to estimate the GARCH model for each return series.  
    * msftgarchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * wmtgarchfit <- ugarchfit(data = wmtret, spec = garchspec)  
    * Step 2: Use residuals() to compute the standardized returns.  
    * stdmsftret <- residuals(msftgarchfit, standardize = TRUE)  
    * stdwmtret <- residuals(wmtgarchfit, standardize = TRUE)  
    * Step 3: Use cor() to estimate \rho? as the sample correlation of the standardized returns.  
    * msftwmtcor <- as.numeric(cor(stdmsftret, stdwmtret))  
    * msftwmtcor  
    * Step 4: Compute the GARCH covariance by multiplying the estimated correlation and volatilities  
    * msftwmtcov <- msftwmtcor * sigma(msftgarchfit) * sigma(wmtgarchfit)  
* Covariance has many applications in finance  
	* Optimization of portfolio variance  
    * msftvar <- sigma(msftgarchfit)^2  
    * wmtvar <- sigma(wmtgarchfit)^2  
    * msftwmtcov <- msftwmtcor * sigma(msftgarchfit) * sigma(wmtgarchfit)  
    * msftweight <- (wmtvar - msftwmtcov) / (msftvar + wmtvar - 2 * msftwmtcov)  
    * Dynamic beta (systematic risk) of a specific stock  
    * msftsp500cor <- as.numeric(cor(stdmsftret, stdsp500ret))  
    * msftsp500cov <- msftsp500cor * sigma(msftgarchfit) * sigma(sp500garchfit)  
    * sp500var <- sigma(sp500garchfit)^2  
    * msftbeta <- msftsp500cov / sp500var  
  
Wrap Up:  
  
* Language of GARCH models  
	* Volatility and volatility clustering  
    * Information set and predictions  
    * Leverage effect and GJR GARCH  
    * Skewness, fat tails, and student t  
    * Ljung-Box test, MSE, and other model validation methods  
* Language of rugarch  
	* ugarchspec()  
    * ugarchfit()  
    * ugarchroll()  
    * ugarchforecast()  
    * ugarchfilter()  
    * ugarchpath()  
    * Many useful methods sigma(), fitted(), coef(), infocriteria(), likelihood(), setfixed(), setbounds(), quantile()...  
  
Example code includes:  
```{r eval=FALSE}

flexgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)),
                                     variance.model = list(model = "gjrGARCH"),
                                     distribution.model = "sstd"
                                     )
garchroll <- rugarch::ugarchroll(flexgarchspec, data = msftret, 
                                 n.start = 2000, refit.window = "moving",  refit.every = 2500
                                 )

# Extract the dataframe with predictions from the rolling GARCH estimation
garchpreds <- rugarch::as.data.frame(garchroll)

# Extract the 5% VaR 
garchVaR <- rugarch::quantile(garchroll, probs = 0.05)

# Extract the volatility from garchpreds
garchvol <- xts(garchpreds$Sigma, order.by = time(garchVaR))

# Analyze the comovement in a time series plot
garchplot <- plot(garchvol, ylim = c(-0.1, 0.1))
garchplot <- addSeries(garchVaR, on = 1, col = "blue")
plot(garchplot, main = "Daily vol and 5% VaR")


# Take a default specification a with a normal and skewed student t distribution
normgarchspec <- rugarch::ugarchspec(distribution.model = "norm")
sstdgarchspec <- rugarch::ugarchspec(distribution.model = "sstd")

# Do rolling estimation
normgarchroll <- rugarch::ugarchroll(normgarchspec, data = msftret, n.start = 2500, 
                                     refit.window = "moving", refit.every = 2000
                                     )
sstdgarchroll <- rugarch::ugarchroll(sstdgarchspec, data = msftret, n.start = 2500, 
                                     refit.window = "moving", refit.every = 2000
                                     )

# Compute the 5% value at risk
normgarchVaR <- rugarch::quantile(normgarchroll, probs = 0.05)
sstdgarchVaR <- rugarch::quantile(sstdgarchroll, probs = 0.05)

# Compute the coverage
actual <- xts(rugarch::as.data.frame(normgarchroll)$Realized, time(normgarchVaR))
mean(actual < normgarchVaR)
mean(actual < sstdgarchVaR)


garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)), 
                                 variance.model = list(model = "gjrGARCH"), 
                                 distribution.model = "sstd"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data = sp500ret["/2006-12"], spec = garchspec)

# Fix the parameters
progarchspec <- garchspec
rugarch::setfixed(progarchspec) <- as.list(rugarch::coef(garchfit))

# Use ugarchfilter to obtain the estimated volatility for the complete period
garchfilter <- rugarch::ugarchfilter(data = sp500ret, spec = progarchspec)
plot(rugarch::sigma(garchfilter))

# Compare the 252 days ahead forecasts made at the end of September 2008 and September 2017
garchforecast2008 <- rugarch::ugarchforecast(data = sp500ret["/2008-09"], 
                                             fitORspec = progarchspec, n.ahead = 252
                                             )
garchforecast2017 <- rugarch::ugarchforecast(data = sp500ret["/2017-09"], 
                                             fitORspec = progarchspec, n.ahead = 252
                                             )
par(mfrow = c(2, 1), mar = c(3, 2, 3, 2))
plot(rugarch::sigma(garchforecast2008), main = "/2008-09", type = "l")
plot(rugarch::sigma(garchforecast2017), main = "/2017-09", type = "l")
par(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))


simgarchspec <- garchspec
rugarch::setfixed(simgarchspec) <- as.list(rugarch::coef(garchfit))

# Complete the code to simulate 4 time series of 10 years of daily returns
simgarch <- rugarch::ugarchpath(spec=simgarchspec, m.sim = 4, n.sim = 10*252, rseed = 210) 

# Plot the simulated returns of the four series
simret <- rugarch::fitted(simgarch)
plot.zoo(simret)
plot.zoo(rugarch::sigma(simgarch))

# Compute the corresponding simulated prices and plot them
simprices <- exp(apply(simret, 2, "cumsum"))
matplot(simprices, type = "l", lwd = 3)


# Specify model with constant mean, standard GARCH and student t
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0, 0)),
                                 variance.model = list(model = "sGARCH", variance.targeting = FALSE),
                                 distribution.model = "std"
                                 )

# Estimate using default starting values
garchfit <- rugarch::ugarchfit(spec=garchspec, data=EURUSDret)

# Print the estimated parameters and the likelihood
rugarch::coef(garchfit)
rugarch::likelihood(garchfit)

# Set other starting values and re-estimate
rugarch::setstart(garchspec) <- list(alpha1 = 0.05, beta1 = 0.9, shape = 6) 
garchfit <- rugarch::ugarchfit(spec=garchspec, data=EURUSDret)

# Print the estimated parameters and the likelihood
rugarch::coef(garchfit)
rugarch::likelihood(garchfit)


garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)), 
                                 variance.model = list(model = "gjrGARCH"), 
                                 distribution.model = "sstd"
                                 )
usgarchfit <- rugarch::ugarchfit(spec=garchspec, data=ret["2009/2017"])
eugarchfit <- rugarch::ugarchfit(spec=garchspec, data=msftret["2009/2017"])

# Compute the standardized US and EU returns, together with their correlation 
stdusret <- rugarch::residuals(usgarchfit, standardize = TRUE)
stdeuret <- rugarch::residuals(eugarchfit, standardize = TRUE)

useucor <- as.numeric(cor(stdusret, stdeuret))
print(useucor)

# Compute the covariance and variance of the US and EU returns 
useucov <- useucor * rugarch::sigma(usgarchfit) * rugarch::sigma(eugarchfit)
usvar <- rugarch::sigma(usgarchfit)**2
euvar <- rugarch::sigma(eugarchfit)**2

# Compute the minimum variance weight of the US ETF in the US-EU ETF portfolio 
usweight <- (euvar - useucov) / (usvar + euvar - 2*useucov)
plot(usweight)


# Compute standardized returns
# stdmsftret <- residuals(msftgarchfit, standardize=TRUE)
# stdwmtret <- residuals(wmtgarchfit, standardize=TRUE)

# Print the correlation
# cor(stdmsftret, stdwmtret)

# Load the package PerformanceAnalytics
# library(PerformanceAnalytics)

# Plot the 3-month rolling correlation
# chart.RollingCorrelation(stdmsftret, stdwmtret, width = 66, main = "3-month rolling correlation between MSFT and WMT daily returns")

```
  
  
  
***
  
###_RNA-Seq Differential Expression Analysis_  
  
Chapter 1 - Introduction to RNA-Seq Theory and Workflow  
  
Introduction to RNA-Seq:  
  
* Discovery of genes that are differentially expressed between groups  
* The genome contains the instructions for life - double-stranded DNA with chromosomes built of nucleotides (G-A-C-T)  
	* A-T and G-C nucleotides are paired up; the order is called the DNA sequence  
    * Within the sequence, there are genes, which create messenger RNA which produces proteins  
    * To convert to proteins, messenger RNA must undergo transcription (DNA -> pre-mRNA -> Mature-mRNA -> Protein)  
* Muscle cells, nerve cells, and other cells activate different portions of the DNA  
	* Mutations can effect the types of quantities of DNA that are produced  
* Differential expression analysis looks at differences in gene expression across groups, over time, correlated with other changes, driven by what pathways, etc.  
  
RNA-Seq Workflow:  
  
* Experiment planning is important  
	* Technical replicates: Generally low technical variation, so unnecessary  
    * Biological replicates: Crucial to the success of RNA-Seq differential expression analyses. The more replicates the better, but at the very least have 3  
    * Batch effects: Avoid as much as possible and note down all experimental variables  
* Biological sample preparation - isolate RNA, generate cDNA, FASTQ sequencing, RNA reads (sequence ends)  
	* Followed by mapping to the genome (exon and intron alignment is needed)  
    * Counting reads associated with genes - drives estimates of gene counts  
* Can run the process using Bioconductor  
	* wt_rawcounts <- read.csv("fibrosis_wt_rawcounts.csv")  
  
Differential Gene Expression Theory:  
  
* Differential expression is based on normalized counts, adjusted for variation in the data  
	* Are the differences between groups significant?  
* Course will use a publicly available dataset based on SMOC2 data as related to kidney fibrosis  
* Often, there is a long right tail - Poisson might be a good model if there were no biological variation  
	* ggplot(raw_counts) + geom_histogram(aes(x = wt_normal1), stat = "bin", bins = 200) + xlab("Raw expression counts") + ylab("Number of genes")  
* The negative binomial model works better for the real-world data  
	* wt_rawcounts <- read.csv("fibrosis_wt_rawcounts.csv")  
    * genotype <- c("wt", "wt", "wt", "wt", "wt", "wt", "wt")  
    * condition <- c("normal", "fibrosis", "normal", "fibrosis", "normal", "fibrosis", "fibrosis")  
    * wt_metadata <- data.frame(genotype, wildtype)  
  
Example code includes:  
```{r eval=FALSE}

# Load library for DESeq2
library(DESeq2)

# Load library for RColorBrewer
library(RColorBrewer)

# Load library for pheatmap
library(pheatmap)

# Load library for tidyverse
library(tidyverse)


# Explore the first six observations of smoc2_rawcounts
head(smoc2_rawcounts)

# Explore the structure of smoc2_rawcounts
str(smoc2_rawcounts)


# Create genotype vector
genotype <- rep("smoc2_oe", 7)

# Create condition vector
condition <- c(rep("fibrosis", 4), rep("normal", 3))

# Create data frame
smoc2_metadata <- data.frame(genotype, condition)

# Assign the row names of the data frame
rownames(smoc2_metadata) <- paste0("smoc2_", condition, c(1:4, 1, 3, 4))

```
  
  
  
***
  
Chapter 2 - Exploratory Data Analysis  
  
Introduction to Differential Expression Analysis:  
  
* The DESeq2 is a common method for running differential expression analysis  
	* vignette(DESeq2)  
* The DE analysis comes after quality control  
	* Quality control - normalization and unsupervised clustering analysis  
    * DE analysis - modelling raw counts, shrinking log2 fold changes, differential expression analysis  
* Need to bring in the raw counts  
	* wt_rawcounts <- read.csv("fibrosis_wt_rawcounts.csv")  
    * wt_metadata <- read.csv("fibrosis_wt_metadata_unordered.csv")  
  
Organizing the data for DESeq2:  
  
* Need to have the metadata and the sample data in the same order and with the same rownames/colnames  
	* all(rownames(wt_metadata) == colnames(wt_rawcounts))  
    * match(vector1, vector2)  # vector 1 has the desired order, vector2 has the target vector for reordering, output are the indices for reogranizing vector2 to align with vector1  
    * idx <- match(colnames(wt_rawcounts), rownames(wt_metadata))  
    * reordered_wt_metadata <- wt_metadata[idx, ]  
    * all(rownames(reordered_wt_metadata) == colnames(wt_rawcounts))  
* Can then create the DESeq2 object  
	* dds_wt <- DESeqDataSetFromMatrix(countData = wt_rawcounts, colData = reordered_wt_metadata, design = ~ condition)  
  
Count Normalization:  
  
* First step is to normalize raw counts to assess sample-level consistencies  
	* Factors other than RNA expression can drive the expression of genes - library depth, gene length (longer gene means more fragments for sequencing), RNA composition  
    * dds_wt <- estimateSizeFactors(dds_wt)  
    * sizeFactors(dds_wt)  
    * normalized_wt_counts <- counts(dds_wt, normalized=TRUE)  
  
Hierarchical Heatmap:  
  
* Can compare the normalized counts across samples - starting with visualization and clustering methods such as hclust() or PCA  
	* vsd_wt <- vst(dds_wt, blind=TRUE)  # vst is a logarithmic transformation that moderates variance; blind=TRUE means that it should be blinded to the sampling metadata, and should be set for QC  
* Can run hierarchical clustering combines with heatmaps to assess clustering - generally, should see correlations > 0.8 unless there are outliers suggestive of quality errors (can confirm with PCA)  
	* vsd_mat_wt <- assay(vsd_wt)  
    * vsd_cor_wt <- cor(vsd_mat_wt)  
    * library(pheatmap)  
    * pheatmap(vsd_cor_wt, annotation = select(wt_metadata, condition))  
  
Principal Component Analysis:  
  
* PCA is a technique to emphasize the variation within a dataset - first component represents the greatest variance  
	* Genes can be given scores based on the degree to which they influence eachprincipal component  
* PCA can be performed using native DESeq2 functions  
	* plotPCA(vsd_wt, intgroup="condition")  # intgroup is the metadata variable for coloring  
  
Example code includes:  
```{r eval=FALSE}

# Use the match() function to reorder the columns of the raw counts
match(rownames(smoc2_metadata), colnames(smoc2_rawcounts))

# Reorder the columns of the count data
reordered_smoc2_rawcounts <- smoc2_rawcounts[, match(rownames(smoc2_metadata), colnames(smoc2_rawcounts))]

# Create a DESeq2 object
dds_smoc2 <- DESeqDataSetFromMatrix(countData =  reordered_smoc2_rawcounts,
                              colData =  smoc2_metadata,
                              design = ~ condition)


# Determine the size factors to use for normalization
dds_smoc2 <- estimateSizeFactors(dds_smoc2)

# Extract the normalized counts
smoc2_normalized_counts <- counts(dds_smoc2, normalized=TRUE)


# Transform the normalized counts 
vsd_smoc2 <- vst(dds_smoc2, blind=TRUE)

# Extract the matrix of transformed counts
vsd_mat_smoc2 <- assay(vsd_smoc2)

# Compute the correlation values between samples
vsd_cor_smoc2 <- cor(vsd_mat_smoc2) 

# Plot the heatmap
pheatmap(vsd_cor_smoc2, annotation = select(smoc2_metadata, condition))


# Transform the normalized counts 
vsd_smoc2 <- vst(dds_smoc2, blind = TRUE)

# Plot the PCA of PC1 and PC2
plotPCA(vsd_smoc2, intgroup="condition")

```
  
  
  
***
  
Chapter 3 - Differential Expression Analysis with DESeq2  
  
DE Analysis:  
  
* The DE analysis workflow includes 1) fitting raw counts to the negative binomial model, 2) shrinking the log2 fold changes, and 3) visualizing results  
* Begin by fitting raw counts to the negative binomial model  
	* dds_wt <- DESeqDataSetFromMatrix(countData = wt_rawcounts, colData = reordered_wt_metadata, design = ~ condition)  # can have ~ a + b + c in the design=, with the main component (e.g., treatment) last  
    * ~ strain + sex + treatment + sex:treatment  # interaction term for sex and treatment; include as the last term  
    * dds_wt <- DESeq(dds_wt)  
  
DESeq2 Model - Dispersion:  

* Can begin by exploring the model fit - are genes expressed differently across rather than within groups  
	* Log2(TreatmentMean / ControlMean)  
    * mean_counts <- apply(wt_rawcounts[, 1:3], 1, mean)  
    * variance_counts <- apply(wt_rawcounts[, 1:3], 1, var)  
    * df <- data.frame(mean_counts, variance_counts)  
    * ggplot(df) + geom_point(aes(x=mean_counts, y=variance_counts)) + scale_y_log10() + scale_x_log10() + xlab("Mean counts per gene") + ylab("Variance per gene")  
* Variance vs. mean is called "dispersion" in the DESeq2 modeling  
	* Dispersion: Variance = Mean + Dispersion * Mean**2  
    * Increase in variance leads to increase in dispersion  
    * Increase in mean leads to descrease in dispersion  
    * plotDispEsts(dds_wt)  
  
DESeq2 Model - Contrasts:  
  
* The negative binomial model is good for representing RNA Seq data  
* By default, the Wald test is run based on the condition  
	* results(wt_dds, alpha = 0.05)  # alpha is the desired significance  
* Can supply own contrasts using the contrasts argument  
	* GENERAL SYNTAX: results(dds, contrast = c("condition_factor", "level_to_compare", "base_level"), alpha = 0.05)  
    * wt_res <- results(dds_wt, contrast = c("condition", "fibrosis", "normal"), alpha = 0.05)  
* Can use the MA plot for further explorations  
	* plotMA(wt_res, ylim=c(-8,8))  
* LFC shrinkage can be helpful for addressing high dispersions or low means  
	* wt_res <- lfcShrink(dds_wt, contrast=c("condition", "fibrosis", "normal"), res=wt_res)  
    * plotMA(wt_res, ylim=c(-8,8))  
  
DESeq2 Results:  
  
* Can assess the results using the DESeq2 workflows  
	* mcols(wt_res)  
    * head(wt_res, n=10)  
* Multiple test corrections are run using the BH method  
	* summary(wt_res)  
    * wt_res <- results(dds_wt, contrast = c("condition", "fibrosis", "normal"), alpha = 0.05, lfcThreshold = 0.32)  
    * wt_res <- lfcShrink(dds_wt, contrast=c("condition", "fibrosis", "normal"), res=wt_res)  
    * summary(wt_res)  
* Can annotate the gene names for easier exploration and further analysis  
	* library(annotables)  
    * grcm38  
    * wt_res_all <- data.frame(wt_res) %>% rownames_to_column(var = "ensgene") %>% left_join(x = wt_res_all, y = grcm38[, c("ensgene", "symbol", "description")], by = "ensgene")  
    * wt_res_sig <- subset(wt_res_all, padj < 0.05)  
    * wt_res_sig <- wt_res_sig %>% arrange(padj)  
  
Eample code includes:  
```{r eval=FALSE}

# Create DESeq2 object
dds_smoc2 <- DESeqDataSetFromMatrix(countData = reordered_smoc2_rawcounts, colData = smoc2_metadata, design = ~ condition)

# Run the DESeq2 analysis
dds_smoc2 <- DESeq(dds_smoc2)


# Plot dispersions
plotDispEsts(dds_smoc2)


# Extract the results of the differential expression analysis
smoc2_res <- results(dds_smoc2, 
                contrast = c("condition", "fibrosis", "normal"), 
                alpha = 0.05)


# Shrink the log2 fold change estimates to be more accurate
smoc2_res <- lfcShrink(dds_smoc2, 
                    contrast =  c("condition", "fibrosis", "normal"),
                    res = smoc2_res)


# Explore the results() function
?results

# Extract results
smoc2_res <- results(dds_smoc2, 
                contrast = c("condition", "fibrosis", "normal"), 
                alpha = 0.05, 
                lfcThreshold = 0.32)

# Shrink the log2 fold changes
smoc2_res <- lfcShrink(dds_smoc2, 
                       contrast = c("condition", "fibrosis", "normal"),
                       res = smoc2_res)


# Get an overview of the results                    
summary(smoc2_res)


# Save results as a data frame
smoc2_res_all <- data.frame(smoc2_res)

# Subset the results to only return the significant genes with p-adjusted values less than 0.05
smoc2_res_sig <- subset(smoc2_res_all, padj < 0.05)

```
  
  
  
***
  
Chapter 4 - Exploration of Differential Expression Results  
  
Visualization of Results:  
  
* Expression heatmaps explore the expression of key genes  
	* sig_norm_counts_wt <- normalized_counts_wt[wt_res_sig$ensgene, ]  
    * library(RColorBrewer)  
    * heat_colors <- brewer.pal(6, "YlOrRd")  
    * display.brewer.all()  
    * pheatmap(sig_norm_counts_wt, color = heat_colors, cluster_rows = T, show_rownames = F, annotation = select(wt_metadata, condition), scale = "row")  
* Can also plot using the volcano plot  
	* wt_res_all <- wt_res_all %>% rownames_to_column(var = "ensgene") %>% mutate(threshold = padj < 0.05)  
    * ggplot(wt_res_all) + geom_point(aes(x = log2FoldChange, y = -log10(padj), color = threshold)) + xlab("log2 fold change") + ylab("-log10 adjusted p-value") + theme(legend.position = "none", plot.title = element_text(size = rel(1.5), hjust = 0.5), axis.title = element_text(size = rel(1.25)))  
* The expression plot can also be helpful - e.g., top 20 genes plot  
	* top_20 <- data.frame(sig_norm_counts_wt)[1:20, ] %>% rownames_to_column(var = "ensgene")  
    * top_20 <- gather(top_20, key = "samplename", value = "normalized_counts", 2:8)  
    * top_20 <- inner_join(top_20, rownames_to_column(wt_metadata, var = "samplename"), by = "samplename")  
    * ggplot(top_20) + geom_point(aes(x = ensgene, y = normalized_counts, color = condition)) + scale_y_log10() + xlab("Genes") + ylab("Normalized Counts") + ggtitle("Top 20 Significant DE Genes") + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(plot.title = element_text(hjust = 0.5))  
  
RNA-Seq DE Analysis Setup:  
  
* Can explore and filter for relevant and/or expected genes  
	* Samples/library preparation  
    * Sequence reads  
    * Quality control  
    * Splice-aware mapping to genome  
    * Counting reads associated with genes  
    * Statistical analysis to identify differentially expressed genes  
* The initial preparation uses the DESeq2 library  
	* dds <- DESeqDataSetFromMatrix(countData = rawcounts, colData = metadata, design = ~ condition)  
  
RNA-Seq DE Analysis Summary:  
  
* Need to normalize the data for DESeq analysis  
	* dds <- estimateSizeFactors(dds)  
    * normalized_counts <- counts(dds, normalized=TRUE)  
* Can cluster and plot for quality assurance and exploratory analysis  
	* vsd <- vst(dds, blind=TRUE)  
    * vsd %>% assay() %>% # Extract the vst matrix from the object cor() %>% # Compute pairwise correlation values pheatmap(annotation = metadata[ , c("column_name1", "column_name2])  
    * plotPCA(vsd, intgroup="condition")  
* Can then run the DE analysis  
	* dds <- DESeqDataSetFromMatrix(countData = rawcounts, colData = metadata, design = ~ source_of_variation + condition)  
    * dds <- DESeq(dds)  
* Can then explore fits using dispersions - should decrease with increasing mean - and then run LFC shrinkage  
	* plotDispEsts(dds)  
    * res <- results(dds, contrast=c("condition_factor", "level_to_compare", "base_level"), alpha = 0.05)  
    * res_all <- data.frame(res) %>% rownames_to_column(var = "ensgene")  
    * res_all <- left_join(x=res_all, y=grcm38[, c("ensgene", "symbol", "description")], by = "ensgene")  
    * res_all <- arrange(res_all, padj)  
* Identify significantly differently expressed genes  
	* res_sig <- subset(res_all, padj < 0.05)  
    * plotMA(), volcano plot, expression heatmap  
  
RNA-Seq Next Steps:  
  
* Several resources available for additional exploration  
	* http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html  
    * https://support.bioconductor.org/  
* May want to run more complicated experiments and analyses  
* Course covered differential expression of genes associated with fibrosis in wild type and SMOC2  
	* Can further analyze differentially expressed genes  
    * Can compare with the list of expected genes for differential expression  
  
Example code includes:  
```{r eval=FALSE}

# Create MA plot
plotMA(smoc2_res)

# Generate logical column 
smoc2_res_all <- data.frame(smoc2_res) %>% mutate(threshold = padj < 0.05)
              
# Create the volcano plot
ggplot(smoc2_res_all) + 
        geom_point(aes(x = log2FoldChange, y = -log10(padj), color = threshold)) + 
        xlab("log2 fold change") + 
        ylab("-log10 adjusted p-value") + 
        theme(legend.position = "none", 
              plot.title = element_text(size = rel(1.5), hjust = 0.5), 
              axis.title = element_text(size = rel(1.25)))


# Subset normalized counts to significant genes
sig_norm_counts_smoc2 <- normalized_counts_smoc2[rownames(smoc2_res_sig), ]

# Choose heatmap color palette
heat_colors <- brewer.pal(n = 6, name = "YlOrRd")

# Plot heatmap
pheatmap(sig_norm_counts_smoc2, 
         color = heat_colors, 
         cluster_rows = TRUE, 
         show_rownames = FALSE,
         annotation = select(smoc2_metadata, condition), 
         scale = "row")


# Check that all of the samples are in the same order in the metadata and count data
all(colnames(all_rawcounts) %in% rownames(all_metadata))

# DESeq object to test for the effect of fibrosis regardless of genotype
dds_all <- DESeqDataSetFromMatrix(countData = all_rawcounts,
                        colData = all_metadata,
                        design = ~ genotype + condition)

# DESeq object to test for the effect of genotype on the effect of fibrosis                        
dds_complex <- DESeqDataSetFromMatrix(countData = all_rawcounts,
                        colData = all_metadata,
                        design = ~ genotype + condition + genotype:condition)


# Log transform counts for QC
vsd_all <- vst(dds_all, blind = TRUE)

# Create heatmap of sample correlation values
vsd_all %>% 
        assay() %>%
        cor() %>%
        pheatmap(annotation = select(all_metadata, c("condition", "genotype")))

# Create the PCA plot for PC1 and PC2 and color by condition       
plotPCA(vsd_all, intgroup="condition")

# Create the PCA plot for PC1 and PC2 and color by genotype       
plotPCA(vsd_all, intgroup="genotype")


# Select significant genese with padj < 0.05
smoc2_sig <- subset(res_all, padj < 0.05) %>%
    data.frame() %>%
    rownames_to_column(var = "geneID")

# Extract the top 6 genes with padj values
smoc2_sig %>%
    arrange(padj) %>%
    select(geneID, padj) %>%
    head()

```
  
  
  
***
  
###_Survival Analysis in R_  
  
Chapter 1 - What is Survival Analysis?  
  
The term "survival analysis":  
  
* Survival analysis methods are the same whether they are time to progression, time to finding a job, etc.  
* Can be called "time to event" since the event need not be death; "survival analysis" is just the most commonly used metric  
* Data sets for the course  
	* data(GBSG2, package = "TH.data")  # time to death in breast cancer  
    * data(UnempDur, package = "Ecdat")  # time to re-employment  
    * help(UnempDur, package = "Ecdat")  
  
Why learn survival methods?  
  
* Survival analysis requires something more than a linear model  
	* Times are always positive; cannot be negative, nor have a negative number of survivors  
    * Different measures are of interest - hazard functions  
    * Censoring is almost always an issue - often do not know when the event will happen, only that it did not happen by time X  
* Can use the R package survival  
	* time <- c(5, 6, 2, 4, 4)  
    * event <- c(1, 0, 0, 1, 1)  
    * library("survival")  
    * Surv(time, event)  
* R packages that are available for survival analysis  
	* library("survival")  
    * library("survminer")  
  
Measures used in survival analysis:  
  
* Typical survival analysis questions include  
	* What is the probability that a breast cancer patient survives longer than 5 years?  
    * What is the typical waiting time for a cab?  
    * Out of 100 unemployed people, how many do we expect to have a job again after 2 months?  
* The survival function is S(t) = P(T > t) = 1 - F(t) where F is the CFD  
	* At any point in time, how probable is it to survive at least that long  
    * The median is one of the common metrics used  
  
Example code includes:  
```{r}

# Check out the help page for this dataset
# help(GBSG2, package = "TH.data")

# Load the data
data(GBSG2, package = "TH.data")

# Look at the summary of the dataset
summary(GBSG2)


# Count censored and uncensored data
num_cens <- table(GBSG2$cens)
num_cens

# Create barplot of censored and uncensored data
barplot(table(GBSG2$cens))


# Create Surv-Object
sobj <- survival::Surv(GBSG2$time, GBSG2$cens)

# Look at 10 first elements
sobj[1:10]

# Look at summary
summary(sobj)

# Look at structure
str(sobj)

```
  
  
  
***
  
Chapter 2 - Estimation of Survival Curves  
  
Kaplan-Meier Estimate:  
  
* The KM estimate is the step function derived from the raw data  
	* Censoring puts a mark on the curve, but dose not step down the curve  
    * S(t) = S(t-1) * e(t) / n(t)  where e(t) is events occuring at t and n(t) is the number at-risk (not previously censored) at time t  
* Can run KM estimates using the survival function  
	* km <- survfit(Surv(time, event) ~ 1)  
    * ggsurvplot(km, conf.int = FALSE, risk.table = "nrisk_cumevents", legend = "none")  
  
Understanding and Visualizing Kaplan-Meier Curves:  
  
* The ggsurvplot function can be useful for visualizations  
	* library(survminer)  
    * ggsurvplot(fit)  
    * ggsurvplot( fit, palette = NULL, linetype = 1, surv.median.line = "none", risk.table = FALSE, cumevents = FALSE, cumcensor = FALSE, tables.height = 0.25, ... )  
    * ggsurvplot( fit = km, palette = "blue", linetype = 1, surv.median.line = "hv", risk.table = TRUE, cumevents = TRUE, cumcensor = TRUE, tables.height = 0.1 )  # hv is horizontal and vertical  
* Can also revisit the survfit() object, which produces the KM curve  
	* If object is a formula: Kaplan-Meier estimation  
    * Other options for object - coxph, survreg  
  
Weibull Model for Estimating Survival Curves:  
  
* The Weibull model extends the KM curve through smoothing which can be more valuable for predictions  
	* wb <- survreg(Surv(time, event) ~ 1, data)  # reg is regression which is Weibull  
    * km <- survfit(Surv(time, event) ~ 1, data)  # fit is step-function KM  
    * predict(wb, type = "quantile", p = 1 - 0.9, newdata = data.frame(1))  # dummy newdata since there are no covariates  
* Can also create the survival curve  
	* surv <- seq(.99, .01, by = -.01)  
    * t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))  
    * head(data.frame(time = t, surv = surv))  
  
Visualizing Results of Weibull Model:  
  
* Most of the visualization tools, such as ggsurvplot(), work for step functions rather than on curves  
	* wb <- survreg(Surv(time, cens) ~ 1)  
    * surv <- seq(.99, .01, by = -.01)  
    * t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))  
    * surv_wb <- data.frame(time = t, surv = surv, upper = NA, lower = NA, std.err = NA)  
    * ggsurvplot_df(fit = surv_wb, surv.geom = geom_line)  
  
Example code includes:  
```{r}

# Create time and event data
time <- c(5, 6, 2, 4, 4)
event <- c(1, 0, 0, 1, 1)

# Compute Kaplan-Meier estimate
km <- survival::survfit(survival::Surv(time, event) ~ 1)
km

# Take a look at the structure
str(km)

# Create data.frame
data.frame(time = km$time, n.risk = km$n.risk, n.event = km$n.event, 
           n.censor = km$n.censor, surv = km$surv
           )


# Create dancedat data
dancedat <- data.frame(name = c("Chris", "Martin", "Conny", "Desi", "Reni", "Phil", "Flo", "Andrea", "Isaac", "Dayra", "Caspar"),
                       time = c(20, 2, 14, 22, 3, 7, 4, 15, 25, 17, 12),
                       obs_end = c(1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0)
                       )

# Estimate the survivor function pretending that all censored observations are actual observations.
km_wrong <- survival::survfit(survival::Surv(time) ~ 1, data = dancedat)

# Estimate the survivor function from this dataset via kaplan-meier.
km <- survival::survfit(survival::Surv(time, obs_end) ~ 1, data = dancedat)

# Plot the two and compare
survminer::ggsurvplot_combine(list(correct = km, wrong = km_wrong))


# Kaplan-Meier estimate
km <- survival::survfit(survival::Surv(time, cens) ~ 1, data = GBSG2)

# plot of the Kaplan-Meier estimate
survminer::ggsurvplot(km)

# add the risk table to plot
survminer::ggsurvplot(km, risk.table = TRUE)

# add a line showing the median survival time
survminer::ggsurvplot(km, risk.table = TRUE, surv.median.line = "hv")


# Weibull model
wb <- survival::survreg(survival::Surv(time, cens) ~ 1, data = GBSG2)

# Compute the median survival from the model
predict(wb, type = "quantile", p = 0.5, newdata = data.frame(1))

# 70 Percent of patients survive beyond time point...
predict(wb, type = "quantile", p = 1-0.7, newdata = data.frame(1))

# Retrieve survival curve from model probabilities 
surv <- seq(.99, .01, by = -.01)

# Get time for each probability
t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))

# Create data frame with the information
surv_wb <- data.frame(time = t, surv = surv)

# Look at first few lines of the result
head(surv_wb)


# Create data frame with the information needed for ggsurvplot_df
surv_wb <- data.frame(time = t, surv = surv, upper = NA, lower = NA, std.err = NA)

# Plot
survminer::ggsurvplot_df(fit = surv_wb, surv.geom = geom_line)

```
  
  
  
***
  
Chapter 3 - Weibull Model  
  
Why Use the Weibull Model?  
  
* Often, there is a goal to predict the life expectancy based on covariates such as treatment, tumor size, etc.  
	* wbmod <- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2)  
    * coef(wbmod)  
  
Visualizing Weibull Models:  
  
* Steps to produce the visualization include  
	* Compute Weibull model  
    * Decide on "imaginary patients"  
    * Compute survival curves  
    * Create data.frame with survival curve information  
    * Plot  
* Example steps  
	* wbmod <- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2)  # compute model  
    * newdat <- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75)) )  # create all combinations of therapy and tumor sizes  
    * surv <- seq(.99, .01, by = -.01)  
    * t <- predict(wbmod, type = "quantile", p = 1 - surv, newdata = newdat)  
    * surv_wbmod_wide <- cbind(newdat, t)  
    * surv_wbmod <- reshape2::melt(surv_wbmod_wide, id.vars = c("horTh", "tsize"), variable.name = "surv_id", value.name = "time")  
    * surv_wbmod$surv <- surv[as.numeric(surv_wbmod$surv_id)]  
    * surv_wbmod[, c("upper", "lower", "std.err", "strata")] <- NA  
    * ggsurvplot_df(surv_wbmod, surv.geom = geom_line, linetype = "horTh", color = "tsize", legend.title = NULL)  
  
Other Distributions:  
  
* The choice of distribution will depend on the assumptions around the underlying physical process - will see different curves  
	* survreg(Surv(time, cens) ~ horTh, data = GBSG2)  
    * survreg(Surv(time, cens) ~ horTh, data = GBSG2, dist = "exponential")  
    * survreg(Surv(time, cens) ~ horTh, data = GBSG2, dist = "lognormal")  
* The choice of distribution depends on both domain expertise and goodness of fit; Weibull models are generally more flexible than exponential models  
  
Example code includes:  
```{r}

dfTime <- c(306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654, 728, 71, 567, 144, 613, 707, 61, 88, 301, 81, 624, 371, 394, 520, 574, 118, 390, 12, 473, 26, 533, 107, 53, 122, 814, 965, 93, 731, 460, 153, 433, 145, 583, 95, 303, 519, 643, 765, 735, 189, 53, 246, 689, 65, 5, 132, 687, 345, 444, 223, 175, 60, 163, 65, 208, 821, 428, 230, 840, 305, 11, 132, 226, 426, 705, 363, 11, 176, 791, 95, 196, 167, 806, 284, 641, 147, 740, 163, 655, 239, 88, 245, 588, 30, 179, 310, 477, 166, 559, 450, 364, 107, 177, 156, 529, 11, 429, 351, 15, 181, 283, 201, 524, 13, 212, 524, 288, 363, 442, 199, 550, 54, 558, 207, 92, 60, 551, 543, 293, 202, 353, 511, 267, 511, 371, 387, 457, 337, 201, 404, 222, 62, 458, 356, 353, 163, 31, 340, 229, 444, 315, 182, 156, 329, 364, 291, 179, 376, 384, 268, 292, 142, 413, 266, 194, 320, 181, 285, 301, 348, 197, 382, 303, 296, 180, 186, 145, 269, 300, 284, 350, 272, 292, 332, 285, 259, 110, 286, 270, 81, 131, 225, 269, 225, 243, 279, 276, 135, 79, 59, 240, 202, 235, 105, 224, 239, 237, 173, 252, 221, 185, 92, 13, 222, 192, 183, 211, 175, 197, 203, 116, 188, 191, 105, 174, 177)
dfStatus <- c(2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)
dfSex <- factor(ifelse(c(1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2)==1, "male", "female"), levels=c("male", "female"))
dat <- data.frame(time=dfTime, status=dfStatus, sex=dfSex)


# Look at the data set
str(dat)

# Estimate a Weibull model
wbmod <- survival::survreg(survival::Surv(time, status) ~ sex, data = dat)
coef(wbmod)


# Weibull model
wbmod <- survival::survreg(survival::Surv(time, cens) ~ horTh, data = GBSG2)

# Retrieve survival curve from model
surv <- seq(.99, .01, by = -.01)
t_yes <- predict(wbmod, type = "quantile", p = 1-surv, newdata = data.frame(horTh = "yes"))

# Take a look at survival curve
str(t_yes)


# Weibull model
wbmod <- survival::survreg(survival::Surv(time, cens) ~ horTh + tsize, data = GBSG2)

# Imaginary patients
newdat <- expand.grid(horTh = levels(GBSG2$horTh), 
                      tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75))
                      )

# Compute survival curves
surv <- seq(.99, .01, by = -.01)
t <- predict(wbmod, type = "quantile", p = 1-surv, newdata = newdat)

# How many rows and columns does t have?
dim(t)


# Use cbind() to combine the information in newdat with t
surv_wbmod_wide <- cbind(newdat, t)
  
# Use melt() to bring the data.frame to long format
surv_wbmod <- reshape2::melt(surv_wbmod_wide, id.vars = c("horTh", "tsize"), 
                             variable.name = "surv_id", value.name = "time"
                             )

# Use surv_wbmod$surv_id to add the correct survival probabilities surv
surv_wbmod$surv <- surv[as.numeric(surv_wbmod$surv_id)]

# Add columns upper, lower, std.err, and strata to the data.frame
surv_wbmod[, c("upper", "lower", "std.err", "strata")] <- NA

# Take a look at the structure of the object
str(surv_wbmod)

# Plot the survival curves
survminer::ggsurvplot_df(surv_wbmod, surv.geom = geom_line,
                         linetype = "horTh", color = "tsize", legend.title = NULL
                         )


# Weibull model
wbmod <- survival::survreg(survival::Surv(time, cens) ~ horTh, data = GBSG2)

# Log-Normal model
lnmod <- survival::survreg(survival::Surv(time, cens) ~ horTh, data = GBSG2, dist = "lognormal")

# Newdata
newdat <- data.frame(horTh = levels(GBSG2$horTh))

# Surv
surv <- seq(0.99, .01, by = -.01)

# Survival curve from Weibull model and log-normal model
wbt <- predict(wbmod, type = "quantile", p = 1-surv, newdata = newdat)
lnt <- predict(lnmod, type = "quantile", p = 1-surv, newdata = newdat)


dfWbtLnt <- as.data.frame(rbind(wbt, lnt))
names(dfWbtLnt) <- as.character(1:99)

surv_wide <- cbind(data.frame(horTh=factor(c("no", "yes", "no", "yes"), levels=c("no", "yes"))), 
                   dfWbtLnt, 
                   data.frame(dist=factor(c("weibull", "weibull", "lognormal", "lognormal")))
                   )

# Melt the data.frame into long format.
surv_long <- reshape2::melt(surv_wide, id.vars = c("horTh", "dist"), 
                            variable.name = "surv_id", 
                            value.name = "time"
                            )

# Add column for the survival probabilities
surv_long$surv <- surv[as.numeric(surv_long$surv_id)]

# Add columns upper, lower, std.err, and strata contianing NA values
surv_long[, c("upper", "lower", "std.err", "strata")] <- NA


# Plot the survival curves
survminer::ggsurvplot_df(surv_long, surv.geom = geom_line, 
                         linetype = "horTh", color = "dist", legend.title = NULL
                         )

```
  
  
  
***
  
Chapter 4 - Cox Model  
  
Cox Model - most widely used model in survival analysis:  
  
* Semi-parametric (vs. Weibull model which is fully parametric)  
* Also called the "proportional hazards" model; instantaneous probability is assumed to be proportional, meaning that curves cannot cross  
* Computing the Cox model is very similar to the Weibull model; will be no intercept, and negative coefficient mean positive impact on survival  
	* cxmod <- coxph(Surv(time, cens) ~ horTh, data = GBSG2)  
  
Visualizing the Cox Model:  
  
* Five steps for visualizing the Cox model  
	* Compute Cox model  
    * Decide on covariate combinations ("imaginary patients")  
    * Compute survival curves  
    * Create data.frame with survival curve information  
    * Plot  
* Example code includes  
	* cxmod <- coxph(Surv(time, cens) ~ horTh + tsize, data = GBSG2)  
    * newdat <- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75)) )  
    * rownames(newdat) <- letters[1:6]  
    * cxsf <- survfit(cxmod, data = GBSG2, newdata = newdat, conf.type = "none")  
    * surv_cxmod0 <- surv_summary(cxsf)  
    * surv_cxmod <- cbind(surv_cxmod0, newdat[as.character(surv_cxmod0$strata), ])  
    * ggsurvplot_df(surv_cxmod, linetype = "horTh", color = "tsize", legend.title = NULL, censor = FALSE)  
  
Recap:  
  
* Concepts - survival analysis, censoring, survival curves  
* Methods - Kaplan-Meier, Weibull, Cox  
* Focus - understand, estimate, and visualize survival curves  
  
Wrap Up:  
  
* Ability to interpret model estimates is a valuable skill is a useful follow-up step  
	* Statistical inference, confidence intervals, etc.  
* Competing risk models is a valuable follow-on skill  
* Many additional modeling libraries available on CRAN  
  
Example code includes:  
```{r}

dat$performance <- c(90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80, 80, 90, 50, 60, 90, 80, 100, 70, 90, 90, 90, 100, 60, 80, 70, 90, 60, 60, 50, 70, 50, 70, 70, 50, 80, 80, 60, 90, 70, 60, 60, 90, 80, 90, 90, 90, 80, 90, 100, 90, 90, 100, 70, 80, 90, 70, 90, 80, 90, 80, 70, 70, 90, 100, 80, 90, 80, 70, 80, 90, 90, 100, 80, 90, 90, 100, 70, 80, 80, 80, 80, 80, 100, 90, 70, 100, 80, 90, 80, 100, 80, 80, 90, 90, 90, 100, 80, 70, 90, 50, 80, 80, 90, 100, 60, 90, 80, 80, 90, 80, 70, 70, 60, 70, 80, 90, 70, 70, 60, 90, 80, 80, 80, 80, 90, 80, 80, 100, 80, 90, 60, 80, 80, 90, 100, 70, 80, 70, 80, 80, 90, 100, 90, 100, 100, 70, 90, 90, 80, 70, 70, 90, 70, 80, 80, 90, 90, 60, 90, 80, 90, 80, 100, 90, 100, 90, 90, 90, 100, 90, 80, 60, 80, 80, 100, 100, 100, 90, 80, 90, 90, 70, 90, 80, 90, 80, 60, 90, 90, 90, 100, 80, 90, 100, 90, 90, 60, 90, 100, 100, NA, 80, 60, 80, 90, 100, 80, 90, 70, 80, 90, 90, 80, 70, 80, 80, 80, 80, 80, 90, 60, 90, 80)
str(dat)


# Compute Cox model
cxmod <- survival::coxph(survival::Surv(time, status) ~ performance, data = dat)

# Show model coefficient
coef(cxmod)


# Cox model
cxmod <- survival::coxph(survival::Surv(time, cens) ~ horTh + tsize, data = GBSG2)

# Imaginary patients
newdat <- expand.grid(horTh = levels(GBSG2$horTh), 
                      tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75))
                      )
rownames(newdat) <- letters[1:6]

# Compute survival curves
cxsf <- survival::survfit(cxmod, data = GBSG2, newdata = newdat, conf.type = "none")

# Look at first 6 rows of cxsf$surv and time points
head(cxsf$surv)
head(cxsf$time)


# Remove conf.type="none" per https://github.com/kassambara/survminer/issues/355
cxsf <- survival::survfit(cxmod, data = GBSG2, newdata = newdat)

# Compute data.frame needed for plotting
surv_cxmod0 <- survminer::surv_summary(cxsf)

# Get a character vector of patient letters (patient IDs)
pid <- as.character(surv_cxmod0$strata)

# Multiple of the rows in newdat so that it fits with surv_cxmod0
m_newdat <- newdat[pid, ]

# Add patient info to data.frame
surv_cxmod <- cbind(surv_cxmod0, m_newdat)

# Plot
survminer::ggsurvplot_df(surv_cxmod, linetype = "horTh", color = "tsize", 
                         legend.title = NULL, censor = FALSE
                         )


# Compute Cox model and survival curves
cxmod <- survival::coxph(survival::Surv(time, status) ~ performance, data = dat)
new_lung <- data.frame(performance = c(60, 70, 80, 90))
cxsf <- survival::survfit(cxmod, data = dat, newdata = new_lung)

# Use the summary of cxsf to take a vector of patient IDs
surv_cxmod0 <- survminer::surv_summary(cxsf)
pid <- as.character(surv_cxmod0$strata)

# Duplicate rows in newdat to fit with surv_cxmod0 and add them in
m_newdat <- new_lung[pid, , drop = FALSE]
surv_cxmod <- cbind(surv_cxmod0, m_newdat)

# Plot
survminer::ggsurvplot_df(surv_cxmod, color = "performance", legend.title = NULL, censor = FALSE)


# Compute Kaplan-Meier curve
km <- survival::survfit(survival::Surv(time, status) ~ 1, data = dat)

# Compute Cox model
cxmod <- survival::coxph(survival::Surv(time, status) ~ performance, data = dat)

# Compute Cox model survival curves
new_lung <- data.frame(performance = c(60, 70, 80, 90))
cxsf <- survival::survfit(cxmod, data = dat, newdata = new_lung)

# Plot Kaplan-Meier curve
survminer::ggsurvplot(km, conf.int = FALSE)

# Plot Cox model survival curves
survminer::ggsurvplot(cxsf, censor = FALSE)

```
  
  
  
***
  
###_Building Response Models in R_  
  
Chapter 1 - Response Models for Product Sales  
  
Fundamentals of Market Response Models:  
  
* Market response models are statistical tools to optimize A&P for the marketing mix  
* Dataset for the course will be for retial sales  
	* str(sales.data)  
  
Linear Response Models:  
  
* The model will have sales as the response variable and the other variables as predictors  
* Example of the simple linear model for Sales vs. Price  
	* linear.model <- lm(SALES ~ PRICE , data = sales.data)  
    * coef(linear.model)  
    * coef(linear.model)[1] + 0.95 * coef(linear.model)[2]  
    * plot(SALES ~ PRICE, data = sales.data)  
    * abline(coef(linear.model))  
  
Non-Linear Response Models:  
  
* Non-linear response is a more common real-world marketing mix behavior  
* The exponential response function assumes a constant elasticity  
	* Sales = B0 * exp(B1 * Price)  
    * log(Sales) = log(B0) + (B1*Price)  
    * log.model <- lm(log(SALES) ~ PRICE, data = sales.data)  
    * coef(log.model)  
    * plot(log(SALES) ~ PRICE, data = sales.data)  
    * log.model <- lm(log(SALES) ~ PRICE, data = sales.data)  
    * abline(coef(log.model))  
  
Example code includes:  
```{r}

load("./RInputFiles/sales.data.RData")
load("./RInputFiles/choice.data.RData")
str(sales.data)
str(choice.data)


# Tail of sales.data
tail(sales.data)

# Mean SALES
mean(sales.data$SALES)

# Minimum SALES
min(sales.data$SALES)

# Maximum SALES
max(sales.data$SALES)


# Linear model explaining SALES by PRICE
linear.model <- lm(SALES ~ PRICE, data = sales.data)

# Obtain the model coefficients
coef(linear.model)


# Obtain the intercept coefficient
coef(linear.model)[1]

# Obtain the slope coefficient
coef(linear.model)[2]

# Predict the SALES for the decreased PRICE of 1.05 
coef(linear.model)[1] + 1.05 * coef(linear.model)[2]

# Predict the SALES for the decreased PRICE of 0.95 
coef(linear.model)[1] + 0.95 * coef(linear.model)[2]


# Linear model explaining SALES by PRICE
linear.model <- lm(SALES ~ PRICE, data = sales.data)

# Plot SALES against PRICE
plot(SALES ~ PRICE, data = sales.data)

# Adding the model predictions
abline(coef(linear.model))


# Linear model explaining log(SALES) by PRICE
log.model <- lm(log(SALES) ~ PRICE, data=sales.data)

# Obtaining the model coefficients
coef(log.model)


# Plot log(SALES) against PRICE
plot(log(SALES) ~ PRICE, data=sales.data)

# Linear model explaining log(SALES) by PRICE
log.model <- lm(log(SALES) ~ PRICE, data=sales.data)

# Adding the model predictions
abline(coef(log.model))

```
  
  
  
***
  
Chapter 2 - Extended Sales Response Modeling  
  
Model Extension Part 1: Dummy Variables:  
  
* Dummy variables typically take on the value of 0 or 1  
* Can start by summarizing dummy variables in the sales.data file  
	* table(sales.data$DISPLAY)  
    * table(sales.data$DISPLAY)/sum(table(sales.data$DISPLAY))  
    * mean(sales.data$DISPLAY)  
* Can also include dummy variables in the regression  
	* dummy.model <- lm(log(SALES) ~ DISPLAY, data = sales.data)  
    * coef(dummy.model)  
    * exp(coef(dummy.model)[1])  # Average unit sales for no display  
    * exp(coef(dummy.model)[2] - 1)  # Percentage increase in sales for a display  
* Can add dummy variables for areas like discounts  
	* summary(sales.data[,c("DISPLAY","COUPON","DISPLAYCOUPON")])  
    * dummy.model <- lm(log(SALES) ~ DISPLAY + COUPON + DISPLAYCOUPON, data = sales.data)  
    * coef(dummy.model)  
    * lm(update(dummy.model, . ~ . + PRICE), data = sales.data)  # adds PRICE to the model  
  
Model Extensions Part 2: Dynamic Variables:  
  
* The carryover effect is the time span between marketing activities and response times  
	* Typically evaluated using lags  
    * head(cbind(sales.data$PRICE, lag(sales.data$PRICE, n = 1)))  
    * Price.lag <- lag(sales.data$PRICE)  
    * lag.model <- lm(log(SALES) ~ PRICE + Price.lag, data = sales.data)  
    * Coupon.lag <- lag(sales.data$COUPON)  
    * lm(update(lag.model, . ~ . + COUPON + Coupon.lag), data = sales.data)  
    * lag.model <- lm(log(SALES) ~ PRICE + Price.lag + DISPLAY + Display.lag + COUPON + Coupon.lag + DISPLAYCOUPON + DisplayCoupon.lag, data = sales.data)  
    * plot(log(SALES) ~ OBS, data = sales.data)  
    * lines(c(NA, fitted.values(lag.model)) ~ OBS, data = sales.data)  
  
Number of Extensions Needed:  
  
* Can summarize the model to view top-level findings, including R-squared and p-values for each variable  
	* summary(extended.model)  
    * AIC(extended.model)  # information criteria  
    * AIC(lm(update(extended.model, . ~ . - Coupon.lag), data = sales.data))  
* Can also run an elimination process using MASS  
	* library(MASS)  
    * final.model <- stepAIC(extended.model, direction = "backward", trace = FALSE)  
    * summary(final.model)  
  
Example code includes:  
```{r}

# Proportion of DISPLAY and no-DISPLAY activity
table(sales.data$DISPLAY) / sum(table(sales.data$DISPLAY))

# Mean of DISPLAY activity
mean(sales.data$DISPLAY)

# Mean of no-DISPLAY activity
1 - mean(sales.data$DISPLAY)

# Linear model explaining log(SALES) by DISPLAY
dummy.model <- lm(log(SALES) ~ DISPLAY, data = sales.data)

# Obtaining the coefficients
coef(dummy.model)


# Mean DISPLAY activity
mean(sales.data$DISPLAY)

# Mean COUPON activity
mean(sales.data$COUPON)

# Mean DISPLAY and COUPON activity
mean(sales.data$DISPLAYCOUPON)

# Summarize DISPLAY, COUPON, DISPLAYCOUPON activity
summary(sales.data[,c("DISPLAY", "COUPON", "DISPLAYCOUPON")])


# Linear model explaining log(SALES) by DISPLAY, COUPON and DISPLAYCOUPON
dummy.model <- lm(log(SALES) ~ DISPLAY + COUPON + DISPLAYCOUPON, data = sales.data)

# Obtain the model coefficients
coef(dummy.model)


# Dummy.mod updated for PRICE
update(dummy.model, . ~ . + PRICE)


# Compare lagged PRICE and original PRICE
head(cbind(sales.data$PRICE, lag(sales.data$PRICE)))


# Create the lagged PRICE variable
Price.lag <- lag(sales.data$PRICE)

# Linear model explaining log(SALES) by PRICE and Price.lag
lag.model <- lm(log(SALES) ~ PRICE + Price.lag, data = sales.data)

# Obtain the coefficients
coef(lag.model)


# Create the lagged COUPON variable
Coupon.lag <-  lag(sales.data$COUPON)

# Update lag.model for COUPON and C_lag
update(lag.model, . ~ . + COUPON + Coupon.lag)


sales.data2 <- sales.data %>%
    mutate(Price.lag = lag(PRICE, 1), 
           Display.lag = lag(DISPLAY, 1),
           Coupon.lag = lag(COUPON, 1),
           DisplayCoupon.lag = lag(DISPLAYCOUPON, 1)
           )

# Extended sales resonse model
extended.model <- lm(log(SALES) ~ PRICE + Price.lag + DISPLAY + Display.lag + COUPON + 
                         Coupon.lag + DISPLAYCOUPON + DisplayCoupon.lag, data = sales.data2
                     )

# Plot log(SALES) against OBS
plot(log(SALES) ~ OBS, data = sales.data2)

# Add the model predictions
lines(c(NA, fitted.values(extended.model)) ~ OBS, data = sales.data2)


# Summarize the model
summary(extended.model)

# AIC of the extended response model
AIC(extended.model)

# Single term deletion
AIC(lm(update(extended.model, . ~ . -Coupon.lag), data = sales.data2))


# Backward elemination
final.model <- MASS::stepAIC(extended.model, direction = "backward", trace = FALSE)

# Summarize the final model
summary(final.model)

```
  
  
  
***
  
Chapter 3 - Response Models for Individual Demand  
  
Models for Individual Demand:  
  
* Data are available about customer purchases  
	* str(choice.data)  
    * OBS-ervation week  
    * HOUSEHOLDID of the purchase records  
    * LASTPURCHASE recorded of the household  
* The probability to purchase cannnot be well modeled with a purely linear model  
	* probability.model <- lm(HOPPINESS ~ PRICE.HOP, data = choice.data)  
    * plot(HOPPINESS ~ PRICE.HOP, data = choice.data) abline(probability.model)  
    * abline(probability.model)  
* Can define a new variable, price ratio  
	* price.ratio <- log(choice.data$PRICE.HOP/choice.data$PRICE.BUD)  
    * probability.model <- lm(HOPPINESS ~ price.ratio, data = choice.data)  
    * plot(HOPPINESS ~ price.ratio, data = choice.data)  
    * abline(probability.model)  
  
Logistic Response Models:  
  
* The logistic response function is a better predictor for choice data since it is bounded between 0 and 1  
	* logistic.model <- glm(HOPPINESS ~ price.ratio, family = binomial, data = choice.data)  
    * coef(logistic.model)  
    * plot(HOPPINESS ~ price.ratio, data = choice.data)  
    * curve(predict(logistic.model, data.frame(price.ratio = x), type = "response"), add = TRUE)  
    * margins(logistic.model)  
* Can also look at an effects plot using the margins package  
	* x <- seq(-1.25, 1.25, by = 0.25)  
    * cplot(logistic.model, "price.diff", xvals = x)  
  
Probit Response Models:  
  
* Can use the probit response function in lieu of the logit response function, with slight changes to the tails  
	* probit.model <- glm(HOPPINESS ~ price.ratio, family = binomial(link = probit), data = choice.data)  
    * coef(probit.model)  
    * cbind(coef(logistic.model), coef(probit.model))  
    * margins(logistic.model)  # interpretable log-odds  
    * margins(probit.model)  # non-interpretable z-values  
  
Example code includes:  
```{r}

# Structure of choice.data
str(choice.data)

# Summarize purchases of HOPPINESS, BUD and PRICE.HOP and PRICE.BUD
summary(choice.data[,c("HOPPINESS", "BUD", "PRICE.HOP", "PRICE.BUD")]) 


# Plot HOPPINESS against PRICE.HOP
plot(HOPPINESS ~ PRICE.HOP, data = choice.data)

# Linear probability model explaining HOPPINESS by PRICE.HOP
probability.model <- lm(HOPPINESS ~ PRICE.HOP, data = choice.data)

# Add the model predictions
abline(coef(probability.model))


# Calculate the price ratio for HOPPINESS and BUD
choice.data$price.ratio <- log(choice.data$PRICE.HOP / choice.data$PRICE.BUD)

# Plot HOPPINESS purchases against the price ratio
plot(HOPPINESS ~ price.ratio, data = choice.data)

# Linear probability model explaining HOPPINESS by price.ratio
probability.model <- lm(HOPPINESS ~ price.ratio, data = choice.data)

# Add the model predictions
abline(probability.model)


# Logistic model explaining HOPPINESS by price.ratio
logistic.model <- glm(HOPPINESS ~ price.ratio, family = binomial, data = choice.data)

# Obtain the coefficients
coef(logistic.model)


# Plot HOPPINESS choices against price.diff
plot(HOPPINESS ~ price.ratio, data = choice.data)

# Add the predictions of the logistic model
curve(predict(logistic.model, data.frame(price.ratio = x), type = "response"), add = TRUE)


# Linear probability model
coef(probability.model)

# Logistic model
margins::margins(logistic.model)


# Sequence of x values
x <- seq(-1, 1, by = 0.10)

# Conditional effect plot
margins::cplot(logistic.model, "price.ratio", xvals = x)


# Probit model explaining HOPPINESS by price.ratio
probit.model <- glm(HOPPINESS ~ price.ratio, family = binomial(link=probit), data = choice.data)

# Obtain the coefficients
coef(probit.model)


# Compare the coefficients
cbind(coef(probit.model), coef(logistic.model))


# Logistic model
margins::margins(logistic.model)

# Probit model
margins::margins(probit.model)

```
  
  
  
***
  
Chapter 4 - Extended Demand Modeling  
  
Model Selection:  
  
* Can extend the model to include display feature data  
	* summary(choice.data[,c("FEAT.HOP","DISPL.HOP","FEATDISPL.HOP")])  
    * extended.model <- glm(HOPPINESS ~ price.ratio + DISPL.HOP + FEAT.HOP + FEATDISPL.HOP, family = binomial, data = choice.data)  
    * margins(extended.model)  
    * summary(extended.model)  
* The null deviance and the residual deviance can be assessed using an intercept-only model and an ANOVA  
	* null.model <- glm(HOPPINESS ~ 1, family = binomial, data = choice.data)  
    * anova(extended.model, null.model, test = "Chisq")  
* Can also run a stepwise model and assess using AIC  
	* final.model <- stepAIC(extended.model, direction = "backward", trace = FALSE)  
    * summary(final.model)  
  
Predictive Performance:  
  
* Can use cutoffs on the predicted probabilities to create a classifier  
	* predicted <- ifelse(fitted.values(extended.mod) >= 0.5, 1, 0)  
    * observed <- choice.data$HOPPINESS  
    * table(observed, predicted)/2798  # There are 2798 samples (confusion matrix)  
    * Roc <- pROC::roc(predictor = fitted.values(extended.mod), response = observed)  
    * plot(Roc)  
  
Model Validation:  
  
* Can evaluate model on independent (unseen) datasets - example of subsetting on LASTPURCHASE  
	* train.data <- subset(choice.data, subset = LASTPURCHASE == 0)  
    * test.data <- subset(choice.data, subset = LASTPURCHASE == 1)  
    * train.model <- glm(HOPPINESS ~ price.diff + FEAT.HOP + FEATDISPL.HOP, family = binomial, data = train.data)  
    * margins(train.model)  
    * prob <- predict(train.model, test.data, type = "response")  
    * predicted <- ifelse(prob >= 0.5, 1, 0)   
    * observed <- test.data$HOPPINESS  
    * table(predicted, observed)/300  
  
Wrap Up:  
  
* Linear and non-linear response models  
* Logit and probit models  
* Dummy and lagged variables  
* Test and control approaches  
  
Example code includes:  
```{r}

# Summarizing DISPLAY.HOP, FEAT.HOP, FEATDISPL.HOP actions
summary(choice.data[, c("DISPL.HOP", "FEAT.HOP", "FEATDISPL.HOP")])

# Logistic model explaining HOPPINESS by price.diff, DISPL.HOP, FEAT.HOP, FEATDISPL.HOP
extended.model <- glm(HOPPINESS ~ price.ratio + DISPL.HOP + FEAT.HOP + FEATDISPL.HOP, 
                      family = binomial, data  = choice.data
                      )

# Marginal effects for the extended logistic response model
margins::margins(extended.model)

# Summarize the model
summary(extended.model)


# Null model explaining HOPPINESS by the intercept only
null.model <- glm(HOPPINESS ~ 1, family = binomial, data = choice.data)

# Compare null.mod against extended.mod
anova(extended.model, null.model, test = "Chisq")


# Backward elemination
final.model <- MASS::stepAIC(extended.model, direction = "backward", trace = FALSE)

# Summarize the final model
summary(final.model)


# Classifying the predictions
predicted <- ifelse(fitted.values(extended.model) >= 0.5, 1, 0)

# Obtain the purchase predictions
table(predicted)


# Obtain the observed purchases
observed <- choice.data$HOPPINESS

# Cross-tabulating the observed vs. the predicted purchases
table(predicted, observed)/2798


# Creating the Roc object
Roc <- pROC::roc(predictor = fitted.values(extended.model), response = observed)

# Plot the ROC curve
pROC::plot.roc(Roc)


# Create the training dataset
train.data <- subset(choice.data, LASTPURCHASE == 0)

# Create the test dataset
test.data <- subset(choice.data, LASTPURCHASE == 1)


# Fit logistic response model to the training data set
train.model <- glm(HOPPINESS ~ price.ratio + FEAT.HOP + FEATDISPL.HOP, 
                   family = binomial, data = train.data
                   )


# Predict the purchase probabilities for test.data
prob <- predict(train.model, newdata=test.data, type = "response") 

# Classify the predictions
predicted <- ifelse(prob >= 0.5, 1, 0) 

# Obtain the observed purchases from test.data
observed <- test.data$HOPPINESS

# Cross-tabulate  the predicted vs. the observed purchases
table(predicted, observed)/300

```
  
  
  
***
  
###_Time Series with data.table in R_  
  
Chapter 1 - Review of data.table  
  
Introduction:  
  
* Data frames are a general purpose data structure - rectangular structure reflecting a list of lists, though unlike matrices, the lists may be of multiple data types  
* The data.table package is an extension of data.frame - more efficient memory usage and more expressive syntax  
    * library(data.table)  
    * someDT <- data.table(x = rnorm(100), y = rep(TRUE, 100))  
    * str(someDT)  # Classes 'data.table' and 'data.frame':    100 obs. of  2 variables:  
* Can select columns using .()  
	* baseballDT[, .(timestamp, winning_team)]  
* Can use .SD for column selection  
	* cols <- c("timestamp", "winning_team")  
    * baseballDT[, .SD, .SDcols = cols]  
* Can use grep() to help with the matching  
	* count_cols <- grep('COUNT$', names(baseballDT), value = TRUE)  
    * countDT <- baseballDT[, .SD, .SDcols = count_cols]  
* Can simultaneously select rows and columns, and they are objects inside the data.table environment  
	* cols <- c("timestamp", "winning_team")  
    * baseballDT[ which.max(timestamp), .SD, .SDcols = cols ]  
  
Flexible Data Selection:  
  
* The get() function allows for evaluating a string as a column reference  
	* locDT <- data.table( cities = c("Chicago", "Boston", "Milwaukee"), ppl_mil = c(2.7, 0.673, 0.595) )  
    * city_col <- "cities"  
    * locDT[, get(city_col)]  
    * square_col <- function(DT, col_name){ return(DT[, get(col_name) ^ 2]) }  
* The () mean that you are accessing something that is external to the data.table  
	* add_bil_ppl <- function(DT, new_name){ DT[, (new_name) := ppl_mil * 1000 }  
    * add10 <- function(DT, cols){ for (col in cols){ new_name <- paste0(col, "_plus10") ; DT[, (new_name) := get(col) + 10] } }  
    * add10(locDT, cols = "ppl_mil")  
* Can change names using setnames() - modifies the table in place (no copies)  
	* setnames(locDT, old = "cities", new = "city_names")  
    * tag_important_columns <- function(DT, cols){ setnames(DT, old = cols, new = paste0(cols, "_important")) }  
    * tag_important_columns(locDT, "ppl_mil")  
  
Executing Functions Inside data.tables:  
  
* Can evaluate functions inside the data.table operators  
* Functions in the "I" block can be used to select rows, with booleans used for subsetting  
	* stockDT <- data.table( close_date = seq.POSIXt(as.POSIXct("2017-01-01"), as.POSIXct("2017-01-30"), length.out = 100), MSFT = runif(100, 70, 80), AAPL = runif(100, 140, 180) )  
    * stockDT[close_date > max(close_date) - 60 * 60 * 8]  
* Functions in the "j" block can be used to summarize data  
	* cor(stockDT[, .SD, .SDcols = c('AAPL', 'MSFT')])  
    * corr_mat <- stockDT[, cor(.SD), .SDcols = c('AAPL', 'MSFT')]  
    * stockDT[, rand_noise := AAPL + rnorm(100)]  
* Can use the "by" group to dynamically group data  
	* stockDT[, hour_of_day := as.integer(strftime(close_date, "%H"))]  
    * stockDT[, mean(AAPL), by = hour_of_day][order(hour_of_day)]  
    * stockDT[, mean(AAPL), by = .( hour_of_day = as.integer(strftime(close_date, "%H")) )][order(hour_of_day)]  # same as above, but with a 1-step process  
* Since a data.table is a list of lists, can use lapply or sapply on the data.table  
	* Use lapply() if you want a data.table back  
    * Use sapply() if you want a vector or list back  
    * stockDT[, lapply(.SD, function(x){mean(is.na(x))})]  
    * num_obs <- stockDT[, sapply(.SD, function(x){sum(!is.na(x), na.rm = TRUE)})]  
  
Example code includes:  
```{r}

library(data.table)

diagnosticDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/diagnosticDT.feather"))
str(diagnosticDT)


# Select system voltage directly
voltageDT <- diagnosticDT[, .(timestamp, system_voltage)]

# Select system voltage with .SD
voltageDT <- diagnosticDT[, .SD, .SDcols = c("timestamp", "system_voltage")]

# Select system voltage with .SD + a vector of names
voltage_cols <- c("timestamp", "system_voltage")
voltageDT <- diagnosticDT[, .SD, .SDcols = voltage_cols]

diagnosticDT[which.max(timestamp), .SD, .SDcols=c("timestamp", "system_voltage")]


# Store the names of all columns starting with "engine_" in a vector
engine_cols <- grep(pattern = "engine_", x = names(diagnosticDT), value = TRUE)

# Use that vector to create a new data.table with only engine signals
engineDT <- diagnosticDT[, .SD, .SDcols = engine_cols]


# Complete the function
add_interaction <- function(someDT, col1, col2){
    new_col_name <- paste0(col1, "_times_", col2)
    someDT[, (new_col_name) := get(col1) * get(col2)]
}

# Add an interaction
add_interaction(diagnosticDT, "engine_speed", "engine_temp")

# Check it out!
head(diagnosticDT)


# Write a function to scale a column by 10
scale_by_10 <- function(someDT, col_to_scale, new_col_name){
    someDT[, (new_col_name) := get(col_to_scale) * 10]
}

# Try it out
scale_by_10(diagnosticDT, "engine_temp", "temp10")

# Check the state of the data.table
head(diagnosticDT)


# Write a function that squares every numeric column
add_square_features <- function(someDT, cols){
    for (col_name in cols){
        new_col_name <- paste0(col_name, "_squared")
        someDT[, (new_col_name) := get(col_name)^2 ]
    }
}

# Look at the difference!
add_square_features(diagnosticDT, c("engine_speed", "engine_temp", "system_voltage"))
head(diagnosticDT)


# Change names
setnames(diagnosticDT, old = c("timestamp"), new = "obs_time")

# Tag all the numeric columns with "_NUMERIC"
tag_numeric_cols <- function(DT, cols){
    setnames(DT, old = cols, new = paste0(cols, "_NUMERIC"))
}

# Tag numeric columns
tag_numeric_cols(diagnosticDT, c("engine_speed", "engine_temp", "system_voltage"))


diagnosticDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/diagnosticDT.feather"))
str(diagnosticDT)
diagnosticDT2 <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/diagnosticDT.feather"))
str(diagnosticDT2)


# Mean of engine temp
diagnosticDT[, mean(engine_temp)]

# Correlation between engine_temp and system_voltage
diagnosticDT[, cor(engine_temp, system_voltage)]

# Get classes of column names
correlations <- function(DT){
    # Find numeric columns
    num_cols <- diagnosticDT[, sapply(.SD, is.numeric)]
    numeric_cols <- names(diagnosticDT)[num_cols]
    return(DT[, cor(.SD), .SDcols = numeric_cols])
}


# Mean of system voltage
diagnosticDT[, lapply(.SD, mean), .SDcols = c("system_voltage")]

# Mean of all engine cols
engine_cols <- c("engine_speed", "engine_temp")
meanDT <- diagnosticDT[, lapply(.SD, mean), .SDcols = engine_cols]
print(meanDT)


get_numeric_cols <- function(DT){
    num_cols <- DT[, sapply(.SD, is.numeric)]
    return(names(DT)[num_cols])
}

# Function to get correlation matrix from a data.table
corrmat_from_dt <- function(DT){
    numeric_cols <- get_numeric_cols(DT)
    return(DT[, cor(.SD), .SDcols = numeric_cols])
}

# Get correlation matrices
corrmat_from_dt(diagnosticDT)
corrmat_from_dt(diagnosticDT2)

```
  
  
  
***
  
Chapter 2 - Getting Time Series Data into data.table  
  
Overview of the POSIXct Type:  
  
* The name POSIX means "Portable Operating System for Unix"  
	* POSIXlt = a list object with date-time components like year and day stored in individual attributes  
    * POSIXct = a signed integer representing seconds since 1970-01-01, with a single attribute capturing timezone  
    * POSIXct is generally preferred in databases, since it is easiest to sort and manipulate  
    * This course always uses UTC (constant time zone)  
* Can convert other formats to POSIXct  
	* as.POSIXct("2004-10-27", tz = "UTC")  
    * as.POSIXct(1540153601, origin = "1970-01-01", tz = "UTC")  # if the integer is number of seconds since 1970-01-01  
    * as.POSIXct(as.Date(42885, origin = "1900-01-01"), tz = "UTC")  # if the integer is number of days since 1970-01-01  
* The as.POSIXct is vectorized  
	* dates <- c("2004-10-24", "2004-10-25", "2004-10-26")  
    * as.integer(as.POSIXct(dates, tz = "UTC"))  
    * someDT <- data.table(dates = c("2004-10-24", "2004-10-25", "2004-10-26"))  
    * someDT[, posix := as.POSIXct(dates, tz = "UTC")]  
* Example for converting columns in a dataset without overwriting the original data  
	* gameDT <- data.table( game_date = c("2004-10-23", "2004-10-24", "2004-10-26", "2004-10-27") )  
    * gameDT[, posix_date := as.POSIXct(game_date, tz = "UTC")]  
* The lubridate family of functions can be applied also  
	* the_date <- "10-27-2004 22:29:00"  
    * lubridate::mdy_hms(the_date)  
  
Creating data.tables from vectors:  
  
* Can create data.tables from vectors sinmilar to the process for data.frames  
	* candyDT <- data.table( color = c("red", "blue", "green"), size = c("S", "L", "S"), num = c(100, 50, 210) )  
    * testDT <- data.table( rand_numbers = rnorm(100), rand_strings = sample(LETTERS, n = 100, replace = TRUE), simple_index = 1:100, sample_dates = seq.POSIXt( from = as.POSIXct("1990-01-01"), to = as.POSIXct("1992-08-01"), length.out = 100), fifty_fifty_split = c(rep(TRUE, 50), rep(FALSE, 50)) )  
* The seq.POSIXt() is an extension of seq() for time data  
	* start <- as.POSIXct("2010-06-17", tz = "UTC")  
    * end <- as.POSIXct("2010-06-18", tz = "UTC")  
    * hourlyDT <- data.table( timestamp = seq.POSIXt(start, end, length.out = 1 + 24) )  
    * minuteDT <- data.table( timestamp = seq.POSIXt(start, end, length.out = 1 + 24 * 60) )  
* Can use the .N to dynamically size the input vector  
	* add_stock_data <- function(DT){ DT[, COMPANY1 := rnorm(n = .N)] DT[, COMPANY2 := rnorm(n = .N)] }  
  
Coercing from xts:  
  
* The xts format is popular, and can be used for conversions to data.table  
	* dates <- seq.POSIXt(from = as.POSIXct("2017-06-15"), to = as.POSIXct("2017-06-16"), length.out = 24)  
    * ex_tee_ess<- xts::xts( x = rnorm(24), order.by = dates )  
* The xts object has several attributes  
	* tclass = R class for the date-time index  
    * tzone = timezone for date-time index  
    * attr(ex_tee_ess, "tclass")  
    * attr(ex_tee_ess, "tzone")  
* The expressive subsetting is one of the reasons for xts popularity  
	* ['/'] = "the whole dataset"  
    * ['2017'] = "data from 2017"  
    * ['2017-01/'] = "data from January 2017 to the end of the data"  
    * ['2014/2015'] = "data from 2014 to 2015"  
* Can also use functions for converting the units of time  
	* to.minutes(), to.minutes10(), to.daily()  
    * xts::to.daily(hourlyXTS)  
* Can convert to data.table using as.data.table() - may need to modify column names to be more meaningful  
	* hourlyDT <- data.table::as.data.table( hourlyXTS )  
    * data.table::setnames(hourlyDT, "V1", "stock_price")  
  
Combining datasets with merge() and rbindlist():  
  
* Merging with timestamps depends on numeric precision, since POSIXct is a number  
* Precision-safe merges can be managed using the round() function  
	* secDT[, timestamp := as.POSIXct(round(as.numeric(timestamp)), origin = "1970-01-01")]  
    * milliDT[, timestamp := as.POSIXct(round(as.numeric(timestamp)), origin = "1970-01-01")]  
    * merge(secDT, milliDT, by = "timestamp", all = TRUE)   
* May need to run down-sampling process to match up records appropriately  
	* salesDT[, .(ts, year = year(ts), mday = mday(ts), hour = hour(ts))]  
    * dailySalesDT[, day_int := mday(timestamp)]  
    * dailyPriceDT <- hourlyPriceDT[, .(price = mean(price)), by = mday(timestamp)]  
    * mergeDT <- merge( dailySalesDT, dailyPriceDT, by.x = "day_int", by.y = "day" )  
* Can also use rbindlist to stack data.tables - but, be careful about matching data types and names  
	* allDT <- rbindlist(list(DT1, DT2, DT3), fill = TRUE)  
  
Example code includes:  
```{r}

excelDT <- data.table(timecol=42885:42889, sales=c(105, 92, 500, 81, 230))
stringDT <- data.table(timecol=c("2017-06-01", "2017-06-02", "2017-06-03", "2017-06-04", "2017-06-05"),
                       sales=c(105, 92, 500, 81, 230)
                       )
epochSecondsDT <- data.table(timecol=1496275200 + 24*60*60*0:4, sales=c(105, 92, 500, 81, 230))
epochMillisDT <- data.table(timecol=1000 * c(1496275200 + 24*60*60*0:4), sales=c(105, 92, 500, 81, 230))


# Create POSIXct dates from a hypothetical Excel dataset
excelDT[, posix := as.POSIXct(as.Date(timecol, origin = "1900-01-01"), tz = "UTC")]

# Convert strings to POSIXct
stringDT[, posix := as.POSIXct(timecol, tz = "UTC")]

# Convert epoch seconds to POSIXct
epochSecondsDT[, posix := as.POSIXct(timecol, tz = "UTC", origin = "1970-01-01")]

# Convert epoch milliseconds to POSIXct
epochMillisDT[, posix := as.POSIXct(timecol/1000, origin = "1970-01-01", tz="UTC")]


stringDT <- data.table(timecol1=c("2017-06-01 10", "2017-06-02 5", "2017-06-03 10", 
                                  "2017-06-04 7", "2017-06-05 9"
                                  ),
                       timecol2=c("06-01-2017 10:00:00", "06-02-2017 05:00:00", "06-03-2017 10:00:00", 
                                  "06-05-2017 07:00:00", "06-04-2017 09:00:00"
                                  ),
                       sales=c(105, 92, 500, 81, 230)
                       )
stringDT


# Convert timecol1
str(stringDT)
stringDT[, posix1 := lubridate::ymd_h(timecol1)]
str(stringDT)

# Convert timecol2
str(stringDT)
stringDT[, posix2 := lubridate::mdy_hms(timecol2)]
str(stringDT)


# Generate a series of dates
march_dates <- seq.POSIXt(as.POSIXct("2017-03-01", tz="UTC"), 
                          as.POSIXct("2017-03-31", tz="UTC"), 
                          length.out = 31
                          )

# Generate hourly data
hourly_times <- seq.POSIXt(as.POSIXct("2017-05-01 00:00:00", tz="UTC"), 
                           as.POSIXct("2017-05-02 00:00:00", tz="UTC"), 
                           length.out = 1 + 24
                           )

# Generate sample IoT data
iotDT <- data.table(timestamp = seq.POSIXt(as.POSIXct("2016-04-19 00:00:00", tz="UTC"), 
                                           as.POSIXct("2016-04-20 00:00:00", tz="UTC"), 
                                           length.out = 1 + 24
                                           ),
                    engine_temp = rnorm(n=1+24),
                    ambient_temp = rnorm(n=1+24)
                    )
head(iotDT)


# Create a 500-row data.table
start_date <- "2016-01-01"
end_date <- "2018-01-01"
someDT <- data.table(timestamp = seq.POSIXt(as.POSIXct(start_date), 
                                            as.POSIXct(end_date), 
                                            length.out = 500
                                            )
                     )

# Function to add random columns
add_random_cols <- function(DT, colnames){
    for (colname in colnames){
        DT[, (colname) := rnorm(n = .N)]
    }
}

# Check out the new data.table
add_random_cols(someDT, c("copper", "chopper", "stopper"))


# Simulated data
some_data <- rnorm(100)
some_dates <- seq.POSIXt(from = as.POSIXct("2017-06-15 00:00:00Z", tz = "UTC"),
                         to = as.POSIXct("2017-06-15 01:00:00Z", tz = "UTC"),
                         length.out = 100
                         )

# Make your own 'xts' object
myXTS <- xts::xts(some_data, order.by=some_dates)

# View the timezone
print(attr(myXTS, "tzone"))


nickelXTS <- readRDS("./RInputFiles/nickelXTS.rds")

# All observations after 2018-01-01 00:45:00
fifteenXTS <- nickelXTS["2018-01-01 00:45:00/"]

# Check the structure
str(fifteenXTS)

# 10-minute aggregations
tenMinuteXTS <- xts::to.minutes10(nickelXTS)
print(tenMinuteXTS)

# 1-minute aggregations
oneMinuteXTS <- xts::to.minutes(nickelXTS)


# Convert to a data.table
nickelDT <- as.data.table(nickelXTS)
str(nickelDT)

# Change names
setnames(nickelDT, old="index", new="spot_price_timestamp")
print(nickelDT)


treasuryDT <- data.table(timestamp=as.POSIXct("2018-03-01 00:00:00", tz="UTC") + 0:4 + 0.001, 
                         treasury_10y=c(0.71, 0.8, 0.78, 0.77, 0.73)
                         )
oilDT <- data.table(timestamp=as.POSIXct("2018-03-01 00:00:00", tz="UTC") + 0:4, 
                    oil=c(44.07, 44.15, 44.14, 44.06, 44.09)
                    )

# Naive approach (merge on timestamp)
newDT <- merge(treasuryDT, oilDT, on = "timestamp")
str(newDT)

# Check out the precision
treasuryDT[, as.numeric(timestamp)]
oilDT[, as.numeric(timestamp)]

# Clean up and merge
treasuryDT[, timestamp := as.POSIXct(round(as.numeric(timestamp)), origin = "1970-01-01")]
newDT <- merge(treasuryDT, oilDT, on = "timestamp")
str(newDT)


# Add grouping indicator 
# fxDT[, yearmonth := paste0(year(timestamp), "_", month(timestamp))]
# exportDT[, yearmonth := paste0(year(timestamp), "_", month(timestamp))]

# Monthly exchange rate
# monthlyFXDT <- fxDT[, .(exch_rate = mean(exchange_rate)), by = yearmonth]

# Merge 
# merge(exportDT, monthlyFXDT, by="yearmonth")

```
  
  
  
***
  
Chapter 3 - Generating Lags, Differences, and Windowed Aggregations  
  
Generating Lags:  
  
* The lag represents the value of a time series n time-periods ago (common for forecasting)  
	* dailyDT[, lag15 := shift(sales, type = "lag", n = 15)]  
* The shift capability in data.table can either lag or lead, filling with NA when the data cannot be known  
	* someDT[, col1_lag1 := shift(col1, n = 1, type = "lag")]  
    * someDT[, col1_lead1 := shift(col1, n = 1, type = "lead")]  
* Need to ensure the proper sort order prior to lag/lead; the functions are time-nave  
	* setorderv(backwardsDT, "timestamp")  # operation is run in-place  
    * backwardsDT[, somenums_lag1 := shift(somenums, type = "lag", n = 1)]  
* Can also generate lage on the fly while modeling  
	* mod <- lm(sales ~ shift(sales, n = 21), data = dailyDT)  
* With long datasets, may need to add a "by" so that the lagging is done by subject  
	* experimentDT[, lag1 := shift(result, type = "lag", n = 1), by = subject_id]  
  
Generating Growth Rates and Differences:  
  
* Often a goal to find a stationary time series - same mean, variance, over time  
* Typically, change in metric can make an increasing time series in to a stationary differences series  
	* gdpDT[, lag1 := shift(gdp, type = "lag", n = 1)]  
    * gdpDT[, diff1 := gdp - lag1]  
    * gdpDT[, diff1 := gdp - shift(gdp, type = "lag", n = 1)]  # same as above, but in a single shot  
* May instead want to capture the pace of change, such as a growth rate  
	* gdpDT[, growth1 := (gdp - shift(gdp, type = "lag", n = 1)) / shift(gdp, type = "lag", n = 1) ]  
    * gdpDT[, growth1 := (gdp / shift(gdp, type = "lag", n = 1)) - 1 ]  # algebraic simplification  
  
Windowing with j and by:  
  
* Windowing is the process of evaluating a metric over a time period - e.g., heartbeats per minute  
	* salesDT[, nearest_month := month(timestamp)]  # create a grouping column  
    * aggDT <- salesDT[, .( min = min(sales), total = sum(sales), num_obs = length(sales) ), by = nearest_month ]  
* Can also do on-the-fly grouping by passing an expression to the by-clause  
	* aggDT <- malfunctionDT[, .( min = min(sales), total = sum(sales), num_obs = length(sales) ), by = month(timestamp) ]  
  
Example code includes:  
```{r}

dailyDT <- data.table(timestamp=as.POSIXct("2018-08-01", tz="UTC") + lubridate::days(0:152), 
                      sales=c(483.08, 449.25, 523.6, 498.36, 448, 487.03, 502.91, 475.69, 535.39, 471.54, 494.57, 509.6, 538.43, 603.55, 560.84, 482.39, 456.42, 550.68, 526.83, 577.16, 515.7, 450.44, 522.18, 546.44, 530.86, 452.47, 498.56, 486.58, 523.58, 424.25, 587.53, 533.11, 477.74, 582.16, 449.59, 575.78, 523.92, 475.5, 556.5, 487.27, 515.98, 523.78, 528.1, 548.19, 484.26, 542.97, 540.72, 475.16, 483.19, 598.89, 419.74, 448.57, 494.05, 438.82, 460.1, 343.01, 525.2, 527.51, 461.07, 557.52, 577.24, 499.41, 431.83, 487.61, 412.54, 454.56, 471.44, 520.61, 519.03, 547.59, 541.78, 507.67, 448.96, 468.08, 494.02, 520.78, 442.87, 507.98, 553.78, 486.46, 476.9, 546.92, 502.69, 557.93, 445.11, 501.94, 491.04, 534.49, 533.16, 543.76, 484.38, 610.28, 528.18, 483.56, 509.4, 496.62, 439.98, 488.11, 475.01, 514.07, 567.83, 506.74, 496.28, 417.83, 499.35, 556.75, 511, 596.06, 537.87, 562.97, 496.55, 499.85, 460.23, 478.96, 451.44, 576.34, 466.04, 433.66, 530.26, 554.76, 469.11, 477.79, 542.85, 582.55, 464.29, 458.92, 585.33, 487.18, 576.68, 488.35, 441.12, 509.81, 464.99, 464, 506.53, 459.36, 554.13, 444, 436.21, 528.15, 480.87, 541.93, 496.3, 423.1, 546.8, 499.21, 543.36, 534.85, 523.89, 524.99, 522.67, 524.51, 502.6)
                      )
str(dailyDT)


# Sort by time
setorderv(dailyDT, "timestamp")

# 1-day lag
dailyDT[, sales_lag1 := shift(sales, type = "lag", n = 1)]

# 5-day lag
dailyDT[, sales_lag5 := shift(sales, type = "lag", n = 5)]


experimentDT <- data.table(day=c(1:3, 1:3), 
                           result=c(1, 3.3, 2.5, 1.1, 3.9, 3.8), 
                           subject_id=LETTERS[c(1, 1, 1, 2, 2, 2)]
                           )
experimentDT


# Yesterday
experimentDT[, yesterday := shift(result, type="lag", n=1), by=subject_id]

# Two days ago
experimentDT[, two_days_ago := shift(result, type="lag", n=2), by=subject_id]

# Preview experimentDT
print(experimentDT)


aluminumDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/aluminumDF.feather") %>%
                                rename(timestamp=Date, price=`Cash Buyer`) %>%
                                select(timestamp, price)
                            )
str(aluminumDT)


# Add 1-period and 2-period lags
aluminumDT[, lag1 := shift(price, type = "lag", n = 1)]
aluminumDT[, lag2 := shift(price, type = "lag", n = 2)]

# Fit models with 1 and 2 lags
mod1 <- lm(price ~ lag1, data = aluminumDT)
mod2 <- lm(price ~ lag1 + lag2, data = aluminumDT)

# Compare
stargazer::stargazer(list(mod1, mod2), type = "text")


# One-period lag
dailyDT[, sales_lag1 := shift(sales, type = "lag", n = 1)]

# One-period diff
dailyDT[, sales_diff1 := sales - sales_lag1]

# Two-period diff
dailyDT[, sales_diff2 := sales - shift(sales, type="lag", n=2)]


# Add 1-period percentage change
dailyDT[, sales_pctchng1 := sales_diff1 / sales_lag1]

# Add 2-period percentage change
dailyDT[, sales_pctchng2 := (sales / shift(sales, type="lag", n=2) - 1)]


passengerDT <- data.table(obs_time=lubridate::ymd_hms("2017-08-01 00:00:00") + lubridate::minutes(15*0:96), 
                          passengers=c(506, 513, 554, 427, 439, 476, 509, 382, 457, 498, 398, 385, 529, 442, 393, 500, 557, 439, 453, 488, 520, 546, 542, 492, 528, 493, 498, 530, 515, 537, 535, 518, 396, 623, 499, 467, 523, 499, 535, 383, 546, 552, 436, 556, 452, 512, 514, 476, 437, 432, 522, 492, 537, 480, 543, 485, 491, 512, 555, 498, 452, 502, 514, 452, 446, 458, 538, 414, 499, 433, 503, 466, 553, 473, 473, 546, 447, 545, 492, 554, 466, 618, 530, 568, 541, 433, 524, 433, 571, 506, 485, 466, 490, 467, 528, 427, 480)
                          )
str(passengerDT)


# Generation time in seconds
passengerDT[, obs_time_in_seconds := as.numeric(obs_time)]

# Add floor time for each time stamp
seconds_in_an_hour <- 60 * 60
passengerDT[, hour_end := floor(obs_time_in_seconds / seconds_in_an_hour)]

# Count number of observations in each hour
passengerDT[, .N, by = hour_end]


# Mean passengers per hour
passengerDT[, mean(passengers), by=hour_end]

# Cleaner names
passengerDT[, .(mean_passengers = mean(passengers)), by=hour_end]

# Generate hourly summary statistics
passengerDT[, .(min_passengers = min(passengers), max_passengers = max(passengers)), by=hour_end]

```
  
  
  
***
  
Chapter 4 - Case Study: Financial Data  
  
Modeling Metals Prices:  
  
* Data sets have been pulled from quandl; can then convert to data.table, including fixing variable/column names  
	* aluminumDF <- Quandl::Quandl(code = "LME/PR_AL", start_date = "2001-12-31", end_date = "2018-03-12")  
    * aluminumDT <- as.data.table(aluminumDF)  
    * newDT <- aluminumDT[, .(obstime = Date, aluminum_price = `Cash Seller & Settlement` )]  # The .() allows for both selecting by name and changing names  
* Can also apply functions using .()  
	* newDT <- aluminumDT[, .(obstime = as.POSIXct(Date, tz = "UTC"), aluminum_price = `Cash Seller & Settlement` )]  
* Can also apply merges based on timestamps  
	* mergedDT <- merge( x = aluminumDT, y = nickelDT, all = TRUE, by = "obstime" )  
* Can also use Reduce()  
	* Reduce( f = function(x,y){paste0(x, y, "|")}, x = c("a", "b", "c") )  
  
Time Series Feature Engineering:  
  
* May want to add differences and growth rates to the data  
	* gdpDT[, diff1 := gdp - shift(gdp, type = "lag", n = 1)]  
    * add_diffs <- function(DT){ DT[, diff1 := gdp - shift(gdp, type = "lag", n = 1)] ; return(invisible(NULL)) }  # note that a copy of DT is passed and so DT is edited  
* Can use parentheses for a user-defined name in the data.table  
	* colname <- "abc"  
    * someDT[, (colname) := rnorm(10)]  
    * add_diffs <- function(DT, newcol){ DT[, (newcol) := gdp - shift(gdp, type = "lag", n = 1)] ; return(invisible(NULL)) }  
    * add_diffs(DT, "diff1")  
* Can also use get() to have a flexible column for changing  
	* colname <- "def"  
    * someDT[, random_stuff := get(colname) * rnorm(10)]  
    * add_diffs <- function(DT, newcol, dcol){ DT[, (newcol) := get(dcol) - shift(get(dcol), type = "lag", n = 1)] ; return(invisible(NULL)) }  
    * add_diffs <- function(DT, newcol, dcol, ndiff){ DT[, (newcol) := get(dcol) - shift(get(dcol), type = "lag", n = ndiff)] return(invisible(NULL)) }  # allows for passing the number of time periods  
* Can also extend the methodology to growth rates  
	* add_growth_rates <- function(DT, newcol, dcol, ndiff){ DT[, (newcol) := (get(dcol) / shift(get(dcol), type = "lag", n = ndiff)) - 1 ] return(invisible(NULL)) }  
  
EDA and Model Building:  
  
* Feature selection is sometimes needed for modeling - for example, regressions may perform poorly with too many features  
* Can look at the correlations using data.table  
	* someDT <- data.table(x = rnorm(100), y = rnorm(100), z = rnorm(100))  
    * someDT[complete.cases(someDT)]  
    * cor(someDT)  
    * cmat <- cor(someDT[complete.cases(someDT)])  
    * cmat[, "x"]  
    * feat_cols <- c("var_1", "var_5")  
    * mod1 <- lm(target ~ ., data = trainDT[, .SD, .SDcols = feat_cols])  
  
Wrap Up:  
  
* Modify data.tables by reference  
* Growth rates and differences  
* Flexible code for changes in variables  
  
Example code includes:  
```{r}

copperDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/copperDF.feather"))
str(copperDT)
nickelDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/nickelDF.feather"))
str(nickelDT)
cobaltDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/cobaltDF.feather"))
str(cobaltDT)
tinDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/tinDF.feather"))
str(tinDT)
aluminumDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/aluminumDF.feather"))
str(aluminumDT)


# Rename "Cash Buyer" to "copper_price"
setnames(copperDT, old="Cash Buyer", new="copper_price")
setnames(cobaltDT, old="Cash Buyer", new="cobalt_price")
setnames(tinDT, old="Cash Buyer", new="tin_price")
setnames(aluminumDT, old="Cash Buyer", new="aluminum_price")

# Convert `"Date"` to POSIXct
copperDT[, close_date := as.POSIXct(Date, tz="UTC")]
cobaltDT[, close_date := as.POSIXct(Date, tz="UTC")]
tinDT[, close_date := as.POSIXct(Date, tz="UTC")]
aluminumDT[, close_date := as.POSIXct(Date, tz="UTC")]

# Create copperDT2 with "close_date" and "copper_price"
copperDT2 <- copperDT[, .(close_date, copper_price)]
cobaltDT2 <- cobaltDT[, .(close_date, cobalt_price)]
tinDT2 <- tinDT[, .(close_date, tin_price)]
aluminumDT2 <- aluminumDT[, .(close_date, aluminum_price)]

# Create a new data.table using .() subsetting
nickelDT2 <- nickelDT[, .(
    close_date = as.POSIXct(Date, tz = "UTC"),
    nickel_price = `Cash Buyer`
)]
str(nickelDT2)


# Merge copperDT and cobaltDT with merge()
mergedDT <- merge(cobaltDT2, copperDT2, by="close_date", all=TRUE)

# Merge five tables into one
mergedDT <- Reduce(f = function(x, y) { merge(x, y, by="close_date", all=TRUE) },
                   x = list(aluminumDT2, copperDT2, cobaltDT2, nickelDT2, tinDT2)
                   )


# Function to add differences
add_diffs <- function(DT, cols, ndiff){
    for (colname in cols){
        new_name <- paste0(colname, "_diff", ndiff)
        DT[, (new_name) := get(colname) - shift(get(colname), type = "lag", n = ndiff)]
    }
}

# Add 2-period diffs
add_diffs(mergedDT, paste0(c("aluminum", "cobalt", "copper", "nickel", "tin"), "_price"), 2)


# Function to add growth rates
add_growth_rates <- function(DT, cols, ndiff){
    for (colname in cols){
        new_name <- paste0(colname, "_pctchg", ndiff)
        DT[, (new_name) := (get(colname) / shift(get(colname), type = "lag", n = ndiff)) - 1]
    }
}

# Add 1-period growth rate
add_growth_rates(mergedDT, paste0(c("aluminum", "cobalt", "copper", "nickel", "tin"), "_price"), 1)


# Function to get correlation matrix from a data.table
corrmat_from_dt <- function(DT, cols){
    # Subset to the requested columns
    subDT <- DT[, .SD, .SDcols=cols]
    subDT <- subDT[complete.cases(subDT)]
    return(cor(subDT))
}

# Get correlations of prices
corrmat_from_dt(mergedDT, paste0(c("aluminum", "cobalt", "copper", "nickel", "tin"), "_price"))


# Add 1-period first differences
price_cols <- c("aluminum_price", "cobalt_price", "copper_price", "nickel_price", "tin_price")
add_diffs(DT = mergedDT, cols = price_cols, ndiff = 1)

# Rename aluminum first difference to "target"
setnames(mergedDT, "aluminum_price_diff1", "target")

# Add 1-period growth rates
add_growth_rates(DT = mergedDT, cols = price_cols, ndiff = 1)

# Correlation matrix
diff_cols <- grep("_diff", x = names(mergedDT), value = TRUE)
growth_cols <- grep("_pctchg", x = names(mergedDT), value = TRUE)
corrmat_from_dt(DT = mergedDT, cols = c(diff_cols, growth_cols, "target"))[, "target"]


# Add 1-period differences
add_diffs(mergedDT, paste0(c("cobalt", "copper", "nickel", "tin"), "_price"), 1)

# Add 3-period growth rates
add_growth_rates(mergedDT, paste0(c("cobalt", "copper", "nickel", "tin"), "_price"), 3)

# Add 12-period difference in nickel price
add_diffs(mergedDT, paste0(c("nickel"), "_price"), 12)

# Top 4 difference / growth columns
top_features <- c("copper_price_diff1", "nickel_price_diff1", "tin_price_diff1", "copper_price_pctchg3")

```
  
  
  
***
  
###_Interactive Data Visualization with plotly in R_  
  
Chapter 1 - Introduction to plotly  
  
What is plotly?  
  
* Interface to the plotly javascript library - plots can work in multiple formats  
	* Active development and support community  
* Need to consider the relative values of the static graphic and the interactive graphic  
* Example of the wine plot using ggplot2  
	* static <- wine %>% ggplot(aes(x = Flavanoids, y = Proline, color = Type)) + geom_point()  
* Can use ggplotly() to convert a static object to an interactive plot  
	* plotly::ggplotly(static)  
  
Univariate Graphics:  
  
* May want to look at distributions of a categorical variable - example using the wine dataset  
	* wine %>%  
    *     count(Type) %>%                 # create a frequency table  
    *     plot_ly(x = ~Type, y = ~n) %>%  # specify aesthetics (similar to ggplot aes() function)  
    *     add_bars()                      # add the bars trace  
    * wine %>% count(Type) %>% mutate(Type = forcats::fct_reorder(Type, n, .desc = TRUE)) %>% plot_ly(x = ~Type, y = ~n) %>% add_bars()  # example of using forcats for reordering  
* Can instead run a histogram using plotly  
	* wine %>%  
    *     plot_ly(x = ~Phenols) %>%  # specify aesthetics  
    *     add_histogram()            # add the histogram trace  
    * wine %>% plot_ly(x = ~Phenols) %>% add_histogram(nbinsx = 10)  # adjust the number of bins to a precise number  
    * wine %>% plot_ly(x = ~Phenols) %>% add_histogram(xbins = list(start = 0.8, end = 4, size = 0.25))  # provide specific bin ranges  
  
Bivariate Graphics:  
  
* Can extend to looking at scatterplots, boxplots, and the like  
* Example scatterplot for exploring relationships in numeric variables  
	* winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers()  
* Example stacked bars  
	* winequality %>% count(type, quality_label) %>% plot_ly(x = ~type, y = ~n, color = ~quality_label) %>% add_bars() %>% layout(barmode = "stack")  
* Converting to proportional bars  
	* winequality %>%  
    *     count(type, quality_label) %>%  
    *     group_by(type) %>%                # group the table  
    *     mutate(prop = n / sum(n)) %>%     # calculate the proportions  
    *     plot_ly(x = ~type, y = ~n, color = ~quality_label) %>%  
    *     add_bars() %>%  
    *     layout(barmode = "stack")  
* Can also run boxplots to explore numeric distributions vs. a categorical variable  
	* winequality %>% plot_ly(x = ~quality_label, y = ~alcohol) %>% add_boxplot()  
  
Example code includes:  
```{r eval=FALSE}

vgsales <- readr::read_csv("./RInputFiles/vgsales.csv")
glimpse(vgsales)


# Store the scatterplot of Critic_Score vs. NA_Sales sales in 2016
scatter <- vgsales %>%
  filter(Year == 2016) %>%
  ggplot(aes(x = NA_Sales, y = Critic_Score)) +
  geom_point(alpha = 0.3)

# Convert the scatterplot to a plotly graphic
plotly::ggplotly(scatter)


library(plotly)

# Create a histogram of Critic_Score
vgsales %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram()

# Create a histogram of Critic_Score with at most 25 bins
vgsales %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(nbinsx = 25)

# Create a histogram with bins of width 10 between 0 and 100
vgsales %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(xbins = list(start=0, end=100, size=10))


# Create a frequency for Genre
genre_table <- vgsales %>%
    count(Genre)

# Reorder the bars for Genre by n
genre_table %>%
    filter(!is.na(Genre)) %>%
    mutate(Genre = fct_reorder(Genre, n, .desc=TRUE)) %>%
    plot_ly(x = ~Genre, y = ~n) %>% 
    add_bars()


# Create a scatter plot of User_Score against Critic_Score
vgsales %>% 
    filter(!is.na(Critic_Score) & !is.na(User_Score)) %>%
    plot_ly(x=~Critic_Score, y=~User_Score) %>%
    add_markers()


# Filter out the 2016 video games
vg2016 <- vgsales %>%
    filter(Year == 2016)

# Create a stacked bar chart of Rating by Genre
vg2016 %>%
    count(Genre, Rating) %>%
    plot_ly(x = ~Genre, y = ~n, color = ~Rating) %>%
    add_bars() %>%
    layout(barmode = "stack")

# Create boxplots of Global_Sales by Genre for above data
vg2016 %>% 
  plot_ly(x=~Global_Sales, y=~Genre) %>%
  add_boxplot()

```
  
  
  
***
  
Chapter 2 - Styling and Customizing Graphics  
  
Customize Traces:  
  
* Can change colors; for example, non-blue histograms  
	* winequality %>% plot_ly(x = ~fixed_acidity) %>% add_histogram()  
    * winequality %>% plot_ly(x = ~fixed_acidity) %>% add_histogram(color = I("red"))  # The I() function is for as-is, since plotly otherwise assumes mapping of a color  
* Can change opacity, for example in scatterplots with over-plotting  
	* winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers()  
    * winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers(marker = list(opacity = 0.2))  
* Can change the plotting symbol from filled to open  
	* winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers(marker = list(symbol = "circle-open"))  
* Additional marker options include  
	* opacity  
    * color  
    * symbol (scatter/box)  
    * size (scatter)  
    * width (bar/histogram)  
  
Thoughtful Use of Color:  
  
* Color can be used thoughtfully to represent variables in a dataset  
* Coloring by category can make the trends more evident  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Type) %>% add_markers()  
* Coloring can add a third quantitative variable to the plot (gradient)  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Color) %>% add_markers()  
* Can also add the RColorBrewer palettes or use a custom color palette  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Type) %>% add_markers(colors = "Dark2")  
    * wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Type) %>% add_markers(colors = c("orange", "black", "skyblue"))  # note that as-is I() is not needed here, since color=~Type already requested the aesthetic  
  
Hover Info:  
  
* Hover information is added in plotly automtically - coordinate pairs, but without variable names for scatter  
* Can change the defaults to hovering to make the charts more intuitive  
	* wine %>%  
    *     count(Type) %>%  
    *     plot_ly(x = ~Type, y = ~n, hoverinfo = "y") %>%  
    *     add_bars()  
* Can customize hover to add variable names to a scatter plot  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, hoverinfo = "text", text = ~paste("Flavanoids:", Flavanoids, "<br>", "Alcohol:", Alcohol) ) %>% add_markers()  # uses the html "<br>" and the ~means columns are mapping as aesthetics  
  
Customizing Layout:  
  
* The layout() function is the workhorse for many of the desired changed to plotly - like labs() and theme() in ggplot2  
* Example for adding axis labels (by list) and axis titles(by string)  
	* winequality %>% plot_ly(x = ~free_so2, y = ~total_so2) %>% add_markers(marker = list(opacity = 0.2)) %>% layout(xaxis = list(title = "Free SO2 (ppm)"), yaxis = list(title = "Total SO2 (ppm)"), title = "Does free SO2 predict total SO2 in wine?")  
    * winequality %>% plot_ly(x = ~free_so2, y = ~total_so2) %>% add_markers(marker = list(opacity = 0.2)) %>% layout(xaxis = list(title = "Free SO2 (ppm, log scale)", type = "log"), yaxis = list(title = "Total SO2 (ppm, log scale)", type = "log"), title = "Does free SO2 predict total SO2 in wine?")  
* Can remove the zero lines and grids from the plot  
	* winequality %>% plot_ly(x = ~free_so2, y = ~total_so2) %>% add_markers(marker = list(opacity = 0.5)) %>% layout(xaxis = list(title = "Free SO2 (ppm)", zeroline = FALSE), yaxis = list(title = "Total SO2 (ppm)", zeroline = FALSE, showgrid = FALSE))  
* Can make additional customizations to plotting canvas - paper_bgcolor=toRGB() and plot_bgcolor=toRBG()  
  
Example code includes:  
```{r eval=FALSE}

# Filter out the 2016 video games
vgsales2016 <- vgsales %>%
    filter(Year == 2016)
str(vgsales2016)


# Create a histogram of Critic_Score with navy bars that are 50% transparent
vgsales2016 %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(color = I("navy"), opacity = 0.5)


# Change the color of the histogram using a hex code
vgsales2016 %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(color=I("#111e6c"))

# Change the color of the histogram using rgb()
vgsales2016 %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(marker = list(color = "rgb(17, 30, 108)"))


# Set the plotting symbol to diamond and the size to 4
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score)) %>%
    plot_ly(x = ~User_Score, y = ~Critic_Score) %>% 
    add_markers(marker = list(symbol="diamond", size=4))


# Use color to add Genre as a third variable
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(Genre)) %>%
    plot_ly(x=~Critic_Score, y=~User_Score, color=~Genre) %>%
    add_markers(colors="Dark2")


# Create a scatterplot of User_Score against Critic_Score coded by Rating
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(Rating)) %>%
    plot_ly(x=~Critic_Score, y=~User_Score, symbol=~Rating) %>%
    add_markers()


# Create a scatterplot of User_Score vs. Critic_Score colored by User_Count
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(User_Count)) %>%
    plot_ly(x = ~Critic_Score, y = ~User_Score, color=~User_Count) %>%
    add_markers()

# Create a scatterplot of User_Score vs. Critic_Score colored by log User_Count
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(User_Count)) %>%
    plot_ly(x = ~Critic_Score, y = ~User_Score, color=~log(User_Count)) %>%
    add_markers()


# Create a bar chart of Platform with hoverinfo only for the bar heights
vgsales2016 %>%
    filter(!is.na(Platform)) %>%
    count(Platform) %>%
    plot_ly(x=~Platform, y=~n, hoverinfo="y") %>%
    add_bars()


# Create a scatterplot of User_Score vs. Critic score
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(Name)) %>%
    # Add video game Name to the hover info text
    plot_ly(x=~Critic_Score, y=~User_Score, hoverinfo="text", text=~Name) %>% 
    add_markers()


# Format the hover info for NA_Sales, EU_Sales, and Name
vgsales2016 %>% 
    filter(!is.na(NA_Sales), !is.na(EU_Sales), !is.na(Name)) %>%
    plot_ly(x = ~NA_Sales, y = ~EU_Sales, hoverinfo = "text",
            text = ~paste("NA_Sales:", NA_Sales, "<br>", "EU_Sales:", EU_Sales, "<br>", "Name:", Name)
            ) %>%
    add_markers()


# Polish the scatterplot by transforming the x-axis and labeling both axes
vgsales2016 %>%
    filter(!is.na(Global_Sales), !is.na(Critic_Score)) %>%
    plot_ly(x = ~Global_Sales, y = ~Critic_Score) %>%
    add_markers(marker = list(opacity = 0.5)) %>%
    layout(xaxis = list(title="Global sales (millions of units)", type="log"),
           yaxis = list(title="Critic score")
           )


# Set the background color to #ebebeb and remove the vertical grid
vgsales %>%
    filter(!is.na(Year)) %>%
    group_by(Year) %>%
    summarize(Global_Sales = sum(Global_Sales, na.rm=TRUE)) %>%
    plot_ly(x = ~Year, y = ~Global_Sales) %>%
    add_lines() %>%
    layout(xaxis=list(showgrid=FALSE), paper_bgcolor="#ebebeb")

```
  
  
  
***
  
Chapter 3 - Advanced Charts  
  
Layering Traces:  
  
* Layering traces allows for creating more complex charts - goal is the simplest chart that communicates the message effectively  
* Example for adding a loess smooth to a scatterplot - need to calculate the smooth, then pass it using add_lines with a y aesthetic  
	* m <- loess(Alcohol ~ Flavanoids, data = wine, span = 1.5)  
    * wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol) %>% add_markers() %>% add_lines(y = ~fitted(m)) %>% layout(showlegend = FALSE)  
    * m2 <- lm(Alcohol ~ poly(Flavanoids, 2), data = wine)  
    * wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol) %>% add_markers(showlegend = FALSE) %>% add_lines(y = ~fitted(m), name = "LOESS") %>% add_lines(y = ~fitted(m2), name = "Polynomial")  
* Can also use layers to compare densities and distributions  
	* d1 <- filter(wine, Type == 1)  
    * d2 <- filter(wine, Type == 2)  
    * d3 <- filter(wine, Type == 3)  
    * density1 <- density(d1$Flavanoids)  
    * density2 <- density(d2$Flavanoids)  
    * density3 <- density(d3$Flavanoids)  
    * plot_ly(opacity = 0.5) %>%  
    *     add_lines(x = ~density1$x, y = ~density1$y, name = "Type 1") %>%  
    *     add_lines(x = ~density2$x, y = ~density2$y, name = "Type 2") %>%  
    *     add_lines(x = ~density3$x, y = ~density3$y, name = "Type 3") %>%  
    *     layout(xaxis = list(title = 'Flavonoids'), yaxis = list(title = 'Density'))  
  
Subplots:  
  
* A series of subplots can be a powerful way to explore the interactions of various variables - example of base code that would be better with subplots  
	* vgsales2016 %>% plot_ly(x = ~Critic_Score, y = ~User_Score, color = ~Genre) %>% add_markers()  # too many colors, hard to read  
* Example of creating a single subplot  
	* action_df <- vgsales2016 %>% filter(Genre == "Action")  
	* action_df %>% plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers()  
* Example of creating two subplots  
	* p1 <- action_df %>% plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers()  
    * p2 <- vgsales2016 %>% filter(Genre == "Adventure") %>% plot_ly(x = ~life.expectancy, y = ~happiness) %>% add_markers()  
    * subplot(p1, p2, nrows = 1)  
* Can add legends to the individuals subplots, which are then used by the subplot() command  
	* p1 <- plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers(name = ~Genre)  
    * p2 <- vgsales2016 %>% filter(Genre == "Adventure") %>% plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers(name = ~Genre)  
    * subplot(p1, p2, nrows = 1)  
* Can create shared axis labels  
	* subplot(p1, p2, nrows = 1, shareY = TRUE, shareX = TRUE)  
* Can also create facets more automatically using group_by() and do()  
	* library(dplyr)  
    * vgsales2016 %>%  
    *     group_by(region) %>%  
    *     do(plot = plot_ly(data = ., x = ~Critic_Score, y = ~User_Score) %>%  
    *     add_markers(name = ~Genre)) %>%  
    *     subplot(nrows = 2)  
  
Scatterplot Matrices:  
  
* Can explore the pairwise relationship (pairs plot) for some or all of the variables in the dataset  
	* data %>% plot_ly() %>% add_trace( type = 'splom', dimensions = list( list(label='string-1', values=X1), list(label='string-2', values=X2), . . . list(label='string-n', values=Xn)) )  
    * Need to pass that we want "splom" (scatterplot matrix), then give each of the variables and their names  
    * wine %>% plot_ly() %>% add_trace( type = 'splom', dimensions = list( list(label='Alcohol', values=~Alcohol), list(label='Flavonoids', values=~Flavanoids), list(label='Color', values=~Color) ) )  
* There is linked brushing in plotly, so changes in one panel will carry through to other panels  
* Can also add color to the scatterplot matrices  
	* wine %>% plot_ly(color = ~Type) %>% add_trace( type = 'splom', dimensions = list( list(label='Alcohol', values=~Alcohol), list(label='Flavonoids', values=~Flavanoids), list(label='Color', values=~Color) ) )  
  
Binned Scatterplots:  
  
* With large datasets, some charts (e.g., scatterplots) will perform poorly  
* Binned scatterplots can be a solution to the overplotting problem of a large scatterplot  
	* sim_data %>% plot_ly(x = ~x, y = ~y) %>% add_histogram2d()  
    * sim_data %>% plot_ly(x = ~x, y = ~y) %>% add_histogram2d(nbinsx = 200, nbinsy = 100)  
  
Example code includes:  
```{r eval=FALSE}

vgsales2016 <- vgsales %>%
    mutate(User_Score = as.numeric(User_Score)) %>%
    filter(Year == 2016, !is.na(User_Score), !is.na(Critic_Score))
str(vgsales2016)


# Fit the regression model of User_Score on Critic_Score
m <- lm(User_Score ~ Critic_Score, data = vgsales2016)

# Create the scatterplot with smoother
vgsales2016 %>%
   select(User_Score, Critic_Score) %>%
   na.omit() %>%
   plot_ly(x = ~Critic_Score, y = ~User_Score) %>%
   add_markers(showlegend = FALSE) %>%
   add_lines(y = ~fitted(m))


activision <- vgsales2016 %>% filter(Publisher == "Activision")
ea <- vgsales2016 %>% filter(Publisher == "Electronic Arts")
nintendo <- vgsales2016 %>% filter(Publisher == "Nintendo")

# Compute density curves
d.a <- density(activision$Critic_Score, na.rm = TRUE)
d.e <- density(ea$Critic_Score, na.rm = TRUE)
d.n <- density(nintendo$Critic_Score, na.rm = TRUE)

# Overlay density plots
plot_ly() %>%
  add_lines(x = ~d.a$x, y = ~d.a$y, name = "Activision", fill = 'tozeroy') %>%
  add_lines(x = ~d.e$x, y = ~d.e$y, name = "Electronic Arts", fill = 'tozeroy') %>%
  add_lines(x = ~d.n$x, y = ~d.n$y, name = "Nintendo", fill = 'tozeroy') %>%
  layout(xaxis = list(title = 'Critic Score'),
         yaxis = list(title = 'Density'))


# Create a scatterplot of User_Score against Critic_Score for PS4 games
p1 <- vgsales2016 %>%
   filter(Platform == "PS4") %>%
   plot_ly(x = ~Critic_Score, y = ~User_Score) %>% 
   add_markers(name = "PS4")

# Create a scatterplot of User_Score against Critic_Score for XOne games
p2 <- vgsales2016 %>%
   filter(Platform == "XOne") %>%
   plot_ly(x = ~Critic_Score, y = ~User_Score) %>% 
   add_markers(name = "XOne")

# Create a facted scatterplot containing p1 and p2
subplot(p1, p2, nrows=2)


# Create a faceted scatterplot of User_Score vs. Critic_Score with 3 rows
vgsales2016 %>%
  group_by(Platform) %>%
  do(plot = plot_ly(data = ., x=~Critic_Score, y=~User_Score) %>% 
         add_markers(name = ~Platform)
     ) %>%
  subplot(nrows = 3, shareY = TRUE, shareX = TRUE)


# Add x-axis and y-axis labels, and a title
sp2 <- subplot(p1, p2, nrows = 2, shareX=TRUE, shareY=TRUE) %>%
    layout(title="User score vs. critic score by platform, 2016")
sp2

# Add x-axis and y-axis labels, and a title to  sp2
sp2 %>%
   layout(xaxis = list(title=""), xaxis2 = list(title="Year"), 
          yaxis = list(title="Global Sales (M units)"), yaxis2 = list(title="Global Sales (M units)")
          )


# Create a SPLOM of NA_Sales, EU_Sales, and JP_Sales
vgsales2016 %>%
  plot_ly() %>%
  add_trace(type = "splom", dimensions = list(list(label = "N. America", values = ~NA_Sales),
                                              list(label = "Europe", values = ~EU_Sales),
                                              list(label = "Japan", values = ~JP_Sales)
                                              )
            )


# Color the SPLOM of NA_Sales, EU_Sales, and JP_Sales by nintendo
vgsales2016 %>%
  mutate(nintendo = ifelse(Publisher == "Nintendo", "Nintendo", "Other")) %>%
  plot_ly(color=~nintendo) %>% 
  add_trace(type="splom", dimensions = list(list(label = "N. America", values = ~NA_Sales),
                                            list(label = "Europe", values = ~EU_Sales),
                                            list(label = "Japan", values = ~JP_Sales)
                                            )
            )


# Delete the diagonal plots in splom
splom %>%
   style(diagonal = list(visible=FALSE))

# Delete the plots in the upper half of splom
splom %>%
   style(showupperhalf=FALSE)

# Delete the plots in the lower half of splom
splom %>%
   style(showlowerhalf=FALSE)


# Create a binned scatterplot of User_Score vs. Critic_Score
vgsales %>%
  plot_ly(x=~Critic_Score, y=~User_Score) %>%
  add_histogram2d(nbinsx=50, nbinsy=50)

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Introduction to 2018 Election Data:  
  
* The 2018 US midterm election featured 435 House seats, 35 Senate seats, and 36 governors  
* Can use tunrout data and fundraising data from the US elections project and US FEC respectively  
	* glimpse(turnout)  
    * glimpse(fundraising)  
* Can also see results of key races using  
	* glimpse(senate_winners)  
  
Choropleth Maps:  
  
* The choropleth can be used to show key variables by geography  
	* turnout %>%  
    *     plot_geo(locationmode = 'USA-states') %>%  
    *     add_trace(z = ~turnout, locations = ~state.abbr) %>%  
    *     layout(geo = list(scope = 'usa')) # restricts map only to USA  
* Not all world regions are available in plot_geo - currently just locationmode: "USA-states" | "ISO-3" | "country names"  
* Mapping options can be passed to the geo-layout  
	* scope = "world" | "usa" | "europe" | "asia" | "africa" | "north america" | "south america"  
    * projection = list(type = "mercator")  "conic conformal" | "mercator" | "robinson" | "stereographic" | and 18 more.  
    * scale = 1  (Larger values = tighter zoom)  
    * center = list(lat = ~c.lat, lon = ~c.lon)  # Set c.lat and c.lon to center the map  
  
From Polygons to Maps:  
  
* Since not all regions are available, may need to cutom-create a choropleth  
* Need to start with a boundaries dataset, such as might be available in "us_states"  
	* head(us_states)  
* Can join data, after cleaning up potential merging issues  
	* turnout <- turnout %>% mutate(state = tolower(state)) # make state names lowercase  
    * states_map <- left_join(us_states, turnout, by = c("region" = "state"))  
* Can then plot the data  
	* states_map %>%  
    *     group_by(group) %>%  
    *     plot_ly( x=~long, y=~lat, color=~turnout2018, split=~region ) %>%  
    *     add_polygons(line = list(width = 0.4), showlegend = FALSE)  
* May want to further clean up the map - axes, ticks, etc.  
	* state_turnout_map %>% layout( title = "2018 Voter Turnout by State", xaxis = list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE), yaxis = list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE) ) %>% colorbar(title = "Turnout")  
  
Wrap Up:  
  
* Chapter 1 - Basics of Plotly and conversion of ggplot2 graphics - univariate, bivariate distributions  
* Chapter 2 - Customizing Plotly Charts - reduce over-plotting, change plotting symbols, adding color, hovering, custom layouts  
* Chapter 3 - Advanced Charts - layering traces, subplots, scatterplot matrices, binned scatterplots  
* Chapter 4 - Mapping Data - choropleths, polygons, customizing map apearances  
  
Example code includes:  
```{r eval=FALSE}

turnout <- readr::read_csv("./RInputFiles/TurnoutRates.csv")
str(turnout)


# Create a scatterplot of turnout2018 against turnout2014
p <- turnout %>%
    plot_ly(x=~turnout2014, y=~turnout2018) %>%
    add_markers() %>%
    layout(xaxis = list(title="2014 voter turnout"),
           yaxis = list(title="2018 voter turnout")
           )

p


# Add the line y = x to the scatterplot
p %>%
  add_lines(x = c(0.25, 0.6), y = c(0.25, 0.6)) %>%
  layout(showlegend=FALSE)


# Create a dotplot of voter turnout in 2018 by state ordered by turnout
turnout %>%
    top_n(15, wt = turnout2018) %>%
    plot_ly(x = ~turnout2018, y = ~fct_reorder(state, turnout2018)) %>%
    add_markers() %>%
    layout(yaxis=list(title="State", type="category"), xaxis=list(title="Elgible voter turnout"))


fundraising <- readr::read_csv("./RInputFiles/fec_candidate_summary_2018.csv")
str(fundraising)


# Create a histogram of receipts for the senate races
fundraising %>%
    filter(office=="S") %>%
    plot_ly(x=~receipts) %>%
    add_histogram() %>%
    layout(title="Fundraising for 2018 Senate races", xaxis=list(title="Total contributions received"))


# Create a dotplot of the top 15 Senate campaigns
fundraising %>%
    filter(office == "S") %>%
    top_n(15, wt = receipts) %>%
    plot_ly(x = ~receipts, y = ~fct_reorder(state, receipts), color = ~fct_drop(party), 
            hoverinfo = "text", text = ~paste("Candidate:", name, "<br>", "Party:", party, "<br>",
                                              "Receipts:", receipts, "<br>",
                                              "Disbursements:", disbursement
                                              )
            ) %>%
    add_markers(colors = c("blue", "red")) 


# Create a choropleth map of the change in voter turnout from 2014 to 2018
turnout %>%
    mutate(change = turnout2018 - turnout2014) %>%
    plot_geo(locationmode = 'USA-states') %>%
    add_trace(z=~change, locations=~state.abbr) %>%
    layout(geo = list(scope="usa"))


senate_winners <- readr::read_csv("./RInputFiles/senate_winners.csv")
str(senate_winners)


# Create a choropleth map displaying the Senate results
senate_winners %>%
    plot_geo(locationmode = "USA-states") %>%
    add_trace(z=~as.numeric(as.factor(party)), locations=~state, 
              colors = c("dodgerblue", "mediumseagreen", "tomato"),
              hoverinfo = "text", text = ~paste("Candidate:", name, "<br>",
                                                "Party:", party, "<br>",
                                                "% vote:", round(pct.vote, 1)
                                                )
              ) %>%
    layout(geo = list(scope = 'usa')) %>% 
    hide_colorbar()


# Map President Trump's rallies in 2018
# rallies2018 %>%
#     plot_geo(locationmode = 'USA-states') %>%
#     add_markers(x=~long, y=~lat, size=~no.speakers, 
#                 hoverinfo = "text", text = ~paste(city, state, sep = ",")
#                 ) %>%
#     layout(title = "2018 Trump Rallies", geo = list(scope = "usa"))


# Customize the geo layout
g <- list(scope = 'usa', 
          showland = TRUE, landcolor = "gray90",
          showlakes = TRUE, lakecolor = "white",
          showsubunit = TRUE, subunitcolor = "white"
          )

# Apply the geo layout to the map
# rallies2018 %>%
#     plot_geo(locationmode = 'USA-states') %>%
#     add_markers(x = ~long, y = ~lat, size = ~no.speakers, 
#                 hoverinfo = "text", text = ~paste(city, state, sep = ",")
#                 ) %>%
#     layout(title = "2018 Trump Rallies", geo = list(scope="usa"))


# Customize the geo layout
g <- list(scope = 'usa', 
          showland = TRUE, landcolor = toRGB("gray90"),
          showlakes = TRUE, lakecolor = toRGB("white"),
          showsubunit = TRUE, subunitcolor = toRGB("white")
          )

# Apply the geo layout to the map
# rallies2018 %>%
#     plot_geo(locationmode = 'USA-states') %>%
#     add_markers(x = ~long, y = ~lat, size = ~no.speakers, 
#                 hoverinfo = "text", text = ~paste(city, state, sep = ",")
#                 ) %>%
#     layout(title = "2018 Trump Rallies", geo = g)


fl_boundaries <- readr::read_csv("./RInputFiles/fl_boundaries.csv")
str(fl_boundaries)
fl_results <- readr::read_csv("./RInputFiles/fl_results.csv")
str(fl_results)


# Create a choropleth map displaying the Senate winners
# senate_vote %>%
#     group_by(group) %>%
#     plot_ly(x=~long, y=~lat, color=~PartyCode, split=~region) %>%
#     add_polygons(line = list(width=0.4), showlegend=FALSE)

# Adjust the polygon colors and boundaries
# senate_map %>%
#     group_by(group) %>%
#     plot_ly(x = ~long, y = ~lat, color = ~party, split = ~region, 
#             colors=c("dodgerblue", "mediumseagreen", "tomato")
#             ) %>%
#     add_polygons(line = list(width = 0.4, color=toRGB("gray60")), showlegend = FALSE)

# Define the layout settings to polish the axes
# map_axes <- list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE)

# Apply the layout to both axes
# senate_map %>%
#     group_by(group) %>%
#     plot_ly(x = ~long, y = ~lat, color = ~party, split = ~region, 
#             colors = c("dodgerblue", "mediumseagreen", "tomato")
#             ) %>%
#     add_polygons(line = list(width = 0.4, color = toRGB("gray60")), showlegend = FALSE) %>%
#     layout(xaxis=map_axes, yaxis=map_axes)


# Join the fl_boundaries and fl_results data frames
senate_vote <- left_join(fl_boundaries, fl_results, by = c("subregion" = "CountyName"))

# Specify the axis settings to polish the map
map_axes <- list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE)

# Create a polished county-level choropleth map of Pctvote
senate_vote %>%
    group_by(group) %>%
    plot_ly(x = ~long, y = ~lat, color = ~Pctvote, split = ~subregion) %>%
    add_polygons(line = list(width = 0.4), showlegend = FALSE, colors = c("blue", "red")) %>%
    layout(xaxis = map_axes, yaxis = map_axes)

```
  
  
  
***
  
###_Hyperparameter Tuning in R_  
  
Chapter 1 - Introduction to hyperparameters  
  
Parameters vs. Hyperparameters:  
  
* The hyper-parameters differ from the model parameters  
	* Model parameters are being fit during training  
    * Hyper-parameters are set prior to training and specify how the training should happen  
  
Recap of machine learning basics:  
  
* First step for hyoer-parameter training is to split the data in to test/train subsets  
	* index <- caret::createDataPartition(breast_cancer_data$diagnosis, p = .70, list = FALSE)  
    * bc_train_data <- breast_cancer_data[index, ]  
    * bc_test_data  <- breast_cancer_data[-index, ]  
* Can then train the model using caret  
	* fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * tictoc::tic()  # timing function  
    * set.seed(42)  
    * rf_model <- train(diagnosis ~ ., data = bc_train_data, method = "rf", trControl = fitControl, verbose = FALSE)  
    * tictoc::toc()  # timing function  
* The caret package automatically attempts a few hyper-parameters and picks the best for the final results  
  
Hyperparameter tuning in caret:  
  
* The caret package automatically performs some hyper-parameter tuning (e.g., tries a few values for mtry)  
* Many models are available in caret, with details available at  
	* modelLookup(model)  
    * https://topepo.github.io/caret/available-models.html  
* Example for running an SVM using caret  
	* fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * tictoc::tic()  # timing function  
    * set.seed(42)  
    * svm_model <- train(diagnosis ~ ., data = bc_train_data, method = "svmPoly", trControl = fitControl, verbose= FALSE)  
    * tictoc::toc()  # timing function  
* Can specify the number of values per hyperparameter to attempt for automatic tuning  
	* svm_model_2 <- train(diagnosis ~ ., data = bc_train_data, method = "svmPoly", trControl = fitControl, verbose = FALSE, tuneLength = 5)  # tuneLenght=5 means "try 5 of each"  
* Can also manually specify hyperparameters for tuning  
	* hyperparams <- expand.grid(degree = 4, scale = 1, C = 1)  
    * svm_model_3 <- train(diagnosis ~ ., data = bc_train_data, method = "svmPoly", trControl = fitControl, tuneGrid = hyperparams, verbose = FALSE)  
  
Example code includes:  
```{r}

breast_cancer_data <- readr::read_csv("./RInputFiles/breast_cancer_data.csv")
str(breast_cancer_data)
# bc_train_data <- readr::read_csv("./RInputFiles/bc_train_data.csv")
# str(bc_train_data)


# Fit a linear model on the breast_cancer_data.
linear_model <- lm(concavity_mean ~ symmetry_mean, data=breast_cancer_data)

# Look at the summary of the linear_model.
summary(linear_model)

# Extract the coefficients.
coef(linear_model)


# Plot linear relationship.
ggplot(data = breast_cancer_data, aes(x = symmetry_mean, y = concavity_mean)) +
    geom_point(color = "grey") +
    geom_abline(slope = coef(linear_model)[2], intercept = coef(linear_model)[1])


# Create partition index
index <- caret::createDataPartition(breast_cancer_data$diagnosis, p = 0.7, list = FALSE)

# Subset `breast_cancer_data` with index
bc_train_data <- breast_cancer_data[index, ]
bc_test_data  <- breast_cancer_data[-index, ]

# Define 3x5 folds repeated cross-validation
fitControl <- caret::trainControl(method = "repeatedcv", number = 5, repeats = 3)

# Run the train() function
gbm_model <- caret::train(diagnosis ~ ., data = bc_train_data, method="gbm", 
                          trControl=fitControl, verbose = FALSE
                          )

# Look at the model
gbm_model



set.seed(42)  # Set seed.
tictoc::tic()  # Start timer.
gbm_model <- caret::train(diagnosis ~ ., data = bc_train_data, method = "gbm", 
                          trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3),
                          verbose = FALSE, tuneLength=4
                          )
tictoc::toc()  # Stop timer.


# Define hyperparameter grid.
hyperparams <- expand.grid(n.trees = 200, interaction.depth = 1, 
                           shrinkage = 0.1, n.minobsinnode = 10
                           )

# Apply hyperparameter grid to train().
set.seed(42)
gbm_model <- caret::train(diagnosis ~ ., data = bc_train_data, method = "gbm", 
                          trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3),
                          verbose = FALSE, tuneGrid=hyperparams
                          )

```
  
  
  
***
  
Chapter 2 - Hyperparameter Tuning with caret  
  
Hyperparameter tuning in caret:  
  
* Example for using 2016 US voter turnout dataset  
	* fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * gbm_model_voters <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE) 
    * gbm_model_voters  
* Can use a Cartesian grid for hyperparameters - increases the model run-time  
	* man_grid <- expand.grid(n.trees = c(100, 200, 250), interaction.depth = c(1, 4, 6), shrinkage = 0.1, n.minobsinnode = 10)  
    * fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * gbm_model_voters_grid <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE, tuneGrid = man_grid)  
* Can plot the hyper-parameter performance either on accuracy or Kappa  
	* plot(gbm_model_voters_grid)  
    * plot(gbm_model_voters_grid, metric = "Kappa", plotType = "level")  
 
Grid vs. Random Search:  
  
* Can continue with the grid search methodology using ranges and seq()  
	* big_grid <- expand.grid(n.trees = seq(from = 10, to = 300, by = 50), interaction.depth = seq(from = 1, to = 10, length.out = 6), shrinkage = 0.1, n.minobsinnode = 10)  
* Can instead run random search, looking for good parameters randomly  
	* fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5, search = "random")  
    * gbm_model_voters_random <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE, tuneLength = 5)  # tuneLength would likely need to be ~100  
  
Adaptive Resampling:  
  
* Adaptive resampling is a blend between grid sampling and random sampling  
	* Hyperparameter combinations are resampled with values near combinations that performed well  
    * Adaptive Resampling is, therefore, faster and more efficient!  
* Can implement adaptive resampling using caret  
	* min: minimum number of resamples per hyperparameter  
    * alpha: confidence level for removing hyperparameters  
    * method: "gls" for linear model or "BT" for Bradley-Terry (better for large number of hyper-parameters and/or models that are already close to optimal)  
    * complete: if TRUE generates full resampling set  
    * fitControl <- trainControl(method = "adaptive_cv", adaptive = list(min = 2, alpha = 0.05, method = "gls", complete = TRUE), search = "random")  
    * fitControl <- trainControl(method = "adaptive_cv", number = 3, repeats = 3, adaptive = list(min = 2, alpha = 0.05, method = "gls", complete = TRUE), search = "random")  
    * gbm_model_voters_adaptive <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE, tuneLength = 7)  # tuneLength would generally be at least ~100  
  
Example code includes:  
```{r cache=TRUE}

tgtData <- rep(c("Did not vote", "Voted"), each=40)
vecVoteData <- c(2, 2, 3, 2, 2, 3, 3, 1, 2, 3, 4, 4, 4, 3, 1, 2, 2, 2, 3, 2, 1, 2, 3, 2, 1, 3, 3, 3, 3, 4, 2, 4, 1, 4, 3, 3, 2, 4, 2, 1, 3, 2, 2, 1, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 2, 3, 4, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 4, 2, 3, 3, 3, 3, 3, 2, 4, 1, 3, 4, 3, 3, 2, 2, 3, 3, 2, 2, 1, 2, 4, 2, 3, 2, 3, 4, 3, 2, 2, 2, 4, 1, 2, 2, 3, 2, 1, 3, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 3, 4, 1, 4, 4, 1, 3, 4, 4, 2, 2, 3, 3, 3, 2, 3, 1, 1, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 4, 2, 2, 2, 1, 4, 2, 2, 3, 3, 4, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 3, 1, 3, 2, 1, 1, 3, 2, 2, 1, 2, 2, 1, 1, 1, 3, 2, 2, 1, 1, 2, 1, 3, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 2, 1, 4, 2, 2, 3, 2, 3, 2, 3, 1, 3, 2, 1, 2, 2, 3, 2, 1, 1, 1, 3, 2, 1, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 3, 3, 1, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2, 2, 4, 3, 3, 4, 1, 4, 4, 1, 2, 1, 3, 4, 4, 2, 3, 1, 3, 1, 3, 1, 1, 2, 3, 1, 2, 1, 3, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 4, 1, 1, 3, 1, 2, 2, 2, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 4, 3, 3, 2, 1, 1, 2, 3, 3, 1, 2, 3, 2, 2, 3, 2, 3, 3, 1, 1, 2, 2, 2, 2, 2, 3, 1, 3, 2, 2, 3, 4, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 1, 4, 1, 3, 3, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 3, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 3, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 4, 2, 2, 1, 2, 2, 3, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 1, 1, 2, 3, 2, 2, 2, 3, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 4, 2, 3, 2, 1, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2)
vecVoteData <- c(vecVoteData, 2, 3, 1, 2, 2, 2, 3, 2, 4, 1, 1, 2, 2, 2, 3, 4, 3, 4, 2, 2, 3, 2, 2, 4, 1, 1, 3, 4, 2, 4, 3, 3, 2, 4, 3, 3, 2, 2, 1, 3, 3, 1, 2, 2, 1, 1, 1, 1, 3, 2, 2, 1, 2, 2, 3, 3, 2, 1, 3, 2, 3, 3, 1, 3, 1, 2, 1, 2, 3, 3, 2, 3, 3, 2, 4, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 1, 1, 2, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1, 3, 3, 1, 2, 2, 3, 1, 3, 3, 4, 2, 3, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 2, 2, 3, 2, 4, 1, 1, 3, 3, 3, 3, 4, 3, 2, 3, 3, 1, 3, 1, 4, 1, 1, 4, 4, 4, 4, 3, 3, 4, 3, 2, 3, 4, 2, 4, 1, 1, 3, 4, 3, 1, 1, 3, 4, 4, 4, 1, 1, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 4, 4, 3, 3, 4, 2, 2, 3, 2, 3, 4, 2, 3, 4, 3, 3, 2, 2, 1, 2, 2, 8, 2, 8, 8, 2, 2, 2, 2, 2, 1, 2, 2, 8, 8, 8, 2, 1, 2, 2, 2, 1, 2, 1, 8, 8, 2, 2, 8, 8, 1, 2, 2, 2, 8, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 8, 2, 1, 1, 1, 1, 2, 1, 1, 2, 8, 2, 1, 1, 2, 1, 2, 1, 3, 8, 3, 3, 1, 3, 2, 2, 3, 2, 1, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 8, 2, 3, 8, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 1, 3, 1, 8, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 1, 8, 8, 3, 1, 3, 3, 3, 3, 8, 3, 3, 3, 3, 8, 3, 3, 8, 3, 1, 3, 3, 3, 2, 3, 8, 3, 3, 3, 3, 8, 2, 2, 3, 1, 1, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 8, 2, 1, 1, 1, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 8, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 8, 2)
vecVoteData <- c(vecVoteData, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 8, 2, 1, 2, 2, 1, 2, 2, 2, 8, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 8, 8, 2, 1, 1, 8, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 8, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 8, 2, 1, 1, 2, 2, 1, 1, 8, 2, 1, 1, 8, 1, 1, 1, 2, 8, 1, 1, 8, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 1, 4, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 4, 2, 1, 1, 2, 1, 1, 3, 2, 4, 2, 2, 1, 1, 1, 2, 1, 1, 2, 4, 1, 2, 2, 3, 1, 2, 2, 4, 3, 2, 1, 1, 1, 3, 1, 4, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 4, 1, 3, 1, 1, 2, 2, 2, 3, 2, 2, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 2, 1, 1, 1, 1, 2, 4, 2, 2, 2, 1, 2, 1, 3, 1, 2, 2, 3, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 4, 1, 2, 3, 2, 1, 3, 1, 2, 2, 1, 3, 2, 2, 1, 2, 3, 4, 1, 1, 2, 3, 1, 1, 1, 2, 3, 4, 2, 1, 4, 2, 2, 2, 2, 1, 2, 3, 1, 1, 2, 1, 1, 4, 1, 1, 4, 1, 4, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 3, 4, 1, 2, 1, 1, 3, 1, 1, 3, 1, 1, 2, 1, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 4, 1, 1, 2, 1, 2, 1, 1, 1, 4, 1, 1, 2, 2, 2, 3, 3, 2, 2, 1, 2, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 3, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 3, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 2, 1, 1, 1, 2)
vecVoteData <- c(vecVoteData, 1, 1, 1, 2, 3, 2, 1, 1, 2, 3, 4, 1, 1, 2, 2, 2, 4, 2, 2, 4, 3, 3, 2, 4, 4, 3, 3, 4, 2, 2, 4, 4, 4, 2, 3, 1, 4, 2, 4, 4, 3, 3, 1, 4, 3, 3, 3, 1, 3, 2, 1, 2, 1, 1, 3, 2, 4, 2, 3, 2, 2, 1, 4, 3, 3, 3, 1, 2, 3, 3, 1, 3, 4, 4, 4, 3, 3, 4, 1, 1, 2, 4, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1, 2, 2, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 3, 1, 2, 2, 4, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 3, 2, 2, 1, 2, 1, 3, 2, 1, 3, 1, 1, 4, 2, 2, 1, 3, 2, 2, 3, 2, 1, 2, 2, 2, 3, 2, 2, 1, 1, 4, 1, 1, 2, 4, 2, 3, 2, 2, 2, 4, 4, 2, 1, 1, 2, 1, 2, 2, 2, 3, 3, 3, 4, 2, 4, 3, 1, 4, 3, 3, 2, 2, 2, 2, 3, 1, 2, 2, 4, 1, 1, 3, 4, 1, 1, 1, 3, 2, 4, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 4, 1, 2, 1, 4, 1, 3, 1, 3, 1, 1, 1, 3, 2, 2, 2, 1, 1, 2, 1, 1, 2, 3, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 4, 2, 1, 2, 2, 4, 2, 1, 3, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 3, 4, 2, 1, 1, 3, 2, 2, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 4, 2, 1, 1, 4, 3, 1, 3, 1, 1, 2, 4, 3, 1, 4, 1, 2, 1, 3, 1, 2, 2, 4, 1, 2, 3, 1, 4, 1, 2, 4, 1, 1, 1, 1, 1, 1, 2, 3, 1, 4, 1, 4, 4, 2, 1, 4, 1, 4, 2, 2, 4, 2, 3, 1, 4, 3, 4, 3, 1, 3, 4, 1, 1, 1, 3, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 3, 1, 1, 2, 2, 2, 2, 1, 1, 2, 3, 3, 1, 2, 2, 4, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 3, 2, 2, 1, 2, 1, 2, 2, 3, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 3, 1, 3, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1)
vecVoteData <- c(vecVoteData, 1, 1, 2, 1, 4, 2, 2, 2, 2, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 2, 1, 2, 3, 1, 3, 1, 4, 1, 3, 3, 3, 2, 2, 2, 3, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 4, 1, 2, 2, 4, 3, 1, 3, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 4, 2, 2, 2, 3, 1, 2, 2, 4, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 3, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 3, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 4, 4, 3, 1, 4, 2, 2, 2, 3, 1, 1, 2, 1, 1, 4, 4, 1, 4, 3, 2, 1, 2, 4, 3, 3, 4, 1, 2, 1, 2, 2, 3, 3, 1, 4, 3, 3, 4, 3, 4, 1, 1, 3, 3, 1, 1, 3, 1, 1, 3, 1, 3, 3, 3, 3, 4, 4, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 4, 1, 1, 1, 2, 1, 4, 3, 4, 4, 2, 2, 2, 3, 1, 2, 1, 2, 2, 3, 3, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 3, 2, 2, 1, 1, 1, 2, 1, 3, 2, 1, 1, 2, 1, 1, 3, 3, 2, 2, 2, 2, 1, 3, 3, 4, 3, 3, 3, 3, 1, 3, 2, 3, 1, 1, 2, 2, 1, 3, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 3, 4, 1, 1, 2, 3, 3, 2, 2, 2, 2, 1, 2, 1, 4, 3, 3, 1, 1, 2, 1, 1, 4, 1, 1, 4, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 1, 1, 1, 1, 3, 1, 2, 2, 3, 4, 3, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 3, 2, 3, 3, 3, 4, 2, 1, 3, 2, 2, 1, 2, 2, 1, 3, 1, 2, 4, 4, 2, 1, 1, 3, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 1, 1, 2, 2, 1, 3, 1, 1, 4, 3, 3, 2, 4, 3, 3, 2, 2, 1, 2, 3, 2, 2, 2, 3, 1, 2, 2, 3, 2, 3, 2, 1, 4, 2, 3, 1, 1, 1, 2, 1, 1, 2, 2, 4, 1, 3, 4, 3, 4, 2, 1, 4, 2, 3, 2, 1, 2, 2, 4, 1, 2, 4, 4, 3, 3, 3, 4, 1, 2, 1, 2)
voters_train_data <- tibble(turnout16_2016=tgtData) %>% 
    bind_cols(as.data.frame(matrix(vecVoteData, nrow=80, byrow=FALSE)))
names(voters_train_data) <- c('turnout16_2016', 'RIGGED_SYSTEM_1_2016', 'RIGGED_SYSTEM_2_2016', 'RIGGED_SYSTEM_3_2016', 'RIGGED_SYSTEM_4_2016', 'RIGGED_SYSTEM_5_2016', 'RIGGED_SYSTEM_6_2016', 'track_2016', 'persfinretro_2016', 'econtrend_2016', 'Americatrend_2016', 'futuretrend_2016', 'wealth_2016', 'values_culture_2016', 'US_respect_2016', 'trustgovt_2016', 'trust_people_2016', 'helpful_people_2016', 'fair_people_2016', 'imiss_a_2016', 'imiss_b_2016', 'imiss_c_2016', 'imiss_d_2016', 'imiss_e_2016', 'imiss_f_2016', 'imiss_g_2016', 'imiss_h_2016', 'imiss_i_2016', 'imiss_k_2016', 'imiss_l_2016', 'imiss_m_2016', 'imiss_n_2016', 'imiss_o_2016', 'imiss_p_2016', 'imiss_r_2016', 'imiss_s_2016', 'imiss_t_2016', 'imiss_u_2016', 'imiss_x_2016', 'imiss_y_2016')
glimpse(voters_train_data)


# Define Cartesian grid
man_grid <- expand.grid(degree = c(1, 2, 3), scale = c(0.1, 0.01, 0.001), C = 0.5)

fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5)

# Start timer, set seed & train model
tictoc::tic()
set.seed(42)
svm_model_voters_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "svmPoly", 
                                      trControl = fitControl, verbose= FALSE, tuneGrid = man_grid
                                      )
tictoc::toc()


# Plot default
plot(svm_model_voters_grid)

# Plot Kappa level-plot
plot(svm_model_voters_grid, metric = "Kappa", plotType = "level")


# Define the grid with hyperparameter ranges
big_grid <- expand.grid(size = seq(from = 1, to = 5, by = 1), decay = c(0, 1))

# Train control with grid search
fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5, search = "grid")

# Train neural net
tictoc::tic()
set.seed(42)
nn_model_voters_big_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, 
                                         method = "nnet", trControl = fitControl, verbose = FALSE
                                         )
tictoc::toc()

# Train neural net
tictoc::tic()
set.seed(42)
nn_model_voters_big_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "nnet", 
                                         trControl = fitControl, verbose = FALSE, tuneGrid = big_grid
                                         )
tictoc::toc()


# Train control with random search
fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5, search = "random")

# Test 6 random hyperparameter combinations
tictoc::tic()
nn_model_voters_big_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "nnet", 
                                         trControl = fitControl, verbose = FALSE, tuneLength = 6
                                         )
tictoc::toc()


# Define trainControl function
fitControl <- caret::trainControl(method="adaptive_cv", number = 3, repeats = 3)

# Define trainControl function
fitControl <- caret::trainControl(method = "adaptive_cv", number = 3, repeats = 3, search="random")

# Define trainControl function
fitControl <- caret::trainControl(method = "adaptive_cv", number = 3, repeats = 3,
                                  adaptive = list(min=3, alpha = 0.05, method = "BT", complete = FALSE),
                                  search = "random"
                                  )

# Start timer & train model
tictoc::tic()
svm_model_voters_ar <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "nnet", 
                                    trControl = fitControl, verbose = FALSE, tuneLength = 6
                                    )
tictoc::toc()

```
  
  
  
***
  
Chapter 3 - Hyperparameter Tuning with mlr  
  
Machine Learning with mlr:  
  
* The package mlr can be used as a framework for machine learning in R  
	* 1.  Define the task  
    * 2.  Define the learner  
    * 3.  Fit the model  
* There are many possible tasks in mlr  
	* RegrTask() for regression  
    * ClassifTask() for binary and multi-class classification  
    * MultilabelTask() for multi-label classification problems  
    * CostSensTask() for general cost-sensitive classification  
* Example of making a classification  
	* task <- makeClassifTask(data = knowledge_train_data, target = "UNS")  
    * listLearners()  
    * lrn <- makeLearner("classif.h2o.deeplearning", fix.factors.prediction = TRUE, predict.type = "prob")  
    * model <- train(lrn, task)  
  
Grid and Random Search with mlr:  
  
* There are three things to define for hyperparameter tuning using mlr  
	* the search space for every hyperparameter  
    * the tuning method (e.g. grid or random search)  
    * the resampling method  
* Example of defining the search space for every hyperparameter  
	* makeParamSet( makeNumericParam(), makeIntegerParam(), makeDiscreteParam(), makeLogicalParam(), makeDiscreteVectorParam() )  
    * getParamSet("classif.h2o.deeplearning")  
    * param_set <- makeParamSet( makeDiscreteParam("hidden", values = list(one = 10, two = c(10, 5, 10))), makeDiscreteParam("activation", values = c("Rectifier", "Tanh")), makeNumericParam("l1", lower = 0.0001, upper = 1), makeNumericParam("l2", lower = 0.0001, upper = 1) )  
* Example of tuning - can use either grid search (requires discrete search) or random search (all types)  
	* ctrl_grid <- makeTuneControlGrid()  
    * ctrl_random <- makeTuneControlRandom()  
* Example of defining the resampling strategy  
	* cross_val <- makeResampleDesc("RepCV", predict = "both", folds = 5 * 3)  
    * param_set <- makeParamSet( makeDiscreteParam("mtry", values = c(2,3,4,5)) )  
    * ctrl_grid <- makeTuneControlGrid()  
    * task <- makeClassifTask(data = knowledge_train_data, target = "UNS")  
    * lrn <- makeLearner("classif.h2o.deeplearning", predict.type = "prob", fix.factors.prediction = TRUE)  
    * lrn_tune <- tuneParams(lrn, task, resampling = cross_val, control = ctrl_grid, par.set = param_set)  
  
Evaluating Hyperparameters with mlr:  
  
* Generally, the goal of hyperparameter tuning is to understand impact on performance as well as convergence on optimal parameters  
* Recap of the basic process, this time using a holdout sample rather than repeated cross-validation  
	* getParamSet("classif.h2o.deeplearning")  
    * param_set <- makeParamSet( makeDiscreteParam("hidden", values = list(one = 10, two = c(10, 5, 10))), makeDiscreteParam("activation", values = c("Rectifier", "Tanh")), makeNumericParam("l1", lower = 0.0001, upper = 1), makeNumericParam("l2", lower = 0.0001, upper = 1) )  
    * ctrl_random <- makeTuneControlRandom(maxit = 50)  
    * holdout <- makeResampleDesc("Holdout")  
    * task <- makeClassifTask(data = knowledge_train_data, target = "UNS")  
    * lrn <- makeLearner("classif.h2o.deeplearning", predict.type = "prob", fix.factors.prediction = TRUE)  
    * lrn_tune <- tuneParams(lrn, task, resampling = holdout, control = ctrl_random, par.set = param_set)  
* Can see the impact of each of the hyperparameter tunings  
	* generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)  
    * hyperpar_effects <- generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)  
    * plotHyperParsEffect(hyperpar_effects, partial.dep.learn = "regr.randomForest", x = "l1", y = "mmce.test.mean", z = "hidden", plot.type = "line")  
  
Advanced Tuning with mlr:  
  
* There are several advanced tuning capabilities available in mlr  
	* makeTuneControlCMAES: CMA Evolution Strategy  
    * makeTuneControlDesign: Predefined data frame of hyperparameters  
    * makeTuneControlGenSA: Generalized simulated annealing  
    * makeTuneControlIrace: Tuning with iterated F-Racing  
    * makeTuneControlMBO: Model-based / Bayesian optimization  
* Can choose the evaluation metrics within mlr  
	* ctrl_gensa <- makeTuneControlGenSA()  
    * bootstrap <- makeResampleDesc("Bootstrap", predict = "both")  
    * lrn_tune <- tuneParams(learner = lrn, task = task, resampling = bootstrap, control = ctrl_gensa, par.set = param_set, measures = list(acc, mmce))  
    * lrn_tune <- tuneParams(learner = lrn, task = task, resampling = bootstrap, control = ctrl_gensa, par.set = param_set, measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce, train.mean)))  
* Can also run using nested cross-validation  
	* lrn_wrapper <- makeTuneWrapper(learner = lrn, resampling = bootstrap, control = ctrl_gensa, par.set = param_set, measures = list(acc, mmce))  
    * model_nested <- train(lrn_wrapper, task)  
    * getTuneResult(model_nested)  
    * cv2 <- makeResampleDesc("CV", iters = 2)  
    * res <- resample(lrn_wrapper, task, resampling = cv2, extract = getTuneResult)  
    * generateHyperParsEffectData(res)  
    * lrn_best <- setHyperPars(lrn, par.vals = list(minsplit = 4, minbucket = 3, maxdepth = 6))  
    * model_best <- train(lrn_best, task)  
  
Example code includes:  
```{r cache=TRUE}

vecData <- c(0.08, 0.18, 0.1, 0.12, 0.09, 0.08, 0.2, 0.2, 0.13, 0.18, 0.24, 0.18, 0.31, 0.28, 0.325, 0.323, 0.299, 0.32, 0.329, 0.315, 0.325, 0.325, 0.312, 0.299, 0.48, 0.46, 0.48, 0.49, 0.495, 0.43, 0.4, 0.44, 0.49, 0.44, 0.46, 0.495, 0.49, 0.42, 0.78, 0.85, 0.06, 0.08, 0.2, 0.06, 0.1, 0.15, 0.12, 0.06, 0.15, 0.1, 0.02, 0.09, 0.1, 0.08, 0.09, 0.2, 0.28, 0.265, 0.275, 0.295, 0.32, 0.25, 0.27, 0.27, 0.29, 0.288, 0.255, 0.295, 0.243, 0.295, 0.276, 0.258, 0.28, 0.255, 0.265, 0.255, 0.39, 0.38, 0.37, 0.38, 0.1, 0.1, 0.2, 0.18, 0.1, 0.12, 0.19, 0.14, 0.18, 0.17, 0.1, 0.23, 0.18, 0.2, 0.09, 0.06, 0.15, 0.29, 0.3, 0.27, 0.3, 0.295, 0.29, 0.258, 0.32, 0.3, 0.29, 0.26, 0.305, 0.32, 0.295, 0.285, 0.3, 0.4, 0.4, 0.41, 0.41, 0.44, 0.42, 0.43, 0.08, 0.18, 0.1, 0.12, 0.3, 0.325, 0.45, 0.49, 0.39, 0.34, 0.75, 0.51, 0.1, 0.16, 0.25, 0.32, 0.32, 0.28, 0.55, 0.69, 0.61, 0.9, 0.8, 0.7, 0.12, 0.2, 0.3, 0.245, 0.276, 0.45, 0.33, 0.33, 0.34, 0.55, 0.78, 0.82, 0.9, 0.7, 0.21, 0.05, 0.06, 0.08, 0.14, 0.06, 0.25, 0.32, 0.28, 0.29, 0.295, 0.42, 0.33, 0.55, 0.6, 0.58, 0.61, 0.68, 0.1, 0.06, 0.1, 0.2, 0.12, 0.29, 0.1, 0.31, 0.29, 0.31, 0.305, 0.25, 0.27, 0.29, 0.255, 0.31, 0.65, 0.75, 0.76, 0.72, 0.05, 0.1, 0.06, 0.01, 0.1, 0.1, 0.2, 0.3, 0.27, 0.245, 0.38, 0.49, 0.33, 0.36, 0.39, 0.7, 0.72, 0.52, 0.6, 0.77, 0.79, 0.06, 0.08, 0.12, 0.2, 0.25, 0.3, 0.28, 0.255, 0.27, 0.3, 0.28, 0.255, 0.27, 0.59, 0.64, 0.85, 0.18, 0.12, 0.18, 0.09, 0.08, 0.21, 0.305, 0.1, 0.55, 0.7, 0.75, 0.68, 0.62, 0.28, 0.6, 0.85, 0.71, 0.32, 0.58, 0.41, 0.69, 0.38, 0.89, 0.31, 0.72, 0.02, 0.28, 0.46, 0.52, 0.67, 0.95, 0.28, 0.76, 0.15, 0.38, 0.58, 0.27, 0.12, 0.59, 0.88, 0.11, 0.38, 0.67, 0.52, 0.72, 0.68, 0.91, 0.05, 0.08, 0.35, 0.51, 0.1, 0.05, 0.2, 0.35, 0.75, 0.22, 0.36, 0.12, 0.33, 0.6, 0.53, 0.73, 0.12, 0.57, 0.72, 0.86, 0.79, 0.15, 0.1, 0.32, 0.4, 0.79, 0.86, 0.73, 0.08, 0.31, 0.81, 0.88, 0.4, 0.35, 0.8, 0.72, 0.02, 0.4, 0.32, 0.53, 0.15, 0.52, 0.7, 0.37, 0.31, 0.75, 0.38, 0.55, 0.61, 0.8, 0.75, 0.19, 0.37, 0.36, 0.66, 0.72, 0.78, 0.19, 0.4, 0.37, 0.52, 0.26, 0.52, 0.64, 0.55, 0.31, 0.56, 0.6, 0.63, 0.52, 0.29, 0.18, 0.54, 0.26, 0.41, 0.33, 0.58, 0.8, 0.87, 0.51, 0.24, 0.3, 0.15, 0.35, 0.18, 0.94, 0.31, 0.2, 0.38, 0.71, 0.18, 0.33, 0.42, 0.33, 0.31, 0.32, 0.33, 0.89, 0.4, 0.8, 0.32, 0.49, 0.92, 0.22, 0.7, 0.95, 0.65, 0.14, 0.77, 0.27, 0.3, 0.53, 0.75, 0.26, 0.24, 0.01, 0.9, 0.3, 0.65, 0.8, 0.25, 0.98, 0.72, 0.41, 0.08, 0.27, 0.78, 0.76, 0.65, 0.72, 0.76, 0.78, 0.42, 0.64, 0.75, 0.48, 0.28, 0.75, 0.1, 0.44, 0.76, 0.48, 0.7, 0.41, 0.78, 0.23, 0.62, 0.77, 0.42, 0.76, 0.27, 0.4, 0.65, 0.72, 0.28, 0.63, 0.06, 0.48, 0.78, 0.27, 0.65, 0.78, 0.3, 0.12, 0.29, 0.31, 0.49, 0.29, 0.64, 0.14, 0.31, 0.51, 0.29, 0.84, 0.19, 0.19, 0.3, 0.55, 0.02, 0.29, 0.3, 0.12, 0.09, 0.29, 0.78, 0.31, 0.25, 0.29, 0.4, 0.81, 0.31, 0.61, 0.25, 0.26, 0.1, 0.31, 0.18, 0.22, 0.56, 0.09, 0.9, 0.81, 0.9, 0.8, 0.85, 0.56, 0.78, 0.78, 0.77, 0.9, 0.86, 0.82, 0.75, 0.78, 0.79, 0.8, 0.87, 0.58, 0.79, 0.7, 0.81, 0.76, 0.5, 0.66, 0.71, 0.65, 0.77, 0.86, 0.83, 0.89, 0.9, 0.85, 0.71, 0.83, 0.89, 0.93, 0.47, 0.8, 0.75, 0.68, 0.33, 0.24, 0.25, 0.3, 0.33, 0.29, 0.2, 0.25, 0.24, 0.26, 0.1, 0.05, 0.26, 0.1, 0.01, 0.28, 0.32, 0.1, 0.3, 0.28, 0.24, 0.26, 0.25, 0.28, 0.18, 0.24, 0.15, 0.19, 0.29, 0.1, 0.33, 0.3, 0.13, 0.25, 0.28, 0.14, 0.34, 0.26, 0.1, 0.3, 0.3, 0.34, 0.6, 0.66, 0.65, 0.59, 0.45, 0.6, 0.25, 0.66, 0.62, 0.45, 0.55, 0.25, 0.59, 0.56, 0.51, 0.51, 0.67, 0.58, 0.53, 0.67, 0.67, 0.56, 0.34, 0.54, 0.67, 0.59, 0.54, 0.3, 0.55, 0.45, 0.83, 0.67, 0.65, 0.5, 0.58, 0.56, 0.48, 0.64)

knowledge_train_data <- tibble(UNS=rep(c("High", "Low", "Medium"), each=40))
mtxData <- data.frame(matrix(vecData, nrow=120, byrow=FALSE))
names(mtxData) <- c("STG", "SCG", "STR", "LPR", "PEG")
knowledge_train_data <- bind_cols(as.tibble(mtxData), knowledge_train_data)
glimpse(knowledge_train_data)


library(mlr)

# Create classification taks
task <- mlr::makeClassifTask(data = knowledge_train_data, target = "UNS")

# Call the list of learners
mlr::listLearners() %>%
    as.data.frame() %>%
    select(class, short.name, package) %>%
    filter(grepl("classif.", class))

# Create learner
lrn <- mlr::makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)


# Get the parameter set for neural networks of the nnet package
ParamHelpers::getParamSet("classif.nnet")

# Define set of parameters
param_set <- ParamHelpers::makeParamSet(ParamHelpers::makeDiscreteParam("size", values = c(2,3,5)),
                                        ParamHelpers::makeNumericParam("decay", lower = 0.0001, upper = 0.1)
                                        )

# Print parameter set
print(param_set)

# Define a random search tuning method.
ctrl_random <- mlr::makeTuneControlRandom()


# Define task
task <- makeClassifTask(data = knowledge_train_data, target = "UNS")

# Define learner
lrn <- makeLearner("classif.nnet", predict.type = "prob", fix.factors.prediction = TRUE)

# Define set of parameters
param_set <- makeParamSet(makeDiscreteParam("size", values = c(2,3,5)),
                          makeNumericParam("decay", lower = 0.0001, upper = 0.1)
                          )

# Define a random search tuning method.
ctrl_random <- mlr::makeTuneControlRandom(maxit = 6)

# Define a 2 x 2 repeated cross-validation scheme
cross_val <- mlr::makeResampleDesc("RepCV", folds = 2 * 2)

# Tune hyperparameters
tictoc::tic()
lrn_tune <- mlr::tuneParams(lrn, task, resampling = cross_val, control = ctrl_random, par.set=param_set)
tictoc::toc()


task <- makeClassifTask(data = knowledge_train_data, target = "UNS")
lrn <- makeLearner(cl = "classif.rpart", fix.factors.prediction = TRUE)
param_set <- makeParamSet(makeIntegerParam("minsplit", lower = 1, upper = 30),
                          makeIntegerParam("minbucket", lower = 1, upper = 30),
                          makeIntegerParam("maxdepth", lower = 3, upper = 10)
                          )
ctrl_random <- makeTuneControlRandom(maxit = 10)


# Create holdout sampling
holdout <- makeResampleDesc("Holdout")

# Perform tuning
lrn_tune <- tuneParams(learner = lrn, task = task, resampling = holdout, 
                       control = ctrl_random, par.set = param_set
                       )

# Generate hyperparameter effect data
hyperpar_effects <- generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)

# Plot hyperparameter effects
plotHyperParsEffect(hyperpar_effects, partial.dep.learn = "regr.glm",
                    x = "minsplit", y = "mmce.test.mean", z = "maxdepth", plot.type = "line"
                    )


task <- makeClassifTask(data = knowledge_train_data, target = "UNS")
lrn <- makeLearner(cl = "classif.nnet", fix.factors.prediction = TRUE)
param_set <- makeParamSet(makeIntegerParam("size", lower = 1, upper = 5),
                          makeIntegerParam("maxit", lower = 1, upper = 300),
                          makeNumericParam("decay", lower = 0.0001, upper = 1)
                          )
ctrl_random <- makeTuneControlRandom(maxit = 10)


# Create holdout sampling
holdout <- makeResampleDesc("Holdout", predict = "both")

# Perform tuning
lrn_tune <- tuneParams(learner = lrn, task = task, resampling = holdout, control = ctrl_random, 
                       par.set = param_set,
                       measures = list(mmce, setAggregation(mmce, train.sd), 
                                       acc, setAggregation(acc, train.sd)
                                       )
                       )


task <- makeClassifTask(data = knowledge_train_data, target = "UNS")
lrn <- makeLearner(cl = "classif.nnet", fix.factors.prediction = TRUE)


# Set hyperparameters
lrn_best <- setHyperPars(lrn, par.vals = list(size=1, maxit = 150, decay = 0))

# Train model
model_best <- train(lrn_best, task)

```
  
  
  
***
  
Chapter 4 - Hyperparameter Tuning with h2o  
  
Machine Learning with h2o:  
  
* The h2o package in an open-source package that is designed for scalability (distributed clusters)  
* Need to start by initiating a cluster  
	* library(h2o)  
    * h2o.init()  
* Need to then convert the data to an h2o file and define the feature and target names  
	* glimpse(seeds_data)  
    * seeds_data_hf <- as.h2o(seeds_data)  
    * y <- "seed_type"  
    * x <- setdiff(colnames(seeds_data_hf), y)  
    * seeds_data_hf[, y] <- as.factor(seeds_data_hf[, y])  
* Can also create train, test, and validation sets  
	* sframe <- h2o.splitFrame(data = seeds_data_hf, ratios = c(0.7, 0.15), seed = 42)  
    * train <- sframe[[1]]  
    * valid <- sframe[[2]]  
    * test <- sframe[[3]]  
    * summary(train$seed_type, exact_quantiles = TRUE)  
* There are many model algorithms available in h2o  
	* Gradient Boosted models with h2o.gbm() & h2o.xgboost()  
    * Generalized linear models with h2o.glm()  
    * Random Forest models with h2o.randomForest()  
    * Neural Networks with h2o.deeplearning()  
* Example of running GBM using h2o  
	* gbm_model <- h2o.gbm(x = x, y = y, training_frame = train, validation_frame = valid)  
    * perf <- h2o.performance(gbm_model, test)  
    * h2o.confusionMatrix(perf)  
    * h2o.logloss(perf)  
    * h2o.predict(gbm_model, test)  
  
Grid and random search with h2o:  
  
* Can use the help files to obtain all of the available hyperparameters  
	* ?h2o.gbm  
* Brief recap of data preparation steps  
	* seeds_data_hf <- as.h2o(seeds_data)  
    * y <- "seed_type"  
    * x <- setdiff(colnames(seeds_data_hf), y)  
    * sframe <- h2o.splitFrame(data = seeds_data_hf, ratios = c(0.7, 0.15), seed = 42)  
    * train <- sframe[[1]]  
    * valid <- sframe[[2]]  
    * test <- sframe[[3]]  
* Can then define a hyperparameter grid  
	* gbm_params <- list(ntrees = c(100, 150, 200), max_depth = c(3, 5, 7), learn_rate = c(0.001, 0.01, 0.1))  
    * gbm_grid <- h2o.grid("gbm", grid_id = "gbm_grid", x = x, y = y, training_frame = train, validation_frame = valid, seed = 42, hyper_params = gbm_params)  
    * gbm_gridperf <- h2o.getGrid(grid_id = "gbm_grid", sort_by = "accuracy", decreasing = TRUE)  
    * best_gbm <- h2o.getModel(gbm_gridperf@model_ids[[1]])  # Top GBM model chosen by validation accuracy has id position 1 due to the decreasing accuracy sort above  
    * print(best_gbm@model[["model_summary"]])  
    * h2o.performance(best_gbm, test)  
* Can also run random search using h2o  
	* gbm_params <- list(ntrees = c(100, 150, 200), max_depth = c(3, 5, 7), learn_rate = c(0.001, 0.01, 0.1))  
    * search_criteria <- list(strategy = "RandomDiscrete", max_runtime_secs = 60, seed = 42)  
    * gbm_grid <- h2o.grid("gbm", grid_id = "gbm_grid", x = x, y = y, training_frame = train, validation_frame = valid, seed = 42, hyper_params = gbm_params, search_criteria = search_criteria)  
    * search_criteria <- list(strategy = "RandomDiscrete", stopping_metric = "mean_per_class_error", stopping_tolerance = 0.0001, stopping_rounds = 6)  
    * gbm_grid <- h2o.grid("gbm", x = x, y = y, training_frame = train, validation_frame = valid, seed = 42, hyper_params = gbm_params, search_criteria = search_criteria)  
  
Automatic machine learning with h2o:  
  
* Can also run automatic machine learning algorithms in h2o, merely by specifying a total run time  
	* Generalized Linear Model (GLM)  
    * (Distributed) Random Forest (DRF)  
    * Extremely Randomized Trees (XRT)  
    * Extreme Gradient Boosting (XGBoost)  
    * Gradient Boosting Machines (GBM)  
    * Deep Learning (fully-connected multi-layer artificial neural network)  
    * Stacked Ensembles (of all models & of best of family)  
* Can tune the hyperparameters prior to the full automatic machine learning algorithm runs (for GBM and deep learning)  
	* automl_model <- h2o.automl(x = x, y = y, training_frame = train, validation_frame = valid, max_runtime_secs = 60, sort_metric = "logloss", seed = 42)  
* Can view the leaderboard  
	* lb <- automl_model@leaderboard  
    * model_ids <- as.data.frame(lb)$model_id  
    * aml_leader <- h2o.getModel(model_ids[1])  
  
Wrap up:  
  
* Hyperparameters and benefits of tuning in machine learning models  
	* Cartesian Grid Search  
    * Random Search  
    * Adaptive Resampling  
    * Automatic Machine Learning  
    * Evaluating tuning results with performance metrics  
    * Stopping criteria  
  
Example code includes:  
```{r eval=FALSE}

# Code runs OK in Console, does not run in knitr
library(mlr)

vecSeed <- c(15.26, 14.29, 13.84, 16.14, 14.38, 14.69, 15.26, 13.89, 13.78, 13.74, 14.59, 13.99, 15.69, 14.7, 12.72, 14.11, 15.01, 13.02, 14.11, 13.45, 13.16, 15.49, 14.09, 13.94, 15.05, 17.08, 14.8, 13.5, 13.16, 15.5, 13.8, 15.36, 14.99, 14.43, 15.78, 17.63, 16.84, 19.11, 16.82, 16.77, 20.71, 17.12, 18.72, 20.2, 19.57, 19.51, 18.88, 18.98, 20.88, 18.81, 18.59, 18.36, 16.87, 18.17, 18.72, 19.46, 19.18, 18.95, 18.83, 17.63, 18.55, 18.45, 19.38, 19.13, 19.14, 20.97, 19.06, 18.96, 19.15, 20.24, 13.07, 13.34, 12.22, 11.82, 11.21, 11.43, 12.49, 10.79, 11.83, 12.01, 12.26, 11.18, 11.19, 11.34, 11.75, 11.49, 12.54, 12.02, 12.05, 12.55, 11.14, 12.1, 12.15, 10.8, 11.26, 11.41, 12.46, 12.19, 11.65, 11.56, 11.81, 10.91, 11.23, 11.27, 11.87, 14.84, 14.09, 13.94, 14.99, 14.21, 14.49, 14.85, 14.02, 14.06, 14.05, 14.28, 13.83, 14.75, 14.21, 13.57, 14.26, 14.76, 13.76, 14.18, 14.02, 13.82, 14.94, 14.41, 14.17, 14.68, 15.38, 14.52, 13.85, 13.55, 14.86, 14.04, 14.76, 14.56, 14.4, 14.91, 15.98, 15.67, 16.26, 15.51, 15.62, 17.23, 15.55, 16.19, 16.89, 16.74, 16.71, 16.26, 16.66, 17.05, 16.29, 16.05, 16.52, 15.65, 16.26, 16.34, 16.5, 16.63, 16.42, 16.29, 15.86, 16.22, 16.12, 16.72, 16.31, 16.61, 17.25, 16.45, 16.2, 16.45, 16.91, 13.92, 13.95, 13.32, 13.4, 13.13, 13.13, 13.46, 12.93, 13.23, 13.52, 13.6, 13.04, 13.05, 12.87, 13.52, 13.22, 13.67, 13.33, 13.41, 13.57, 12.79, 13.15, 13.45, 12.57, 13.01, 12.95, 13.41, 13.36, 13.07, 13.31, 13.45, 12.8, 12.82, 12.86, 13.02, 0.87, 0.9, 0.9, 0.9, 0.9, 0.88, 0.87, 0.89, 0.88, 0.87, 0.9, 0.92, 0.91, 0.92, 0.87, 0.87, 0.87, 0.86, 0.88, 0.86, 0.87, 0.87, 0.85, 0.87, 0.88, 0.91, 0.88, 0.89, 0.9, 0.88, 0.88, 0.89, 0.89, 0.88, 0.89, 0.87, 0.86, 0.91, 0.88, 0.86, 0.88, 0.89, 0.9, 0.89, 0.88, 0.88, 0.9, 0.86, 0.9, 0.89, 0.91, 0.85, 0.86, 0.86, 0.88, 0.9, 0.87, 0.88, 0.89, 0.88, 0.89, 0.89, 0.87, 0.9, 0.87, 0.89, 0.89, 0.91, 0.89, 0.89, 0.85, 0.86, 0.87, 0.83, 0.82, 0.83, 0.87, 0.81, 0.85, 0.82, 0.83, 0.83, 0.83, 0.86, 0.81, 0.83, 0.84, 0.85, 0.84, 0.86, 0.86, 0.88, 0.84, 0.86, 0.84, 0.86, 0.87, 0.86, 0.86, 0.82, 0.82, 0.84, 0.86, 0.86, 0.88, 5.76, 5.29, 5.32, 5.66, 5.39, 5.56, 5.71, 5.44, 5.48, 5.48, 5.35, 5.12, 5.53, 5.21, 5.23, 5.52, 5.79, 5.39, 5.54, 5.52, 5.45, 5.76, 5.72, 5.58, 5.71, 5.83, 5.66, 5.35, 5.14, 5.88, 5.38, 5.7, 5.57, 5.58, 5.67, 6.19, 6, 6.15, 6.02, 5.93, 6.58, 5.85, 6.01, 6.29, 6.38, 6.37, 6.08, 6.55, 6.45, 6.27, 6.04, 6.67, 6.14, 6.27, 6.22, 6.11, 6.37, 6.25, 6.04, 6.03, 6.15, 6.11, 6.3, 6.18, 6.26, 6.56, 6.42, 6.05, 6.25, 6.32, 5.47, 5.39, 5.22, 5.31, 5.28, 5.18, 5.27, 5.32, 5.26, 5.41, 5.41, 5.22, 5.25, 5.05, 5.44, 5.3, 5.45, 5.35, 5.27, 5.33, 5.01, 5.11, 5.42, 4.98, 5.19, 5.09, 5.24, 5.24, 5.11, 5.36, 5.41, 5.09, 5.09, 5.09, 5.13, 3.31, 3.34, 3.38, 3.56, 3.31, 3.26, 3.24, 3.2, 3.16, 3.11, 3.33, 3.38, 3.51, 3.47, 3.05, 3.17, 3.25, 3.03, 3.22, 3.06, 2.98, 3.37, 3.19, 3.15, 3.33)
vecSeed <- c(vecSeed, 3.68, 3.29, 3.16, 3.2, 3.4, 3.15, 3.39, 3.38, 3.27, 3.43, 3.56, 3.48, 3.93, 3.49, 3.44, 3.81, 3.57, 3.86, 3.86, 3.77, 3.8, 3.76, 3.67, 4.03, 3.69, 3.86, 3.48, 3.46, 3.51, 3.68, 3.89, 3.68, 3.75, 3.79, 3.57, 3.67, 3.77, 3.79, 3.9, 3.74, 3.99, 3.72, 3.9, 3.82, 3.96, 2.99, 3.07, 2.97, 2.78, 2.69, 2.72, 2.97, 2.65, 2.84, 2.78, 2.83, 2.69, 2.67, 2.85, 2.68, 2.69, 2.88, 2.81, 2.85, 2.97, 2.79, 2.94, 2.84, 2.82, 2.71, 2.77, 3.02, 2.91, 2.85, 2.68, 2.72, 2.67, 2.82, 2.8, 2.95, 2.22, 2.7, 2.26, 1.36, 2.46, 3.59, 4.54, 3.99, 3.14, 2.93, 4.18, 5.23, 1.6, 1.77, 4.1, 2.69, 1.79, 3.37, 2.75, 3.53, 0.86, 3.41, 3.92, 2.12, 2.13, 2.96, 3.11, 2.25, 2.46, 4.71, 1.56, 1.37, 2.96, 3.98, 5.59, 4.08, 4.67, 2.94, 4, 4.92, 4.45, 2.86, 5.32, 5.17, 1.47, 2.96, 1.65, 3.69, 5.02, 3.24, 6, 4.93, 3.7, 2.85, 2.19, 4.31, 3.36, 3.37, 2.55, 3.75, 1.74, 2.23, 3.68, 2.11, 6.68, 4.68, 2.25, 4.33, 3.08, 5.9, 5.3, 6, 5.47, 4.47, 6.17, 2.22, 4.42, 5.46, 5.2, 6.99, 4.76, 3.33, 5.81, 3.35, 4.38, 5.39, 3.08, 4.27, 4.99, 4.42, 6.39, 2.2, 3.64, 4.77, 5.34, 4.96, 4.99, 4.86, 5.21, 4.06, 4.9, 4.18, 7.52, 3.98, 3.6, 5.22, 4.83, 4.8, 5.17, 4.96, 5.22, 5.31, 4.74, 4.87, 4.83, 4.78, 4.78, 5.05, 4.65, 4.91, 5.22, 5, 4.83, 5.04, 5.1, 5.06, 5.23, 5.3, 5.01, 5.36, 5.48, 5.31, 5.18, 4.78, 5.53, 4.96, 5.13, 5.17, 5.14, 5.14, 6.06, 5.88, 6.08, 5.84, 5.8, 6.45, 5.75, 5.88, 6.19, 6.27, 6.18, 6.11, 6.5, 6.32, 6.05, 5.88, 6.45, 5.97, 6.27, 6.1, 6.01, 6.23, 6.15, 5.88, 5.93, 5.89, 5.79, 5.96, 5.92, 6.05, 6.32, 6.16, 5.75, 6.18, 6.19, 5.39, 5.31, 5.22, 5.18, 5.28, 5.13, 5, 5.19, 5.31, 5.27, 5.36, 5, 5.22, 5, 5.31, 5.31, 5.49, 5.31, 5.05, 5.18, 5.05, 5.06, 5.34, 5.06, 5.09, 4.83, 5.15, 5.16, 5.13, 5.18, 5.35, 4.96, 4.96, 5, 5.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)
seeds_train_data <- as.data.frame(matrix(vecSeed, nrow=105, byrow=FALSE))
names(seeds_train_data) <- c('area', 'perimeter', 'compactness', 'kernel_length', 'kernel_width', 'asymmetry', 'kernel_groove', 'seed_type')
seeds_train_data$seed_type <- as.factor(seeds_train_data$seed_type)
glimpse(seeds_train_data)


# Initialise h2o cluster
h2o::h2o.init()

# Convert data to h2o frame
seeds_train_data_hf <- h2o::as.h2o(seeds_train_data)

# Identify target and features
y <- "seed_type"
x <- setdiff(colnames(seeds_train_data_hf), y)

# Split data into train & validation sets
sframe <- h2o::h2o.splitFrame(seeds_train_data_hf, seed = 42)
train <- sframe[[1]]
valid <- sframe[[2]]

# Calculate ratio of the target variable in the training set
summary(seeds_train_data_hf$seed_type, exact_quantiles = TRUE)


# Train random forest model
rf_model <- h2o::h2o.randomForest(x = x, y = y, training_frame = train, validation_frame = valid)

# Calculate model performance
perf <- h2o::h2o.performance(rf_model, valid = TRUE)

# Extract confusion matrix
h2o::h2o.confusionMatrix(perf)

# Extract logloss
h2o::h2o.logloss(perf)


# Define hyperparameters
dl_params <- list(hidden = list(c(50, 50), c(100, 100)), epochs = c(5, 10, 15), 
                  rate = c(0.001, 0.005, 0.01)
                  )


# Define search criteria
search_criteria <- list(strategy = "RandomDiscrete",
                        max_runtime_secs = 10, # this is way too short & only used to keep runtime short!
                        seed = 42
                        )

# Train with random search
dl_grid <- h2o::h2o.grid("deeplearning", grid_id = "dl_grid", x = x, y = y,
                         training_frame = train, validation_frame = valid, seed = 42,
                         hyper_params = dl_params, search_criteria = search_criteria
                         )


# Define early stopping
stopping_params <- list(strategy = "RandomDiscrete", stopping_metric = "misclassification",
                        stopping_rounds = 2, stopping_tolerance = 0.1, seed = 42
                        )


# Run automatic machine learning
automl_model <- h2o::h2o.automl(x = x, y = y, training_frame = train, max_runtime_secs = 10,
                                sort_metric = "mean_per_class_error", leaderboard_frame = valid, seed = 42
                                )


# Extract the leaderboard
lb <- automl_model@leaderboard
head(lb)

# Assign best model new object name
aml_leader <- automl_model@leader

# Look at best model
summary(aml_leader)

```
  
  
  
***
  
###_Intermediate Functional Programming with purrr_  
  
Chapter 1 - Programming with purrr  
  
Refresher of purrr Basics:  
  
* The map() function is one of the most basic purrr calls  
	* map(.x, .f, .)  # for each element of .x do .f  
* OpenData files available from French city St Malo  
	* JSON format; nested list  
* The map() function will always return a list by default  
	* res <- map(visit_2015, sum)  # returns a list  
* Can override to other preferred outputs, such as map_dbl()  
	* res <- map_dbl(visit_2015, sum)  # returns a numeric  
* Can also extend to map2(.x, .y, .f, .) which resolves to do .f(.x, .y, .)  
	* res <- map2(visit_2015, visit_2016, sum)  
    * res <- map2_dbl(visit_2015, visit_2016, sum)  
* Can use pmap() to run operations on 3+ items, though these need to be passed in as a list  
	* l <- list(visit_2014, visit_2015, visit_2016)  
    * res <- pmap(l, sum)  
    * res <- pmap_dbl(l, sum)  
  
Introduction to mappers:  
  
* The .f is the action element - function applied to every element, number n to extract the nth element, character vector of named elements to extract  
* The functions can either be regular functions or lambda (anonymous) functions  
	* map_dbl(visit_2014, function(x) { round(mean(x)) })  
* The anonymous function with a one-sided formula can be written in any of several ways  
	* map_dbl(visits2017, ~ round(mean(.x)))  # typically the default  
    * map_dbl(visits2017, ~ round(mean(.)))  
    * map_dbl(visits2017, ~ round(mean(..1)))  
    * map2(visits2016, visits2017, ~ .x + .y)  
    * map2(visits2016, visits2017, ~ ..1 + ..2)  
* Can extend to data with more than 2 parameters  
	* pmap(list, ~ ..1 + ..2 + ..3)  
* Can use as_mapper to create mapper objects from lambda functions  
	* round_mean <- function(x){ round(mean(x)) }  
    * round_mean <- as_mapper(~ round(mean(.x))))  
* Mappers have several benefits  
	* More concise  
    * Easier to read than functions  
    * Reusable  
  
Using Mappers to Clean Data:  
  
* Can use set_names from purrr to set the names of a list  
	* visits2016 <- set_names(visits2016, month.abb)  
    * all_visits <- list(visits2015, visits2016, visits2017)  
    * named_all_visits <- map(all_visits, ~ set_names(.x, month.abb))  
* The keep() function extracts elements that satisfy a condition  
	* over_30000 <- keep(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * over_mapper <- keep(visits2016, limit)  
* The discard() function removes elements that satisfy a condition  
	* under_30000 <- discard(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * under_mapper <- discard(visits2016, limit)  
    * names(under_mapper)  
* Can use keep() and discard() with map() to clean up lists  
	* df_list <- list(iris, airquality) %>% map(head)  
    * map(df_list, ~ keep(.x, is.factor))  
  
Predicates:  
  
* Predicates return either TRUE or FALSE - example of is.numeric()  
* Predicate functionals take an element and a predicate, and then use the predicate on the element  
	* keep(airquality, is.numeric)  # keep all elements that return TRUE when run against the predicate  
* There are also extensions of every() and some()  
	* every(visits2016, is.numeric)  
    * every(visits2016, ~ mean(.x) > 1000)  
    * some(visits2016, ~ mean(.x) > 1000)  
* The detect_index() returns the first and last element that satisfies a condition  
	* detect_index(visits2016, ~ mean(.x) > 1000)  # index of first element that satisfies  
    * detect_index(visits2016, ~ mean(.x) > 1000, .right = TRUE)  # index of last element that satisfies  
* The detect() returns the value rather than the index  
	* detect(visits2016, ~ mean(.x) > 1000, .right = TRUE)  
* The has_element() tests whether an object contains an item  
	* visits2016_mean <- map(visits2016, mean)  
    * has_element(visits2016_mean,981)  
  
Example code includes:  
```{r}

# Create the to_day function
to_day <- function(x) {
 x*24
}

visit_a <- c(117, 147, 131, 73, 81, 134, 121)
visit_b <- c(180, 193, 116, 166, 131, 153, 146)
visit_c <- c(57, 110, 68, 72, 87, 141, 67)

# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)

# Convert to daily number of visits: all_visits_day
all_visits_day <- map(all_visits, to_day)

# Map the mean() function and output a numeric vector 
map_dbl(all_visits_day, mean)


# You'll test out both map() and walk() for plotting
# Both return the "side effects," that is to say, the changes in the environment (drawing plots, downloading a file, changing the working directory...), but walk() won't print anything to the console.

# Create all_tests list  and modify with to_day() function
all_tests <- list(visit_a, visit_b, visit_c)
all_tests_day <- map(all_tests, to_day)

# Plot all_tests_day with map
map(all_tests_day, barplot)

# Plot all_tests_day
walk(all_tests_day, barplot)

# Get sum of all visits and class of sum_all
sum_all <- pmap(all_tests_day, sum)
class(sum_all)


# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) {
  x*24
})

# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)

# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)

# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
map(visit_c, to_day)


# Round visit_a to the nearest tenth with a mapper
map_dbl(visit_a, ~ round(.x, -1))

# Create to_ten, a mapper that rounds to the nearest tenth
to_ten <- as_mapper(~ round(.x, -1))

# Map to_ten on visit_b
map_dbl(visit_b, to_ten)

# Map to_ten on visit_c
map_dbl(visit_c, to_ten)


# Create a mapper that test if .x is more than 100 
is_more_than_hundred <- as_mapper(~ .x > 100)

# Run this mapper on the all_visits object
all_visits <- list(visit_a, visit_b, visit_c)
map(all_visits, ~ keep(.x, is_more_than_hundred) )

# Use the  day vector to set names to all_list
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
full_visits_named <- map(all_visits, ~ set_names(.x, day))

# Use this mapper with keep() 
map(full_visits_named, ~ keep(.x, is_more_than_hundred))


# Set the name of each subvector
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
all_visits_named <- map(all_visits, ~ set_names(.x, day))

# Create a mapper that will test if .x is over 100 
threshold <- as_mapper(~.x > 100)

# Run this mapper on the all_visits object: group_over
group_over <- map(all_visits, ~ keep(.x, threshold) )

# Run this mapper on the all_visits object: group_under
group_under <-  map(all_visits, ~ discard(.x, threshold) )


# Create a threshold variable, set it to 160
threshold <- 160

# Create a mapper that tests if .x is over the defined threshold
over_threshold <- as_mapper(~ .x > threshold)

# Are all elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ every(.x, over_threshold))

# Are some elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ some(.x, over_threshold))

```
  
  
  
***
  
Chapter 2 - Functional Programming from Theory to Practice  
  
Functional Programming in R:  
  
* Everything that exists is an object and everything that happens is a function call  
	* This means that a function is an object and can be treated as such  
    * Every action in R is performed by a function  
    * Functions are first-class citizens, and behave like any other object  
    * Functions can be manipulated, stored as variables, lambda (anonymous), stored in a list, arguments of a function, returned by a function  
    * R is a functional programming language  
* In a "pure function", output depends only on input, and there are no side-effects (no changes to the environment)  
	* Sys.Date() depends on the enviornment and is thus not pure  
    * write.csv() is called solely for the side effect (writing a file) and is thus not pure  
  
Tools for Functional Programming in purrr:  
  
* A high order function can take functions as input and return functions as output  
	* nop_na <- function(fun) {  
    *     function(...){fun(..., na.rm = TRUE)}  
    * }  
    * sd_no_na <- nop_na(sd)  
    * sd_no_na( c(NA, 1, 2, NA) )  
* There are three types of high-order functions  
	* Functionals take another function and return a vector - like map()  
    * Function fatories take a vector and create a function  
    * Function operators take functions and return functions - considered to be "adverbs"  
* Two of the most common adverbs in purrr are safely() and possibly()  
	* The safely() call returns a function that will return $result and $error when run; helpful for diagnosing issues with code rather than losing the information  
    * safe_log <- safely(log)  
    * safe_log("a")  # there will be $result of NULL and $error with the error code  
    * map( list(2, "a"), safely(log) )  
  
Using possibly():  
  
* The possibly() function is an adverb that returns either the value of the function OR the value specified in the otherwise element  
	* possible_sum <- possibly(sum, otherwise = "nop")  
    * possible_sum("a")  # result will be "nop"  
* Note that possibly() cannot be made to run a function; it will just return a pre-specified value  
  
Handling adverb results:  
  
* Can use transpose() to change the output (converts the list to inside out)  
	* Transpose turn a list of n elements a and b to a list of a and b, with each n elements  
* The compact() function will remove the NULL elements  
	* l <- list(1,2,3,"a")  
    * possible_log <- possibly(log, otherwise = NULL)  
    * map(l, possible_log) %>% compact()  
* Can use the httr package specifically for http requests  
	* httr::GET(url) will return the value from attempting to connect to url - 200 is good, 404 is unavailable, etc.  
  
Example code includes:  
```{r cache=TRUE}

# `$` is a function call, of a special type called 'infix operator', as they are put between two elements, and can be used without parenthesis.

# Launch Sys.time(), Sys.sleep(1), & Sys.time()
Sys.time()
Sys.sleep(1)
Sys.time()

# Launch nrow(iris), Sys.sleep(1), & nrow(iris)
data(iris)
nrow(iris)
Sys.sleep(1)
nrow(iris)

# Launch ls(), create an object, then rerun the ls() function
# ls()
# this <- 12
# ls()

# Create a plot of the iris dataset
plot(iris)


urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Create a safe version of read_lines()
safe_read <- safely(read_lines)

# Map it on the urls vector
res <- map(urls, safe_read)

# Set the name of the results to `urls`
named_res <-  set_names(res, urls)

# Extract only the "error" part of each sublist
map(named_res, "error")


# Code a function that discard() the NULL from safe_read()
safe_read_discard <- function(url){
    safe_read(url) %>%
        discard(is.null)
}

# Map this function on the url list
res <- map(urls, safe_read_discard)


# Create a possibly() version of read_lines()
possible_read <- possibly(read_lines, otherwise = 404)

# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)

# Paste each element of the list 
res_pasted <- map(res, paste, collapse=" ")

# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)


url_tester <- function(url_list){
    url_list %>%
        # Map a version of read_lines() that otherwise returns 404
        map( possibly(read_lines, otherwise = 404) ) %>%
        # Set the names of the result
        set_names( urls ) %>% 
        # paste() and collapse each element
        map(paste, collapse =" ") %>%
        # Remove the 404 
        discard(~.x==404) %>%
        names() # Will return the names of the good ones
}

# Try this function on the urls object
url_tester(urls)


url_tester <- function(url_list, type = c("result", "error")){
    res <- url_list %>%
        # Create a safely() version of read_lines() 
        map( safely(read_lines) ) %>%
        set_names( url_list ) %>%
        # Transpose into a list of $result and $error
        purrr::transpose() 
    # Complete this if statement
    if (type == "result") return( res$result ) 
    if (type == "error") return( res$error ) 
}

# Try this function on the urls object
url_tester(urls, type = "error") 


url_tester <- function(url_list){
    url_list %>%
        # Map a version of GET() that would otherwise return NULL 
        map( possibly(httr::GET, otherwise=NULL) ) %>%
        # Set the names of the result
        set_names( urls ) %>%
        # Remove the NULL
        compact() %>%
        # Extract all the "status_code" elements
        map("status_code")
}

# Try this function on the urls object
url_tester(urls)

```
  
  
  
***
  
Chapter 3 - Better Code with purrr  
  
Rationale for cleaner code:  
  
* Cleaner code is easier to debug (spot typos), easier to interpret, and easier to modify  
	* tidy_iris_lm <- compose( as_mapper(~ filter(.x, p.value < 0.05)), tidy, partial(lm, data=iris, na.action = na.fail) )  
    * list( Petal.Length ~ Petal.Width, Petal.Width ~ Sepal.Width, Sepal.Width ~ Sepal.Length ) %>% map(tidy_iris_lm)  
* Clean code characteristics  
	* Light - no unnecessary code  
    * Readable - less repition makes for easier reading (one piece of code for one task)  
    * Interpretable  
    * Maintainable  
* The compose() function is used to compose a function from two or more functions  
	* rounded_mean <- compose(round, mean)  
  
Building functions with compose() and negate():  
  
* There is a limitless amount of functions that can be included in compose()  
	* clean_aov <- compose(tidy, anova, lm)  
* Can use negate() to flip the predicate - TRUE becomes FALSE and FALSE becomes TRUE  
	* is_not_na <- negate(is.na)  
    * under_hundred <- as_mapper(~ mean(.x) < 100)  
    * not_under_hundred <- negate(under_hundred)  
    * map_lgl(98:102, under_hundred)  
    * map_lgl(98:102, not_under_hundred)  
* The "good" status return codes from GET() are in the low-200s  
	* good_status <- c(200, 201, 202, 203)  
    * status %in% good_status  
  
Prefilling functions:  
  
* The partial() allows for pre-filling a function  
	* mean_na_rm <- partial(mean, na.rm = TRUE)  
    * lm_iris <- partial(lm, data = iris)  
* Can also combine partial() and compose()  
	* rounded_mean <- compose( partial(round, digits = 2), partial(mean, na.rm = TRUE) )  
* Can use functions from rvest for web scraping  
	* read_html()  
    * html_nodes()  
    * html_text()  
    * html_attr()  
  
List columns:  
  
* A list column is part of a nested data frame - one or more of the data frame columns is itself a list (requires use of tibble rather than data.frame)  
	* df <- tibble( classic = c("a", "b","c"), list = list( c("a", "b","c"), c("a", "b","c", "d"), c("a", "b","c", "d", "e") ) )  
    * a_node <- partial(html_nodes, css = "a")  
    * href <- partial(html_attr, name = "href")  
    * get_links <- compose( href, a_node,  read_html )  
    * urls_df <- tibble( urls = c("https://thinkr.fr", "https://colinfay.me", "https://datacamp.com", "http://cran.r-project.org/") )  
    * urls_df %>% mutate(links = map(urls, get_links))  
* Can also unnest the data from the list columns  
	* urls_df %>% mutate(links = map(urls, get_links)) %>% unnest()  
* Can also nest() a standard data.frame  
	* iris_n <- iris %>% group_by(Species) %>% tidyr::nest()  
* Since the list column is a list, the purrr functions can be run on them  
	* iris_n %>% mutate(lm = map(data, ~ lm(Sepal.Length ~ Sepal.Width, data = .x)))  
    * summary_lm <- compose(summary, lm)  
    * iris %>% group_by(Species) %>% nest() %>% mutate(data = map(data, ~ summary_lm(Sepal.Length ~ Sepal.Width, data = .x)), data = map(data, "r.squared")) %>% unnest()  
  
Example code includes:  
```{r cache=TRUE}

urls <- c('https://thinkr.fr', 'https://colinfay.me', 'https://datacamp.com', 'http://cran.r-project.org/')

    
# Compose a status extractor 
status_extract <- purrr::compose(httr::status_code, httr::GET)

# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")

# Map it on the urls vector, return a vector of numbers
map_dbl(urls, status_extract)


# Negate the %in% function 
`%not_in%` <- negate(`%in%`)

# Complete the function
strict_code <- function(url){
    code <- status_extract(url)
    if (code %not_in% c(200:203)){
        return(NA)
    } else {
        return(code)
    } 
}


# Map the strict_code function on the urls vector
res <- map(urls, strict_code)

# Set the names of the results using the urls vector
res_named <- set_names(res, urls)

# Negate the is.na function
is_not_na <- negate(is.na)

# Run is_not_na on the results
is_not_na(res_named)


# Prefill html_nodes() with the css param set to h2
get_h2 <- partial(rvest::html_nodes, css="h2")

# Combine the html_text, get_h2 and read_html functions
get_content <- purrr::compose(rvest::html_text, get_h2, xml2::read_html)

# Map get_content to the urls list
res <- map(urls, get_content) %>%
    set_names(urls)

# Print the results to the console
res


# Create a partial version of html_nodes(), with the css param set to "a"
a_node <- partial(rvest::html_nodes, css="a")

# Create href(), a partial version of html_attr()
href <- partial(rvest::html_attr, name = "href")

# Combine href(), a_node(), and read_html()
get_links <- purrr::compose(href, a_node, xml2::read_html)

# Map get_links() to the urls list
res <- map(urls, get_links) %>%
    set_names(urls)


# Create a "links" columns, by mapping get_links() on urls
df2 <- tibble::tibble(urls=urls) %>%
    mutate(links = map(urls, get_links)) 

# Print df2 to see what it looks like
df2

# unnest() df2 to have a tidy dataframe
df2 %>%
    unnest()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Discovering the Dataset:  
  
* The dataset is available from https://github.com/ThinkR-open/datasets  
	* rstudioconf: a list of 5055 tweets  
    * length(rstudioconf)  
    * length(rstudioconf[[1]])  
    * purrr::vec_depth(rstudioconf)  
* JSON is a standard data format for the web, and typically consists of key-value pairs which are read as nested lists by R  
* Refresher of keep() and discard() usage  
	* keep(1:10, ~ .x < 5)  
    * discard(1:10, ~ .x < 5)  
  
Extracting Information from the Dataset:  
  
* Can manipulate functions for list cleaning using high-order functions - includes partial() and compose()  
	* sum_no_na <- partial(sum, na.rm = TRUE)  
    * map_dbl(airquality, sum_no_na)  
    * rounded_sum <- compose(round, sum_no_na)  
    * map_dbl(airquality, rounded_sum)  
* Can also clean lists using compact() to remove NULL and flatten() to remove one level from a nested list  
	* l <- list(NULL, 1, 2, 3, NULL)  
    * compact(l)  
    * my_list <- list( list(a = 1), list(b = 2) )  
    * flatten(my_list)  
  
Manipulating URL:  
  
* Can use the mapper functions to create a re-usable function  
	* mult <- as_mapper(~ .x * 2)  
* Can use str_detect inside the mapper function  
	* lyrics <- c("Is this the real life?", "Is this just fantasy?", "Caught in a landslide", "No escape from reality")  
    * stringr::str_detect(a, "life")  
  
Identifying Influencers:  
  
* Can use the map_at() function to run a function at a specific portion of the list  
	* my_list <- list( a = 1:10, b = 1:100, c = 12 )  
    * map_at(.x = my_list, .at = "b", .f = sum)  
* Can also use negate() to reverse the actio of a predicate  
	* not_character <- negate(is.character)  
    * my_list <- list( a = 1:10, b = "a", c = iris )  
    * map(my_list, not_character)  
  
Wrap up:  
  
* Lambda functions and reusable mappers  
	* map(1:5, ~ .x*10)  
    * ten_times <- as_mapper(~ .x * 10)  
    * map(1:5, ten_times)  
* Function manipulation using functionals (functions that take functions as inputs and return vectors)  
	* map() & friends  
    * keep() & discard()  
    * some() & every()  
* Function operators take functions and return (modified) functions  
	* safely() & possibly()  
    * partial()  
    * compose()  
    * negate()  
* Cleaner code is easier to read, understand, and maintain  
	* rounded_mean <- compose( partial(round, digits = 1), partial(mean, trim = 2, na.rm = TRUE) )  
    * map( list(airquality, mtcars), ~ map_dbl(.x, rounded_mean) )  
  
Example code includes:  
```{r cache=TRUE}

rstudioconfDF <- readRDS("./RInputFiles/#RStudioConf.RDS")
rstudioconfListA <- split(rstudioconfDF, seq(nrow(rstudioconfDF)))
rstudioconf <- lapply(rstudioconfListA, FUN=as.list)


# Print the first element of the list to the console 
rstudioconf[[1]]

# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")

# Extract the favorite count element of each non_rt sublist
fav_count <- map_dbl(non_rt, "favorite_count")

# Get the median of favorite_count for non_rt
median(fav_count)


# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
    map("user_id") %>%
    unique()

# Remove the RT, extract the user id, remove the duplicate
non_rt <- discard(rstudioconf, "is_retweet") %>%
    map("user_id") %>% 
    unique()

# Determine the total number of users
union(rt, non_rt) %>% 
    length()

# Determine the number of users who has just retweeted
setdiff(rt, non_rt) %>% 
    length()


# Prefill mean() with na.rm, and round() with digits = 1
mean_na_rm <- partial(mean, na.rm=TRUE)
round_one <- partial(round, digits=1)

# Compose a rounded_mean function
rounded_mean <- purrr::compose(round_one, mean_na_rm)

# Extract the non retweet  
non_rt <- discard(rstudioconf, "is_retweet")

# Extract "favorite_count", and pass it to rounded_mean()
map_dbl(non_rt, "favorite_count") %>%
    rounded_mean()


# Combine as_vector(), compact(), and flatten()
flatten_to_vector <- purrr::compose(as_vector, compact, flatten)

# Complete the fonction
extractor <- function(list, what = "mentions_screen_name"){
    map(list, what) %>%
        flatten_to_vector()
}

# Create six_most, with tail(), sort(), and table()
six_most <- purrr::compose(tail, sort, table)

# Run extractor() on rstudioconf
extractor(rstudioconf) %>% 
    six_most()


# Extract the "urls_url" elements, and flatten() the result
urls_clean <- map(rstudioconf, "urls_url") %>%
    lapply(FUN=function(x) { ifelse(is.na(x), list(NULL), x) }) %>%
    flatten()

# Remove the NULL
compact_urls <- compact(urls_clean)

# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x[1], "github"))

# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
    sum()


# Complete the function
ratio_pattern <- function(vec, pattern){
    n_pattern <- str_detect(vec, pattern) %>%
        sum()
    n_pattern / length(vec)
}

# Create flatten_and_compact()
flatten_and_compact <- purrr::compose(compact, flatten)

# Complete the pipe to get the ratio of URLs with "github"
map(rstudioconf, "urls_url") %>%
    lapply(FUN=function(x) { ifelse(is.na(x), list(NULL), x) }) %>%
    flatten_and_compact() %>% 
    ratio_pattern("github")


# Create mean_above, a mapper that tests if .x is over 3.3
mean_above <- as_mapper(~ . > 3.3)

# Prefil map_at() with "retweet_count", mean_above for above, 
# and mean_above negation for below
above <- partial(map_at, .at = "retweet_count", .f = mean_above )
below <- partial(map_at, .at = "retweet_count", .f = negate(mean_above) )

# Map above() and below() on non_rt, keep the "retweet_count"
# ab <- map(non_rt, above) %>% keep("retweet_count")
# bl <- map(non_rt, below) %>% keep("retweet_count")

rtCounts <- sapply(map(non_rt, "retweet_count"), FUN=function(x) { x })
ab <- rtCounts[rtCounts > 3.3]
bl <- rtCounts[rtCounts <= 3.3]

# Compare the size of both elements
length(ab)
length(bl)


# Get the max() of "retweet_count" 
max_rt <- map_dbl(non_rt, "retweet_count") %>% 
    max()

# Prefill map_at() with a mapper testing if .x equal max_rt
# max_rt_calc <- partial(map_at, .at = "retweet_count", .f = ~.x==max_rt )

idxMatch <- which(map(non_rt, "retweet_count") == max_rt)

# Map max_rt_calc on non_rt, keep the retweet_count & flatten
# res <- map(non_rt, max_rt_calc) %>% 
#     keep("retweet_count") %>% 
#     flatten()

# Print the "screen_name" and "text" of the result
res <- non_rt[[idxMatch]]
res$screen_name
res$text

```
  
  	
  
***
  
###_Longitudinal Analysis in R_  
  
Chapter 1 - Introduction to Longitudinal Data  
  
Introduction to Longitudinal Data:  
  
* Longitudinal data are data with 3+ measurements on the same unit (individual)  
	* Blood pressure every week for 6 weeks  
    * Match scores for a student from grade 3-8  
    * Extracurricular yes/no for each semester of high school  
* Dichotomous values are TRUE/FALSE while continuous have multiple values  
	* Multiple measurements allow for time-series modeling  
    * Two measurements allow for ANCOVA or t-tests  
* Example longitudinal data for rat weights from nlme  
	* library(nlme)  
    * head(BodyWeight, n = 10)  
    * count(BodyWeight, Rat)  
    * count(BodyWeight, Time)  
    * count(BodyWeight, Diet)  
  
Data Restructuring and Correlations:  
  
* Data are often stored in wide format, with each individual being a row  
* Analysis in R is generally best in long format, with time as one of the columns  
	* tidyr::gather() function for wide to long  
    * tidyr::spread() function for long to wide  
* Long to wide transforms can be helpful for calculating correlations  
	* BodyWeight %>% mutate(Time = paste0('Time_', Time)) %>% spread(Time, weight) %>% select(Rat, Diet, Time_1, Time_8, everything())  
* Wide to long transforms are more common  
	* gather(BodyWeight_wide, key = Time, value = weight, Time_1:Time_64)  # the colon operator : can be very helpful for these transforms  
* Can explore correlations over time and how the dependencies change over time  
* The corrr package can help with exploring correlations  
	* correlate(): to compute correlation matrix  
    * shave(): to remove extra information from matrix  
    * fashion(): to format correlation matrix  
    * BodyWeight %>% mutate(Time = paste0('T_', Time)) %>% spread(Time, weight) %>% select(Time_1, Time_8, Time_15:Time_64) %>% correlate() %>% shave(upper = FALSE) %>% fashion(decimals = 3)  
  
Descriptive Statistics:  
  
* Numeric summaries are often the most useful when broken down by time or other factors of interest  
* The group_by() and summarize() functions can be very helpful  
	* BodyWeight %>% group_by(Time) %>% summarize(mean_wgt = mean(weight, na.rm = TRUE), med_wgt = median(weight, na.rm = TRUE), min_wgt = min(weight, na.rm = TRUE), max_wgt = max(weight, na.rm = TRUE), sd_wgt = sd(weight, na.rm = TRUE), num_miss = sum(is.na(weight)), n = n())  
* Violin plots can be useful for assessing distributions at each point in time  
	* ggplot(BodyWeight, aes(x = factor(Time), y = weight)) + geom_violin(aes(fill = Diet)) + xlab("Time (in days)") + ylab("Weight") + theme_bw(base_size = 16)  
  
Example code includes:  
```{r}

data(calcium, package="lava")
str(calcium)


# Individuals with data at each visit number
count(calcium, visit)

# Individuals in each group
count(calcium, person)

# Individuals in each group
count(calcium, group)

# Individuals with each visit number in each group
count(calcium, visit, group)


# Restructure data into wide format for correlations
calcium_wide <- calcium %>%
    mutate(visit_char = paste0('visit_', visit)) %>%
    select(bmd, person, visit_char) %>%
    spread(visit_char, bmd)

# Calculate correlations across time
calcium_corr <- calcium_wide %>%
    select(-person) %>%
    corrr::correlate(method="pearson") %>%
    corrr::shave(upper=FALSE) %>%
    corrr::fashion(decimals=3)


# Convert data from wide to long format
calcium_wide %>% 
    gather(key="visit", value="bmd", -person)


# Calculate descriptive statistics
calcium %>%
    group_by(visit, group) %>%
    summarize(avg_bmd = mean(bmd, na.rm = TRUE), median_bmd = median(bmd, na.rm = TRUE),
              minimum_bmd = min(bmd, na.rm = TRUE), maximum_bmd = max(bmd, na.rm = TRUE),
              standev_bmd = sd(bmd, na.rm = TRUE), num_miss = sum(is.na(bmd)), n = n()
              )


# Visualize distributions of outcome over time
ggplot(calcium, aes(x = factor(visit), y = bmd)) + 
    geom_violin(aes(fill=group)) + 
    xlab("Visit Number") + 
    ylab("Bone Mineral Density") + 
    theme_bw(base_size = 16)

```
  
  
  
***
  
Chapter 2 - Modeling Continuous Longitudinal Outcomes  
  
Longitudinal Analysis for Continuous Outcomes:  
  
* Dependencies are introduced based on repeated observations of the same individual  
	* library(nlme)  
    * ggplot(BodyWeight, aes(x = Time, y = weight)) + geom_line(aes(group = Rat), alpha = 0.6) + geom_smooth(se = FALSE, size = 2) + theme_bw(base_size = 16) + xlab("Number of Days") + ylab("Weight (grams)")  # se is invalid due to the dependencies  
* The lmer stands for "Linear Mixed Effects Regression" - aka Hierarchical Linear Model, Linear Mixed Models, Multi-level Models, Growth Models  
	* lmer(outcome ~ fixed_effects + (random_effects | individual), data = data)  
    * outcome ~ fixed_effects + (random_effects | individual)  # outcome is the variable to be explained, fixed_effects are like a simple regression, random_effects represents deviation per individual, individual are the individual IDs  
* Example of model fitting with the rat data  
	* BodyWeight <- mutate(BodyWeight, Time = Time - 1)  
    * body_ri <- lmer(weight ~ 1 + Time + (1 | Rat), data = BodyWeight)  
    * summary(body_ri)  
* The fixed effects represent the average effects across all the rats  
  
Addition of Random Slope Terms:  
  
* Random slopes are commonly used along with random intercepts - each individual can have their own random slopes  
	* weight ~ 1 + Time + (1 + Time | Rat)  # fixed stays the same (though terms may be added)  
    * BodyWeight <- mutate(BodyWeight, Time = Time - 1)  
    * body_rs <- lmer(weight ~ 1 + Time + (1 + Time | Rat), data = BodyWeight)  
* Sometimes a question of whether random intercepts or random slopes fit the data better - can use anova()  
	* Akaike Information Criterion (AIC): smaller is better (recommended when the true model is unknown)  
    * Bayesian Information Criterion (BIC): smaller is better (imposes larger penalties for adding predictors)  
    * Log Likelihood: value minimized during estimation: smaller is better  
* Nested models can be compared using anova() - subset or simplification of another model  
	* body_ri <- lmer(weight ~ 1 + Time + (1 | Rat), data = BodyWeight)  
    * body_rs <- lmer(weight ~ 1 + Time + (1 + Time | Rat), data = BodyWeight)  
    * anova(body_rs, body_ri)  
  
Visualize and Interpret Output:  
  
* Visualizing model results is an important component of presenting and interpreting the data  
	* body_agg <- BodyWeight %>% mutate(pred_values = predict(body_ri))  
    * ggplot(body_agg, aes(x = Time, y = pred_values)) + geom_line(aes(group = Rats), alpha = 0.6) + theme_bw(base_size = 16) + xlab("Number of Days") + ylab("Model Implied Values")  
* Random effects help control for dependencies due to repeated measurements  
	* Custom function (next slide) will help explore model implied correlations  
* Example of creating and applying a custom function for correlations ("compound symmetry")  
	* corr_structure <- function(object, num_timepoints, intercept_only = TRUE) {  
    *     variance <- VarCorr(object)  
    *     if(intercept_only) {  
    *         random_matrix <- as.matrix(object@pp$X[1:num_timepoints, 1])  
    *         var_cor <- random_matrix %*% variance[[1]][1] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)  
    *     } else {  
    *     random_matrix <- as.matrix(object@pp$X[1:num_timepoints, ])  
    *     var_cor <- random_matrix %*% variance[[1]][1:2, 1:2] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)  
    *     }  
    *     Matrix::cov2cor(var_cor)  
    * }  
    * body_ri <- lmer(weight ~ 1 + Time + (1 | Rat), data = BodyWeight)  
    * corr_structure(body_ri, 11) %>% round(3)  
* Can visually show correlation dependencies  
	* Ggally::ggcorr(data = NULL, cor_matrix = corr_structure(body_ri, 11), label = TRUE, label_round = 3, label_size = 3.5, palette = 'Set2', nbreaks = 5)  
  
Example code includes:  
```{r}

# Visualize trajectories
ggplot(calcium, aes(x = visit, y = bmd)) +
    geom_line(aes(group = person), alpha = .4) +
    geom_smooth(se = FALSE, size = 2) +
    theme_bw(base_size = 14) +
    xlab('Visit Number') +
    ylab('Bone Mineral Density (g/cm^2)')


# Unconditional model
uncond_model <- lme4::lmer(bmd ~ 1 + visit + (1 | person), data = calcium)

# Show model output
summary(uncond_model)


# Alter the visit variable to start at 0
calcium <- calcium %>%
    mutate(visit_0 = visit - 1)

# Fit random intercept model with new time variable
uncond_model_0 <- lme4::lmer(bmd ~ 1 + visit_0 + (1 | person), data = calcium)
summary(uncond_model_0)


# Random slope
uncond_model_rs <- lme4::lmer(bmd ~ 1 + visit_0 + (1 + visit_0 | person), data = calcium)
summary(uncond_model_rs)


# Compare random slopes and random intercept only models
anova(uncond_model_rs, uncond_model_0)


# Create predicted values for random intercept only model
calcium_vis <- calcium %>%
    mutate(pred_values_ri = predict(uncond_model_0))
  
# Visualize random intercepts
ggplot(calcium_vis, aes(x = visit_0, y = pred_values_ri)) + 
    geom_line(size = 1, color = 'gray70', aes(group = person)) + 
    theme_bw() + 
    xlab("Visit Number") +
    ylab("Model Predicted Bone Mineral Density (g/cm^2)")


# Create predicted values for random intercept and slope model
calcium_vis <- calcium %>%
    mutate(pred_values_rs = predict(uncond_model_rs))

# Visualize random intercepts and slopes
ggplot(calcium_vis, aes(x = visit_0, y = pred_values_rs)) + 
    geom_line(size = 1, color = 'gray70', aes(group = person)) + 
    theme_bw() + 
    xlab("Visit Number") +
    ylab("Model Predicted Bone Mineral Density (g/cm^2)")


corr_structure <- function(object, num_timepoints, intercept_only = TRUE) {
    variance <- lme4::VarCorr(object)
    if(intercept_only) {
        random_matrix <- as.matrix(object@pp$X[1:num_timepoints, 1])
        var_cor <- random_matrix %*% variance[[1]][1] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)
    } else {
        random_matrix <- as.matrix(object@pp$X[1:num_timepoints, ])
        var_cor <- random_matrix %*% variance[[1]][1:2, 1:2] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)
    }
    Matrix::cov2cor(var_cor)
}

# Random intercept and slope model
random_slope <- lme4::lmer(bmd ~ 1 + visit_0 + (1 + visit_0 | person), data = calcium)

# Generate model implied correlation matrix
mod_corr <- corr_structure(random_slope, num_timepoints = 5, intercept_only = FALSE)
round(mod_corr, 3)

# Create visualization for correlation structure
GGally::ggcorr(data = NULL, cor_matrix = mod_corr, midpoint = NULL, 
               limits = NULL, label = TRUE, label_round = 3, label_size = 5, 
               nbreaks = 100, palette = 'PuBuGn'
               )

```
  
  
  
***
  
Chapter 3 - Add Fixed Predictor Variables  
  
Adding Predictors:  
  
* Predictors will build up the fixed effects to help explain the outcomes - reduces the variance of the intercepts/slopes  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + (1 + Time | Rat), data = BodyWeight)  
* Visualizing predictors can be especially helpful  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + (1 + Time | Rat), data = BodyWeight)  
    * bodyweight_agg <- BodyWeight %>% mutate(pred_values = predict(body_weight, re.form = NA)) %>% group_by(Time, Diet) %>% summarize(mean_diet_pred = mean(pred_values))  
    * ggplot(bodyweight_agg, aes(x = Time, y = mean_diet_pred, color = Diet)) + geom_point(data = BodyWeight, aes(x = Time, y = weight)) + geom_line(size = 2) + ylab("Body Weight") + xlab("Time (in days)") + theme_bw(base_size = 16)  
  
Adding Predictors - Interactions:  
  
* Can add predictor variables to the model  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight)  
* Need to check assumptions to ensure that model results are trustworthy  
	* Ensure model results are trustworthy  
    * Explore the distribution of residuals and random effects  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight)  
    * BodyWeight <- BodyWeight %>% mutate(model_residuals = residuals(body_weight))
ggplot(BodyWeight, aes(x = model_residuals)) + geom_density(aes(color = diet_f), size = 1.25) + xlab("Residuals") + theme_bw(base_size = 14) + scale_color_brewer(palette = "Set2")  
* Can use the ranef() function to extract random effects from the model  
	* body_weight <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight)  
    * random_effects <- ranef(body_weight)$Rat %>% mutate(id = 1:n()) %>% gather("variable", "value", -id) 
* Can also run Q-Q plots to asses normality  
	* ggplot(random_effects, aes(sample = value)) + geom_qq() + geom_qq_line() + facet_wrap(~variable, scales = 'free_y') + theme_bw(base_size = 14)  
  
Model Comparisons and Eplained Variance:  
  
* Model comparisons may be made using AIC, AICc (corrected AIC), or BIC  
	* AICc converges to AIC with large samples  
* Example of model comparisons  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight_rs <- lmer(weight ~ 1 + Time + (1 + Time | Rat), data = BodyWeight, REML = FALSE)  # REML=TRUE would be for only changes in random effects; REML=FALSE is needed when changing fixed effects (random effects should stay the same)  
    * body_weight_diet <- lmer(weight ~ 1 + Time + diet_f + (1 + Time | Rat), data = BodyWeight, REML = FALSE)  
    * body_weight_diet_int <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight, REML = FALSE)  
* Can calculate AICc from AICcmodavg  
	* AICcmodavg::aictab(list(body_weight_rs, body_weight_diet, body_weight_diet_int), modnames = c('random slope', 'diet intercept', 'diet interaction'))  
* Explained variance can also be helpful in model assessment - larger values are better  
	* MuMIn::r.squaredGLMM(body_weight_rs)  
    * MuMIn::r.squaredGLMM(body_weight_diet) 
    * MuMIn::r.squaredGLMM(body_weight_diet_int)  
  
Example code includes:  
```{r}

# Add a categorical predictor
bmd_group <- lme4::lmer(bmd ~ 1 + visit_0 + group + (1 + visit_0 | person), data = calcium)
summary(bmd_group)

# Add a continuous predictor
bmd_group_age <- lme4::lmer(bmd ~ 1 + visit_0 + group + age + (1 + visit_0 | person), data = calcium)
summary(bmd_group_age)


# Calculate aggregate trends
calcium_agg <- calcium %>%
    mutate(pred_values = predict(bmd_group_age, re.form = NA)) %>%
    group_by(visit_0, group) %>%
    summarize(pred_group = mean(pred_values))

# Visualize the model results
ggplot(calcium_agg, aes(x = visit_0, y = pred_group, color = group)) +
    geom_point(data = calcium, aes(x = visit_0, y = bmd, color = group)) +
    geom_line(size = 1.25) +
    xlab('Visit Number') +
    ylab('Model Predicted Bone Mineral Density (g/cm^2)')


# Add an interaction
bmd_group_age_int  <- lme4::lmer(bmd ~ 1 + visit_0 + age + group + visit_0:group + visit_0:age + (1 + visit_0 | person), data = calcium)
summary(bmd_group_age_int)


# Add residuals original data
calcium <- calcium %>%
    mutate(model_residuals = residuals(bmd_group_age_int))

# Visualize residuals
ggplot(calcium, aes(x = model_residuals)) + 
    geom_density(aes(color = group), size = 1.25) + 
    theme_bw(base_size = 14) + 
    xlab("Model Residuals")


# Extract random effects
random_effects <- lme4::ranef(bmd_group_age_int)$person %>%
    mutate(id = 1:n()) %>%
    gather("variable", "value", -id)

# Visualize random effects
ggplot(random_effects, aes(sample = value)) + 
    geom_qq() + 
    geom_qq_line() + 
    facet_wrap(~variable, scales = 'free_y') + 
    theme_bw(base_size = 14)


# Compare random slope, model with group variable, model with group and age, and model with interactions.
AICcmodavg::aictab(list(uncond_model_rs, bmd_group, bmd_group_age, bmd_group_age_int),
                   modnames = c('random slope', 'group intercept', 'group and age', 
                                'group and age interaction'
                                )
                   )


# Compute explained variance for random slope only model
MuMIn::r.squaredGLMM(uncond_model_rs)

# Compute explained variance for group and age predicting intercepts model
MuMIn::r.squaredGLMM(bmd_group_age)

# Compute explained variance for interaction model
MuMIn::r.squaredGLMM(bmd_group_age_int)

```
  
  
  
***
  
Chapter 4 - Modeling Longitudinal Dichotomous Outcomes  
  
Exploring and Modeling Dichotomous Outcomes:  
  
* Binary outcomes include yes/no, presence/absence, etc.  
	* library(HSAUR2)  
    * head(toenail, n = 10)  
* Generalized Linear Mixed Models (GLMM) explore the log-odds of success  
	* toenail <- toenail %>% mutate(outcome_dich = ifelse(outcome == "none or mild", 1, 0), visit_0 = visit - 1)  
    * toenail %>% group_by(visit_0) %>% summarise(prop_outcome = mean(outcome_dich), num = n())  
* Two modificiations to lmer to run GLMM  
	* use glmer instead of lmer  
    * specify family = binomial argument  
    * toe_output <- glmer(outcome_dich ~ 1 + visit_0 + treatment + ( 1 | patientID), data = toenail, family = binomial)  
    * summary(toe_output)  
  
Generalized Estimating Functions (GEE):  
  
* GEE is another way to estimate dichotomous data that are not continuous  
* The geepack package contains data for running GEE  
	* toenail <- toenail %>% mutate(outcome_dich = ifelse(outcome == "none or mild", 1, 0), visit_0 = visit - 1)  
    * gee_toe <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE)  
    * summary(gee_toe)  
* An optional argument, corstr is used to control the working correlation matrix  
	* Accounts for the dependency due to repeated measures  
    * The default is independence  
    * gee_toe <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, corstr = 'exchangeable', scale.fix = TRUE)  
    * summary(gee_toe)  
* Additional correlation matrices can be specified  
	* corstr = "ar1"  # specified correlation for each time lag  
    * corstr = "unstructured"  # unique correlation for each time lag  
  
Model Selection:  
  
* The working correlation matrix can significantly impact study results  
* The QIC statistic is the quasi-likelihood under the independence model criterion - lower (smaller) is better  
	* toenail <- toenail %>% mutate(outcome_dich = ifelse(outcome == "none or mild", 1, 0), visit_0 = visit - 1)  
    * gee_toe <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE)  
    * MuMIn::QIC(gee_toe)  
* Can evaluate model criterion  
	* gee_ind <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE)  
    * gee_exch <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE, corstr = 'exchangeable')  
    * gee_ar1 <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE, corstr = 'ar1')  
    * MuMIn::QIC(gee_ind, gee_exch, gee_ar1)  
* Can also use the aictab() from AICcmodavg  
	* toe_baseline <- glmer(outcome_dich ~ 1 + visit_0 + ( 1 | patientID), data = toenail, family = binomial)  
    * toe_output <- glmer(outcome_dich ~ 1 + visit_0 + treatment + ( 1 | patientID), data = toenail, family = binomial)  
    * AICcmodavg::aictab(list(toe_baseline, toe_output), c("no treatment", "treatement"))  
  
Interpreting and Visualizing Model Results:  
  
* Can visualize the GLMM outputs  
	* toe_output <- glmer(outcome_dich ~ 1 + visit_0 + treatment + ( 1 | patientID), data = toenail, family = binomial)  
    * toenail <- toenail %>% mutate(pred_values = predict(toe_output))  
    * ggplot(toenail, aes(x = visit_0, y = pred_values)) + geom_line(aes(group = patientID), linetype = 2) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Predicted Values")  
* Probabilities are often more intuitive and can be found using type="response"  
	* toenail <- toenail %>% mutate(pred_values = predict(toe_output, type = "response"))  
    * ggplot(toenail, aes(x = visit_0, y = pred_values)) + geom_line(aes(group = patientID), linetype = 2) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Prob of none or mild separation")  
* Can use predict() using GEE similar to GLMM  
	* gee_toe <- geeglm(outcome_dich ~ 1 + visit_0 + treatment, data = toenail, id = patientID, family = binomial, corstr = 'exchangeable', scale.fix = TRUE)  
    * toenail_gee <- toenail %>% mutate(pred_gee = predict(gee_toe, type = "response"))  
    * ggplot(toenail_gee, aes(x = visit_0, y = pred_gee)) + geom_line(aes(color = treatment)) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Probability of none or mild separation")  
* Compare GLMM and GEE  
	* toenail_glmm <- toenail %>% group_by(visit_0, treatment) %>% summarise(prob = mean(pred_values))  
    * toenail_gee <- toenail_gee %>% group_by(visit_0, treatment) %>% summarise(prob = mean(pred_values))  
    * toenail_agg = bind_rows( mutate(toenail_glmm, model = "GLMM"), mutate(toenail_gee, model = "GEE") )  
    * ggplot(toenail_agg, aes(x = visit_0, y = prob)) + geom_line(aes(color = treatment, linetype = model), size = 1) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Prob of non or mild separation")  
  
Example code includes:  
```{r}

ids <- rep(c(1:82, 85:87, 90), each=12)
months <- rep(0:11, times=length(unique(ids)))
symps <- c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, NA, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, NA, NA, NA, NA, NA, NA, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0)
symps <- c(symps, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, NA, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, 0, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, NA, NA, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)
age <- rep(c('Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20'), each=12)
sex <- rep(c('Male', 'Female', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male'), each=12)
madras <- data.frame(id=ids, symptom=symps, month=months, age=age, sex=sex)


# Explore the first few rows of the madras data
str(madras)
head(madras)

# Descriptives about symptom prevalence over time
summary_stats <- madras %>%
    group_by(month) %>%
    summarize(num_symptom = sum(symptom, na.rm = TRUE), 
              num = n(),
              prop_symptom = mean(symptom, na.rm = TRUE)
              )
  
# Print out summary statistics
summary_stats


# Build models
uncond_ri <- lme4::glmer(symptom ~ 1 + month + (1|id), data = madras, family = binomial)
summary(uncond_ri)


# Add in covariates based on trend plot
cond_model <- lme4::glmer(symptom ~ 1 + month + sex + age + sex:age + sex:month + (1 | id), data = madras, family = binomial)

# Generate summary of output
summary(cond_model)


# Fit a GEE model with intercept and time variable
gee_mod <- geepack::geeglm(symptom ~ 1 + month, id = id, family=binomial, data = madras, scale.fix = TRUE)

# Extract model results
summary(gee_mod)

# Fit a GEE model with an ar(1) working correlation matrix
gee_mod_ar1 <- geepack::geeglm(symptom ~ 1 + month, id = id, family = binomial, data = madras, corstr="ar1", scale.fix = TRUE)

# Fit a GEE model with an unstructured working correlation matrix
gee_mod_un <- geepack::geeglm(symptom ~ 1 + month, id = id, family = binomial, data = madras, corstr="unstructured", scale.fix = TRUE)

# Extract model results
summary(gee_mod_ar1)
summary(gee_mod_un)


# Fit a GEE model with an ar(1) working correlation matrix
gee_mod_ar1 <- geepack::geeglm(symptom ~ 1 + month + age + sex + age:sex + month:age + month:sex, id = id, data = madras, family = binomial, corstr = 'ar1', scale.fix = TRUE)

# Extract model results
summary(gee_mod_ar1)


# Fit a GEE model with an exchangeable working correlation matrix
gee_mod_exch <- geepack::geeglm(symptom ~ 1 + month + age + sex, data = madras, id = id, family = binomial, scale.fix = TRUE, corstr = 'exchangeable')


glmm_age <- lme4::glmer(symptom ~ 1 + month + age + month:age + (1 | id), data=madras, family=binomial)

# Generate model implied probabilities
madras <- madras %>%
    na.omit() %>%
    mutate(prob=predict(glmm_age, type="response"))

# Visualize subject specific probabilities
ggplot(madras, aes(x=month, y=prob)) + 
    geom_line(aes(group=id)) + 
    theme_bw() +
    xlab("Month") + 
    ylab("Probabilities")


# Compute the average trajectories
glmm_prob <- madras %>%
    group_by(month, age) %>%
    summarize(prob = mean(prob))

# Visualize average trajectories
ggplot(glmm_prob, aes(x = month, y = prob)) + 
    geom_line(aes(color = age)) + 
    theme_bw() +
    xlab("Month") + 
    ylab("Probability")


gee_age_sex <- geepack::geeglm(formula = symptom ~ 1 + month + age + month:age + sex, 
                               family = binomial, data = madras, id = id, corstr = "ar1", scale.fix = TRUE
                               )

# Compute model implied probabilites using gee_age_sex
madras_gee <- madras %>% 
    select(month, symptom, age, sex) %>% 
    na.omit() %>%
    mutate(prob = predict(gee_age_sex, type = "response"))

# Visualize trajectories
ggplot(madras_gee, aes(x = month, y = prob)) + 
    geom_line(aes(color = age)) + 
    facet_wrap(~ sex) + 
    theme_bw() +
    xlab("Month") +
    ylab("Probability")


# Fit a GLMM mdoel
glmm_age_sex <- lme4::glmer(symptom ~ 1 + month + age + sex + (1 | id), data = madras, family = binomial)

# Generate model implied probabilites
madras <- madras %>% mutate(prob_gee = predict(gee_age_sex, type = "response"),
                            prob_glmm = predict(glmm_age_sex, type = "response")
                            )

# Compute average GEE probabilities
madras_gee <- madras %>%
    group_by(month, age, sex) %>%
    summarize(prob = mean(prob_gee))

# Compute average GLMM probabilities
madras_glmm <- madras %>%
    group_by(month, age, sex) %>%
    summarize(prob = mean(prob_glmm))

# Create combined data object
madras_agg = bind_rows(
    mutate(madras_glmm, model = "GLMM"), 
    mutate(madras_gee, model = "GEE") 
)


# Visualize differences in trajectories across model types
ggplot(madras_agg, aes(x = month, y = prob)) + 
    geom_line(aes(color = sex, linetype = model)) + 
    facet_wrap(~ age) + 
    theme_bw() + 
    xlab("Month") + 
    ylab("Probability")

```
  
  
  
***
  
###_Data Manipulation in R with data.table_  
  
Chapter 1 - Introduction to data.table  
  
Introduction:  
  
* A data.table is a data.frame with extended capabilities  
	* DT[i, j, by]  # which rows, what to do (operates on columns), grouped by what  
* The data.table is fast and parallelizes operations where possible, and is feature-rich  
* There are at least three ways to create a data.table  
	* data.table()  
    * as.data.table()  
    * fread()  
    * x <- data.table(id = 1:2, name = c("a", "b"))  
* All functions that can be used on data.frame are available on data.table  
	* Note that data.table does not convert characters to factors, does not set row names, and uses colon to separate row number from the row data  
  
Filtering Rows in a data.table:  
  
* The data.table syntax consists of DT[i, j, by]  # which rows, what to do (operates on columns), grouped by what  
	* batrips[3:4]  
    * batrips[3:4, ]  # same as above  
    * batrips[-(1:5)]  # exclude rows 1:5  
    * batrips[!(1:5)]  # exclude rows 1:5  
* The data.table has some special symbols that help with calculations  
	* .N is an integer value that contains the number of rows in the data.table (Particularly useful alternative to nrow(x) in i)  
    * batrips[.N]  
    * ans <- batrips[1:(.N-10)]  
    * batrips[subscription_type == "Subscriber"]  # if this is a data.table  
    * batrips[batrips$subscription_type == "Subscriber", ]  # required if it is only a data.frame  
    * batrips[start_terminal == 58 & end_terminal != 65]  # if this is a data.table  
* The data.table automatically creates a key of the columns used to subset the data (future operations on that column will be much faster)  
	* dt <- data.table(x = sample(10000, 10e6, TRUE), y = sample(letters, 1e6, TRUE))  
    * indices(dt)  
    * system.time(dt[x == 900])  
    * indices(dt)  
    * system.time(dt[x == 900])  
  
Helpers for filtering:  
  
* The %like% operator allows for matching a pattern in a column  
	* batrips[start_station %like% "^San Francisco"]  # for data.table, with the ^ being from regex for "starts with"  
    * batrips[grepl("^San Francisco", start_station)]  # equivalent for data.frame  
* The %between% operator allows for finding items in the closed interval (a, b)  
	* batrips[duration %between% c(2000, 3000)]  # using data.table  
    * batrips[duration >= 2000 & duration <= 3000]  # equivalent for data.frame  
* The %chin% operator allows for %in% for character vectors  
	* batrips[start_station %chin% c("Japantown", "Mezes Park", "MLK Library")]  # for data.table, runs MUCH faster  
    * batrips[start_station %in% c("Japantown", "Mezes Park", "MLK Library")]  # alternate, slower syntax  
  
Example code includes:  
```{r}

# Load data.table
library(data.table)

# Create the data.table X 
X <- data.table(id = c("a", "b", "c"), value = c(0.5, 1.0, 1.5))

# View X
X


data(batrips, package="bikeshare14")
batrips <- as.data.table(batrips)

# Get number of columns in batrips
col_number <- ncol(batrips)

# Print the first 8 rows
head(batrips, 8)

# Print the last 8 rows
tail(batrips, 8)

# Print the structure of batrips
str(batrips)


# Filter third row
row_3 <- batrips[3]
row_3

# Filter rows 10 through 20
rows_10_20 <- batrips[10:20]
rows_10_20

# Filter the 1st, 6th and 10th rows
rows_1_6_10 <- batrips[c(1, 6, 10)]
rows_1_6_10


# Select all rows except the first two
not_first_two <- batrips[-c(1:2)]
not_first_two

# Select all rows except 1 through 5 and 10 through 15
exclude_some <- batrips[-c(1:5, 10:15)]
exclude_some

# Select all rows except the first and last
not_first_last <- batrips[-c(1, .N)]
not_first_last


# Filter all rows where start_station is "MLK Library"
trips_mlk <- batrips[start_station == "MLK Library"]
trips_mlk

# Filter all rows where start_station is "MLK Library" AND duration > 1600
trips_mlk_1600 <- batrips[start_station == "MLK Library" & duration > 1600]
trips_mlk_1600

# Filter all rows where `subscription_type` is not `"Subscriber"`
customers <- batrips[subscription_type != "Subscriber"]
customers

# Filter all rows where start_station is "Ryland Park" AND subscription_type is not "Customer"
ryland_park_subscribers <- batrips[start_station=="Ryland Park" & subscription_type != "Customer"]
ryland_park_subscribers


# Filter all rows where end_station contains "Market"
any_markets <- batrips[end_station %like% "Market"]
any_markets

# Filter all rows where end_station ends with "Market" 
end_markets <- batrips[end_station %like% "Market$"]
end_markets


# Filter all rows where trip_id is 588841, 139560, or 139562
filter_trip_ids <- batrips[trip_id %in% c(588841, 139560, 139562)]
filter_trip_ids


# Filter all rows where duration is between [5000, 6000]
duration_5k_6k <- batrips[duration %between% c(5000, 6000)]
duration_5k_6k

# Filter all rows with specific start stations
two_stations <- batrips[start_station %chin% c("San Francisco City Hall", "Embarcadero at Sansome")]
two_stations

```
  
  
  
***
  
Chapter 2 - Selecting and Computing on Columns  
  
Selecting columns from a data.table:  
  
* The data.table syntax consists of DT[i, j, by]  # which rows, what to do (operates on columns), grouped by what  
* The j argument can be passed a vector of column names or column numbers  
	* ans <- batrips[, "trip_id"]  # will still be a data.table (no need for drop=FALSE)  
    * ans <- batrips[, c(2, 4)]  # not recommended, since column numbers may change over time  
    * ans <- batrips[, -c("start_date", "end_date", "end_station")]  # excludes these columns  
* The data.table way allows for computations on columns as part of the j column  
	* ans <- batrips[, list(trip_id, dur = duration)]  # duration will be renamed to dur while selecting  
    * ans <- batrips[, list(trip_id)]  # will return a data.table since it is inside ()  
* The .() is an alias to list()  
	* ans <- batrips[, .(trip_id, duration)]  # select columns trip_id, duration  
    * ans <- batrips[, list(trip_id, duration)]  # same as above  
* To get the return of a vector (like drop=TRUE in data.frame), pass the column name as an unquoted variable, not inside list() and not inside .()  
	* batrips[, duration]  # will return a vector  
  
Computing on columns the data.table way:  
  
* Can perform computations directly on columns in j  
	* ans <- batrips[, mean(duration)]  # will return a single value, the mean of duration (will be a vector)  
* Can run computations for only a subset of rows by combining I and j  
	* batrips[start_station == "Japantown", mean(duration)]  # row subsetting happens PRIOR to mean() calculation  
* Can use .N as part of the j argument  
	* batrips[start_station == "Japantown", .N]  # number of trips starting from Japantown  
  
Advanced computations in j:  
  
* Can compute in j and return a data.table  
	* ans <- batrips[, .(trip_id, dur = duration)]  
    * batrips[, .(mn_dur = mean(duration), med_dur = median(duration))]  
* Can combine with I so that only a subset of rows have the calculations in j selected on them  
	* batrips[start_station == "Japantown", .(mn_dur = mean(duration), med_dur = median(duration))]  
  
Example code includes:  
```{r}

# Select bike_id and trip_id using a character vector
df_way <- batrips[, c("bike_id", "trip_id")]
df_way

# Select start_station and end_station cols without a character vector
dt_way <- batrips[, .(start_station, end_station)]
dt_way


# You can also drop or deselect columns by prepending the character vector of column names with the - or ! Operators
# For e.g., dt[, -c("col1", "col2")] or dt[, !c("col1", "col2")] would both return all columns except col1 and col2

# Deselect start_terminal and end_terminal columns
drop_terminal_cols <- batrips[, -c("start_terminal", "end_terminal")]
drop_terminal_cols


# Calculate median duration using the j argument
median_duration <- batrips[, mean(duration)]
median_duration

# Get median duration after filtering
median_duration_filter <- batrips[end_station == "Market at 10th" & subscription_type == "Subscriber", median(duration)]
median_duration_filter


# Compute duration of all trips
trip_duration <- batrips[, difftime(end_date, start_date, units="min")]
head(trip_duration)


# Have the column mean_durn
mean_duration <- batrips[, .(mean_durn=mean(duration))]
mean_duration

# Get the min and max duration values
min_max_duration <- batrips[, .(min(duration), max(duration))]
min_max_duration

# Calculate the number of unique values
other_stats <- batrips[, .(mean_duration=mean(duration), last_ride=max(end_date))]
other_stats


duration_stats <- batrips[start_station == "Townsend at 7th" & duration < 500, 
                          .(min_dur=min(duration), max_dur=max(duration))]
duration_stats

# Plot the histogram of duration based on conditions
batrips[start_station == "Townsend at 7th" & duration < 500, hist(duration)]

```
  
  
  
***
  
Chapter 3 - Groupwise Operations  
  
Computations by Groups:  
  
* The by argument allows for separate computations by each group number  
	* ans <- batrips[, .N, by = "start_station"]  
    * ans <- batrips[, .N, by = .(start_station)]  # can use .() or list() instead of the character vector  
* Can rename columns on the fly, including calculations and group by  
	* ans <- batrips[, .(no_trips = .N), by = .(start = start_station)]  
* Can calculate grouping variables on the fly as part of the by expression  
	* ans <- batrips[ , .N, by = .(start_station, mon = month(start_date))]  
  
Chaining data.table expressions:  
  
* Chaining expressions is the process of running multiple operations to get a single output  
	* batrips[duration > 360][order(duration)][1:3]  
    * batrips[, .(mn_dur = mean(duration)), by = "start_station"][order(mn_dur)][1:3]  
* The uniqueN() is a helper function that returns the number of unique objects  
	* ans <- batrips[, uniqueN(bike_id), by = month(start_date)]  
  
Computations in j using .SD:  
  
* The .SD means "subset of data" which simplifies calculations  
	* x[, print(.SD), by = id]  # each of the groups is a data.table, which is the power of the .SD helper  
    * x[, .SD[1], by = id]  # returns the first row for each group  
* Can use .SDcols to select just a subset of the columns for return  
	* batrips[, .SD[1], by = start_station]  
    * batrips[, .SD[1], by = start_station, .SDcols = c("trip_id", "duration")]  
    * batrips[, .SD[1], by = start_station, .SDcols = - c("trip_id", "duration")]  
  
Example code includes:  
```{r}

# Compute the mean duration for every start_station
mean_start_stn <- batrips[, .(mean_duration=mean(duration)), by = "start_station"]
mean_start_stn


# Compute the mean duration for every start and end station
mean_station <- batrips[, .(mean_duration=mean(duration)), by = .(start_station, end_station)]
mean_station

# Compute the mean duration grouped by start_station and month
mean_start_station <- batrips[, .(mean_duration=mean(duration)), by=.(start_station, month(start_date))]
mean_start_station


# Compute mean of duration and total trips grouped by start and end stations
aggregate_mean_trips <- batrips[, .(mean_duration=mean(duration), total_trips=.N), by=.(start_station, end_station)]
aggregate_mean_trips

# Compute min and max duration grouped by start station, end station, and month
aggregate_min_max <- batrips[, .(min_duration=min(duration), max_duration=max(duration)), by=.(start_station, end_station, month(start_date))]
aggregate_min_max


# Arrange the total trips grouped by start_station and end_station in decreasing order
trips_dec <- batrips[, .N, by = .(start_station, end_station)][order(-N)]
trips_dec


# Top five most popular destinations
top_5 <- batrips[, .N, by = .(end_station)][order(-N)][1:5]
top_5


# Compute most popular end station for every start station
popular_end_station <- trips_dec[, .(end_station = head(end_station, 1)), by = .(start_station)]
popular_end_station


# Find the first and last ride for each start_station
first_last <- batrips[order(start_date), 
                      .(start_date = c(head(start_date, 1), tail(start_date, 1))), 
                      by = .(start_station)]
first_last


relevant_cols <- c("start_station", "end_station", "start_date", "end_date", "duration")

# Find the row corresponding to the shortest trip per month
shortest <- batrips[, .SD[which.min(duration)], by = month(start_date), .SDcols = relevant_cols]
shortest


# Find the total number of unique start stations and zip codes per month
unique_station_month <- batrips[, lapply(.SD, FUN=uniqueN), 
                                by = month(start_date), 
                                .SDcols = c("start_station", "zip_code")]
unique_station_month

```
  
  
  
***
  
Chapter 4 - Reference Semantics  
  
Adding and Updating Columns by Reference:  
  
* Can add, delete, and update columns in place  
	* df <- data.frame(x = 1:5, y = 6:10)  
    * df <- data.frame(a = 1:3, b = 4:6, c = 7:9, d = 10:12)  
    * df[1:2] <- lapply(df[1:2], function(x) ifelse(x%%2, x, NA))  
* The data.table internals are such that neither the whole frame nor entire columns are deep copied simply to update a few values  
	* data.table updates columns in place, i.e., by reference  
    * This means, you don't need the assign the result back to a variable  
    * No copy of any column is made while their values are changed  
    * data.table uses a new operator := to add/update/delete columns by reference  
* Two ways to use the := operator  
	* batrips[, ("is_dur_gt_1hour", "week_day") := list(duration > 3600, wday(start_date)]  
    * batrips[, is_dur_gt_1hour := duration > 3600]  # can skip the parentheses and quotes if there is just a single variable for processing  
    * batrips[, `:=`(is_dur_gt_1hour = NULL, start_station = toupper(start_station))]  # note that `:=` is the function that is being called and that NULL means "delete the column"  
  
Grouped Aggregations:  
  
* Can run grouped aggregations by combining := and by  
	* batrips[, n_zip_code := .N, by = zip_code]  # nothing will be printed to the console, though number of columns increased by 1  
    * zip_1000 <- batrips[n_zip_code > 1000][, n_zip_code := NULL]  # can delete the intermediate columns by setting them to NULL  
  
Advanced Aggregations:  
  
* Can add multiple columns by reference, each within a specified by group  
	* batrips[, `:=`(end_dur_first = duration[1], end_dur_last = duration[.N]), by = end_station]  
    * batrips[, c("end_dur_first", "end_dur_last") := list(duration[1], duration[.N]), by = end_station]  
* Can use ifelse statements, and the j argument can have multi-line expressions with LHS=RHS wrapped inside {}  
	* batrips[, trip_category := { med_dur = median(duration, na.rm = TRUE) if (med_dur < 600) "short" else if (med_dur >= 600 & med_dur <= 1800) "medium" else "long" }, by = .(start_station, end_station)]  
* Can also apply user-defined functions to achieve the same tasks  
	* bin_median_duration <- function(dur) {  
    *     med_dur <- median(dur, na.rm = TRUE)  
    *     if (med_dur < 600) "short"  
    * else if (med_dur >= 600 & med_dur <= 1800) "medium"  
    * else "long"  
    * }  
    * batrips[, trip_category := bin_median_duration(duration), by = .(start_station, end_station)]  
* Can combine I, j, by all for a single operation  
	* batrips[duration > 500, min_dur_gt_500 := min(duration), by = .(start_station, end_station)]  
  
Example code includes:  
```{r}

data(batrips, package="bikeshare14")
batrips <- as.data.table(batrips)

batrips_new = batrips
makeNA <- sample(1:nrow(batrips), round(0.05*nrow(batrips)), replace=FALSE)
batrips_new[makeNA, "duration"] <- NA


# Add a new column, duration_hour
batrips[, duration_hour := duration/3600]


# Print untidy
# untidy[1:2]

# Fix spelling in the second row of start_station
# untidy[2, start_station:="San Francisco City Hall"]

# Replace negative duration values with NA
# untidy[duration < 0, duration:=NA]


# Add a new column equal to total trips for every start station
batrips[, trips_N:=.N, by = start_station]

# Add new column for every start_station and end_station
batrips[, duration_mean:=mean(duration), by = .(start_station, end_station)]


# Calculate the mean duration for each month
batrips_new[, mean_dur:=mean(duration, na.rm=TRUE), by = month(start_date)]

# Replace NA values in duration with the mean value of duration for that month
batrips_new[, mean_dur := mean(duration, na.rm = TRUE), by = month(start_date)][is.na(duration), duration:=round(mean_dur,0)]

# Delete the mean_dur column by reference
batrips_new[, mean_dur := mean(duration, na.rm = TRUE), by = month(start_date)][is.na(duration), duration := mean_dur][, mean_dur:=NULL]


# Add columns using the LHS := RHS form
batrips[, c("mean_duration", "median_duration"):=.(mean(duration), as.integer(round(median(duration), 0))), by=start_station]

# Add columns using the functional form
batrips[, `:=`(mean_duration=mean(duration), median_duration=as.integer(round(median(duration), 0))), by = start_station]

# Add the mean_duration column
batrips[duration > 600, mean_duration:=mean(duration), by=.(start_station, end_station)]

```
  
  
  
***
  
Chapter 5 - Importing and Exporting Data  
  
Fast data reading with fread():  
  
* The fread() function is a fast flat-file reader since it imports files in parallel (default is to use all available threads)  
	* Can import local files, files from the web, and strings  
    * Intelligent defaults - colClasses, sep, nrows etc.  
    * Note: Dates and Datetimnes are read as character columns but can be converted later with the excellent fasttime or anytime packages  
    * DT1 <- fread("https://bit.ly/2RkBXhV")  
    * DT2 <- fread("data.csv")  
    * DT3 <- fread("a,b\n1,2\n3,4")  
    * DT4 <- fread("1,2\n3,4")  
* The nrows and skip arguments can be helpful for finer control  
	* fread("a,b\n1,2\n3,4", nrows = 1)  # nrows=1 means read 1 row in addition to the header  
    * str <- "# Metadata\nTimestamp: 2018-05-01 19:44:28 GMT\na,b\n1,2\n3,4"  
    * fread(str, skip = 2)  # skips the first two lines entirely before attempting to parse the file  
* Can also pass a string argument to skip  
	* str <- "# Metadata\nTimestamp: 2018-05-01 19:44:28 GMT\na,b\n1,2\n3,4"  
    * fread(str, skip = "a,b")  # everything before the line "a,b" will be skipped  
* The select and drop arguments allow for control of columns to read  
	* str <- "a,b,c\n1,2,x\n3,4,y"  
    * fread(str, select = c("a", "c"))  
    * fread(str, drop = "b")  # same as above for this chunk of data  
    * str <- "1,2,x\n3,4,y"  
    * fread(str, select = c(1, 3))  
    * fread(str, drop = 2)  # same as above for this chunk of data  
  
Advanced file reading:  
  
* By default, R can only read integers less than 2**31 - 1  
	* Large integers are automatically read in as integer64 type, provided by the bit64 package  
* Can override the colClasses() guessing that is otherwise automatic in fread()  
	* str <- "x1,x2,x3,x4,x5\n1,2,1.5,true,cc\n3,4,2.5,false,ff"  
    * ans <- fread(str, colClasses = c(x5 = "factor"))  
    * ans <- fread(str, colClasses = c("integer", "integer", "numeric", "logical", "factor"))  
    * str <- "x1,x2,x3,x4,x5,x6\n1,2,1.5,2.5,aa,bb\n3,4,5.5,6.5,cc,dd"  
    * ans <- fread(str, colClasses = list(numeric = 1:4, factor = c("x5", "x6"))) str(ans)  # specifies that columns 1:4 are numeric and that "x5" and "x6" will be factors  
* The fill argument can be used to direct fread() to fill missing values  
	* str <- "1,2\n3,4,a\n5,6\n7,8,b"  
    * fread(str)  # throws a warning since fill=FALSE is the default  
    * fread(str, fill = TRUE)  # will assume empty strings for the missing fields  
* Can override the defaults for what to treat as NA  
	* str <- "x,y,z\n1,###,3\n2,4,###\n#N/A,7,9"  
    * ans <- fread(str, na.strings = c("###", "#N/A"))  
  
Fast data writing with fwrite():  
  
* The fwrite() function is a fast parallel flat file writer  
	* dt <- data.table(id = c("x", "y", "z"), val = list(1:2, 3:4, 5:6))  
    * fwrite(dt, "fwrite.csv")  # the list is flattened by using the secondary separtor (default is |)  
    * fread("fwrite.csv") 
* Dates and datetimes are saved in ISO format for further clarity  
	* fwrite() provides three additional ways of writing date and datetime format - ISO, squash and epoch  
    * Encourages the use of ISO standards with ISO as default  
    * now <- Sys.time()  
    * dt <- data.table(date = as.IDate(now), time = as.ITime(now), datetime = now)  
    * fwrite(dt, "datetime.csv", dateTimeAs = "ISO")  
    * fread("datetime.csv")  
* Additional date and time formats available - squash  
	* squash writes yyyy-mm-dd hh:mm:ss as yyyymmddhhmmss, for example.  
    * Read in as integer. Very useful to extract month, year etc by simply using modulo arithmetic. e.g., 20160912 %/% 10000 = 2016  
    * Also handles milliseconds (ms) resolution.  
    * POSIXct type (17 digits with ms resolution) is automatically read in as integer64 by fread  
* Additional date and time formats available - epoch  
	* epoch counts the number of days (for dates) or seconds (for time and datetime) since relevant epoch  
    * Relevant epoch is 1970-01-01, 00:00:00 and 1970-01-01T00:00:00Z for date, time and datetime, respectively  
    * fwrite(dt, "datetime.csv", dateTimeAs = "epoch")  
    * fread("datetime.csv")  
  
Example code includes:  
```{r cache=TRUE}

data(batrips, package="bikeshare14")
batrips <- as.data.table(batrips)
readr::write_csv(batrips, "./RInputFiles/_batrips.csv")


# Use read.csv() to import batrips
system.time(read.csv("./RInputFiles/_batrips.csv"))

# Use fread() to import batrips
system.time(fread("./RInputFiles/_batrips.csv"))


cat('id,"name",val
29192,"Robert Whitaker", 200
49301 ,"Elisa Waters",190
', file="./RInputFiles/_sample.csv")


# Import using read.csv()
csv_file <- read.csv("./RInputFiles/_sample.csv", fill = NA, quote = "", stringsAsFactors = FALSE, strip.white = TRUE, header = TRUE)
csv_file

# Import using fread()
csv_file <- fread("./RInputFiles/_sample.csv")
csv_file


cat("id,name,val
29192,Robert Whitaker, 200
49301 ,Elisa Waters,190  
34456 , Karla Schmidt,458
", file="./RInputFiles/_sample.csv")


# Select "id" and "val" columns
select_columns <- fread("./RInputFiles/_sample.csv", select=c("id", "val"))
select_columns

# Drop the "val" column
drop_column <- fread("./RInputFiles/_sample.csv", drop=c("val"))
drop_column


cat('id,"name",val
29192,"Robert Whitaker", 200
49301 , Elisa Waters,190  
34456 , Karla Schmidt,458  

END-OF-DATA
METADATA
attr;value
date;"2018-01-01"
data;"cash payment" 
', file="./RInputFiles/_sample.csv")


# Import the file
entire_file <- fread("./RInputFiles/_sample.csv")
entire_file

# Import the file while avoiding the warning
only_data <- fread("./RInputFiles/_sample.csv", nrows=3)
only_data

# Import only the metadata
only_metadata <- fread("./RInputFiles/_sample.csv", skip="attr;value")
only_metadata


cat('id,name,val
9002019291929192,Robert Whitaker, 200
9200129401349301 ,Elisa Waters,190  
9200149429834456 , Karla Schmidt,458
', file="./RInputFiles/_sample.csv")


# Import the file using fread 
fread_import <- fread("./RInputFiles/_sample.csv")

# Import the file using read.csv 
base_import <- read.csv("./RInputFiles/_sample.csv")

# Check the class of id column
class(fread_import$id)
class(base_import$id)


cat('c1,c2,c3,c3.1,c5,n1,n2,n3,n4,n5
aa,bb,cc,dd,ee,1,2,3,4,5
ff,gg,hh,ii,jj,6,7,8,9,10
', file="./RInputFiles/_sample.csv")


# Import using read.csv with defaults
base_r_defaults <- read.csv("./RInputFiles/_sample.csv")
str(base_r_defaults)

# Import using read.csv
base_r <- read.csv("./RInputFiles/_sample.csv", 
                   colClasses = c(rep("factor", 4), "character", "integer", rep("numeric", 4))
                   )
str(base_r)

# Import using fread
import_fread <- fread("./RInputFiles/_sample.csv", colClasses = list(factor=1:4, numeric=7:10))
str(import_fread)


cat('id,name,val
9002019291929192,Robert Whitaker,
9200129401349301 ,Elisa Waters,190  
9200149429834456 , Karla Schmidt
', file="./RInputFiles/_sample.csv")


# Import the file and note the warning message
incorrect <- fread("./RInputFiles/_sample.csv")
incorrect

# Import the file correctly
correct <- fread("./RInputFiles/_sample.csv", fill=TRUE)
correct


# Import the file using na.strings
missing_values <- fread("./RInputFiles/_sample.csv", na.strings="##")
missing_values


dt <- data.table(id=c(29192L, 49301L, 34456L), 
                 name=c("Robert, Whitaker", "Elisa, Waters", "Karla, Schmidt"), 
                 vals=list(c(144, 48, 32), c(22, 289), 458)
                 )
dt


# Write dt to fwrite.txt
fwrite(dt, "./RInputFiles/_fwrite.txt")

# Import the file using readLines()
readLines("./RInputFiles/_fwrite.txt")

# Import the file using fread()
fread("./RInputFiles/_fwrite.txt")


batrips_dates <- batrips[1:5, c("start_date", "end_date")]
batrips_dates


# Write batrips_dates to file using "ISO" format
fwrite(batrips_dates, "./RInputFiles/_iso.txt", dateTimeAs="ISO")

# Import the file back
iso <- fread("./RInputFiles/_iso.txt")
iso

# Write batrips_dates to file using "squash" format
fwrite(batrips_dates, "./RInputFiles/_squash.txt", dateTimeAs="squash")

# Import the file back
squash <- fread("./RInputFiles/_squash.txt")
squash

# Write batrips_dates to file using "epoch" format
fwrite(batrips_dates, "./RInputFiles/_epoch.txt", dateTimeAs="epoch")

# Import the file back
epoch <- fread("./RInputFiles/_epoch.txt")
epoch


# Use write.table() to write batrips
system.time(write.table(batrips, "./RInputFiles/_base-r.txt"))

# Use fwrite() to write batrips
system.time(fwrite(batrips, "./RInputFiles/_data-table.txt"))

```
  
  
  
***
  
###_Probability Puzzles in R_  
  
Chapter 1 - Introduction and Classic Puzzles  
  
Introduction:  
  
* Chapter 1 - Classic Problems  
* Chapter 2 - Dice Puzzles  
* Chapter 3 - Web Puzzles  
* Chapter 4 - Poker Games  
* Built-in combinatorics functions will help  
	* factorial(3)  
    * choose(5,3)  
* Can run simulations using sample(), rbinom(), replicate(), for, while, set.seed() and the like  
	* for(i in 1:10){ sum(sample(x = c(1,2,3,4,5,6), size = 2, replace = TRUE)) }  
    * rolls <- rep(NA, 10)  
    * for(i in 1:10){ rolls[i] <- sum(sample(x = c(1,2,3,4,5,6), size = 2, replace = TRUE)) }  
  
Birthday Problem:  
  
* Suppose that there are n people in the room, and we want to know the probability that 2+ people share a birthday  
	* Ignore February 29th  
    * Birthdays are uniformly distributed across the remaining 365 days  
    * Each individual in the room is independent  
* Can run a simulation-based approach to the problem  
* There is a built-in function pbirthday() that calculates the exact probability  
	* pbirthday(10)  
    * room_sizes <- c(1:10)  
    * match_probs <- sapply(room_sizes, pbirthday)  
    * plot(match_probs ~ room_size)  
  
Monty Hall:  
  
* Three door problem - one with a prize, and two with nothing  
	* Contestant picks a door  
    * Host opens a door with nothing  
    * Contestant has the choice to switch or not switch  
* Can manage this problem in R with reverse indexing  
	* doors <- 1:3  
    * reveal <- doors[-c(1,2)]  # assumes contestant chose 1 and actual prize is in 2  
    * reveal <- sample(x = doors[-1], size = 1)  # assumes contestant chose 1 and actual prize is in 1  
  
Example code includes:  
```{r}

# Set seed to 1
set.seed(1)


# Write a function to roll k dice
roll_dice <- function(k){
    all_rolls <- sample(c(1,2,3,4,5,6), k, replace = TRUE)
    final_answer <- sum(all_rolls)
    return(final_answer)
}

# Run the function to roll five dice
roll_dice(5)


# Initialize a vector to store the output
output <- rep(NA, 10000)

# Loop for 10000 iterations
for(i in 1:10000){
    # Fill in the output vector with the result from rolling two dice
    output[i] <- roll_dice(2)
}


set.seed(1)
n <- 50
match <- 0

# Simulate 10000 rooms and check for matches in each room
for(i in 1:10000){
    birthdays <- sample(1:365, n, replace = TRUE)
    if(length(unique(birthdays)) < n){ match <- match + 1 } 
}

# Calculate the estimated probability of a match and print it
p_match <- match/10000
print(p_match)


# Calculate the probability of a match for a room size of 50
pbirthday(50)


# Define the vector of sample sizes
room_sizes <- 1:50

# Run the pbirthday function within sapply on the vector of sample sizes
match_probs <- sapply(room_sizes, FUN=pbirthday)

# Create the plot
plot(match_probs ~ room_sizes)


set.seed(1)
doors <- c(1,2,3)

# Randomly select one of the doors to have the prize
prize <- sample(x = doors, size = 1)
initial_choice <- 1

# Check if the initial choice equals the prize
if(prize == initial_choice){
    print("The initial choice was correct!")
}

print(prize)


set.seed(1)
doors <- c(1,2,3)

# Define counter
win_count <- 0

# Run 10000 iterations of the game
for(i in 1:10000){
    prize <- sample(x = doors, size = 1)
    initial_choice <- 1
    if(initial_choice == prize){ win_count <- win_count + 1 }
}

# Print the answer
print(win_count / 10000)


reveal_door <- function(doors, prize, initial_choice){
    if(prize == initial_choice){
        # Sample at random from the two remaining doors
        reveal <- sample(doors[-prize], 1)
    } else {
        reveal <- doors[-c(prize, initial_choice)]
    }  
}

set.seed(1)
prize <- sample(doors,1)
initial_choice <- 1

# Use the reveal_door function to do the reveal
reveal <- reveal_door(doors, prize, initial_choice)

# Switch to the remaining door
final_choice <- doors[-c(initial_choice, reveal)]
print(final_choice)

# Check whether the final choice equals the prize
if(final_choice==prize){
    print("The final choice is correct!")
}

# Initialize the win counter
win_count <- 0

for(i in 1:10000){
    prize <- sample(doors,1)
    initial_choice <- 1
    reveal <- reveal_door(doors, prize, initial_choice)
    final_choice <- doors[-c(initial_choice, reveal)]
    if(final_choice == prize){
        # Increment the win counter
        win_count <- win_count + 1
    }
}

# Print the estimated probability of winning
print(win_count / 10000)

```
  
  
  
***
  
Chapter 2 - Games with Dice  
  
Yahtzee:  
  
* Yahtzee is based on rolling 5 dice, and then optionally re-rolling 0-5 of the dice for two more turns  
* When rolling 3 dice, there are 6**3 possible permutations of the dice  
* Can use factorial functions for combinatorics  
	* factorial(3)  
* Probability of rolling exactly {3, 4, 5} or {2, 3, 4} - probabilities can be added since they are MECE  
    * factorial(3)/6^3 + factorial(3)/6^3  
* The probability of rolling 3-dice Yahtzee  
	* 1/6^3 + 1/6^3 + 1/6^3 + 1/6^3 + 1/6^3 + 1/6^3  
* Can also use the choose() function for combinatorics in R  
	* choose(3,2)  
* Number of ways to roll 5 of one denomiation and 5 of the other denomination  
	* n_denom <- factorial(6) / factorial(4)  
    * n_groupings <- choose(10,5) * choose(5,5)  
    * n_total <- n_denom * n_groupings  
  
Settlers of Catan:  
  
* Typical games lasts ~60 rolls, and each spot is labelled 2-12  
* Can simulate dice rolls  
	* roll_dice <- function(k){  
    *     all_rolls <- sample(c(1,2,3,4,5,6), k, replace = TRUE)  
    *     final_answer <- sum(all_rolls)  
    * }  
    * replicate(10, roll_dice(2))  
    * table(rolls)  
    * sum(rolls == 3)  
  
Craps:  
  
* Pass line bet made prior to the start of a roll  
	* 7 or 11 on the first roll wins  
    * 2, 3, or 12 on the first roll loses  
    * All others become the point, which then needs to be rolled before the next 7 to be a winner  
* There is no set number of rolls, so replicate() and for() od not work as per the previous examples  
	* while(roll != 6){ roll <- roll_dice(1); print(roll) }  
  
Example code includes:  
```{r}

# Calculate the size of the sample space
s_space <- 6**5

# Calculate the probability of a Yahtzee
p_yahtzee <- 6 / s_space

# Print the answer
print(p_yahtzee)


s_space <- 6^5

# Calculate the probabilities
p_12345 <- factorial(5) / s_space
p_23456 <- factorial(5) / s_space
p_large_straight <- p_12345 + p_23456

# Print the large straight probability
print(p_large_straight)


s_space <- 6^5

# Calculate the number of denominations possible
n_denom <- factorial(6) / factorial(4)

# Calculate the number of ways to form the groups
n_groupings <- choose(5, 3)

# Calculate the total number of full houses
n_full_house <- n_denom * n_groupings

# Calculate and print the answer
print(n_full_house / s_space)


set.seed(1)

# Simulate one game (60 rolls) and store the result
rolls <- replicate(60, roll_dice(2))

# Display the result
table(rolls)


set.seed(1)
counter <- 0

for(i in 1:10000){
    # Roll two dice 60 times
    rolls <- replicate(60, roll_dice(2))
    # Check whether 2 or 12 was rolled more than twice
    if(sum(rolls==2) > 2 | sum(rolls==12) > 2) { counter <- counter + 1 }
}

# Print the answer
print(counter/10000)


roll_after_point <- function(point){
    new_roll <- 0
    # Roll until either a 7 or the point is rolled 
    while( (new_roll != point) & (new_roll != 7) ){
        new_roll <- roll_dice(2)
        if(new_roll == 7){ won <- FALSE }
        # Check whether the new roll gives a win
        if(new_roll == point){ won <- TRUE }
    }
    return(won)
}


evaluate_first_roll <- function(roll){
    # Check whether the first roll gives an immediate win
    if(roll %in% c(7, 11)){ won <- TRUE }
    # Check whether the first roll gives an immediate loss
    if(roll %in% c(2, 3, 12)){ won <- FALSE }
    if(roll %in% c(4,5,6,8,9,10) ){
        # Roll until the point or a 7 is rolled and store the win/lose outcome
        won <- roll_after_point(roll)
    }
    return(won)
}


set.seed(1)
won <- rep(NA, 10000)

for(i in 1:10000){
    # Shooter's first roll
    roll <- roll_dice(2)
    # Determine result and store it
    won[i] <- evaluate_first_roll(roll)
}

sum(won)/10000

```
  
  
  
***
  
Chapter 3 - Inspired from the Web  
  
Factoring a Quadratic:  
  
* Given random integers a, b, c, what is the probability that the quadratic will factor?  
	* The definition of "factorable" is that the solution is rational (can be expressed as the ratio of two integers; is not imaginary)  
* The solution is rational only when the discriminant (b**2 - 4*a*c) is a perfect square  
	* sqrt_dscr <- sqrt(3^2 - 4*1*2)  
    * sqrt_dscr == round(sqrt_dscr)  # basically, is it an integer; note that is.integer() will fail since the integer is reprsented as a float, making this FALSE  
  
Four Digit iPhone Passcodes:  
  
* Smudge marks on an iPhone can leave clues as to the passcode  
* Research suggests that using a single repeated digit can ENHANCE the security of the passcode  
* The identical function checks whether the full vector is equivalent, as opposed to the element-wise ==  
	* identical(set1, set2)  
  
Sign Error Cancellations:  
  
* Suppose that the possibility of a sign flip is p < 0.5 and suppose that an even number of sign flips gets a correct answer, is the student guaranteed greater than 50% likelihood of getting the right answer?  
	* sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)  
    * rbinom(n, size, prob)  
    * result <- sapply(X = c(0.25, 0.75, 0.1, 0.9), FUN = rbinom, n = 1, size = 1)  
  
Example code includes:  
```{r}

is_factorable <- function(a,b,c){
    # Check whether solutions are imaginary
    if(b^2 - 4*a*c < 0){ 
        return(FALSE)
        # Designate when the next section should run
    } else {
        sqrt_discriminant <- sqrt(b^2 - 4*a*c) 
        # return TRUE if quadratic is factorable
        return(sqrt_discriminant == round(sqrt_discriminant))
    }
}

counter <- 0

# Nested for loop
for(a in 1:100){
    for(b in 1:100){
        for(c in 1:100){
            # Check whether factorable
            if(is_factorable(a, b, c)){ counter <- counter + 1 }
        }
    }
}

print(counter / 100^3)


counter <- 0

# Store known values 
values <- c(3, 4, 5, 9)
passcode = values

for(i in 1:10000){
    # Create the guess
    guess <- sample(values, replace=FALSE)
    # Check condition 
    if(identical(passcode, guess)){ counter <- counter + 1 }
}

print(counter/10000)


counter <- 0
# Store known values
unique_values <- c(2, 4, 7)
passcode = c(unique_values, unique_values[1])

for(i in 1:10000){
    # Pick repeated value
    all_values <- c(unique_values, sample(unique_values, 1))
    # Make guess
    guess <- sample(all_values, replace=FALSE)
    if(identical(passcode, guess)){ counter <- counter + 1 }
}

print(counter / 10000)


set.seed(1)

# Run 10000 iterations, 0.1 sign switch probability
switch_a <- rbinom(10000, 3, prob=0.1)

# Calculate probability of correct answer
mean(switch_a/2==round(switch_a/2))

# Run 10000 iterations, 0.45 sign switch probability
switch_b <- rbinom(10000, 3, prob=0.45)

# Calculate probability of correct answer
mean(switch_b/2==round(switch_b/2))


set.seed(1)
counter <- 0

for(i in 1:10000){
    # Simulate switches
    each_switch <- sapply(c(0.49, 0.1), FUN=rbinom, size=1, n=1)
    # Simulate switches
    num_switches <- sum(each_switch)
    # Check solution
    if(num_switches/2 == round(num_switches/2)){ counter <- counter + 1 }
}

print(counter/10000)

```
  
  
  
***
  
Chapter 4 - Poker  
  
Texas Hold'em:  
  
* Texas Hold'em is a variant of poker where each player has 2 personal cards and 5 communal cards, with rounds of betting to start and then as the communal cards are shown  
* Can look at probabilities when there are 2 cards left to go or one card left to go  
* Can look at varying numbers of "outs" (winning cards)  
	* outs <- c(0,1,2,3)  
    * p_lose <- choose(10-outs,2) / choose(10,2)  
    * p_win <- 1 - p_lose  
* Can further calculate the expected value of the possible outcomes (sum of profit*probability across all possible outcomes)  
  
Consecutive Cashes:  
  
* WSOP Main Event held in Las Vegas - survival time is all that matters (not number of chips left at any given time)  
	* Roughly the top 10% of players "cash" (win prize money)  
    * Ronnie Bardah cashed five straight years - how likely is that?  
* Simplifying assumptions include  
	* 6000 players, same by year, all of same talent  
    * cash_year1 <- sample(players, 4)  
    * cash_year2 <- sample(players, 4)  
    * intersect(cash_year1, cash_year2)  
* Challenge for this lesson is to get the intersection of multiple years; can use a matrix for this  
	* cashes <- replicate(3, sample(players, 4))  
    * in_all_three <- Reduce(intersect, list(cashes[, 1], cashes[, 2], cashes[, 3]))  # input to Reduce() must be a list  
    * length(in_all_three)  
  
von Neumann Model of Poker:  
  
* Application of game theory to poker - model stipulates each hand is a random draw from Uniform(0, 1)  
	* runif(n, min = 0, max = 1)  
    * runif(n = 1)  
* Model assumes that runif() are compared iff both players wagered; otherwise, the wagering player beats the non-wagering player  
* Can use the ifelse() function for modeling  
  
Wrap Up:  
  
* Combinatorics - choose(), factorial()  
* Simulation - sample(), replicate(), runif()  
* Can continue with more complex combinatorics puzzles  
* Can transfer directly to Monte Carlo and Markov chains  
  
Example code includes:  
```{r}

p_win <- 8 / 46
curr_pot <- 50
bet <- 10

# Define vector of probabilities
probs <- c(p_win, 1-p_win)

# Define vector of values
values <- c(curr_pot, -bet)

# Calculate expected value
sum(probs*values)


outs <- c(0:25)

# Calculate probability of not winning
p_no_outs <- choose(47-outs, 2) /choose(47, 2)

# Calculate probability of winning
p_win <- 1 - p_no_outs

print(p_win)


players <- c(1:60)
count <- 0

for(i in 1:10000){
    cash_year1 <- sample(players, 6)
    cash_year2 <- sample(players, 6)
    # Find those who cashed both years
    cash_both <- intersect(cash_year1, cash_year2)
    # Check whether anyone cashed both years
    if(length(cash_both) > 0){ count <- count + 1 }
}

print(count/10000)


check_for_five <- function(cashed){
    # Find intersection of five years
    all_five <- Reduce(intersect, list(cashed[, 1], cashed[, 2], cashed[, 3], cashed[, 4], cashed[, 5]))
    # Check intersection
    if(length(all_five) > 0){ 
        return(TRUE)
        # Specify when to return FALSE
    } else { return(FALSE) }
}


players <- c(1:6000)
count <- 0

for(i in 1:10000){
    # Create matrix of cashing players
    cashes <- replicate(5, sample(players, 600, replace=FALSE))
    # Check for five time winners
    if(check_for_five(cashes)){ count <- count + 1 }
}

print(count/10000)


# Generate values for both players
A <- runif(1)
B <- runif(1)

# Check winner
if(A > B){
    print("Player A wins")
} else {
    print("Player B wins")
}

print(A)
print(B)


one_round <- function(bet_cutoff){
    a <- runif(n = 1)
    b <- runif(n = 1)
    # Fill in betting condition
    if(b > bet_cutoff){
        # Return result of bet
        return(ifelse(b > a, 1, -1))
    } else {
        return(0)
    }  
}


b_win <- rep(NA, 10000)

for(i in 1:10000){
    # Run one and store result
    b_win[i] <- one_round(0.5)
}

# Print expected value
mean(b_win)

```
  
  
  
***
  
###_Highcharter for Finance in R_  
  
Chapter 1 - Introduction to Highcharter  
  
Introduction:  
  
* Highcharts build strong visualizations - requires a license for professional use  
* The highcharter package on CRAN wraps the highcharts package  
	* Also called an htmlwidget  
    * Extends on package like ggplot by enabling zooming, hovering, etc.  
* Can make an OHLC chart (assuming appropriate underlying data) simply  
	* hchart(spy_prices)  
    * hchart(spy_prices$open, type = "line", color = "purple")  
  
Two highcharter paradigms:  
  
* Can use either highchart() to draw a blank canvas or hchart(object) to plot the object while creating the highchart()  
	* highchart(type = "stock") %>% hc_add_series(spy_prices)  # spy_prices is formatted as xts data  
    * hchart(spy_prices)  # will guess the best type based on the data (if not specified)  
    * hchart(spy_prices_tibble, hcaes(x = date, y = open), type = "line")  # hcaes() is the hchart() equivalent of aes()  
  
Data going forward:  
  
* The xts data holds the full stock data for the portfolio, indexed by date (all xts objects must have an index)  
	* etf_prices_xts  
    * etf_prices_xts$SPY  # will include the index (date)  
    * index(etf_prices_xts)  # will show the index  
* Can also store data as a wide tibble  
	* etf_prices_wide_tibble  # no index, date is an explicit column  
    * etf_prices_wide_tibble$SPY  # date is not shown since it is not selected  
* Can also store data as a tidy (long) tibble  
	* etf_prices_tidy_tibble  
    * etf_tidy_tibble_prices %>% filter(symbol == "SPY")  # grab just the SPY data  
  
Example code includes:  
```{r eval=FALSE}

load("./RInputFiles/stock_prices_xts.RData")
load("./RInputFiles/stock_tidy_tibble_prices.RData")
load("./RInputFiles/stock_wide_tibble_returns.RData")

str(stock_prices_xts)
str(stock_tidy_tibble_prices)
str(stock_wide_tibble_returns)


load("./RInputFiles/commodities_returns.RData")
load("./RInputFiles/commodities-returns-tidy.RData")
load("./RInputFiles/commodities-xts.RData")

str(commodities_returns)
str(commodities_returns_tidy)
str(commodities_xts)


# Load the highcharter package
library(highcharter)
library(zoo)


quantmod::getSymbols("XLK", src="yahoo", from="2012-12-31", to="2017-12-31")
fix_vars <- function(x) { tolower(str_split(x, fixed("."))[[1]][2]) }
xlk_prices <- XLK
names(xlk_prices) <- sapply(names(XLK), FUN=fix_vars) %>% unname()


# Build a candlestick chart
hchart(xlk_prices, type = "candlestick")

# Build a ohlc chart
hchart(xlk_prices, type = "ohlc")

# Build a line chart
hchart(xlk_prices$close, type = "line")


# Show the dates
head(index(xlk_prices))

# Use the base function and set the correct chart type
highchart(type = "stock") %>%
    hc_add_series(xlk_prices)


xlk_prices_tibble <- fortify.zoo(xlk_prices) %>%
    as_tibble() %>%
    rename("date"="Index")
head(xlk_prices_tibble)


# Create a line chart of the 'close' prices
hchart(xlk_prices_tibble, hcaes(x = date, y = close), type = "line")

# Create a line chart of the open prices
hchart(xlk_prices_tibble, hcaes(x = date, y = open), type = "line")


# Inspect the first rows of the xts data object
head(stock_prices_xts)

# Extract and show the GOOG column from the xts object
head(stock_prices_xts$GOOG)

# Display the date index from the xts object
head(index(stock_prices_xts))

# Extract and show the DIS column from the xts object
head(stock_prices_xts$DIS)


stock_wide_tibble_prices <- stock_tidy_tibble_prices %>%
    tidyr::spread(symbol, price)

# Inspect the first rows of the wide tibble object
head(stock_wide_tibble_prices)

# Extract and show the GOOG column from the wide tibble data
head(stock_wide_tibble_prices$GOOG)

# Display the date information from the wide tibble data
head(stock_wide_tibble_prices$date)

# Extract and show the DIS column from the wide tibble data
head(stock_wide_tibble_prices$DIS)


# Inspect the first rows of the tidy tibble object
head(stock_tidy_tibble_prices)

# Extract and show the GOOG price data from the tidy tibble data
stock_tidy_tibble_prices %>%
    filter(symbol == "GOOG") %>%
    head()

# Display the date information from the tidy tibble
head(stock_tidy_tibble_prices$date)

# Extract and show the DIS price data from the tidy tibble data
stock_tidy_tibble_prices %>%
    filter(symbol == "DIS") %>%
    head()

```
  
  
  
***
  
Chapter 2 - Highcharter for xts data  
  
Chart the price of one stock in an xts object:  
  
* Dataset has five ETF prices in a single object  
	* etf_prices_xts  
* Can create charts for just a single series in the ETF data  
	* highchart(type = "stock")  # create a blank chart canvas, informs to look for a date index since type="stock"  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$SPY)  # include data for series SPY, date is grabbed automatically from the index  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$EEM, color = "green")  
  
Chart the price of many stocks from xts:  
  
* Can add multiple series using multiple ncalls to hc_add_series()  
	* highchart(type="stock") %>% hc_add_series(etf_prices_xts$SPY) %>% hc_add_series(etf_prices_xts$IJS)  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$SPY, color = "blue") %>% hc_add_series(etf_prices_xts$IJS, color = "red")  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$SPY, color = "blue", name = "SPY") %>% hc_add_series(etf_prices_xts$IJS, color = "red", name = "IJS")  
  
Adding a title, subtitle, and axis labels:  
  
* Good labelling makes charts easier to read and interpret - best practice is to add title, then subtitle, then the rest of the data  
	* highchart(type = "stock") %>% hc_title(text = "5 ETFs Price History")  
    * highchart(type = "stock") %>% hc_title(text = "5 ETFs Price History") %>% hc_subtitle(text = "daily prices")  
    * highchart(type = "stock") %>%  
    *     hc_title(text = "5 ETFs Price History") %>%  
    *     hc_subtitle(text = "daily prices") %>%  
    *     hc_add_series(etf_prices_xts$SPY, color = "blue", name = "SPY") %>%  
    *     hc_add_series(etf_prices_xts$IJS, color = "red", name = "IJS") %>%  
    *     hc_add_series(etf_prices_xts$EEM, color = "green", name = "EEM") %>%  
    *     hc_add_series(etf_prices_xts$EFA, color = "purple", name = "EFA") %>%  
    * hc_add_series(etf_prices_xts$AGG, color = "orange", name = "AGG")  
* Can modify axes - labels, positions, number formats, etc. - add hc_yAxis() at the end of plotting code  
	* hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)  # opposite=FALSE puts the y-axis on the left (default opposite=TRUE goes on the right)  
  
Tooltips and legends:  
  
* The tooltip is like a magnifying glass for the viewer  
	* hc_tooltip(pointFormat = "text in the tooltip")  
    * hc_tooltip(pointFormat = "${point.y}")  # the text should be a '$' followed by the point the user is hovering over  
    * hc_tooltip(pointFormat = "${point.y: .2f}")  # round the text that is hovered over to 2 decimal points, and format with a $ sign in front  
    * hc_tooltip(pointFormat = "{point.series.name}: ${point.y: .2f})  # will show the series name followed by$price, rounded to 2 digits  
* Can also add legends to the chart - legends are interactive, and clicking them turns series on or off  
	* hc_legend(enabled = TRUE)  
  
Example code includes:  
```{r eval=FALSE}

# Chart the price of KO
highchart(type = "stock") %>%
    hc_add_series(stock_prices_xts$KO)


# Fill in the complete highchart code flow to chart GOOG in green
highchart(type = "stock") %>%
    hc_add_series(stock_prices_xts$GOOG, color = "green")

# Fill in the complete highchart code flow to chart DIS in purple
highchart(type = "stock") %>%
    hc_add_series(stock_prices_xts$DIS, color = "purple")


highchart(type = "stock") %>% 
    # Add the price of GOOG, colored orange
    hc_add_series(stock_prices_xts$GOOG, color = "orange") %>% 
    # Add the price of DIS, colored black
    hc_add_series(stock_prices_xts$DIS, color = "black")

highchart(type = "stock") %>% 
    # Add the price of KO, colored green
    hc_add_series(stock_prices_xts$KO, color = "green") %>%
    # Add the price of JPM, colored pink
    hc_add_series(stock_prices_xts$JPM, color = "pink")


highchart(type = "stock") %>%
    # Add JPM as a blue line called JP Morgan
    hc_add_series(stock_prices_xts$JPM, color = "blue", name = "JP Morgan") %>%
    # Add KO as a red line called Coke
    hc_add_series(stock_prices_xts$KO, color = "red", name = "Coke") %>%
    # Add GOOG as a green line named Google
    hc_add_series(stock_prices_xts$GOOG, color = "green", name = "Google") %>%
    # Add DIS as a purple line named Disney
    hc_add_series(stock_prices_xts$DIS, color = "purple", name = "Disney")


highchart(type = "stock") %>%
    # Add the stocks to the chart with the correct color and name
    hc_add_series(stock_prices_xts$JPM, color = "blue", name = "jpm") %>%
    hc_add_series(stock_prices_xts$KO, color = "red", name = "coke") %>%
    hc_add_series(stock_prices_xts$GOOG, color = "green", name = "google") %>%
    hc_add_series(stock_prices_xts$DIS, color = "purple", name = "disney") %>%
    hc_add_series(stock_prices_xts$AMZN, color = "black", name = "amazon")


highchart(type = "stock") %>%
    # Supply the text of the title to hc_title()
    hc_title(text = "A history of two stocks") %>%
    # Supply the text of the subtitle to hc_subtitle()
    hc_subtitle(text = "told with lines") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>%
    # Supply the text and format of the y-axis
    hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)


highchart(type = "stock") %>%
    # Add a title
    hc_title(text = "A history of two stocks") %>% 
    # Add a subtitle
    hc_subtitle(text = "told with lines") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>%
    # Change the y-axis title
    hc_yAxis(title = list(text = "in $$$s"), labels = list(format = "{value} USD"), opposite = FALSE)


highchart(type = "stock") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>%
    # Add the dollar sign and y-values on a new line
    hc_tooltip(pointFormat = "Daily Price:<br> ${point.y}")


highchart(type = "stock") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>% 
    hc_add_series(stock_prices_xts$GOOG, color = "green", name = "GOOG") %>%
    # Add stock names and round the price
    hc_tooltip(pointFormat = "{point.series.name}: ${point.y: .2f}") %>%
    # Enable the legend
    hc_legend(enabled = TRUE)


# Choose the type of highchart
highchart(type = "stock") %>%
    # Add gold, platinum and palladium
    hc_add_series(commodities_xts$gold, color = "yellow", name= "Gold") %>% 
    hc_add_series(commodities_xts$platinum, color = "grey", name= "Platinum") %>% 
    hc_add_series(commodities_xts$palladium, color = "blue", name= "Palladium") %>%
    # Customize the pointFormat of the tooltip
    hc_tooltip(pointFormat = "{point.series.name}: ${point.y} ") %>%
    hc_title(text = "Gold, Platinum and Palladium 2017") %>%
    hc_yAxis(labels = list(format = "${value}"))


```
  
  
  
***
  
Chapter 3 - Highcharter for wide tibble data
  
Visualizing one stock from wide tibble data:  
  
* The tibble format includes a column for date, since there is no index as in the xts - requires use of hcaes() to map the columns to x and y  
	* hchart(etf_prices_wide_tibble, hcaes(x = date, y = SPY), type = "line")  
    * hchart(etf_prices_wide_tibble, hcaes(x = date, y = SPY), type="line", color = "green", name = "SPY")  
  
Visualizing multiple stocks from wide tibble data:  
  
* Can plot multiple series on the same chart, using hchart() %>% hc_add_series() - can use multiple hc_add_series()  
	* hchart(etf_prices_wide_tibble, hcaes(x = date, y = SPY), type = "line") %>%  
    *     hc_add_series(etf_prices_wide_tibble, hcaes(x = date, y = EEM) type = "line")  
* The tooltip will hover only over a specific line, rather than all lines for that time point as with xts plotting  
	* hc_tooltip(shared = TRUE)  # will display all series tooltips at the same time  
    * hc_tooltip(shared = TRUE, pointFormat = "{point.series.name}: ${point.y: .2f}")  # format acordingly  
    * hc_tooltip(shared = TRUE, pointFormat = "{point.series.name}: ${point.y: .2f}<br>")  # <br> is html for line break  
* Can also customize the y-axis labels and formats, as well as adding a legend  
	* hc_yAxis(title = list(text = "prices (USD)"), labels = list(format = "${value}"))  
    * hc_legend(enabled = TRUE)  
* Each of the hc_add_series() calls needs to also specify inclusion in the legend  
	* hc_add_series(etf_prices_wide_tibble, hcaes(x = date, y = EEM) name = "EEM", type = "line", showInLegend = TRUE)  
  
Scatterplots from etf_wide_tibble:  
  
* The wide tiblle is more flexible, specifically in allowing for different chart types including scatter  
	* hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter")  
    * hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter", color = "pink", name = "EEM v. SPY")  
* Can format the tooltips for better readability  
	* hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter", color = "pink", name = "EEM v. SPY") %>% hc_tooltip(pointFormat = "{point.date} <br> EEM: {point.y: .2f}% <br> SPY: {point.x: .2f}%")  
  
Mixing chart types from wide tibble data:  
  
* Can create multiple chart types on the same plot; for example, regression line on a scatter plot  
	* model <- lm(EEM ~ SPY, data = etf_wide_tibble_returns)  
    * slope <- coef(model)[2]  
    * hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter", color = "pink", name = "EEM v. SPY") %>% hc_add_series(etf_wide_tibble_returns, hcaes(x = SPY, y = (SPY * slope), type = "line", color = "blue", lineWidth = 3)  
* Can further customize titles, axes, tooltips and the like, by using %>% to the next commands  
	* hc_title(text = "Scatter plot with regression line") %>%  
    *     hc_yAxis(title = list(text = "EEM Daily returns (%)"), labels = list(format = "{value}%"), opposite = FALSE) %>%  
    *     hc_xAxis(title = list(text = "SPY Daily returns (%)"), labels = list(format = "{value}%")) %>%  
    *     hc_tooltip(pointFormat = "{point.date} <br> EEM {point.y: .2f}% <br> SPY: {point.x: .2f}%")  
* May want to change the tooltip for the regression line, so that it shows the relevant data for the regression rather than the scatter  
	* hc_add_series(etf_wide_tibble_returns, hcaes(x = SPY, y = (SPY * slope), type = "line", color = "blue", lineWidth = 3, tooltip = list( headerFormat = "", pointFormat = ""))  
    * tooltip = list( headerFormat = "regression line", pointFormat = "{point.y: .2f}%"))  
    * hc_add_series(etf_wide_tibble_returns, hcaes(x = SPY, y = (SPY * coef(model)[2])), type = "line", color="blue", lineWidth=3, tooltip=list( headerFormat="regression line", pointFormat = "{point.y: .2f}%"))  
  
Example code includes:  
```{r eval=FALSE}

# Visualize DIS as a line chart	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = DIS), 	
       type = "line",
       # Specify the name
       name = "DIS", 
       # Specify the color
       color = "orange"
       )


# Create a green line chart of KO	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = KO), type = "line", color = "green", name = "KO")

# Create a black line chart of JPM	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = JPM), type = "line", color = "black", name = "JPM")


# Create a line chart of KO	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = KO), name = "KO", type = "line") %>%
    # Add JPM to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x = date, y = JPM), name = "JPM", type = "line") %>%
    # Add DIS to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x = date, y = DIS), name = "DIS", type = "line") %>%
    # Add AMZN to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x = date, y = AMZN), name = "AMZN", type = "line") %>%
    # Enable a shared tooltip
    hc_tooltip(shared = TRUE)


hchart(stock_wide_tibble_prices, hcaes(x=date, y=KO), name="KO", type="line", showInLegend = TRUE) %>%	
    # Add JPM to the chart and show it in the legend
    hc_add_series(stock_wide_tibble_prices, hcaes(x=date, y=JPM), name="JPM", type="line", showInLegend=TRUE) %>%
    # Add DIS to the chart and show it in the legend
    hc_add_series(stock_wide_tibble_prices, hcaes(x=date, y=DIS), name="DIS", type="line", showInLegend=TRUE) %>%
    # Add a legend to the chart
    hc_legend(enabled = TRUE)


hchart(stock_wide_tibble_prices, hcaes(x = date, y = KO), name = "KO", type = "line") %>%
    # Add JPM to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x=date, y=JPM), name = "JPM", type = "line") %>%
    # Enable a shared tooltip
    hc_tooltip(shared = TRUE, pointFormat = "{point.series.name}: ${point.y: .2f}<br>") %>%
    # Change the text of the title of the y-axis
    hc_yAxis(title = list(text = "prices (USD)"))


# Specify a green scatter plot	
hchart(stock_wide_tibble_returns, hcaes(x = GOOG, y = JPM),	
       type = "scatter", color = "green", name = "GOOG v. JPM"
       ) %>%
    # Make the tooltip display the x and y points and percentage sign
    hc_tooltip(pointFormat = "GOOG: {point.x: .2f}% <br>JPM: {point.y: .2f}%")


hchart(stock_wide_tibble_returns, hcaes(x = KO, y = AMZN), type = "scatter", 
       color = "pink",	name = "GOOG v. AMZN"
       ) %>%
    # Add a custom tooltip format
    hc_tooltip(pointFormat = "{point.date} <br>AMZN: {point.y: .2f}% <br>KO: {point.x: .2f}%")


# Create a scatter plot	
hchart(stock_wide_tibble_returns, hcaes(x = KO, y = GOOG), type = "scatter") %>%
    # Add the slope variable
    hc_add_series(stock_wide_tibble_returns, hcaes(x = KO, y = (KO * 1.15)), type =  "line") %>%
    # Customize the tooltip to show the date, x-, and y-values
    hc_tooltip(pointFormat = "{point.date} <br> GOOG {point.y: .2f}% <br> KO: {point.x: .2f}%")


hchart(stock_wide_tibble_returns, hcaes(x = AMZN, y = DIS),	type = "scatter") %>%	
    hc_add_series(stock_wide_tibble_returns, hcaes(x = AMZN, y = (AMZN * .492)), type =  "line",
                  # Add the tooltip argument
                  tooltip = list(
                      # Change the header of the line tooltip
                      headerFormat = "DIS/AMZN linear relationship<br>",
                      # Customize the y value display
                      pointFormat = "{point.y: .2f}%"
                      )
                  ) %>%
    # Customize the scatter tooltip
    hc_tooltip(pointFormat = "{point.date} <br> DIS: {point.y: .2f}% <br> AMZN: {point.x: .2f}%")


# Start the hchart flow for the returns data	
hchart(commodities_returns, type = "scatter", 
       hcaes(x = gold, y = palladium, date = date), color = "pink"
       ) %>%
    # Customize the tooltip
    hc_tooltip(pointFormat = "date: {point.date} <br>palladium: {point.y:.4f} <br>gold: {point.x:.4f} ") %>%
	hc_title(text = "Palladium Versus Gold 2017")

```
  
  
  
***
  
Chapter 4 - Highcharter for tidy tibble data  
  
Tidy data:  
  
* Financial data tends to be in wide format, though the tidy format is increasingly common in the R world  
* Need to change the mapping approach, with price mapped to the y-axis  
	* etf_tidy_tibble_prices %>% filter(symbol == "SPY") %>% hchart(., hcaes(x = date, y = price), type = "line")  # the . Is how highcharter takes in data from previous step  
    * etf_tidy_tibble_prices %>% filter(symbol == "EEM") %>% hchart(., hcaes(x = date, y = price), type = "line", color = "green")  
  
Chart many ETF from a tidy tibble:  
  
* Tidy format allows for easy plotting of all series  
	* etf_tidy_tibble_prices %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line")  
    * etf_tidy_tibble_prices %>% filter(symbol != "AGG") %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line")  
* Can also customize axes  
	* etf_tidy_tibble_prices %>% filter(symbol != "AGG" & symbol ! = "EFA") %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>% hc_title(text = "Tidy Line Charts") %>% hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)  
  
Creativity with tidy data:  
  
* Tidy format allows for easy manipulation and transformation of the underlying data  
	* etf_long_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns))  
    * etf_tidy_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns)) %>% hchart(., hcaes(x = symbol, y = mean, group = symbol), type = "scatter")  
    * etf_tidy_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns)) %>% hchart(., hcaes(x = symbol, y = mean, group = symbol, size = std_dev), type = "scatter")  
* Can further display the return to risk ratio  
	* etf_tidy_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns), return_risk = mean/std_dev) %>% hchart(., hcaes(x = symbol, y = return_risk, group = symbol), type = "column")  
  
Tidy tooltips:  
  
* Can customize the tooltips similar to what was done with xts or wide data  
	* etf_tidy_returns_tibble %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>% hc_tooltip(pointFormat = "${point.price: .2f}")  
    * etf_tidy_prices_tibble %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>% hc_tooltip(pointFormat = "{point.symbol}: ${point.price: .2f}<br>", shared = TRUE)  
* Can change labels using mutate  
	* etf_tidy_prices_tibble %>%  
    *     mutate(type = case_when(symbol == "EFA" ~ "international", symbol == "EEM" ~ "emerging", symbol == "AGG" ~ "bond", symbol == "IJS" ~ "small-cap", symbol == "SPY" ~ "market"))  
    * etf_tidy_prices_tibble %>%  
    *     mutate(type = case_when(symbol == "EFA" ~ "international", symbol == "EEM" ~ "emerging", symbol == "AGG" ~ "bond", symbol == "IJS" ~ "small-cap", symbol == "SPY" ~ "market")) %>%  
    *     hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>%  
    *     hc_tooltip(pointFormat = " {point.symbol}: ${point.price: .2f <br> fund type: {point.type}")  
* Can further run this process using summary stats (underlying tibble is grouped by symbol)  
	* etf_tidy_returns_tibble %>% summarize(mean = mean(returns), st_dev = sd(returns), max_return = max(returns), min_return = min(returns))  
    * etf_tidy_returns_tibble %>%  
    *     summarize(mean = mean(returns), st_dev = sd(returns), max_return = max(returns), min_return = min(returns)) %>%  
    *     hchart(., hcaes(x = symbol, y = mean, group = symbol), type = "column") %>%  
    *     hc_tooltip(pointFormat = "sd: {point.st_dev: .4f}% <br> max: {point.max_return: .4f}% <br> min: {point.min_return: .4f}%")  
  
Wrap up:  
  
* Tibbles and xts objects for highcharter - wide tibbles, tidy tibles, xts  
	* highchart(type="stock")  
    * hchart()  
* Customized tooltips  
  
Example code includes:  
```{r eval=FALSE}

stock_tidy_tibble_prices %>% 
    # Filter by the symbol
    filter(symbol == "KO") %>%
    # Pass the data, choose the mappings and create a line chart
    hchart(., hcaes(x = date, y = price), type = "line", color = "red")


stock_tidy_tibble_prices %>% 
    # Filter the data by symbol
    filter(symbol == "GOOG") %>%
    # Pass the data
    hchart(., hcaes(x = date, y = price), type = "line", color = "purple")


# Chart AMZN as a black line
stock_tidy_tibble_prices %>% 
    filter(symbol == "AMZN") %>%
    hchart(., hcaes(x = date, y = price), type = "line", color = "black")


stock_tidy_tibble_prices %>%
    # Pass in the data
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>%
    # Title the chart
    hc_title(text = "Daily Prices from Tidy Tibble") %>% 
    # Customize the y-axis and move the labels to the left
    hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)


stock_tidy_tibble_prices %>%
    # Filter the data so it doesn't inclue JP Morgan
    filter(symbol != "JPM") %>%
    # Pass in the data and define the aesthetic mappings
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line")

stock_tidy_tibble_prices %>%
    # Filter the data so it doesn't include Disney and Coke
    filter(!(symbol %in% c("DIS", "KO"))) %>%
    # Pass in the data and define the aesthetic mappings
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line")


stock_tidy_tibble_returns <- stock_tidy_tibble_prices %>%
    arrange(symbol, date) %>%
    group_by(symbol) %>%
    mutate(returns = price / lag(price) - 1) %>%
    filter(!is.na(returns))
str(stock_tidy_tibble_returns)


stock_tidy_tibble_returns %>%
    # Calculate the standard deviation and mean of returns
    summarize(std_dev = sd(returns), mean = mean(returns)) %>%
    hchart(., hcaes(x = symbol, y = std_dev, color = symbol, size = mean), type = "scatter") %>% 
    hc_title(text = "Standard Dev and Mean Return")


stock_tidy_tibble_returns %>%
    summarize(avg_returns = mean(returns), vol_risk = sd(returns), risk_return = vol_risk/avg_returns) %>%
    # Pass the summary statistics to hchart
    hchart(., hcaes(x = symbol, y = risk_return, group = symbol), type = "column") %>% 
    hc_title(text = "Risk/Return") %>% 
    hc_subtitle(text = "lower bars are better")


stock_tidy_tibble_prices %>%
    mutate(sector = case_when(symbol == "AMZN" ~ "tech", symbol == "GOOG" ~ "tech", symbol == "DIS" ~ "fun",
                              symbol == "JPM" ~ "bank", symbol == "KO" ~ "food")) %>%
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>%
    # Set the tooltip display with curly braces
    hc_tooltip(pointFormat = "{point.symbol}: ${point.y: .2f}<br> sector: {point.sector}")


# Calculate the mean, sd, max and min returns
stock_tidy_tibble_returns %>% 
    summarize(mean = mean(returns), st_dev = sd(returns), 
              max_return = max(returns), min_return = min(returns)
              ) %>%
    hchart(., hcaes(x = symbol, y = st_dev, group = symbol), type = "column") %>% 
    hc_tooltip(pointFormat = "mean: {point.mean: .4f}% <br>max: {point.max_return: .4f}% <br>min: {point.min_return: .4f}%")


# Pass the tidy tibble to hchart()
hchart(commodities_returns_tidy, hcaes(x=date, y=return, group=metal, date=date), type="scatter") %>% 
    hc_title(text = "Gold, Palladium and Platinum Returns 2017") %>%
    # Customize the tooltip
    hc_tooltip(pointFormat = "date: {point.date} <br>{point.metal}: {point.return: .4f}")

```
  
  
  
***
  
###_Advanced Dimensionality Reduction in R_  
  
Chapter 1 - Introduction to Advanced Dimensionality Reduction  
  
Exploring the MNIST Dataset:  
  
* The t-SNE technique is for t-Distributed Stochastic Neighbor Embedding  
* The GLRM is the Generalized Low Rank Model  
* There are multiple benefits to running dimensionality reduction - feature selection, data compression, vizualization, etc.  
* The MNIST dataset contains 70,000 images of 28x28 pixel handwritten digits  
  
Distance Metrics:  
  
* Can use distance metrics to quantify similarity between MNIST digits  
* A distance metric is a function for points x, y, z where the output satisfies all of  
	* Triangle inequality: d(x,z) <= d(x,y) + d(y,z)  
    * Symmetric property: d(x,y) = d(y,x)  
    * Non-negativity and identity: d(x,y) >= 0 and d(x,y)=0 only if x=y  
* Euclidean distance is an example - length of the connecting line segment (square root of sum-squared distances by dimension)  
	* distances <- dist(mnist_sample[195:200 ,-1])  
    * heatmap(as.matrix(distances), Rowv = NA, symm = T, labRow = mnist_sample$label[195:200], labCol = mnist_sample$label[195:200])  
* Minkowski family of distances  
	* Sum-over-all-dimensions-of[ (absolute-value-distance-on-dimension)**p ]**(1/p)  # referred to as the Manhattan distance for p=1  
    * distances <- dist(mnist_sample[195:200 ,-1, method = "minkowski", p = 3])  
    * distances <- dist(mnist_sample[195:200 ,-1], method = "manhattan")  
* The Kullback-Leibler Divergence fails to meet all the criteria  
	* Not a metric since it does not satisfy the symmetric and triangle inequality properties  
    * Measures differences in probability distributions  
    * A divergence of 0 indicates that the two distributions are identical  
    * A common distance metric in Machine Learning (t-SNE). For example, in decision trees it is called Information Gain  
    * library(philentropy)  
    * mnist_6 <- mnist_sample[195:200, -1]  
    * mnist_6 <- mnist_6 + 1  
    * sums <- rowSums(mnist_6)  
    * distances <- distance(mnist_6/sums, method = "kullback-leibler")  
    * heatmap(as.matrix(distances), Rowv = NA, symm = T, labRow = mnist_sample$label, labCol = mnist_sample$label)  
  
PCA and t-SNE:  
  
* The "curse of dimensionality" is that distance metrics do not perform well with high-dimension datasets  
* Principal Component Analysis (PCA) is a well-known technique for dimension reduction  
	* pca_result <- prcomp(mnist[, -1])  
    * pca_result <- prcomp(mnist[, -1], rank = 2)  # get only the first two components  
    * plot(pca_result$x[,1:2], pch = as.character(mnist$label), col = mnist$label, main = "PCA output")  
* The t-SNE process (not shown) can sometimes better differentiate the underlying data  
	* plot(tsne$tsne_x, tsne$tsne_y, pch = as.character(mnist$label), col = mnist$label+1, main = "t-SNE output")  
  
Exampe code includes:  
```{r eval=FALSE}

load("./RInputFiles/mnist-sample-200.RData")
load("./RInputFiles/fashion_mnist_500.RData")
load("./RInputFiles/creditcard.RData")

str(mnist_sample)
str(fashion_mnist)
str(creditcard)


# Have a look at the MNIST dataset names
names(mnist_sample)

# Show the first records
str(mnist_sample)

# Labels of the first 6 digits
head(mnist_sample[, 1])


# Plot the histogram of the digit labels
hist(mnist_sample$label)

# Compute the basic statistics of all records
# summary(mnist_sample)

# Compute the basic statistics of digits with label 0
# summary(mnist_sample[mnist_sample$label==0,])


# Show the labels of the first 10 records
mnist_sample$label[1:10]

# Compute the Euclidean distance of the first 10 records
distances <- dist(mnist_sample[1:10, -1], method="euclidean")

# Show the distances values
distances

# Plot the numeric matrix of the distances in a heatmap
heatmap(as.matrix(distances), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )


# Minkowski distance or order 3
distances_3 <- dist(mnist_sample[1:10, -1], method="minkowski", p=3)
distances_3
heatmap(as.matrix(distances_3), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )

# Minkowski distance of order 2
distances_2 <- dist(mnist_sample[1:10, -1], method="minkowski", p=2)
distances_2
heatmap(as.matrix(distances_2), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )


# Get the first 10 records
mnist_10 <- mnist_sample[1:10, -1]

# Add 1 to avoid NaN when rescaling
mnist_10_prep <- mnist_10 + 1

# Compute the sums per row
sums <- rowSums(mnist_10_prep)

# Compute KL divergence
distances <- philentropy::distance(mnist_10_prep/sums, method="kullback-leibler")
heatmap(as.matrix(distances), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )


# Get the principal components from PCA
pca_output <- prcomp(mnist_sample[, -1])

# Observe a summary of the output
summary(pca_output)

# Store the first two coordinates and the label in a data frame
pca_plot <- data.frame(pca_x = pca_output$x[, 1], pca_y = pca_output$x[, 2], 
                       label = as.factor(mnist_sample$label)
                       )

# Plot the first two principal components using the true labels as color and shape
ggplot(pca_plot, aes(x = pca_x, y = pca_y, color = label)) + 
    ggtitle("PCA of MNIST sample") + 
    geom_text(aes(label = label)) + 
    theme(legend.position = "none")


tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 2)  # modifying the default parameters


# Explore the tsne_output structure
str(tsne_output)

# Have a look at the first records from the t-SNE output
head(tsne_output$Y)

# Store the first two coordinates and the label in a data.frame
tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], 
                        label = as.factor(mnist_sample$label)
                        )

# Plot the t-SNE embedding using the true labels as color and shape
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = label)) + 
    ggtitle("T-Sne output") + 
    geom_text(aes(label = label)) + 
    theme(legend.position = "none")

```
  
  
  
***
  
Chapter 2 - Introduction to t-SNE  
  
Building a t-SNE Embedding:  
  
* The t-SNE method was published in 2008  
	* Non-linear dimensionality reduction technique  
    * Works well for most of the problems and is a very good method for visualizing high dimensional datasets  
    * Rather than keeping dissimilar points apart (like PCA) it keeps the low-dimensional representation of similar points together  
* Generally, the t-SNE method starts with PCA, then moves on to additional steps  
	* Use PCA to reduce the input dimensions into a small number  
    * Construct a probability distribution over pairs of original high dimensional records  
    * Define a similarity probability distribution of the points in the low-dimensional embedding  
    * Minimize the K-L divergence between the two distributions using gradient descent method  
* Can run t-SNE in R using the Rtsne package  
	* library(Rtsne)  
    * tsne_output <- Rtsne(mnist[, -1])  
    * tsne_output <- Rtsne(mnist[, -1], PCA = FALSE, dims = 3)  # modifying the default parameters  
    * tsne_output$itercosts  # K-L divergence cost, samples after 50 iterations  
    * head(tsne_output$costs)  # cost of each record after the final iteration  
  
Optimal Number of t-SNE Iterations:  
  
* Hyper-parameters are common in machine learning, and t-SNE has several of them  
	* Number of iterations  
    * Perplexity  
    * Learning rate  
    * Optimization criterium: K-L divergence  
* Because t-SNE is not deterministic, running with the same parameters can drive different results  
	* set.seed(1234)  
    * tsne_output_1 <- Rtsne(mnist[, -1], max_iter = 1500)   
    * set.seed(1234)  
    * tsne_output_2 <- Rtsne(mnist[, -1], max_iter = 1500)  
    * identical(tsne_output_1, tsne_output_2)  # TRUE  
* One of the important parameters for t-SNE is the number of iterations (default is 1000)  
  
Effect of Perplexity Parameter:  
  
* Perplexity is a hyper-parameter for balancing global and local criteria  
	* A guess about the number of close neighbors  
    * In a real setting is important to try different values  
    * Must be lower than the number of input records  
    * tsne_output <- Rtsne(mnist[, -1], perplexity = 50, max_iter = 1300)  
  
Classifying Digits with t-SNE:  
  
* Can classify digits (and run other tasks) using t-SNE  
	* Build a t-SNE model and calculate the centroids  
    * Classify unknown data points based on proximity to the centroid  
* Example for step 1 (building the t-SNE)  
	* tsne <- Rtsne(mnist_10k[, -1], perplexity = 5)  
    * tsne_plot <- data.frame(tsne_x= tsne_out$Y[1:5000,1], tsne_y = tsne_out$Y[1:5000,2], digit = as.factor(mnist_10k[1:5000,]$label))  
    * ggplot(tsne_plot, aes(x= tsne_x, y = tsne_y, color = digit)) + ggtitle("MNIST embedding of the first 5K digits") + geom_text(aes(label = digit)) + theme(legend.position="none")  
* Example for step 1b (calculating the centroids)  
	* centroids <- as.data.table(tsne_out$Y[1:5000,])  
    * setnames(centroids, c("X", "Y"))  
    * centroids[, label := as.factor(mnist_10k[1:5000,]$label)]  
    * centroids[, mean_X := mean(X), by = label]  
    * centroids[, mean_Y := mean(Y), by = label]  
    * centroids <- unique(centroids, by = "label")  
    * ggplot(centroids, aes(x= mean_X, y = mean_Y, color = label)) + ggtitle("Centroids coordinates") + geom_text(aes(label = label)) + theme(legend.position = "none")  
* Example for step 2 (classifying new digits)  
	* distances <- as.data.table(tsne_out$Y[5001:10000,])  
    * setnames(distances, c("X", "Y"))  
    * distances[, label := mnist_10k[5001:10000,]$label]  
    * distances <- distances[label == 4 | label == 9]  
    * distances[, dist_4:=sqrt(((X - centroids[label==4,]$mean_X) + (Y - centroids[label==4,]$mean_Y))^2)]  
    * ggplot(distances, aes(x=dist_4, fill = as.factor(label))) + geom_histogram(binwidth=5, alpha=.5, position="identity", show.legend = F)  
  
Example code includes:  
```{r eval=FALSE}

# Compute t-SNE without doing the PCA step
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)

# Show the obtained embedding coordinates
head(tsne_output$Y)

# Store the first two coordinates and plot them 
tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], 
                        digit = as.factor(mnist_sample$label)
                        )

# Plot the coordinates
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("t-SNE of MNIST sample") + 
geom_text(aes(label = digit)) + 
theme(legend.position = "none")


# Inspect the output object's structure
str(tsne_output)

# Show the K-L divergence of each record after the final iteration
tsne_output$itercosts
tsne_output$costs

# Plot the K-L divergence of each record after the final iteration
plot(tsne_output$itercosts, type = "l")
plot(tsne_output$costs, type = "l")


# Generate a three-dimensional t-SNE embedding without PCA
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA=FALSE, dims=3)

# Generate a new t-SNE embedding with the same hyper-parameter values
tsne_output_new <- Rtsne::Rtsne(mnist_sample[, -1], PCA=FALSE, dims=3)

# Check if the two outputs are identical
identical(tsne_output, tsne_output_new)

# Generate a three-dimensional t-SNE embedding without PCA
set.seed(1234)
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)

# Generate a new t-SNE embedding with the same hyper-parameter values
set.seed(1234)
tsne_output_new <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)

# Check if the two outputs are identical
identical(tsne_output, tsne_output_new)


# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with 2000 iterations
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA=TRUE, dims=2, max_iter=2000)

# Observe the output costs 
tsne_output$itercosts

# Get the 50th iteration with the minimum K-L cost
which.min(tsne_output$itercosts)


# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with perplexity 5
tsne_output_5 <- Rtsne::Rtsne(mnist_sample[, -1], perplexity=5, max_iter=1200)

# Observe the returned K-L divergence costs at every 50th iteration
tsne_output_5$itercosts

# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with perplexity 20
tsne_output_20 <- Rtsne::Rtsne(mnist_sample[, -1], perplexity=20, max_iter=1200)

# Observe the returned K-L divergence costs at every 50th iteration
tsne_output_20$itercosts

# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with perplexity 50
tsne_output_50 <- Rtsne::Rtsne(mnist_sample[, -1], perplexity=50, max_iter=1200)

# Observe the returned K-L divergence costs at every 50th iteration
tsne_output_50$itercosts


# Observe the K-L divergence costs with perplexity 5 and 50
tsne_output_5$itercosts
tsne_output_50$itercosts

# Generate the data frame to visualize the embedding
tsne_plot_5 <- data.frame(tsne_x = tsne_output_5$Y[, 1], tsne_y = tsne_output_5$Y[, 2], digit = as.factor(mnist_sample$label))
tsne_plot_50 <- data.frame(tsne_x = tsne_output_50$Y[, 1], tsne_y = tsne_output_50$Y[, 2], digit = as.factor(mnist_sample$label))

# Plot the obtained embeddings
ggplot(tsne_plot_5, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("MNIST t-SNE with 1300 iter and Perplexity=5") + geom_text(aes(label = digit)) + 
theme(legend.position="none")
ggplot(tsne_plot_50, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("MNIST t-SNE with 1300 iter and Perplexity=50") + geom_text(aes(label = digit)) + 
theme(legend.position="none")


# Prepare the data.frame
tsne_plot <- data.frame(tsne_x = tsne_output_50$Y[1:100, 1], 
                        tsne_y = tsne_output_50$Y[1:100, 2], 
                        digit = as.factor(mnist_sample$label[1:100])
                        )

# Plot the obtained embedding
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("MNIST embedding of the first 100 digits") + 
geom_text(aes(label = digit)) + 
theme(legend.position="none")


# Get the first 5K records and set the column names
dt_prototypes <- as.data.table(tsne_output_50$Y[1:100, ])
setnames(dt_prototypes, c("X","Y"))

# Paste the label column as factor
dt_prototypes[, label := as.factor(mnist_sample[1:100,]$label)]

# Compute the centroids per label
dt_prototypes[, mean_X := mean(X), by = label]
dt_prototypes[, mean_Y := mean(Y), by = label]

# Get the unique records per label
dt_prototypes <- unique(dt_prototypes, by = "label")
dt_prototypes


# Store the last 100 records in distances and set column names
distances <- as.data.table(tsne_output_50$Y[101:200, ])
setnames(distances, c("X", "Y"))

# Paste the true label
distances[, label := mnist_sample[101:200,]$label]

# Filter only those labels that are 1 or 0 
distances <- distances[label == 1 | label == 0]

# Compute Euclidean distance to prototype of digit 1
distances[, dist_1 := sqrt(( (X - dt_prototypes[label == 1,]$mean_X) + 
                             (Y - dt_prototypes[label == 1,]$mean_Y))^2)]


# Compute the basic statistics of distances from records of class 1
summary(distances[label == 1]$dist_1)

# Compute the basic statistics of distances from records of class 1
summary(distances[label == 0]$dist_1)

# Plot the histogram of distances of each class
ggplot(distances, aes(x = dist_1, fill = as.factor(label))) +
geom_histogram(binwidth = 5, alpha = .5, position = "identity", show.legend = FALSE) + 
ggtitle("Distribution of Euclidean distance 1 vs 0")

```
  
  
  
***
  
Chapter 3 - Using t-SNE with Predictive Models  
  
Credit Card Fraud Detection:  
  
* There are benefits of using t-SNE for feature engineering for further analysis  
	* Less correlation of input features  
    * Reduction in computation time  
* Database of European credit card farud available from 2013  
	* Released by Andrea Dal Pozzolo, et al. and available in Kaggle datasets  
    * Highly unbalanced: 492 fraud cases out of 248,807 (0.172%)  
    * Anonymized numerical features which are the result of a PCA  
    * 30 features plus the Class (1 fraud, 0 not-fraud)  
    * We only know the meaning of two features: time and amount of the transaction  
* Need to manage the class imbalances, for example by over-smapling the minority or under-sampling the majority  
	* set.seed(1234)  
    * idx <- sample(1:nrow(creditcard), nrow(creditcard)*.20)  
    * creditcard.test <- creditcard[idx]  
    * creditcard.train <- creditcard[!idx]  
    * creditcard.pos <- creditcard.train[Class==1]  
    * creditcard.neg <- creditcard.train[Class==0]  
    * creditcard.neg.bal <- creditcard.neg[sample(1:nrow(creditcard.neg), nrow(creditcard.pos))]  
    * creditcard.train <- rbind(creditcard.pos, creditcard.neg.bal)  
  
Training Random Forest Models:  
  
* The random forest model can help with classification (widely used method that does not require as much fine-tuning of parameters)  
	* randomForest the most common R package for random forests  
    * library(randomForest)  
    * train_x <- creditcard_train[, -31]  
    * train_y <- creditcard_train$Class  
    * rf_model <- randomForest(x = train_x, y = train_y, ntree = 100)  
* Can plot performance based on the number of trees  
	* plot(rf_model, main = "Error evolution vs number of trees")  
    * legend("topright", colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)  
    * varImpPlot(rf_model, main = "Variable importance")  
  
Predicting Data:  
  
* Can make predictions using the random forest and evaluate the model  
	* prop.table(table(creditcard_test$Class))  
    * pred_rf <- predict(rf_model, creditcard_test, type = "prob")  
    * hist(pred_rf[, 2], main = "Histogram of predictions on the test set", xlab = "prediction value")  
* Can assess quality of predictions using AUC  
	* pred <- prediction(pred_rf[,2], creditcard_test$Class)  
    * perf <- performance(pred, measure = "auc")  
    * perf@y.values  
  
Visualizing Neural Network Layers:  
  
* Neural networks can also be used for classification, with layers of neurons producing a final output  
* There are many types of activation functions - sigmoid, tanh, relu, leaky relu, etc.  
* Visualizing the weights for each neuron in each layer can be valuable, and dimensionality reduction helps with that  
	* head(layer_128_train[, 1:7])  
    * summary(layer_128_train[, 1:4])  
    * tsne_nn_layer_train <- Rtsne(as.matrix(layer_128_train), perplexity = 50, max_iter = 400, check_duplicates = F, dims = 2, verbose = T)  
    * tsne_plot_train <- data.frame(tsne_x = tsne_nn_layer_train$Y[,1], tsne_y = tsne_nn_layer_train$Y[,2], y_col = creditcard_train$Class)  
    * ggplot(tsne_plot_train, aes(x = tsne_x, y = tsne_y, color = y_col)) + geom_point() + ggtitle("Credit card embedding 128 neurons layer") + theme(legend.position="none")  
  
Example code includes:  
```{r eval=FALSE}

# Look at the data dimensions
dim(creditcard)

# Explore the column names
names(creditcard)

# Observe some records
str(creditcard)

# Generate a summary
# summary(creditcard)

# Plot a histogram of the transaction time
ggplot(creditcard, aes(x = Time)) + 
    geom_histogram()


# Extract positive and negative instances of fraud
creditcard_pos <- creditcard[Class == 1]
creditcard_neg <- creditcard[Class == 0]

# Fix the seed
set.seed(1234)

# Create a new negative balanced dataset by undersampling
creditcard_neg_bal <- creditcard_neg[sample(1:nrow(creditcard_neg), nrow(creditcard_pos)), ]

# Generate a balanced train set
creditcard_train <- rbind(creditcard_pos, creditcard_neg_bal)


# Fix the seed
set.seed(1234)

# Separate x and y sets
train_x <- creditcard_train[, -31]
train_y <- as.factor(creditcard_train$Class)

# Train a random forests
rf_model <- randomForest::randomForest(train_x, train_y, ntree=100)

# Plot the error evolution and variable importance
plot(rf_model)
randomForest::varImpPlot(rf_model)


# Set the seed
set.seed(1234)

# Generate the t-SNE embedding 
tsne_output <- Rtsne::Rtsne(as.matrix(creditcard_train[, -31]), check_duplicates = FALSE, PCA=FALSE)

# Generate a data frame to plot the result
tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1],
                        tsne_y = tsne_output$Y[, 2],
                        Class = creditcard_train$Class
                        )

# Plot the embedding usign ggplot and the label
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + 
    ggtitle("t-SNE of credit card fraud train set") + 
    geom_text(aes(label = Class)) + theme(legend.position = "none")


# Fix the seed
set.seed(1234)

# Train a random forest
rf_model_tsne <- randomForest::randomForest(tsne_plot[, c("tsne_x", "tsne_y")], 
                                            as.factor(creditcard_train$Class), ntree=100
                                            )

# Plot the error evolution
plot(rf_model_tsne)

# Plot the variable importance
randomForest::varImpPlot(rf_model_tsne)


# Predict on the test set using the random forest 
# pred_rf <- predict(rf_model, creditcard_test, type = "prob")

# Plot a probability distibution of the target class
# hist(pred_rf[, 2])

# Compute the area under the curve
# pred <- prediction(pred_rf[, 2], creditcard_test$Class)
# perf <- performance(pred, measure = "auc") 
# perf@y.values


# Predict on the test set using the random forest generated with t-SNE features
# pred_rf <- predict(rf_model_tsne, test_x, type = "prob")

# Plot a probability distibution of the target class
# hist(pred_rf[, 2])

# Compute the area under the curve
# pred <- prediction(pred_rf[, 2], creditcard_test$Class)
# perf <- performance(pred, measure="auc") 
# perf@y.values


# Observe the dimensions
# dim(layer_128_train)

# Show the first six records of the last ten columns
# head(layer_128_train[, 118:128])

# Generate a summary of all columns
# summary(layer_128_train)


# Set the seed
# set.seed(1234)

# Generate the t-SNE
# tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates=FALSE, max_iter=400, perplexity=50)

# Prepare data.frame
# tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], 
#                         Class = creditcard_train$Class
#                         )

# Plot the data 
# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + 
#     geom_point() + 
#     ggtitle("Credit card embedding of Last Neural Network Layer")

```
  
  
  
***
  
Chapter 4 - Generalized Low Rank Models  
  
Exploring Fashion MNIST dataset:  
  
* The fashion MNIST dataset contains 70,000 grayscale images at 28x28 pixels of 10 clothing categories  
	* Identical format to traditional MNIST  
    * Released by Zalando  
    * With the goal of replacing MNIST, because:  
    * MNIST is easy to predict  
    * MNIST is overused  
* Exploring and modeling with the fashion MNIST data  
	* class_names <- c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')  
    * xy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1], y = expand.grid(1:28, 28:1)[,2])  
    * plot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[1, -1]))[,1])  
    * plot_theme <- list( raster = geom_raster(hjust = 0, vjust = 0), gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = FALSE), theme = theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.background = element_blank()) )  
    * ggplot(plot_data, aes(x, y, fill = fill)) + ggtitle(class_names[as.integer(fashion_mnist[1,1])+1]) + plot_theme  
  
Generalized Low Rank Models (GLRM):  
  
* There are many benefits to the GLRM approach, including  
	* Reduces the required storage  
    * Enables data visualization  
    * Removes noise  
    * Imputes missing data  
    * Simplifies data processing  
* The low-rank structure converts an mxn matrix to a combination of mxk and kxn  
	* Parallelized dimensionality reduction algorithm  
    * Categorical columns are transformed into binary columns  
* Can run GLRM in R with H2O  
	* H2O is an open source machine learning framework with R interfaces  
    * Has a good parallel implementation of GLRM  
    * Steps: (1) initialize the cluster and (2) store the input data  
    * h2o.init()  
    * fashion_mnist.hex <- as.h2o(fashion_mnist, "fashion_mnist.hex")  
    * model_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, cols = 2:ncol(fashion_mnist), k = 2, max_iterations = 100)  # k is the rank (dimension) of the space  
  
Visualizing a GLRM Model:  
  
* Often helpful to extract the XY representation of the GLRM  
	* X <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))  # dim will be nxk  
    * Y <- model_glrm@model$archetypes  # dim will be kxm  
    * ggplot(X, aes(x= Arch1, y = Arch2, color = fashion_mnist$label)) + ggtitle("Fashion Mnist GLRM Archetypes") + geom_text(aes(label = fashion_mnist$label)) + theme(legend.position="none")  
* Can grab the centroids by class, and use as prototypes for classification  
	* X[, label := as.numeric(fashion_mnist$label)]  
    * X[, mean_x := mean(Arch1), by = label]  
    * X[, mean_y := mean(Arch2), by = label]  
    * X_mean <- unique(X, by = "label")  
    * class_names = c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')  
    * ggplot(X_mean, aes(x = mean_x, y = mean_y, color = as.factor(X_mean$label))) + ggtitle("Fashion Mnist GLRM class centroids") + geom_text(aes(label = class_names[label])) + theme(legend.position="none")  
* Can reconstruct the original data using X*Y, with the predict method  
	* fashion_pred <- predict(model_glrm, fashion_mnist.hex)  
    * head(fashion_pred[1:2, 1:4])  
    * xy_axis <- data.frame(x = expand.grid(1:28,28:1)[,1], y = expand.grid(1:28,28:1)[,2])  
    * data_reconstructed <- cbind(xy_axis, fill = as.data.frame(t(fashion_pred[1000,]))[,1])  
    * plot_reconstructed <- ggplot(plot_data, aes(x, y, fill = fill)) + ggtitle("Reconstructed Pullover (K=2)") + plot_theme  
    * data_original <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[1000, -1]))[,1])  
    * plot_original <- ggplot(plot_data_2, aes(x, y, fill = fill)) + ggtitle("Original Pullover") + plot_theme  
    * grid.arrange(plot_reconstructed, plot_original, nrow = 1)  
* Higher values of k will typically lead to better reconstructions - assessed visually or by using the resulting error values  
  
Dealing with Missing Data and Speeding-Up Models:  
  
* Missing data can confound the analysis, and a good approach to it is required  
* Common in real-world datasets  
	* May be Intentionally not provided  
    * May be Due to an error  
    * With GLRM we can impute missing data and assign an estimation  
* Example of randomly generated missing data using the fashion_mnist dataset  
	* fashion_mnist_miss.hex <- h2o.insertMissingValues(fashion_mnist.hex[,-1], fraction=0.2, seed = 1234)  
    * summary(fashion_mnist_miss[,781:784])  
    * model_glrm <- h2o.glrm(training_frame = fashion_mnist_miss.hex, transform = "NORMALIZE", ignore_const_cols = FALSE, k = 64, max_iterations = 200, seed = 123)  
    * fashion_pred <- h2o.predict(model_glrm, fashion_mnist_miss.hex)  
    * summary(fashion_pred[,782:784])  
* Another advantage of GLRM is to speed up training models (lower volumes of data for processing)  
	* Training machine learning models is faster using a low-dimensional representation  
    * Key is to have a good compressed representation  
* Example of training a random forest  
	* Trained several h2o random forests, 4-Fold Cross-Validation  
    * Fashion MNIST (60.000) was compressed with GLRM and changing the value of K from 2 to 256  
    * We measure the accuracy and the required time  
    * perf_metrics  
  
Summary and Wrap-Up:  
  
* Can use t-SNE and GLRM to reduce dimensionality  
* Simplify data processing, enhace visualizing data, etc.  
  
Example code includes:  
```{r eval=FALSE}

# Show the dimensions
dim(fashion_mnist)

# Create a summary of the last five columns 
summary(fashion_mnist[, 780:785])

# Table with the class distribution
table(fashion_mnist$label)


xy_axis <- data.frame(x=rep(1:28, times=28), y=rep(28:1, each=28))
plot_theme <- list( raster = geom_raster(hjust = 0, vjust = 0), 
                    gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = FALSE), 
                    theme = theme(axis.line = element_blank(), axis.text = element_blank(), 
                                  axis.ticks = element_blank(), axis.title = element_blank(),
                                  panel.background = element_blank(), panel.border = element_blank(),
                                  panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                  plot.background = element_blank()
                                  )
                    )  
class_names <- c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
                 )  

# Get the data from the last image
plot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[500, -1]))[,1])

# Observe the first records
head(plot_data)

# Plot the image using ggplot()
ggplot(plot_data, aes(x, y, fill = fill)) + 
    ggtitle(class_names[as.integer(fashion_mnist[500, 1])]) + 
    plot_theme 


# Start a connection with the h2o cluster
h2o::h2o.init()

# Store the data into h2o cluster
fashion_mnist.hex <- h2o::as.h2o(fashion_mnist, "fashion_mnist.hex")

# Launch a GLRM model over fashion_mnist data
model_glrm <- h2o::h2o.glrm(training_frame = fashion_mnist.hex, cols = 2:ncol(fashion_mnist), 
                            k = 2, seed = 123, max_iterations = 100
                            )

# Plotting the convergence
plot(model_glrm)


# Launch a GLRM model with normalized fashion_mnist data  
model_glrm <- h2o::h2o.glrm(training_frame = fashion_mnist.hex, transform = "NORMALIZE",
                            cols = 2:ncol(fashion_mnist), k = 2, seed = 123, max_iterations = 100
                            )

# Plotting the convergence
plot(model_glrm)


X_matrix <- as.data.table(h2o::h2o.getFrame(model_glrm@model$representation_name))

# Dimension of X_matrix
dim(X_matrix)

# First records of X_matrix
head(X_matrix)

# Plot the records in the new two dimensional space
ggplot(as.data.table(X_matrix), aes(x= Arch1, y = Arch2, color = fashion_mnist$label)) + 
    ggtitle("Fashion Mnist GLRM Archetypes") + 
	geom_text(aes(label = fashion_mnist$label)) + 
	theme(legend.position="none")


# Store the label of each record and compute the centroids
X_matrix[, label := as.numeric(fashion_mnist$label)]
X_matrix[, mean_x := mean(Arch1), by = label]
X_matrix[, mean_y := mean(Arch2), by = label]

# Get one record per label and create a vector with class names
X_mean <- unique(X_matrix, by = "label")
label_names <- c("T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", 
                 "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
                 )

# Plot the centroids
ggplot(X_mean, aes(x = mean_x, y = mean_y, color = as.factor(label))) + 
	ggtitle("Fashion Mnist GLRM class centroids") + 
	geom_text(aes(label = label_names[label])) +
	theme(legend.position="none")


makeNA <- function(x) {
    vecNA <- sort(unique(sample(1:length(x), round(0.225*length(x)), replace=TRUE)))
    x[vecNA] <- NA
    return(x)
}

fashion_mnist_miss <- fashion_mnist %>%
    select(-label) %>%
    apply(1, FUN=makeNA)


# Store the input data in h2o
fashion_mnist_miss.hex <- h2o::as.h2o(fashion_mnist_miss, "fashion_mnist_miss.hex")

# Build a GLRM model
model_glrm <- h2o::h2o.glrm(training_frame = fashion_mnist_miss.hex, transform="NORMALIZE", 
                            k=2, max_iterations=100
                            )

# Impute missing values
fashion_pred <- h2o::h2o.predict(model_glrm, fashion_mnist_miss.hex)

# Observe the statistics of the first 5 pixels
summary(fashion_pred[, 1:5])


# Get the starting timestamp
time_start <- proc.time()

# Train the random forest
rf_model <- randomForest::randomForest(x = fashion_mnist[, -1], y = fashion_mnist$label, ntree = 20)

# Get the end timestamp
time_end <- timetaken(time_start)

# Show the error and the time
rf_model$err.rate[20]
time_end


# Get the starting timestamp
# time_start <- proc.time()

# Train the random forest
# rf_model <- randomForest(x = train_x, y = train_y, ntree = 500)

# Get the end timestamp
# time_end <- timetaken(time_start)

# Show the error and the time
# rf_model$err.rate[500]
# time_end

```
  
  
  
***
  
###_Optimizing R Code with Rcpp_  
  
Chapter 1 - Introduction  
  
Introduction:  
  
* R is an interpretative language which can lead to slow run-times  
* C++ is a compiled language, making it much faster at the expense of requiring compiling (harder to learn and write)  
* The Rcpp package simplifies the process of using C++ from R  
	* Introduction - basic C++ syntax  
    * C++ functions and control flow  
    * Vector classes  
    * Case studies  
* Can use the library(microbenchmark) to help see the processing times of various code snippets  
	* library(microbenchmark)  
    * x <- rnorm(1e6)  
    * microbenchmark( slowmax(x), max(x) )  # slowmax was written in a very inefficient manner  
* Since it is a compiled language, C++ typically does not have console capbilities, though Rcpp build in some helper functions for this  
	* evalCpp( "40 + 2" )  
    * evalCpp( "exp(1.0)" )  
    * evalCpp( "std::numeric_limits<int>::max()")  
* Basic number types differ between R and C++  
	* Literal numbers are doubles in R, and require the L to cast as integers (32 is a double, 32L is an integer)  
    * Literal numbers are integers in C++, and require a trailing .0 to cast as doubles (32 is an integer, 32.0 is a double)  
* Can explicitly cast numbers between double and integer in C++  
	* y <- evalCpp( "(double)(40 + 2)" )  
    * evalCpp( "13 / 4" )  # Integer division, will result in 3  
    * evalCpp( "(double)13 / 4" )  # Casted to float division if either operand is a double, will result in 3.25  
  
Inline Functions with cppFunction:  
  
* Can define C++ functions using Rcpp, either scripted or using the R console  
	* library(Rcpp)  
    * cppFunction("int fun(){  
    *     int x = 37 ;  
    *     return x ;  
    * }" )  
    * fun()  
* There are many languages of engineering that happen automatically behind the scenes using Rcpp  
* Variables in C++ are statically typed (contrast to R which is dynamically typed), meaning they may not undergo a change of type at any time  
	* Functions must declare the types of all inputs and outputs, allowing the compiler to optimize code  
* Example functions using Rcpp  
	* cppFunction("double add( double x, double y){  
    *     double res = x + y ;  
    *     return res ;  
    * }  
    * )  
    * add( 30, 12 )  
    * See below for the equivalent R code  
    * addr <- function(x, y) {  
    *     res <- x + y  
    *     res  
    * }  
  
Debugging:  
  
* Light debugging includes print outs as the loop runs or message printing at key points  
* The Rprintf() functions in Rcpp allows for printing to the screen  
	* cppFunction( 'int fun(){  
    *     // Some values  
    *     int x = 42 ;  
    *     // Printing a message to the R console  
    *     Rprintf( "some message in the console, x=%d\\n", x ) ;  
    *     // Return some int  
    * return 76 ;  
    * }  
    * ')  
* Integer placeholders are %d while string placeholders are %s  
* Error handling in C++ allows for checking that key parameters are inside a defined key range  
	* cppFunction( 'int fun(int x){  
    *     // A simple error message  
    *     if( x < 0 ) stop( "sorry x should be positive" ) ;  
    *     // A formatted error message  
    *     if( x > 20 ) stop( "x is too big (x=%d)", x ) ;  
    *     // Return some int  
    * return x ;  
    * }')  
  
Example code includes:  
```{r eval=FALSE}

# Load microbenchmark
library(microbenchmark)
library(Rcpp)


# Define the function sum_loop
sum_loop <- function(x) {
  result <- 0
  for (i in x) result <- result + i
  result
}

x <- rnorm(100000)

# Check for equality 
all.equal(sum_loop(x), sum(x))

# Compare the performance
microbenchmark(sum_loop = sum_loop(x), R_sum = sum(x))


# Evaluate 2 + 2 in C++
x <- evalCpp("2+2")

# Evaluate 2 + 2 in R
y <- 2+2

# Storage modes of x and y
storage.mode(x)
storage.mode(y)

# Change the C++ expression so that it returns a double
z <- evalCpp("2.0 + 2")


# Evaluate 17 / 2 in C++
evalCpp("17/2")

# Cast 17 to a double and divide by 2
evalCpp("(double)17/2")

# Cast 56.3 to an int
evalCpp("(int)56.3")


# Define the function the_answer()
cppFunction('
  int the_answer() {
    return 42 ;
  }
')

# Check the_answer() returns the integer 42
the_answer() == 42L


# Define the function euclidean_distance()
cppFunction('
  double euclidean_distance(double x, double y) {
    return sqrt(x*x + y*y) ;
  }
')

# Calculate the euclidean distance
euclidean_distance(1.5, 2.5)


# Define the function add()
cppFunction('
  int add(int x, int y) {
    int res = x + y ;
    Rprintf("** %d + %d = %d\\n", x, y, res) ;
    return res ;
  }
')

# Call add() to print THE answer
add(40, 2)


cppFunction('
  // adds x and y, but only if they are positive
  int add_positive_numbers(int x, int y) {
      // if x is negative, stop
      if( x < 0 ) stop("x is negative") ;
    
      // if y is negative, stop
      if( y < 0 ) stop("y is negative") ;
     
      return x + y ;
  }
')

# Call the function with positive numbers
add_positive_numbers(2, 3)

# Call the function with a negative number
add_positive_numbers(-2, 3)

```
  
  
  
***
  
Chapter 2 - Functions and Control Flow  
  
C++ Functions Belong to C++ Files:  
  
* Can use .cpp files to save and source C++ functions  
	* sourceCpp( "code.cpp" )  
    * timesTwo( 21 )  
* To write a .cpp file, the following structure should be included  
	* #include <Rcpp.h>  
    * using namespace Rcpp ;  
    * // [[Rcpp::export]]  
    * int timesTwo( int x ){  
    *     return 2*x ;  
    * }  
* Note that the return requires a semicolon afterwards  
* Note that // [[Rcpp::export]] is a comment to C++ but is also picked up as meaningfull by Rcpp (defines that the following functions should be exported)  
  
Writing Functions in C++:  
  
* Only the exported functions by way of //[[Rcpp::export]] are available to R; the others are internal (private) to the C++ session  
* Can run single-line comments using // and multi-line comments using /* */  
* Rcpp includes a special comment that embeds R code to the C++ file  
	* /*** R <insert R code /*  
* The if-else syntax for Rcpp is very similar to the base R syntax  
	* if( condition ){  
    * // code if true  
    * } else {  
    * // code otherwise  
    * }  
* Can also have a void function that is called only for side effects such as printing  
	* // [[Rcpp::export]]  
    * void info( double x){  
    *     if( x < 0 ){  
    *         Rprintf( "x is negative" ) ;  
    *     } else if( x == 0 ){  
    *         Rprintf( "x is zero" ) ;  
    *     } else if( x > 0 ){  
    *         Rprintf( "x is positive" ) ;  
    *     } else {  
    *         Rprintf( "x is not a number" ) ;  
    *     }  
    * }  
  
For Loops:  
  
* There are four components to a C++ for loop, and the process has meaningful differences from R for loops  
	* Initialization - happens once, at the very beginning of the for loop  
    * Continue condition - Logical condition to control if the loop continues  
    * Increment - Executed at the end of each iteration (often to add 1 to an index)  
    * Body - statements to be executed at each iteration  
* Example of a very typicaly for loop using C++ (note that i++ is shorthand for "increment I by 1"  
	* for (int i=0; i<n; i++ ){  
    *     // some code using i  
    * }  
* Example function to calculate the sum of the first n integers  
	* // [[Rcpp::export]]  
    * int nfirst( int n ){  
    *     if( n < 0 ) {  
    *         stop( "n must be positive, I see n=%d", n ) ;
    *     }  
    *     int result = 0 ;  
    *     for( int i=0; i<n; i++){  
    *         if( i == 13 ){  
    *             Rprintf( "I cannot handle that, I am superstitious" ) ;  
    *             break ;  
    *         }  
    *         result = result + (i+1) ;  
    *     }  
    *     return result ;  
    * }  
* Example of the iterative approach from Newton to calculating square roots (see example code below)  
  
While Loops:  
  
* While loop is conceptually simpler, and there is only a continue condition and a body of the function  
* The for loop from above can be conceptually re-written as a while loop  
	* init  
    * while( condition ){  
    *     body  
    *     increment  
    * }  
* Can also run the do-while loop, where the condition comes after the body (particularly useful when the body must be run at least once)  
	* do {  
    *     body  
    * } while( condition ) ;  
  
Example code includes:  
```{r eval=FALSE}

# file should be included as 'script.cpp')
# file called as sourceCpp('script.cpp')


#include <Rcpp.h>
using namespace Rcpp ; 

// Export the function to R
//[[Rcpp::export]]
double twice(double x) {
    // Fix the syntax error
    return x+x;
}


// Include the Rcpp.h header
#include <Rcpp.h>

// Use the Rcpp namespace
using namespace Rcpp;

// [[Rcpp::export]]
int the_answer() {
    // Return 42
    return 42;
}

/*** R
# Call the_answer() to check you get the right result
the_answer()
*/


#include <Rcpp.h>
using namespace Rcpp; 

// Make square() accept and return a double
double square(double x) {
  // Return x times x
  return x*x ;
}

// [[Rcpp::export]]
double dist(double x, double y) {
  // Change this to use square()
  return sqrt(square(x) + square(y));
}


#include <Rcpp.h>
using namespace Rcpp; 

double square(double x) {
  return x * x ;
}

// [[Rcpp::export]]
double dist(double x, double y) {
  return sqrt(square(x) + square(y));
}

// Start the Rcpp R comment block
/*** R
# Call dist() to the point (3, 4)
dist(3, 4)
# Close the Rcpp R comment block
*/


#include <Rcpp.h>
using namespace Rcpp ;

// [[Rcpp::export]]
double absolute(double x) {
  // Test for x greater than zero
  if(x > 0) {
    // Return x
    return x; 
  // Otherwise
  } else {
    // Return negative x
    return -x;
  }
}

/*** R  
absolute(pi)
absolute(-3)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sqrt_approx(double value, int n) {
    // Initialize x to be one
    double x = 1;
    // Specify the for loop
    for(int i = 0; i < n; i++) {
        x = (x + value / x) / 2.0;
    }
    return x;
}

/*** R
sqrt_approx(2, 10)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List sqrt_approx(double value, int n, double threshold) {
    double x = 1.0;
    double previous = x;
    bool is_good_enough = false;
    int i;
    for(i = 0; i < n; i++) {
        previous = x;
        x = (x + value / x) / 2.0;
        is_good_enough = fabs(previous - x) < threshold;
        
        // If the solution is good enough, then "break"
        if(is_good_enough) break;
    }
    return List::create(_["i"] = i , _["x"] = x);
}

/*** R
sqrt_approx(2, 1000, 0.1)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sqrt_approx(double value, double threshold) {
    double x = 1.0;
    double previous = x;
    bool is_good_enough = false;
    
    // Specify the while loop
    while(is_good_enough == false) {
        previous = x;
        x = (x + value / x) / 2.0;
        is_good_enough = fabs(x - previous) < threshold;
    }
    
    return x ;
}

/*** R
sqrt_approx(2, 0.00001)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sqrt_approx(double value, double threshold) {
    double x = 1.0;
    double previous = x;
    bool is_good_enough = false;
    
    // Initiate do while loop
    do {
        previous = x;
        x = (x + value / x) / 2.0;
        is_good_enough = fabs(x - previous) < threshold;
    // Specify while condition
    } while (is_good_enough == false);
    
    return x;
}

/*** R
sqrt_approx(2, 0.00001)
*/

```
  
  
  
***
  
Chapter 3 - Vector Classes  
  
Rcpp Classes and Vectors:  
  
* Rcpp defines a number of C++ classes that greatly simplify processing and interfacing between C++ and R  
	* NumericVector to manipulate numeric vectors, e.g. c(1,2,3)  
    * IntegerVector for integer e.g. 1:3  
    * LogicalVector for logical e.g. c(TRUE, FALSE)  
    * CharacterVector for strings e.g. c("a", "b", "c")  
    * List for lists, aka vectors of arbitrary R objects  
* There is an API for vectors, which allows for running some key methods  
	* x.size() gives the number of elements of the vector x  
    * x[i] gives the element on the ith position in the vector x  
    * Indexing in C++ starts at 0. The index is an offset to the first position  
* Example pseudo-code for looping around a vector  
	* // x comes from somewhere  
    * NumericVector x = ... ;   
    * int n = x.size() ;  
    * for( int i=0; i<n; i++){  
    *     // manipulate x[i]  
    * }  
  
Creating Vectors:  
  
* Exported Rcpp classes are meant to be called from R  
* Because C++ is a type-sepcific language, Rcpp attempts to coerce types to the needed type prior to passing the data to C++  
* Example for creating a vector of a given size in Rcpp (numeric vectors are initialized to 0 and string vectors are initialized to blank strings)  
	* // [[Rcpp::export]]  
    * NumericVector ones(int n){  
    *     // create a new numeric vector of size n  
    *     NumericVector x(n) ;  
    *     // manipulate it  
    *     for( int i=0; i<n; i++){  
    *         x[i] = 1 ;  
    *     }  
    *     return x ;  
    * }  
* Can override the default initialization values for a new vector by passing an additional argument  
	* double value = 42.0 ;  
    * int n = 20 ;  
    * // create a numeric vector of size 20  
    * // with all values set to 42  
    * NumericVector x( n, value ) ;  
* Can use a static method to initialize the class (???)  
	* NumericVector x = NumericVector::create( 1, 2, 3 ) ;  
    * CharacterVector s = CharacterVector::create( "pink", "blue" ) ;  
    * NumericVector x = NumericVector::create( _["a"] = 1, _["b"] = 2, _["c"] = 3 ) ;  
    * IntegerVector y = IntegerVector::create( _["d"] = 4, 5, 6, _["f"] = 7 ) ;  
* Can also clone vectors to avoid changing the original vector (creates a "deep copy")  
	* // [[Rcpp::export]]  
    * NumericVector positives( NumericVector x ){  
    *     // clone x into y  
    *     NumericVector y = clone(x) ;  
    *     for( int i=0; i< y.size(); i++){  
    *         if( y[i] < 0 ) y[i] = 0 ;  
    *     }  
    *     return y ;  
    * }  
  
Weighted Mean:  
  
* Example for using Rcpp to run the weighted means  
	* weighted_mean_R <- function(x, w){ sum(x*w) / sum(w) }  # R code version  
* Example of very inefficient R code  
	* weighted_mean_loop <- function(x, w){  
    *     total_xw <- 0  
    *     total_w  <- 0  
    *     for( i in seq_along(x)){  
    *         total_xw <- total_xw + x[i]*w[i]  
    *         total_w  <- total_w  + w[i]  
    *     }  
    *     total_xw / total_w  
    * }  
* Translation to C++ code using Rcpp  
	* // [[Rcpp::export]]  
    * double weighted_mean_cpp( NumericVector x, NumericVector w){  
    *     double total_xw = 0.0 ;  
    *     double total_w  = 0.0 ;  
    *     int n = ___ ;  
    *     for( ___ ; ___ ; ___ ){  
    *         // accumulate into total_xw and total_w  
    *     }  
    *     return total_xw / total_w ;  
    * }  
* Missing values need to be tested based on special functions (similar to R)  
	* Each type of vector has its own special missing values  
    * bool test = NumericVector::is_na(x) ;  
    * double y = NumericVector::get_na() ;  // The representation of NA in double  
  
Vectors From the STL:  
  
* Rcpp vectors are thin wrappers around R vectors  
	* Cannot (cost effectively) change size: data copy every time  
* STL (Standard Template Library) vectors are independent of R vectors  
	* Cheap to grow and shrink: amortized copies  
* Generally, growing vectors is very expensive and should be avoided (in R or C++)  
	* Can even be more efficient to have two functions; first finds the vector size, second creates vector of that size and then fills it  
  
Example code includes:  
```{r eval=FALSE}

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double first_plus_last(NumericVector x) {
    // The size of x
    int n = x.size();
    // The first element of x
    double first = x[0];
    // The last element of x
    double last = x[n-1];
    return first + last;
}

/*** R
x <- c(6, 28, 496, 8128)
first_plus_last(x)
# Does the function give the same answer as R?
all.equal(first_plus_last(x), x[1] + x[4])
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sum_cpp(NumericVector x) {
  // The size of x
  int n = x.size();
  // Initialize the result
  double result = 0;
  // Complete the loop specification
  for(int i = 0; i<n; i++) {
    // Add the next value
    result = result + x[i];
  }
  return result;
}

/*** R
set.seed(42)
x <- rnorm(1e6)
sum_cpp(x)
# Does the function give the same answer as R's sum() function?
all.equal(sum_cpp(x), sum(x))
*/


#include <Rcpp.h>
using namespace Rcpp;

// Set the return type to IntegerVector
// [[Rcpp::export]]
IntegerVector seq_cpp(int lo, int hi) {
  int n = hi - lo + 1;
    
  // Create a new integer vector, sequence, of size n
  IntegerVector sequence(n);
    
  for(int i = 0; i < n; i++) {
    // Set the ith element of sequence to lo plus i
    sequence[i] = lo + i;
  }
  
  return sequence;
}

/*** R
lo <- -2
hi <- 5
seq_cpp(lo, hi)
# Does it give the same answer as R's seq() function?
all.equal(seq_cpp(lo, hi), seq(lo, hi))
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List create_vectors() {
  // Create an unnamed character vector
  CharacterVector polygons = CharacterVector::create("triangle", "square", "pentagon");
  // Create a named integer vector
  IntegerVector mersenne_primes = IntegerVector::create(_["first"] = 3, _["second"] = 7, _["third"] = 31);
  // Create a named list
  List both = List::create(_["polygons"] = polygons, _["mersenne_primes"] = mersenne_primes);
  return both;
}

/*** R
create_vectors()
*/


# Unlike R, C++ uses a copy by reference system, meaning that if you copy a variable then make changes to the copy, the changes will also take place in the original.
# To prevent this behavior, you have to use the clone() function to copy the underlying data from the original variable into the new variable
# The syntax is y = clone(x). In this exercise, we have defined two functions for you:
# change_negatives_to_zero(): Takes a numeric vector, modifies by replacing negative numbers with zero, then returns both the original vector and the copy.
# change_negatives_to_zero_with_cloning(): Does the same thing as above, but clones the original vector before modifying it.

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List change_negatives_to_zero(NumericVector the_original) {
  // Set the copy to the original
  NumericVector the_copy = the_original;
  int n = the_original.size();
  for(int i = 0; i < n; i++) {
    if(the_copy[i] < 0) the_copy[i] = 0;
  }
  return List::create(_["the_original"] = the_original, _["the_copy"] = the_copy);
}

// [[Rcpp::export]]
List change_negatives_to_zero_with_cloning(NumericVector the_original) {
  // Clone the original to make the copy
  NumericVector the_copy = clone(the_original);
  int n = the_original.size();
  for(int i = 0; i < n; i++) {
    if(the_copy[i] < 0) the_copy[i] = 0;
  }
  return List::create(_["the_original"] = the_original, _["the_copy"] = the_copy);
}

/*** R
x <- c(0, -4, 1, -2, 2, 4, -3, -1, 3)
change_negatives_to_zero_with_cloning(x)
change_negatives_to_zero(x)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double weighted_mean_cpp(NumericVector x, NumericVector w) {
  // Initialize these to zero
  double total_w = 0.0;
  double total_xw = 0.0;
  
  // Set n to the size of x
  int n = x.size();
  
  // Specify the for loop arguments
  for(int i = 0; i<n; i++) {
    // Add ith weight
    total_w += w[i];
    // Add the ith data value times the ith weight
    total_xw += w[i]*x[i];
  }
  
  // Return the total product divided by the total weight
  return total_xw / total_w;
}

/*** R 
x <- c(0, 1, 3, 6, 2, 7, 13, 20, 12, 21, 11)
w <- 1 / seq_along(x)
weighted_mean_cpp(x, w)
# Does the function give the same results as R's weighted.mean() function?
all.equal(weighted_mean_cpp(x, w), weighted.mean(x, w))
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double weighted_mean_cpp(NumericVector x, NumericVector w) {
  double total_w = 0;
  double total_xw = 0;
  
  int n = x.size();
  
  for(int i = 0; i < n; i++) {
    // If the ith element of x or w is NA then return NA
    if (NumericVector::is_na(x[i]) | NumericVector::is_na(w[i])) return NumericVector::get_na();
    total_w += w[i];
    total_xw += x[i] * w[i];
  }
  
  return total_xw / total_w;
}

/*** R 
x <- c(0, 1, 3, 6, 2, 7, 13, NA, 12, 21, 11)
w <- 1 / seq_along(x)
weighted_mean_cpp(x, w)
*/


NumericVector bad_select_positive_values_cpp(NumericVector x) {
  NumericVector positive_x(0);
  for(int i = 0; i < x.size(); i++) {
    if(x[i] > 0) {
      positive_x.push_back(x[i]);
    }
  }
  return positive_x;
}

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector good_select_positive_values_cpp(NumericVector x) {
  int n_elements = x.size();
  int n_positive_elements = 0;
  
  // Calculate the size of the output
  for(int i = 0; i < n_elements; i++) {
    // If the ith element of x is positive
    if(x[i] > 0) {
      // Add 1 to n_positive_elements
      n_positive_elements++;
    }
  }
  
  // Allocate a vector of size n_positive_elements
  NumericVector positive_x(n_positive_elements);
  
  // Fill the vector
  int j = 0;
  for(int i = 0; i < n_elements; i++) {
    // If the ith element of x is positive
    if(x[i] > 0) {
      // Set the jth element of positive_x to the ith element of x
      positive_x[j] = x[i];
      // Add 1 to j
      j++;
    }
  }
  return positive_x;
}

/*** R
set.seed(42)
x <- rnorm(1e4)
# Does it give the same answer as R?
all.equal(good_select_positive_values_cpp(x), x[x > 0])
# Which is faster?
microbenchmark(
  bad_cpp = bad_select_positive_values_cpp(x),
  good_cpp = good_select_positive_values_cpp(x)
)
*/


# The standard template library (stl) is a C++ library containing flexible algorithms and data structures
# For example, the double vector from the stl is like a "native C++" equivalent of Rcpp's NumericVector
# The following code creates a standard double vector named x with ten elements
std::vector<double> x(10);
# Usually it makes more sense to stick with Rcpp vector types because it gives you access to many convenience methods that work like their R equivalents, including mean(), round(), and abs()
# However, the stl vectors have an advantage that they can dynamically change size without paying for data copy every time


#include <Rcpp.h>
using namespace Rcpp;

// Set the return type to a standard double vector
// [[Rcpp::export]]
std::vector<double> select_positive_values_std(NumericVector x) {
  int n = x.size();
  
  // Create positive_x, a standard double vector
  std::vector<double> positive_x(0);
    
  for(int i = 0; i < n; i++) {
    if(x[i] > 0) {
      // Append the ith element of x to positive_x
      positive_x.push_back(x[i]);
    }
  }
  return positive_x;
}

/*** R
set.seed(42)
x <- rnorm(1e6)
# Does it give the same answer as R?
all.equal(select_positive_values_std(x), x[x > 0])
# Which is faster?
microbenchmark(
  good_cpp = good_select_positive_values_cpp(x),
  std = select_positive_values_std(x)
)
*/

```
  
  
  
***
  
Chapter 4 - Case Studies  
  
Random Number Generation:  
  
* Can use functions from the R namespace for features like drawing random numbers from a distribution  
	* // one number from a N(0,1) - function only gives a single number  
    * double x = R::rnorm( 0, 1 ) ;  
* Can also use the functions of the same name available in the Rcpp namespace - Rcpp::rnorm() and the like  
* Example for creating numbers from a truncated distribution (e.g., a normal with only positive values) - rejected sampling  
	* // we generate n numbers  
    * NumericVector x(n) ;  
    * // fill the vector in a loop  
    * for( int i=0; i<n; i++){  
    *     // keep generating d until it gets positive  
    *     double d  ;  
    *     do {  
    *         d = ... ;  
    *     } while( d < 0 ) ;  
    * x[i] = d ;  
    * }  
* Example for creating numbers from a mixture of distributions  
	* int component( NumericVector weights, double total_weight ){  
    * // return the index of the selected component  
    * }  
    * NumericVector rmix( int n, NumericVector weights, NumericVector means, NumericVector sds ){  
    *     NumericVector res(n) ;  
    *     for( int i=0; i<n; i++){  
    *         // find which component to use  
    *         ...  
    *         // simulate using the mean and sd from the selected component  
    *         ...  
    *     }  
    *     return res ;  
    * }  
  
Rolling Operations:  
  
* Rolling means are vectors where the value at position j is the mean of the next n elements  
	* The process runs much faster if just the first element is deleted and then the last element is added  
* Using C++ code can help make processes like these easy to read while maintaining very strong performance  
* Example of running last observation carried forward (LOCF)  
	* # iterative version (can be converted to C++ code)  
    * na_meancf2 <- function(x){  
    *     total <- 0  
    *     n <- 0  
    *     for( i in seq_along(x) ){  
    *         if( is.na(x[i]) ){  
    *             x[i] <- total / n  
    *         } else {  
    *             total <- x[i] + total  
    *             n <- n + 1  
    *         }  
    *     }  
    * }  
  
Auto-Regressive Model:  
  
* Example for AR code in R (works well, but is inefficient)  
	* ar <- function(n, phi, sd){  
    *     x <- epsilon <- rnorm(n, sd = sd)  
    *     np <- length(phi)  
    *     for( i in seq(np+1, n)){  
    *         x[i] <- sum(x[seq(i-1, i-np)] * phi) + epsilon[i]  
    *     }  
    *     x  
    * }  
* Example skeleton for writing the same code using C++  
	* NumericVector x(n) ;  
    * // initial loop  
    * for( ___ ; __ < np ; ___ ){  
    *     x[i] = R::rnorm(___) ;  
    * }  
    * // outer loop  
    * for( ___ ; ___ ; ___ ){  
    *     double value = rnorm(___) ;  
    *     // inner loop  
    *     for( ___ ; ___ ; ___ ){  
    *         value += ___ ;  
    *     }  
    *     x[i] = value ;  
    * }  
* Similar example for the R code for an MA (moving average)  
	* ma <- function(n, theta, sd){  
    *     epsilon <- rnorm(n, sd = sd)  
    *     x <- numeric(n)  
    *     nq <- length(theta)  
    *     for( i in seq(nq+1, n)){  
    *         x[i] <- sum(epsilon[seq(i-1, i-nq)] * theta) + epsilon[i]  
    *     }  
    *     x  
    * }  
* Example of running the MA (skeleton) using C++  
	* #include <Rcpp.h>  
    * using namespace Rcpp ;  
    * // [[Rcpp::export]]  
    * NumericVector ma( int n, double mu, NumericVector theta, double sd ){  
    *     int nq = theta.size() ;  
    *     // generate the noise vector at once  
    *     // using the Rcpp::rnorm function, similar to the R function  
    *     NumericVector eps = Rcpp::rnorm(n, 0.0, sd) ;  
    *     // init the output vector of size n with all 0.0  
    *     NumericVector x(___) ;  
    *     // start filling the values at index nq + 1  
    *     for( int i=nq+1; i<n; i++){  
    *         ____  
    *     }  
    *     return x ;  
    * }  
* Can also combine AR-MA in to an ARMA model  
  
Wrap Up:  
  
* The Rcpp package combines the ease of R syntax with the speed and efficiency of C++  
* Vectorized code is basically a loop in a compiled language such as C++  
* Recap of key topics from this course - for loops in C++ are an extremely common use case  
	* evalCpp and cppFunction  
    * for loops in C++ - for( init ; condition ; increment ){ body }  
    * Vectors, including that vector indexing starts at 0  
* C++ files with Rcpp  
	* #include <Rcpp.h>  
    * using namespace Rcpp ;  
    * // [[Rcpp::export]]  
    * double add( double x, double y){  
    *     return x + y ;  
    * }  
    * // [[Rcpp::export]]  
    * double twice( double x){  
    *     return 2.0 * x;  
    * }  
  
Example code includes:  
```{r eval=FALSE}

# When you write R code, it usually makes sense to generate random numbers in a vectorized fashion
# When you are in C++ however, you are allowed (even by your guilty conscience) to use loops and process the data element by element

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector positive_rnorm(int n, double mean, double sd) {
    // Specify out as a numeric vector of size n
    NumericVector out(n);
    // This loops over the elements of out
    for(int i = 0; i < n; i++) {
        // This loop keeps trying to generate a value
        do {
            // Call Rs rnorm()
            out[i] = R::rnorm(mean, sd);
            // While the number is negative, keep trying
        } while(out[i] < 0);
    }
    return out;
}

/*** R
positive_rnorm(10, 2, 2)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
int choose_component(NumericVector weights, double total_weight) {
    // Generate a uniform random number from 0 to total_weight
    double x = R::runif(0, total_weight);
    // Remove the jth weight from x until x is small enough
    int j = 0;
    while(x >= weights[j]) {
        // Subtract jth element of weights from x
        x -= weights[j];
        j++;
    }
    return j;
}

/*** R
weights <- c(0.3, 0.7)
# Randomly choose a component 5 times
replicate(5, choose_component(weights, sum(weights)))
*/


#include <Rcpp.h>
using namespace Rcpp;

// From previous exercise; do not modify
// [[Rcpp::export]]
int choose_component(NumericVector weights, double total_weight) {
    double x = R::runif(0, total_weight);
    int j = 0;
    while(x >= weights[j]) {
        x -= weights[j];
        j++;
    }
    return j;
}

// [[Rcpp::export]]
NumericVector rmix(int n, NumericVector weights, NumericVector means, NumericVector sds) {
    // Check that weights and means have the same size
    int d = weights.size();
    if(means.size() != d) {
        stop("means size != weights size");
    }
    // Do the same for the weights and std devs
    if(sds.size() != d) {
        stop("sds size != weights size");
    }
    // Calculate the total weight
    double total_weight = 0.0;
    for (int i=0; i<d; i++) { 
        total_weight += weights[i];
    };
    // Create the output vector
    NumericVector res(n);
    // Fill the vector
    for(int i = 0; i < n; i++) {
        // Choose a component
        int j = choose_component(weights, total_weight);
        // Simulate from the chosen component
        res[i] = R::rnorm(means[j], sds[j]);
    }
    return res;
}

/*** R
weights <- c(0.3, 0.7)
means <- c(2, 4)
sds <- c(2, 4)
rmix(10, weights, means, sds)
*/


# Complete the definition of rollmean3()
rollmean3 <- function(x, window = 3) {
    # Add the first window elements of x
    initial_total <- sum(head(x, window))
    # The elements to add at each iteration
    lasts <- tail(x, - window)
    # The elements to remove
    firsts <- head(x, - window)
    # Take the initial total and add the 
    # cumulative sum of lasts minus firsts
    other_totals <- initial_total + cumsum(lasts - firsts)
    # Build the output vector
    c(rep(NA, window - 1), # leading NA
      initial_total / window, # initial mean
      other_totals / window   # other means
      )
}

# From previous step; do not modify
rollmean3 <- function(x, window = 3) {
    initial_total <- sum(head(x, window))   
    lasts <- tail(x, - window)
    firsts <- head(x, - window)
    other_totals <- initial_total + cumsum(lasts - firsts)
    c(rep(NA, window - 1), initial_total / window, other_totals / window)
}

# This checks rollmean1() and rollmean2() give the same result
all.equal(rollmean1(x), rollmean2(x))

# This checks rollmean1() and rollmean3() give the same result
all.equal(rollmean1(x), rollmean3(x))

# Benchmark the performance
microbenchmark(rollmean1(x), rollmean2(x), rollmean3(x), times = 5)


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector rollmean4(NumericVector x, int window) {
    int n = x.size();
    // Set res as a NumericVector of NAs with length n
    NumericVector res(n, NumericVector::get_na());
    // Sum the first window worth of values of x
    double total = 0.0;
    for(int i = 0; i < window; i++) {
        total += x[i];
    }
    // Treat the first case seperately
    res[window - 1] = total / window;
    // Iteratively update the total and recalculate the mean 
    for(int i = window; i < n; i++) {
        // Remove the (i - window)th case, and add the ith case
        total += - x[i-window] + x[i];
        // Calculate the mean at the ith position
        res[i] = total / window;
    }
    return res;  
}

/*** R
# Compare rollmean2, rollmean3 and rollmean4   
set.seed(42)
x <- rnorm(1e4)
microbenchmark(rollmean2(x, 4), rollmean3(x, 4), rollmean4(x, 4), times = 5)   
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector na_locf2(NumericVector x) {
    // Initialize to NA
    double current = NumericVector::get_na();
    int n = x.size();
    NumericVector res = no_init(n);
    for(int i = 0; i < n; i++) {
        // If ith value of x is NA
        if(NumericVector::is_na(x[i])) {
            // Set ith result as current
            res[i] = current
        } else {
            // Set current as ith value of x
            current = x[i];
            res[i] = x[i]
        }
    }
    return res ;
}

/*** R
library(microbenchmark)
set.seed(42)
x <- rnorm(1e5)
# Sprinkle some NA into x
x[sample(1e5, 100)] <- NA  
microbenchmark(na_locf1(x), na_locf2(x), times = 5)
*/


#include <Rcpp.h>
using namespace Rcpp; 

// [[Rcpp::export]]
NumericVector na_meancf2(NumericVector x) {
    double total_not_na = 0.0;
    double n_not_na = 0.0;
    NumericVector res = clone(x);
    int n = x.size();
    for(int i = 0; i < n; i++) {
        // If ith value of x is NA
        if(NumericVector::is_na(x[i])) {
            // Set the ith result to the total of non-missing values 
            // divided by the number of non-missing values
            res[i] = total_not_na / n_not_na;
        } else {
            // Add the ith value of x to the total of non-missing values
            total_not_na += x[i];
            // Add 1 to the number of missing values
            n_not_na ++;
        }
    }
    return res;
}

/*** R
library(microbenchmark)
set.seed(42)
x <- rnorm(1e5)
x[sample(1e5, 100)] <- NA  
microbenchmark(na_meancf1(x), na_meancf2(x), times = 5)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector ar2(int n, double c, NumericVector phi, double eps) {
    int p = phi.size();
    NumericVector x(n);
    // Loop from p to n
    for(int i = p; i < n; i++) {
        // Generate a random number from the normal distribution
        double value = R::rnorm(c, eps);
        // Loop from zero to p
        for(int j = 0; j < p; j++) {
            // Increase by the jth element of phi times 
            // the "i minus j minus 1"th element of x
            value += phi[j] * x[i-j-1];
        }
        x[i] = value;
    }
    return x;
}

/*** R
d <- data.frame(x = 1:50, y = ar2(50, 10, c(1, -0.5), 1))
ggplot(d, aes(x, y)) + 
    geom_line()
*/


#include <Rcpp.h>
using namespace Rcpp ;

// [[Rcpp::export]]
NumericVector ma2( int n, double mu, NumericVector theta, double sd ){
    int q = theta.size(); 
    NumericVector x(n);
    // Generate the noise vector
    NumericVector eps = rnorm(n, 0.0, sd);
    // Loop from q to n
    for(int i = q; i < n; i++) {
        // Value is mean plus noise
        double value = mu + eps[i];
        // Loop from zero to q
        for(int j = 0; j < q; j++) {
            // Increase by the jth element of theta times
            // the "i minus j minus 1"th element of eps
            value += theta[j] * eps[i - j - 1];
        }
        // Set ith element of x to value
        x[i] = value;
    }
    return x ;
}

/*** R
d <- data.frame(x = 1:50, y = ma2(50, 10, c(1, -0.5), 1))
ggplot(d, aes(x, y)) + 
    geom_line()
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector arma(int n, double mu, NumericVector phi, NumericVector theta, double sd) {
    int p = phi.size();
    int q = theta.size();
    NumericVector x(n);
    // Generate the noise vector
    NumericVector eps = rnorm(n, 0.0, sd);
    // Start at the max of p and q plus 1
    int start = std::max(p, q) + 1;
    // Loop i from start to n
    for(int i = start; i < n; i++) {
        // Value is mean plus noise
        double value = mu + eps[i];
        // The MA(q) part
        for(int j = 0; j < q; j++) {
            // Increase by the jth element of theta times
            // the "i minus j minus 1"th element of eps
            value += theta[j] * eps[i - j - 1];
        }
        // The AR(p) part
        for(int j = 0; j < p; j++) {
            // Increase by the jth element of phi times
            // the "i minus j minus 1"th element of x
            value += phi[j] * x[i - j - 1];
        }
        x[i] = value;
    }
    return x;
}

/*** R
d <- data.frame(x = 1:50, y = arma(50, 10, c(1, -0.5), c(1, -0.5), 1))
ggplot(d, aes(x, y)) + 
    geom_line()
*/

```
  
  
  
***
  
###_Regression Modeling in R: Case Studies_  
  
Chapter 1 - GLMs  
  












