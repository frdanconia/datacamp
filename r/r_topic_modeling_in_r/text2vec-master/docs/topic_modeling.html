<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Dmitriy Selivanov" />

<meta name="date" content="2018-12-21" />

<title>Topic modeling</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #0000ff; } /* Keyword */
code > span.ch { color: #008080; } /* Char */
code > span.st { color: #008080; } /* String */
code > span.co { color: #008000; } /* Comment */
code > span.ot { color: #ff4000; } /* Other */
code > span.al { color: #ff0000; } /* Alert */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #008000; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #008080; } /* SpecialChar */
code > span.vs { color: #008080; } /* VerbatimString */
code > span.ss { color: #008080; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #0000ff; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ff4000; } /* Preprocessor */
code > span.do { color: #008000; } /* Documentation */
code > span.an { color: #008000; } /* Annotation */
code > span.cv { color: #008000; } /* CommentVar */
code > span.at { } /* Attribute */
code > span.in { color: #008000; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">text2vec</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="vectorization.html">Vectorization</a>
</li>
<li>
  <a href="glove.html">GloVe</a>
</li>
<li>
  <a href="collocations.html">Collocations</a>
</li>
<li>
  <a href="topic_modeling.html">Topic modeling</a>
</li>
<li>
  <a href="similarity.html">Similarity</a>
</li>
<li>
  <a href="api.html">API</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-language"></span>
     
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="http://text2vec.org/">English</a>
    </li>
    <li>
      <a href="https://cndocr.github.io/text2vec-doc-cn/">Chinese</a>
    </li>
  </ul>
</li>
<li>
  <a href="https://github.com/dselivanov/text2vec">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="http://stackoverflow.com/questions/tagged/text2vec">
    <span class="fa fa-stack-overflow"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Topic modeling</h1>
<h4 class="author"><em>Dmitriy Selivanov</em></h4>
<h4 class="date"><em>2018-12-21</em></h4>

</div>


<p>Topic modeling is technique to extract abstract topics from a collection of documents. In order to do that input Document-Term matrix usually decomposed into 2 low-rank matrices: document-topic matrix and topic-word matrix.</p>
<div id="latent-semantic-analysis" class="section level1">
<h1>Latent Semantic Analysis</h1>
<p>Latent Semantic Analysis is the oldest among topic modeling techniques. It decomposes Document-Term matrix into a product of 2 <strong>low rank</strong> matrices <span class="math inline">\(X \approx D \times T\)</span>. Goal of LSA is to receive approximation with a <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Low-rank_matrix_approximation">respect to minimize Frobenious norm</a>: <span class="math inline">\(error = \left\lVert X - D \times T \right\rVert _F\)</span>. Turns out this can be done with <strong>truncated SVD</strong> decomposition.</p>
<p><code>text2vec</code> borrows SVD from <a href="https://github.com/dselivanov/rsparse">rsparse</a> package and adds convenient interface with an ability to fit model and apply it to new data.</p>
<div id="example" class="section level2">
<h2>Example</h2>
<p>As usual we will use built-in <code>text2vec::moview_review</code> dataset. Let’s clean it a bit and create DTM:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(stringr)
<span class="kw">library</span>(text2vec)
<span class="kw">data</span>(<span class="st">&quot;movie_review&quot;</span>)
<span class="co"># select 1000 rows for faster running times</span>
movie_review_train =<span class="st"> </span>movie_review[<span class="dv">1</span><span class="op">:</span><span class="dv">700</span>, ]
movie_review_test =<span class="st"> </span>movie_review[<span class="dv">701</span><span class="op">:</span><span class="dv">1000</span>, ]
prep_fun =<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="co"># make text lower case</span>
  x =<span class="st"> </span><span class="kw">str_to_lower</span>(x)
  <span class="co"># remove non-alphanumeric symbols</span>
  x =<span class="st"> </span><span class="kw">str_replace_all</span>(x, <span class="st">&quot;[^[:alpha:]]&quot;</span>, <span class="st">&quot; &quot;</span>)
  <span class="co"># collapse multiple spaces</span>
  x =<span class="st"> </span><span class="kw">str_replace_all</span>(x, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">s+&quot;</span>, <span class="st">&quot; &quot;</span>)
}
movie_review_train<span class="op">$</span>review =<span class="st"> </span><span class="kw">prep_fun</span>(movie_review_train<span class="op">$</span>review)
it =<span class="st"> </span><span class="kw">itoken</span>(movie_review_train<span class="op">$</span>review, <span class="dt">progressbar =</span> <span class="ot">FALSE</span>)
v =<span class="st"> </span><span class="kw">create_vocabulary</span>(it)
v =<span class="st"> </span><span class="kw">prune_vocabulary</span>(v, <span class="dt">doc_proportion_max =</span> <span class="fl">0.1</span>, <span class="dt">term_count_min =</span> <span class="dv">5</span>)
vectorizer =<span class="st"> </span><span class="kw">vocab_vectorizer</span>(v)
dtm =<span class="st"> </span><span class="kw">create_dtm</span>(it, vectorizer)</code></pre></div>
<p>Now we will perform tf-idf scaling and fit LSA model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tfidf =<span class="st"> </span>TfIdf<span class="op">$</span><span class="kw">new</span>()
lsa =<span class="st"> </span>LSA<span class="op">$</span><span class="kw">new</span>(<span class="dt">n_topics =</span> <span class="dv">10</span>)

<span class="co"># pipe friendly transformation</span>
doc_embeddings =<span class="st">  </span><span class="kw">fit_transform</span>(dtm, tfidf)
doc_embeddings =<span class="st">  </span><span class="kw">fit_transform</span>(doc_embeddings, lsa)</code></pre></div>
<pre><code>## INFO [2018-12-21 20:44:49] soft_als: iter 001, frobenious norm change 1.986
## INFO [2018-12-21 20:44:49] soft_als: iter 002, frobenious norm change 1.658
## INFO [2018-12-21 20:44:49] soft_als: iter 003, frobenious norm change 0.115
## INFO [2018-12-21 20:44:49] soft_als: iter 004, frobenious norm change 0.042
## INFO [2018-12-21 20:44:49] soft_als: iter 005, frobenious norm change 0.022
## INFO [2018-12-21 20:44:49] soft_als: iter 006, frobenious norm change 0.013
## INFO [2018-12-21 20:44:49] soft_als: iter 007, frobenious norm change 0.008
## INFO [2018-12-21 20:44:49] soft_als: iter 008, frobenious norm change 0.005
## INFO [2018-12-21 20:44:49] soft_als: iter 009, frobenious norm change 0.004
## INFO [2018-12-21 20:44:49] soft_als: iter 010, frobenious norm change 0.003
## INFO [2018-12-21 20:44:49] soft_als: iter 011, frobenious norm change 0.002
## INFO [2018-12-21 20:44:49] soft_als: iter 012, frobenious norm change 0.001
## INFO [2018-12-21 20:44:49] soft_als: iter 013, frobenious norm change 0.001
## INFO [2018-12-21 20:44:49] soft_als: iter 014, frobenious norm change 0.001
## INFO [2018-12-21 20:44:49] soft_impute: converged with tol 0.001000 after 14 iter
## INFO [2018-12-21 20:44:49] running final svd
## INFO [2018-12-21 20:44:49] final rank = 10</code></pre>
<p><code>doc_embeddings</code> contains matrix with document embeddings (document-topic matrix) and <code>lsa$components</code> contains topic-word matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(doc_embeddings)</code></pre></div>
<pre><code>## [1] 700  10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(lsa<span class="op">$</span>components)</code></pre></div>
<pre><code>## [1]   10 3029</code></pre>
<p>Usually we need not only analyze a fixed dataset, but also apply model to new data. For instance we may need to embed unseen documents into the same latent space in order to use their representation in some downstream task (for example classification). <code>text2vec</code> keep in mind such task from the very first days of development. We can elegantly <strong>perform exactly the same transformation on the new data</strong> with <code>transform()</code> method (and possibly with <code>%&gt;%</code> pipe):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new_data =<span class="st"> </span>movie_review_test
it =<span class="st"> </span><span class="kw">itoken</span>(new_data<span class="op">$</span>review, <span class="dt">preprocessor =</span> prep_fun, <span class="dt">progressbar =</span> <span class="ot">FALSE</span>)
new_doc_embeddings =<span class="st"> </span><span class="kw">create_dtm</span>(it, vectorizer)
  <span class="co"># apply exaxtly same scaling wcich was used in train data</span>
new_doc_embeddings =<span class="st"> </span><span class="kw">transform</span>(new_doc_embeddings, tfidf)
  <span class="co"># embed into same space as was in train data</span>
new_doc_embeddings =<span class="st"> </span><span class="kw">transform</span>(new_doc_embeddings, lsa)
<span class="kw">dim</span>(new_doc_embeddings)</code></pre></div>
<pre><code>## [1] 300  10</code></pre>
</div>
<div id="pros-and-cons" class="section level2">
<h2>Pros and cons</h2>
<p><strong>Pros:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>LSA</strong> is easy to train and tune (no hyperparameters except rank)</li>
<li>Embeddings usually work fine in dowstream tasks such as clusterization, classification, regression, similarity-search</li>
</ol>
<p><strong>Cons:</strong></p>
<ol style="list-style-type: decimal">
<li>Major drawback is that embeddings are <strong>not interpretable</strong> (components might be negative)</li>
<li>Could be quite slow to train on <strong>very large</strong> collections of documents</li>
<li>The probabilistic model of LSA does not match observed data: LSA assumes that words and documents form a joint Gaussian model (ergodic hypothesis), while a Poisson distribution has been observed</li>
</ol>
</div>
</div>
<div id="latent-dirichlet-allocation" class="section level1">
<h1>Latent Dirichlet Allocation</h1>
<p><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA (Latent Dirichlet Allocation)</a> model also decomposes document-term matrix into two low-rank matrices - document-topic distribution and topic-word distribution. Bit it is more complex <strong>non-linear generative model</strong>. We won’t go into gory details behind LDA probabilistic model, reader can find a lot of material on the internet. For example <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">wikipedia article</a> is pretty good. We will rather focus on practical details.</p>
<p>There several important hyper-parameters:</p>
<ol style="list-style-type: decimal">
<li><code>n_topics</code> - Number of latent topics.</li>
<li><code>doc_topic_prior</code> - document-topic prior. Normally a number less than 1, e.g. 0.1, to prefer sparse topic distributions, i.e. few topics per document.</li>
<li><code>topic_word_prior</code> - topic-word prior. Normally a number much less than 1, e.g. 0.001, to strongly prefer sparse word distributions, i.e. few words per topic.</li>
</ol>
<p>LDA in <code>text2vec</code> is implemented using iterative sampling algorithm - it improves log-likelihood with every pass over the data. So user can set <code>convergence_tol</code> parameter for early stopping - algorithm will stop iteration if improvement is not significant. For example setting <code>lda$fit_transform(x, n_iter = 1000, convergence_tol = 1e-3, n_check_convergence = 10)</code> will stop earlier if log-likelihood at iteration <code>n</code> is within 0.1% of the log-likelihood of iteration <code>n - 10</code>.</p>
<div id="remark-on-implementation" class="section level3">
<h3>Remark on implementation</h3>
<p><code>text2vec</code> implementation is based on the state-of-the-art <a href="https://arxiv.org/abs/1510.08628">WarpLDA</a> sampling algorithm. It has O(1) sampling complexity which means <strong>run-time does not depend on the number of topics</strong>. Current implementation is single-threaded and reasonably fast. However it can be improved in future versions.</p>
</div>
<div id="example-1" class="section level3">
<h3>Example</h3>
<p>Let us create topic model with <code>10</code> topics:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tokens =<span class="st"> </span><span class="kw">tolower</span>(movie_review<span class="op">$</span>review[<span class="dv">1</span><span class="op">:</span><span class="dv">4000</span>])
tokens =<span class="st"> </span><span class="kw">word_tokenizer</span>(tokens)
it =<span class="st"> </span><span class="kw">itoken</span>(tokens, <span class="dt">ids =</span> movie_review<span class="op">$</span>id[<span class="dv">1</span><span class="op">:</span><span class="dv">4000</span>], <span class="dt">progressbar =</span> <span class="ot">FALSE</span>)
v =<span class="st"> </span><span class="kw">create_vocabulary</span>(it)
v =<span class="st"> </span><span class="kw">prune_vocabulary</span>(v, <span class="dt">term_count_min =</span> <span class="dv">10</span>, <span class="dt">doc_proportion_max =</span> <span class="fl">0.2</span>)
  
vectorizer =<span class="st"> </span><span class="kw">vocab_vectorizer</span>(v)
dtm =<span class="st"> </span><span class="kw">create_dtm</span>(it, vectorizer, <span class="dt">type =</span> <span class="st">&quot;dgTMatrix&quot;</span>)

lda_model =<span class="st"> </span>LDA<span class="op">$</span><span class="kw">new</span>(<span class="dt">n_topics =</span> <span class="dv">10</span>, <span class="dt">doc_topic_prior =</span> <span class="fl">0.1</span>, <span class="dt">topic_word_prior =</span> <span class="fl">0.01</span>)
doc_topic_distr =<span class="st"> </span>
<span class="st">  </span>lda_model<span class="op">$</span><span class="kw">fit_transform</span>(<span class="dt">x =</span> dtm, <span class="dt">n_iter =</span> <span class="dv">1000</span>, 
                          <span class="dt">convergence_tol =</span> <span class="fl">0.001</span>, <span class="dt">n_check_convergence =</span> <span class="dv">25</span>, 
                          <span class="dt">progressbar =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## INFO [2018-12-21 20:45:01] early stopping at 275 iteration
## INFO [2018-12-21 20:45:03] early stopping at 50 iteration</code></pre>
<p>Now <code>doc_topic_distr</code> matrix represents distribution of topics in documents. Each row is document and values are proportions of corresponding topics.</p>
<p>For example topic distribution for first document:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">barplot</span>(doc_topic_distr[<span class="dv">1</span>, ], <span class="dt">xlab =</span> <span class="st">&quot;topic&quot;</span>, 
        <span class="dt">ylab =</span> <span class="st">&quot;proportion&quot;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), 
        <span class="dt">names.arg =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(doc_topic_distr))</code></pre></div>
<p><img src="topic_modeling_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="describing-topics---top-words" class="section level2">
<h2>Describing topics - top words</h2>
<p>Also we can get top words for each topic. They can be sorted by probability of the chance to observe word in a given topic (<code>lambda = 1</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_model<span class="op">$</span><span class="kw">get_top_words</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">topic_number =</span> <span class="kw">c</span>(1L, 5L, 10L), <span class="dt">lambda =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>##       [,1]        [,2]     [,3]      
##  [1,] &quot;know&quot;      &quot;war&quot;    &quot;director&quot;
##  [2,] &quot;horror&quot;    &quot;before&quot; &quot;films&quot;   
##  [3,] &quot;why&quot;       &quot;years&quot;  &quot;quite&quot;   
##  [4,] &quot;your&quot;      &quot;still&quot;  &quot;while&quot;   
##  [5,] &quot;worst&quot;     &quot;world&quot;  &quot;little&quot;  
##  [6,] &quot;guy&quot;       &quot;man&quot;    &quot;horror&quot;  
##  [7,] &quot;nothing&quot;   &quot;match&quot;  &quot;though&quot;  
##  [8,] &quot;something&quot; &quot;best&quot;   &quot;such&quot;    
##  [9,] &quot;scene&quot;     &quot;these&quot;  &quot;may&quot;     
## [10,] &quot;ever&quot;      &quot;part&quot;   &quot;enough&quot;</code></pre>
<p>Also top-words could be sorted by “relevance” which also takes into account frequency of word in the corpus (<code>0 &lt; lambda &lt; 1</code>). From my experience in most cases setting <code>0.2 &lt; lambda &lt; 0.4</code> works best. See <a href="http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf">LDAvis: A method for visualizing and interpreting topics</a> paper for details.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_model<span class="op">$</span><span class="kw">get_top_words</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">topic_number =</span> <span class="kw">c</span>(1L, 5L, 10L), <span class="dt">lambda =</span> <span class="fl">0.2</span>)</code></pre></div>
<pre><code>##       [,1]      [,2]         [,3]        
##  [1,] &quot;zombie&quot;  &quot;war&quot;        &quot;atmosphere&quot;
##  [2,] &quot;gore&quot;    &quot;match&quot;      &quot;thriller&quot;  
##  [3,] &quot;stupid&quot;  &quot;anti&quot;       &quot;viewer&quot;    
##  [4,] &quot;cheesy&quot;  &quot;army&quot;       &quot;page&quot;      
##  [5,] &quot;flick&quot;   &quot;green&quot;      &quot;narrative&quot; 
##  [6,] &quot;horror&quot;  &quot;jackson&quot;    &quot;identity&quot;  
##  [7,] &quot;zombies&quot; &quot;hitler&quot;     &quot;images&quot;    
##  [8,] &quot;slasher&quot; &quot;soldiers&quot;   &quot;france&quot;    
##  [9,] &quot;killer&quot;  &quot;historical&quot; &quot;engaging&quot;  
## [10,] &quot;worst&quot;   &quot;kelly&quot;      &quot;caine&quot;</code></pre>
</div>
<div id="apply-learned-model-to-new-data" class="section level2">
<h2>Apply learned model to new data</h2>
<p>As with other decompositions we can apply model to new data and obtain document-topic distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">it =<span class="st"> </span><span class="kw">itoken</span>(movie_review<span class="op">$</span>review[<span class="dv">4001</span><span class="op">:</span><span class="dv">5000</span>], tolower, word_tokenizer, <span class="dt">ids =</span> movie_review<span class="op">$</span>id[<span class="dv">4001</span><span class="op">:</span><span class="dv">5000</span>])
new_dtm =<span class="st">  </span><span class="kw">create_dtm</span>(it, vectorizer, <span class="dt">type =</span> <span class="st">&quot;dgTMatrix&quot;</span>)
new_doc_topic_distr =<span class="st"> </span>lda_model<span class="op">$</span><span class="kw">transform</span>(new_dtm)</code></pre></div>
<pre><code>## INFO [2018-12-21 20:45:06] early stopping at 40 iteration</code></pre>
</div>
<div id="cross-validation-and-hyper-parameter-tuning" class="section level2">
<h2>Cross-validation and hyper-parameter tuning</h2>
<p>One widely used approach for model hyper-parameter tuning is validation of per-word <strong>perplexity</strong> on hold-out set. This is quite easy with <code>text2vec</code>.</p>
<div id="perplexity-example" class="section level3">
<h3>Perplexity example</h3>
<p>Remember that we’ve fitted model on first 4000 reviews (learned <code>topic_word_distribution</code> which will be fixed during <code>transform</code> phase) and predicted last 1000. We can calculate perplexity on these 1000 docs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">perplexity</span>(new_dtm, <span class="dt">topic_word_distribution =</span> lda_model<span class="op">$</span>topic_word_distribution, <span class="dt">doc_topic_distribution =</span> new_doc_topic_distr)</code></pre></div>
<pre><code>## [1] 2552.401</code></pre>
<p>The lower perplexity the better. Goal could to find set of hyper-parameters (<code>n_topics</code>, <code>doc_topic_prior</code>, <code>topic_word_prior</code>) which minimize per-word perplexity on hold-out dataset. However it is worth to keep in mind that perplexity is not always correlated with people judgement about topics interpretability and coherence. I personally usually use <strong>LDAvis</strong> for parameter tuning - see next section.</p>
</div>
</div>
<div id="visualization" class="section level2">
<h2>Visualization</h2>
<p>Finally <code>text2vec</code> wraps <code>LDAvis</code> package in order to provide interactive tool for topic exploration. Usually it worth to play with it in order to find meaningfull model hyperparameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lda_model<span class="op">$</span><span class="kw">plot</span>()</code></pre></div>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>
LDAvis
</title>
<script src="topic_modeling_files/ldavis/d3.v3.js"></script>
<script src="topic_modeling_files/ldavis/ldavis.js"></script>
<link rel="stylesheet" type="text/css" href="topic_modeling_files/ldavis/lda.css">
</head>
<body>
<div id="lda">

</div>
<script>
    var vis = new LDAvis("#lda", "topic_modeling_files/ldavis/lda.json");
  </script>
</body>
</div>
</div>





<footer class="footer">
  <div class="text-muted"><strong>text2vec</strong> is created by <a href="http://www.dsnotes.com">Dmitry Selivanov</a> and contributors. &copy;  2016.</div>
  <div class="text-muted"> If you have found any BUGS please report them <a href="https://github.com/dselivanov/text2vec/issues">here</a>.</div>
</footer>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');


  ga('create', 'UA-56994099-2', 'auto');
  ga('send', 'pageview');


</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
