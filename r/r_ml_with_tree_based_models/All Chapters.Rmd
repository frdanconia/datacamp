---
title: 'ML with Tree based Models'
author: "Amit Agni"
date: "24/02/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(rpart,data.table,rpart.plot,here,caret,Metrics,ipred, randomForest,gbm)
```

***

### Chapter 1 : Classification Trees

**Prepare Dataset**
* We will use heart data from the UCI ML library

* Attribute Information   
    1. age  
    2. sex  
    3. chest pain type (4 values)   
    4. resting blood pressure   
    5. serum cholestoral in mg/dl   
    6. fasting blood sugar > 120 mg/dl   
    7. resting electrocardiographic results (values 0,1,2)  
    8. maximum heart rate achieved   
    9. exercise induced angina 
    10. oldpeak = ST depression induced by exercise relative to rest 
    11. the slope of the peak exercise ST segment 
    12. number of major vessels (0-3) colored by flourosopy 
    13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 
    14. Target



```{r}


heartDT <- fread("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat",
                 col.names = c("age","sex","pain_type","bp","cholestoral","sugar","ecg","maxheartrate","angina","oldpeak","slopepeak","noofvessels","thal","attack"))

heartDT[,attack := factor(attack)]

set.seed(1)
ind <- sample(seq_len(nrow(heartDT)),size = 0.8*nrow(heartDT))
trainDT <- heartDT[ind]
testDT <- heartDT[-ind]


```

* Decision tree :
    + Root Node
    + Internal nodes
    + Lead nodes

* Recursive partitioning using rpart package

* rpart package to fit the decision tree and the rpart.plot package to visualize the tree

* Changing the numeric to factor did not change the tree 

* **In the rpart.plot, Each node shows**
    - the predicted class (1 or 2),
    - the predicted probability of the opposite class
    - the percentage of observations in the node.


```{r}

nrow(trainDT)
model <-rpart(attack~.,
              data=trainDT,
              method = "class",
              minsplit = 20)

rpart.plot(model)

#fact_cols <- c("sex","pain_type","sugar","ecg","noofvessels","thal","attack")
#trainDT[, (fact_cols) := lapply(.SD,function(x) { as.factor(as.character(x))}), .SDcols = fact_cols]


```

If extra argument is specified, we can get the number and percentage of observations 
```{r}
rpart.plot(x = model, extra = 101)

```


We can also show the tree in a simplified form as below
```{r}
rpart.plot(x = model, 
           yesno = 2, type = 0, extra = 0)

```




**Building classification tree**

* Advantages
    + No normalisation / transformation
    + No dummies required for categorical
    + Groups also handled by some algos
    + Missing data handled handling by some algos
    + Robust to outliers
    + Recursively partitions
    
* Disadvantages
    + High variance, small change in data can result in totally different splits  
    + If not tuned properly, can result in overfitting
    


**Evaluating model**

* type = class or type = prob  
* Methods  
    + Accuracy  
    + Confusion Matrix  
    + Logloss  
    + AUC  
* White diagonal is good, red diagonal is bad  

![](`r here("confusion_matrix.jpeg")`)
<br>
<br>

**Note that the target has to be a factor else caret gives  error : # this gives "Error: `data` and `reference` should be factors with the same levels."**



```{r}

predictions <- predict(model,
                       newdata = testDT,
                       type="class")

caret::confusionMatrix(data=predictions,
                       reference = testDT$attack)


```


**Splitting criteria**

* How to determine best split
    + More homogeneous sub-sets = More pure 
    + Minimise Impurity measure : Gini Index
    + Other measures include Entropy, Misclassification rates etc
    
```{r}

# Train a gini-based model
m1 <- rpart(formula = attack ~ ., 
                       data = trainDT, 
                       method = "class",
                       parms = list(split = "gini"))

# Train an information-based model
m2 <- rpart(formula = attack ~ ., 
                       data = trainDT, 
                       method = "class",
                       parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = m1, 
             newdata = testDT,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = m2, 
             newdata = testDT,
             type = "class")


# Compare classification error
ModelMetrics::ce(actual = testDT$attack, 
   predicted = pred1)

ModelMetrics::ce(actual = testDT$attack, 
   predicted = pred2)  


```


***



### Chapter 2 : Regression Trees

**Prepare dataset**


```{r}

DT <- fread(here("student","student-mat.csv"),sep=";",header=TRUE)

temp <- c("sex","age","address","studytime","schoolsup","famsup","paid","absences","G3")

grade <- DT[,..temp]

setnames(grade,old="G3",new="final_grade")



```


**splitting dataset into train,validate and test**

```{r}

# Set seed and create assignment
set.seed(1)
assignment <- sample(1:3, size = nrow(grade), prob = c(0.7,0.15,0.15), replace = TRUE)


# Create a train, validation and tests from the original data frame 
grade_train <- grade[assignment == 1, ]    # subset grade to training indices only
grade_valid <- grade[assignment == 2, ]  # subset grade to validation indices only
grade_test <- grade[assignment == 3, ]   # subset grade to test indices only

```




*  Homegenity in classfication is measured using entropy (or gini) but for regression it is undefined. Hence, in regression trees its measures by variance or std deviation or absolute mean deviation
    + method = "anova"

>The goal of this exercise is to predict a student's final Mathematics grade based on the following variables: sex, age, address, studytime (weekly study time), schoolsup (extra educational support), famsup (family educational support), paid (extra paid classes within the course subject) and absences.

* In the rpart.plot, Each node shows
    - **the predicted value**,
    - the percentage of observations in the node.

```{r}
# Train the model
grade_model <- rpart(formula = final_grade ~ ., 
                     data = grade_train, 
                     method = "anova")

# Look at the model output                      
print(grade_model)

# Plot the tree model
rpart.plot(x = grade_model, 
           yesno = 2, type = 0, extra = 0)

rpart.plot(x = grade_model)

```

With the number of observations. 
```{r}
nrow(trainDT)
rpart.plot(x = grade_model, extra = 101)

```



**Performance metrics for regression**  

* Commonly used :  
    + $MAE = \frac{1}{n}\sum \left | actual -predicted \right |$  
    
    + RMSE is sample standard deviation between actual and predicted. 
    
    + $RMSE = \sqrt{\frac{1}{n}}\sum  \left ( actual - predicted \right ) ^{2}$
    + **Metrics package**

```{r}
# Generate predictions on a test set
pred <- predict(object = grade_model,   # model object 
                newdata = grade_test)  # test dataset

# Compute the RMSE
rmse(actual = grade_test$final_grade, 
     predicted = pred)
```

**Hyperparameters for a Decision Tree**

* Control paramter can be used to the hyperparameters :
    + minsplit : minimum no of data points required to attempt a split (default = 20)
    + cp : complexity paramter (default = 0.1)
    + maxdepth : max no of nodes between the root node and the leaf node (default = 30)


* **Cost-Complexity Parameter (CP)**  
    + Penatly term to control the tree size  
    + Always monotonic with the number of splits
    + Small the value the more complex tree
    + rpart uses 10 fold cross validation to determine the optimal value of cp
    + plotcp(model) : Plot the cross validated error accross diff values of cp. Optimal value of CP
    + print(model$cptable) : Extract the optimal value of cp where xerror is the minimum
    + prune(tree = model, cp = cp_optimal) :  Prune the model to the optimised value 

    
```{r}
# Plot the "CP Table"
plotcp(grade_model)

# Print the "CP Table"
print(grade_model$cptable)

# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(grade_model$cptable[, "xerror"])
cp_opt <- grade_model$cptable[opt_index, "CP"]
cp_opt

# Prune the model (to optimized cp value)
grade_model_opt <- prune(tree = grade_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)
```

**Grid Search for model selection**

* Use expand.grid() to generate a grid of maxdepth and minsplit values.
* Alternatively use caret

```{r}

# Establish a list of possible values for minsplit and maxdepth
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

# Print the number of grid combinations
num_models<-nrow(hyper_grid)


# Create an empty list to store models
grade_models <- list()


# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    # Get minsplit, maxdepth values at row i
    minsplit <- hyper_grid$minsplit[i]
    maxdepth <- hyper_grid$maxdepth[i]

    # Train a model and store in the list
    grade_models[[i]] <- rpart(formula = final_grade ~ ., 
                               data = grade_train, 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}


# Number of potential models in the grid
num_models <- length(grade_models)

# Create an empty vector to store RMSE values
rmse_values <- c()

# Write a loop over the models to compute validation RMSE
for (i in 1:num_models) {

    # Retrieve the i^th model from the list
    model <- grade_models[[i]]
    
    # Generate predictions on grade_valid 
    pred <- predict(object = model,
                    newdata = grade_valid)
    
    # Compute validation RMSE and add to the 
    rmse_values[i] <- rmse(actual = grade_valid$final_grade, 
                           predicted = pred)
}

# Identify the model with smallest validation set RMSE
best_model <- grade_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_model$control

# Compute test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = grade_test)
rmse(actual = grade_test$final_grade, 
     predicted = pred)

```


***



### Chapter 3 : Bagged Trees


* Decision trees suffer from **high variance**, a small change in data results in completely different splits

* Bagging = **B**ootstrap **AGG**regatat**ING**
    + Averages many trees to reduce variance (one form of ensemble methods). Also helps in reducing overfitting
    + Bootstrap sampling : With replacement, means single training sample can be picked up more than once
    + Average the predicted values accross the bootstrapped trees
    + Averaging reduces variance and leaves bias as is


> If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the **coob parameter to TRUE**. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the bagging() function).


[Source for below | SE](https://stats.stackexchange.com/questions/321687/how-is-bagging-different-from-cross-validation)

> The big difference between bagging and validation techniques is that bagging averages models (or predictions of an ensemble of models) in order to reduce the variance the prediction is subject to while resampling validation such as cross validation and out-of-bootstrap validation evaluate a number of surrogate models assuming that they are equivalent (i.e. a good surrogate) for the actual model in question which is trained on the whole data set.

>Bagging uses bootstrapped subsets (i.e. drawing with replacement of the original data set) of training data to generate such an ensemble but you can also use ensembles that are produced by drawing without replacement, i.e. cross validation: 

>Whether an ensemble model can be better than single models depends entirely on what the dominant "problem" of the single model is. If it is variance (overfitting, random error, unstable predictions), then ensemble prediction can help. If the problem is bias (systematic error, underfitting, stable but wrong predictions), pretty much all models of the ensemble will give the same prediction and the ensemble prediction is just as wrong.

>Both out-of-bootstrap and iterated/repeated cross validation allow to measure the stability of predictions by comparing predictions for the same input (test) data by a number of different surrogate models. These surrogate models differ in that they were trained on slightly different data sets, that can be described as exchanging a few training cases between any two of the surrogate models for other training cases.


* **bagging()  from the ipred package**. The number of bagged trees can be specified using the nbagg parameter (default = 25)


_Will use the heart attack dataset_

```{r}
# Bagging is a randomized model, so let's set a seed (123) for reproducibility
set.seed(123)

# Train a bagged model
model <- bagging(formula = attack ~ ., 
                        data = trainDT,
                        coob = TRUE)

# Print the model
print(model)


```

**Evaluating Performance of bagged tree models**

* Accuracy improved from 0.72 to 0.77

```{r}

# Generate predicted classes using the model object
class_prediction <- predict(object = model,    
                            newdata = testDT,  
                            type = "class")  # return classification labels

# Print the predicted classes
print(class_prediction)

# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = testDT$attack)  

```

**Computing AUC (area under ROC) **

> The predicted label is generated by applying a threshold to the predicted value, such that all tests points with predicted value greater than that threshold get a predicted label of "1" and, points below that threshold get a predicted label of "0".


**TO DO : Find the AUC for the model in chapter 1**


```{r}


# Generate predictions on the test set
pred <- predict(object = model,
                newdata = testDT,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(testDT$attack == 1, 1, 0), 
    predicted = pred[,1])  

bagged_AUC <- auc(actual = ifelse(testDT$attack == 1, 1, 0), 
    predicted = pred[,1])  

```


**Cross-validation using caret**

* Variability in data due to size of the sample 
* K-fold cross validation, k separate learning experiments
* More useful for smaller datasets as larger datasets will take time to train
* Use caret::train() with the "treebag" method to train a model and evaluate the model using cross-validated AUC. 

* caret was giving Error :At least one of the class levels is not a valid R variable name. Hence the factor levels were converted.

* The AUC improved

```{r}

# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility

str(trainDT)

levels(trainDT$attack) <- c("Yes","No")

caret_model <- train(attack ~ .,
                            data = trainDT, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(caret_model)

# Inspect the contents of the model list 
names(caret_model)

# Print the CV AUC
caret_model$results[,"ROC"]


```


**Generate predictions from the caret model**

AUC improved to 0.898

```{r}

# Generate predictions on the test set
pred <- predict(object = caret_model, 
                newdata = testDT,
                type = "prob")

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(testDT$attack == 1, 1, 0), 
                    predicted = pred[,"Yes"])


caret_auc <- auc(actual = ifelse(testDT$attack == 1, 1, 0), 
                    predicted = pred[,"Yes"])

```

**Compare test set performance to CV performance**

>In this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.

>The credit_ipred_model_test_auc object stores the test set AUC from the model trained using the ipred::bagging() function.
>The credit_caret_model_test_auc object stores the test set AUC from the model trained using the caret::train() function with method = "treebag".
Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the credit_caret_model object. This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.

>On small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced. When using small data, it's recommended to use cross-validated estimates of performance because they are more stable.


```{r}

# Print ipred::bagging test set AUC estimate
print(bagged_AUC)

# Print caret "treebag" test set AUC estimate
print(caret_auc)
                
# Compare to caret 5-fold cross-validated AUC
# Print the CV AUC
caret_model$results[,"ROC"]



```



***




### Chapter 4 : Random Forest

**Introduction**
Erin LeDell

* Improvement upon bagged trees
* Both Bagged trees and RFs are ensembles of trees trained on bootstrapped samples of the training data
* In Random Forest, only a subset of features are selected at random at each split in a decision tree. In bagging, all features are used.
* This means the trees are different and there is a reduced correlation between the sampled trees

* default ntrees = 500 
* More trees almost always means better performance


```{r}

# Train a Random Forest
set.seed(1)  # for reproducibility
model <- randomForest(formula = attack ~ ., 
                             data = trainDT,
                      ntree = 500)
                             
# Print the model output                             
print(model)


```


**Understanding model output**

* `mtry` : No. of variables tried at each split: 3
    + default = determined dynamically, in classification forest is the $\sqrt{no of features}$


* Confusion matrix is based on the OOB samples


* **Out of Bag OOB estimate**
    + Error rate computed across the samples that were not selected in the bootstrap training sets
    + Since each tree in the RF is trained on the bootstrapped sample of the original training set. This means some samples will be duplicated in the bootstrapped set and some will be absent.
    + The absent samples are form the Out of Bag set
    + RF provides you with a built-in validation set without any extra work. Since OOB samples were not used to train the trees, they can be used to evaulate the models performance on the unseen data. 
    + The classification error across all the OOB samples is called the OOB error. The OOB error matrix is stored in RF model and the rows represent the number of trees
    + ith row represents the OOB error for all the trees upto and including the ith tree. 
    + In classification, the first column shows the error across the "classes" and the additional columns shows the error for the individual error per class
    + The last row is the final OOB error which is the same value printed in the model output
   

    
```{r}

# Grab OOB error matrix & take a look
err <- model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)



```
    
    
     
* Plot of OOB error rate vs number of trees in the forest, helps to decide how many trees to include in the ensemble
* Too many trees, computing predictions for each tree takes time so do not include ore trees than you actually need
    
```{r}

# Plot the model trained in the previous exercise
plot(model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))

```


**Evaulate model performance**

Use the caret::confusionMatrix() function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.

* OOB error is a quick builtin snap shot of the model performance
* Adv :
    + Provided with validation set, so you can evaulate mode without a separate test set
    + OOB error is computed builtin
* DisAdv :
    + OOB error only estimated error (not AUC or log-loss etc)
    + RF keep track of the samples that were part of OOB
    + Cant compare RF to other models if builtin sample is used a validation set


**TO DO : Why the Test accuracy is lower than OOB accuracy**

```{r}

# Generate predicted classes using the model object
class_prediction <- predict(object = model,   # model object 
                            newdata = testDT,  # test dataset
                            type = "class") # return classification labels
                            
class_prediction
testDT$attack

levels(testDT$attack) <-  c("Yes","No")

# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
                      reference = testDT$attack)  # actual classes
print(cm)

# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)



```

**Evaluate test set AUC**


    
```{r}

# Generate predictions on the test set
pred <- predict(object = model,
            newdata = testDT,
            type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(testDT$attack == "Yes", 1, 0), 
    predicted = pred[,"Yes"])                    



```


**Tuning a RF model**

* Most important hyperparatmeters
    + `ntree` : default of 500 is recommended
    + `mtry` : no of variables (features) sampled as candidates at each split
    + `sampsize` : no of samples to train on (63.2%)
    + `mtry` and `sampsize` control the variability or randomness goes into the model
    + `nodesize` : minimum size (no of samples) of the terminal nodes. When nodesize is small, allows deeper more complex trees
    + `maxnodes` : max no of terminal nodes, limit tree growth and avoid overfitting
    + `nodesize` and maxnodes control the complexity of the tree.

* **mtry with tuneRF()**
    + In RF, we choose the variable (feature) that splits the data into most pure form
    + **`tuneRF()`** tunes the model based on OOB error
    + Increases the mtry stepsize by default = 
    + Use the `tuneRF()` function in place of the `randomForest()` function to train a series of models with different mtry values and examine the the results
    + Note that (unfortunately) the `tuneRF()` interface does not support the typical formula input that we've been using, but instead uses two arguments, x (matrix or data frame of predictor variables) and y (response vector; must be a factor for classification).
    + If you just want to return the best RF model (rather than results) you can set **`doBest = TRUE`** in `tuneRF()` to return the best RF model instead of a set performance matrix.
    + Keep in mind that if we want to evaluate the model based on AUC instead of error (accuracy), then this is not the best way to tune a model, as the selection only considers (OOB) error.


```{r}

# Execute the tuning process
set.seed(1)              
res <- tuneRF(x = subset(trainDT, select = -attack),
              y = trainDT$attack,
              ntreeTry = 500)
               
# Look at results
print(res)

# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]

print(mtry_opt)




```


**Tuning a Random Forest via tree depth**

> In Chapter 2, we created a manual grid of hyperparameters using the expand.grid() function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of mtry, nodesize and sampsize values. In this example, we will identify the "best model" based on OOB error. The best model is defined as the model from our grid which minimizes OOB error.

> Keep in mind that there are other ways to select a best model from a grid, such as choosing the best model based on validation AUC. **However, for this exercise, we will use the built-in OOB error calculations instead of using a separate validation set.**



```{r}

# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(4, ncol(trainDT) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- round(nrow(trainDT) * c(0.7, 0.8))

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()



# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

  
                          
    # Train a Random Forest model
    model <- randomForest(formula = attack ~ ., 
                          data = trainDT,
                          
                           mtry = hyper_grid$mtry[i],
                           nodesize = hyper_grid$nodesize[i],
                           sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])


```


### Chapter 5 : Boosting

Boosting is an iterative algorithm that considers past fits to improve performance.

* Tree based ensemble method
* Adaboost - longer, easier to understand
* GBM - new and popular

* Adaboost Algorithm
    + Train decision tree with equal weight
    + Increase the weights of the observations that are difficult to classify & lower the weights of the observations that are easy to classify
    + Grow second tree on the weighted data
        + To improve upon the predictions of Tree 1
        + **New model : Tree 1 + Tree 2**
    + Compute the Classification error from this new model and
    + Grow 3rd tree to predict the revised residuals
    + Repeat for specified number of times
        + Subsequent tree help in classifying the observations that are not classifed by the preceding trees
        + Prediction of the final ensemble model is the weighted sum of the predictions made by previous tree models
        
* Gradient Boosting 
    + Gradient Descent + Boosting
    + Fit an additive model (ensemble) in a forward, stage-wise manner
    + In each stage, introduce a "weak learning" (eg decision tree) to compensate the shortcomings of existing weak learners
    + In Adaboost, "shortcomings" are identified by high-weight data points
    + In Gradient Boosting, the "shortcomings" are identified by the gradients

* Advantages of GBM
    + If properly tune can even outperform best deep learning algorithms
    + Whereas other models optimise loss functions, GBM can optimise user defined cost function

* Disdvantages of GBM
    + Overfits, need to find a proper stopping point
    + Sensitive to extreme values and noises

>Here you will use the gbm() function to train a GBM classifier to predict loan default. You will train a 10,000-tree GBM on the credit_train dataset, which is pre-loaded into your workspace.

>Using such a large number of trees (10,000) is probably not optimal for a GBM model, but we will build more trees than we need and then select the optimal number of trees based on early performance-based stopping. The best GBM model will likely contain fewer trees than we started with.

>For binary classification, gbm() requires the response to be encoded as 0/1 (numeric), so we will have to convert from a "no/yes" factor to a 0/1 numeric response column.

>Also, the the gbm() function requires the user to specify a distribution argument. For a binary classification problem, you should set distribution = "bernoulli". The Bernoulli distribution models a binary response.

 **GBM output : Variables with noise or no correlation with the response variable will show up here**

```{r}
#Distribution of response variable

# Convert "yes" to 1, "no" to 0
trainDT$attack <- ifelse(trainDT$attack == "Yes", 1, 0)

# Train a 10000-tree GBM model
set.seed(1)
model <- gbm(formula = attack ~ ., 
                    distribution = "bernoulli", 
                    data = trainDT,
                    n.trees = 50000)

# Print the model object                    
print(model)

# summary() prints variable importance
summary(model)


```



>One thing that's particular to the predict.gbm() however, is that you need to specify the number of trees used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).

>Another argument that you can specify is type, which is only relevant to Bernoulli and Poisson distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it's on the log scale. If instead you specify type = "response", then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.

```{r}

# Since we converted the training response col, let's also convert the test response col
testDT$attack <- ifelse(testDT$attack == "Yes", 1, 0)

# Generate predictions on the test set
preds1 <- predict(object = model, 
                  newdata = testDT,
                  n.trees = 10000)

# Generate predictions on the test set (scale to response)
preds2 <- predict(object = model, 
                  newdata = testDT,
                  n.trees = 10000,
                  type = "response")

# Compare the range of the two sets of predictions
range(preds1)
range(preds2)


```

**Evaluate test set AUC**

>Compute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That's because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.

>**However, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.**

**Why has the AUC has reduced !!! **

```{r}

# Generate the test set AUCs using the two sets of preditions & compare
auc(actual = testDT$attack, predicted = pred1)  #default
auc(actual = testDT$attack, predicted = pred2) #rescaled


```

** GBM Hyperparamters - Tuning GBM**

* GBM are prone to overfitting
* Tuning is commonly used in iterative algorithms

* Reducing or shrinking the impact of the addition of each tree. Penalises the importance of each step.

![](`r here("gbm_hyperparamters.jpeg")`)

### Early Stopping
![](`r here("early_stopping.jpeg")`)

GBM uses "error" for early stopping purposes but other implementations like h2o or xgboost allows auc or logloss

>Use the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that's called "early stopping". The term "early stopping" is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.

> OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds>1 when calling gbm usually results in improved predictive performance.


** OOB estimated only 1 tree :( **

```{r}

# Optimal ntree estimate based on OOB
ntree_opt_oob <- gbm.perf(object = model, 
                          method = "OOB", 
                          oobag.curve = TRUE)

# Train a CV GBM model
set.seed(1)
model_cv <- gbm(formula = attack ~ ., 
                       distribution = "bernoulli", 
                       data = trainDT,
                       n.trees = 10000,
                       cv.folds = 5)

# Optimal ntree estimate based on CV
ntree_opt_cv <- gbm.perf(object = model_cv, 
                         method = "cv")
 
# Compare the estimates                         
print(paste0("Optimal n.trees (OOB Estimate): ", ntree_opt_oob))                         
print(paste0("Optimal n.trees (CV Estimate): ", ntree_opt_cv))



```

>In the previous exercise, we used OOB error and cross-validated error to estimate the optimal number of trees in the GBM. These are two different ways to estimate the optimal number of trees, so in this exercise we will compare the performance of the models on a test set. We can use the same model object to make both of these estimates since the predict.gbm() function allows you to use any subset of the total number of trees (in our case, the total number is 10,000).

** Weird the AUC has now improved !!!**

```{r}

# Generate predictions on the test set using ntree_opt_oob number of trees
preds1 <- predict(object = model, 
                  newdata = testDT,
                  n.trees = ntree_opt_oob)
                  
# Generate predictions on the test set using ntree_opt_cv number of trees
preds2 <- predict(object = model, 
                  newdata = testDT,
                  n.trees = ntree_opt_cv)   

# Generate the test set AUCs using the two sets of preditions & compare
auc1 <- auc(actual = testDT$attack, predicted = preds1)  #OOB
auc2 <- auc(actual = testDT$attack, predicted = preds2)  #CV 

# Compare AUC 
print(paste0("Test set AUC (OOB): ", auc1))                         
print(paste0("Test set AUC (CV): ", auc2))



```


** Model comparison accross all models learnt so far**

* Decision Trees  
* Bagged Trees  
* Random Forest  
* Gradient Boosting Machine (GBM).

> In this case, we see that the Random Forest performed the best on the test set. With a bit more tuning of the GBM, the performance might be closer to that of the Random Forest. To save time, we only used 2-fold cross-validation to choose the optimal number of trees, but we encourage you to use more folds (at least 5 or 10) in practice. With more folds, you will have a better estimate of the optimal number of trees.


>Plot & compare ROC curves

> We conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.

> The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.


>The ROCR package can plot multiple ROC curves on the same plot if you plot several sets of predictions as a list.

>The prediction() function takes as input a list of prediction vectors (one per model) and a corresponding list of true values (one per model, though in our case the models were all evaluated on the same test set so they all have the same set of true values). The prediction() function returns a "prediction" object which is then passed to the performance() function.

>The performance() function generates the data necessary to plot the curve from the "prediction" object. For the ROC curve, you will also pass along two measures, "tpr" and "fpr".

>Once you have the "performance" object, you can plot the ROC curves using the plot() method. We will add some color to the curves and a legend so we can tell which curves belong to which algorithm.

```{r eval = FALSE}


# List of predictions
preds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(credit_test$default), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)



```


# Summary

***



### Chapter 1 : Classification

* `rpart()` and `rpart.plot()`  

* `caret::confusionMatrix()`  

* specify split criteria using parms eg  
    + `rpart(formula = attack ~ .,data = trainDT,method = "class",parms = list(split = "gini"))`  
                       
* Compare classification error using  
    + `ModelMetrics::ce(actual = testDT$attack,predicted = pred1)`  
    + `ModelMetrics::ce(actual = testDT$attack,predicted = pred2)`  

***


### Chapter 2 : Regression

* Splitting into train,validate and test  
    + `ind <- sample(1:3, size = nrow(grade), prob = c(0.7,0.15,0.15), replace = TRUE)`  
    + `DT[ind == 1], DT[ind == 2] and DT[ind == 3]`
    
* **`rpart.plot()`** by default shows percentage of observations and 
    + For classification : the predicted class and the predicted probability of the **opposite class**
    + For regression : **the predicted value**,

* The **`extra = 101` parameter in `rpart.plot()`**, will show the number of observations

* **`Metrics`** package has commonly used performance metrics

* Hyperparamters of Decision tree : minsplit, maxdepth, cp

* **Cost-Complexity Parameter (CP)**  
    + Penatly term to control the tree size  
    + Always monotonic with the number of splits
    + Smaller the value the more complex tree
    + rpart uses 10 fold cross validation to determine the optimal value of cp
    + **`plotcp(model)`** : Plots the cross validated error accross diff values of cp. 
    + **`print(model$cptable)`** : Extract the optimal value of cp where xerror is the minimum
    + **`prune(tree = model, cp = cp_optimal)`** :  Prune the model to the optimised value   
    
    
* **Grid Search for Model selection**
    + Use the **`base::expand.grid()`** to create cartesian product of all hyperparamter values and then use a loop to find the model having best performance metric value
    + `hyper_grid <- expand.grid(minsplit = seq(1,4,1), maxdepth = seq(1,6,1))`
    + alternatively, use caret

***


### Chapter 3 : Bagged Trees

* Bagging = **B**ootstrap **AGG**regatat**ING**
    + Bootstrap sampling : With replacement, means single training sample can be picked up more than once
    + Averages the predicted values across many bootstrapped trees to reduce the variance. 

* ipred::bagging()
    + `nbagg` (default = 25) specifies the number of bagged trees
    + `coob = TRUE` (default = FALSE) will estimate the model's accuracy using "out of bag" OOB samples.
    + Out-of-bag estimates help avoid the need for an independent validation dataset
    + `bagged_model <- bagging(formula = attack ~ ., data = trainDT,coob = TRUE)`
* Cross-validation using caret
    + Use `caret::train()` with the “treebag” method to train a model and evaluate the model using cross-validated AUC.
    + `ctrl <- trainControl(method = "cv", number = 5,`  
                        `classProbs = TRUE,`                  # For AUC  
                        `summaryFunction = twoClassSummary)`  # For AUC  
    + `caret_model <- train(attack ~ .,data = trainDT, method = "treebag", metric = "ROC",trControl = ctrl)`

* Bagging vs Cross validation
    + Bagging averages models (or predictions of an ensemble of models) in order to reduce the variance the prediction is subject to. Bagging is with replacement.
    + Cross validation and out-of-bootstrap validation evaluate a number of surrogate models assuming that they are equivalent (i.e. a good surrogate) for the actual model in question which is trained on the whole data set. Cross validation is without replacement
    
***
    

### Chapter 4 : Random Forest

* Both Bagged trees and RFs are ensembles of trees trained on bootstrapped samples of the training data
* In Random Forest, only a subset of features are selected at random at each split in a decision tree. In bagging, all features are used.
* This means the trees are different and there is a reduced correlation between the sampled trees
* default `ntrees = 500` but more trees almost always means better performance
    
* `model <- randomForest(formula = attack ~ ., data = trainDT,ntree = 500)`

* **Out of Bag OOB estimate**
    + Since each tree in the RF is trained on the bootstrapped sample of the original training set. This means some samples will be duplicated in the bootstrapped set and some will be absent. The absent samples are form the Out of Bag set
    + RF provides you with a built-in validation set without any extra work. Since OOB samples were not used to train the trees, they can be used to evaulate the models performance on the unseen data. 
    + The classification error across all the OOB samples is called the OOB error. The OOB error matrix is stored in RF model for each tree
        + which can be displayed using **`model$err.rate`**
    + ith row represents the OOB error for all the trees upto and including the ith tree. 
    + In classification, the first column shows the error across the "classes" and the additional columns shows the error for the individual error per class
    + The last row is the final OOB error which is the same value printed in the model output
    
* **`plot(model)`** Plot of OOB error rate vs number of trees in the forest, helps to decide how many trees to include in the ensemble
* Too many trees, computing predictions for each tree takes time so do not include ore trees than you actually need


**Tuning a RF model**

* Most important hyperparatmeters
    + `ntree` : default of 500 is recommended
    + `mtry` : no of variables (features) sampled as candidates at each split
    + `sampsize` : no of samples to train on (63.2%)
    + `mtry` and `sampsize` control the variability or randomness goes into the model
    + `nodesize` : minimum size (no of samples) of the terminal nodes. When nodesize is small, allows deeper more complex trees
    + `maxnodes` : max no of terminal nodes, limit tree growth and avoid overfitting
    + `nodesize` and `maxnodes` control the complexity of the tree.

* **mtry with tuneRF()**
    + In RF, we choose the variable (feature) that splits the data into most pure form
    + **`tuneRF()`** tunes the model based on OOB error
    + Increases the mtry stepsize by default = 2
    + If you just want to return the best RF model (rather than results) you can set **`doBest = TRUE`** in `tuneRF()` to return the best RF model instead of a set performance matrix.
    + Keep in mind that if we want to evaluate the model based on AUC instead of error (accuracy), then this is not the best way to tune a model, as the selection only considers (OOB) error.
    + `tuneRF(x = subset(trainDT, select = -attack),y = trainDT$attack,ntreeTry = 500)`
    + mtry value that minimizes OOB Error `res[,"mtry"][which.min(res[,"OOBError"])]`


***

### Chapter 5 : Boosting

* Boosting is an interative algorithm that considers past fits to improve performance
    + Adaboost (older, easier to understand) and GBM (newer and popular)  
* `gbm()` requires the response to be encoded as 0/1 (numeric), so we will have to convert from a “no/yes” factor to a 0/1 numeric response column
    + `gbm(formula = attack ~ ., distribution = "bernoulli", data = trainDT, n.trees = 10000)`
    + `predict(object = model,newdata = testDT,n.trees = 10000)` need to specify n.trees
* GBM hyperparamters 
    + `n.trees` = `ntree`
    + `bag.fraction` = `sampsize`
    + `n.minobsinnode` = `nodesize`
    + `interaction.depth` = `maxnodes` ?
    + `shrinkage`  is the learning rate
* GBM uses “error” for early stopping purposes but other implementations like h2o or xgboost allows auc or logloss
* `gbm.perf()` can be usedfunction to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error.
    + `gbm.perf(object = model,method = "OOB",oobag.curve = TRUE)`
    +  `gbm(formula = attack ~ .,distribution = "bernoulli",data = trainDT,n.trees = 10000,cv.folds = 5)`
    + and then optimal ntree estimate based on the CV model `gbm.perf(object = model_cv, method = "cv")`
                         
***
### TO DO
* Cheatsheet
* [Gradient boosting in R](https://datascienceplus.com/gradient-boosting-in-r/)
* [An Introduction to Recursive Partitioning Using the RPART Routines | PDF] (https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)

***

    