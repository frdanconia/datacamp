---
title: 'Chapter 5 : Advanced Methods'
author: "Amit Agni"
date: "21/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(fpp2)
library(here)
```

### Dynamic Regression

One way of combining additional features like effects of holidays, competitor activity, wider economy etc with the time series forecasting

### Forecasting sales allowing for advertising expenditure

>The auto.arima() function will fit a dynamic regression model with ARIMA errors. The only change to how you used it previously is that you will now use the xreg argument containing a matrix of regression variables.

>In this exercise, you will model sales data regressed against advertising expenditure, with an ARMA error to account for any serial correlation in the regression errors. 

```{r}
# Time plot of both variables
#  The variables are on different scales, so use facets = TRUE

autoplot(advert, facets = TRUE)

# Fit ARIMA model
fit <- auto.arima(advert[,"sales"], xreg = advert[, "advert"], stationary = TRUE)

# Check model. Increase in sales for each unit increase in advertising
#Check that the fitted model is a regression with AR(1) errors. What is the increase in sales for every unit increase in advertising? This coefficient is the third element in the coefficients() output.

salesincrease <- coefficients(fit)[3]

# Forecast fit as 
#Forecast from the fitted model specifying the next 6 months of advertising expenditure as 10 units per month as fc. To repeat 10 six times, use the rep() function inside xreg
fc <- forecast(fit, xreg = rep(10,6))

# Plot fc with x and y labels
autoplot(fc) + xlab("Month") + ylab("Sales")

```

>Great job. The dynamic regression allows you to include other outside information into your forecast.


### Forecasting electricity demand (__Using a matrix of multiple indepedent variables for xreg__)


>You can also model daily electricity demand as a function of temperature. As you may have seen on your electric bill, more electricity is used on hot days due to air conditioning and on cold days due to heating.

> In this exercise, you will fit a quadratic regression model with an ARMA error. One year of daily data are stored as elec including total daily demand, an indicator variable for workdays (a workday is represented with 1, and a non-workday is represented with 0), and daily maximum temperatures. Because there is weekly seasonality, __the frequency has been set to 7__.

The elec dataset used in the course has different data in the package, so we will use elecdaily


```{r}

# Time plots of demand and temperatures
autoplot(elecdaily[, c("Demand", "Temperature")], facets = TRUE)

# Matrix of regressors
#Index elec accordingly to set up the matrix of regressors to include MaxTemp for the maximum temperatures, MaxTempSq which represents the squared value of the maximum temperature, and Workday, in that order. 
xreg <- cbind(MaxTemp = elecdaily[, "Temperature"]
              ,MaxTempSq = elecdaily[,"Temperature"]^2
              ,WorkDay = elecdaily[,"WorkDay"])


# Fit model
fit <- auto.arima(elecdaily[,"Demand"], xreg = xreg)

# Forecast fit one day ahead
#If the next day is a working day (indicator is 1) with maximum temperature forecast to be 20°C, what is the forecast demand? Fill out the appropriate values in cbind() for the xreg argument in forecast().
forecast(fit, xreg = cbind(20, 20^2, 1))


```

### Dynamic harmonic regression
>When there are long seasonal periods, a dynamic regression with Fourier terms is often better than other models we have considered in this book.

>For example, daily data can have annual seasonality of length 365, weekly data has seasonal period of approximately 52, while half-hourly data can have several seasonal periods, the shortest of which is the daily pattern of period 48.

>Seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data. The ets() function restricts seasonality to be a maximum period of 24 to allow hourly data but not data with a larger seasonal frequency. The problem is that there are  m−1 parameters to be estimated for the initial seasonal states where m is the seasonal period. So for large m, the estimation becomes almost impossible.

 >The Arima() and auto.arima() functions will allow a seasonal period up to  
m
=
350
 , but in practice will usually run out of memory whenever the seasonal period is more than about 200. In any case, seasonal differencing of high order does not make a lot of sense — for daily data it involves comparing what happened today with what happened exactly a year ago and there is no constraint that the seasonal pattern is smooth.

>So for such time series, we prefer a harmonic regression approach where the seasonal pattern is modelled using Fourier terms with short-term time series dynamics handled by an ARMA error.

 > The advantages of this approach are:

* it allows any length seasonality;  
+ for data with more than one seasonal period, Fourier terms of different frequencies can be included;  
+ the seasonal pattern is smooth for small values of  
K (but more wiggly seasonality can be handled by increasing K);  
+ the short-term dynamics are easily handled with a simple ARMA error.  

>The only real disadvantage (compared to a seasonal ARIMA model) is that the seasonality is assumed to be fixed — the seasonal pattern is not allowed to change over time. But in practice, seasonality is usually remarkably constant so this is not a big disadvantage except for long time series.

> Notes from video  

* Dynamic harmonic regression uses Fourier terms to handle seasonality  
* __Fourier showed that a series of sine and cosine termsof right frequencies can approximate any periodic function__  
* Fourier terms come in pairs, consisting of a sine and a cosine. The frequencies of these terms are called as harmonic frequencies, they increase with k  
* The Fourier terms are predictors in the dynamic regression model, the more terms include in the model the more complicated the model becomes  
* Uppercase K controls the number of terms to include 
* Since the seasonality is handled by the fourier terms, we use non-seasonal ARIMA model for the error  

### Forecasting weekly data

>The fourier() function makes it easy to generate the required harmonics. The higher the order (K), the more "wiggly" the seasonal pattern is allowed to be. With K=1, it is a simple sine curve. You can select the value of K by minimizing the AICc value. As you saw in the video, fourier() takes in a required time series, required number of Fourier terms to generate, and optional number of rows it needs to forecast

```{r}

# Set up harmonic regressors of order 13
#Set up an xreg matrix called harmonics using the fourier() method on gasoline with order K=13 which has been chosen to minimize the AICc.
harmonics <- fourier(gasoline, K = 13)

# Fit regression model with ARIMA errors
fit <- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE)

# Forecasts next 3 years
#Set up a new xreg matrix called newharmonics in a similar fashion, and then compute forecasts for the next three years as fc.
newharmonics <- fourier(gasoline, K = 13, h = 52*3)
fc <- forecast(fit, xreg = newharmonics)

# Plot forecasts fc
autoplot(fc)

```

>Great. The point predictions look to be a bit low.


### Harmonic regression for multiple seasonality

Harmonic regressions are also useful when time series have multiple seasonal patterns. For example, taylor contains half-hourly electricity demand in England and Wales over a few months in the year 2000. The seasonal periods are 48 (daily seasonality) and 7 x 48 = 336 (weekly seasonality). There is not enough data to consider annual seasonality.

auto.arima() would take a long time to fit a long time series such as this one, so instead you will fit a standard regression model with Fourier terms using the tslm() function. This is very similar to lm() but is designed to handle time series. With multiple seasonality, you need to specify the order K for each of the seasonal periods.

```{r}
autoplot(taylor)
# Fit a harmonic regression using order 10 for each type of seasonality
fit <- tslm(taylor ~ fourier(taylor, K = c(10, 10)))

# Forecast 20 working days ahead
#Remember that the data are half-hourly in order to set the correct value for h.
fc <- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10, 10), h = 24*2*20)))

# Plot the forecasts
autoplot(fc)

# Check the residuals of fit
checkresiduals(fit)

```

>As you can see, auto.arima() would have done a better job. Nice job! The residuals from the fitted model fail the tests badly, yet the forecasts are quite good.


### Forecasting call bookings

> Another time series with multiple seasonal periods is calls, which contains 20 consecutive days of 5-minute call volume data for a large North American bank. There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845. The weekly seasonality is relatively weak, so here you will just model daily seasonality

> The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant. This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series. The effect of the remaining correlations on the forecasts will be negligible.


```{r}

# Plot the calls 
#Plot the calls data to see the strong daily seasonality and weak weekly seasonality.

autoplot(calls)

# Set up the xreg matrix
#Set up the xreg matrix using order 10 for daily seasonality and 0 for weekly seasonality.
xreg <- fourier(calls, K = c(10,0))

# Fit a dynamic regression model
#Fit a dynamic regression model called fit using auto.arima() with seasonal = FALSE and stationary = TRUE

fit <- auto.arima(calls, xreg = xreg, seasonal = FALSE, stationary = TRUE)

# Check the residuals
checkresiduals(fit)

# Plot forecasts for 10 working days ahead
#Create the forecasts for 10 working days ahead as fc, and then plot it.
fc <- forecast(fit, xreg =  fourier(calls, K=c(10, 0), h = 169 *10 ))

autoplot(fc)

```

### TBATS 

Fully Automated time series model. 

![](`r here("images","tbats_chp5.jpeg")`)



>a TBATS model is a special kind of time series model. It can be very slow to estimate, especially with multiple seasonal time series, so in this exercise you will try it on a simpler series to save time. Let's break down elements of a TBATS model in TBATS(1, {0,0}, -, {<51.18,14>}), one of the graph titles from the video:

* 1 : Box-Cox transformation paramter  
* {0,0} : ARMA error , p and q  
* \- : Damping parameter, - means no damping  
* {<51.18,14} : Seasonal period of 51.18 and K=14 fourier terms

>The gas data contains Australian monthly gas production. A plot of the data shows the variance has changed a lot over time, so it needs a transformation. The seasonality has also changed shape over time, and there is a strong trend. This makes it an ideal series to test the tbats() function which is designed to handle these features.

```{r}
# Plot the gas data
autoplot(gas)

# Fit a TBATS model to the gas data
fit <- tbats(gas)

# Forecast the series for the next 5 years
fc <- forecast(fit,h=12*5)

# Plot the forecasts
autoplot(fc)

# Record the Box-Cox parameter and the order of the Fourier terms
lambda <- 0.082
K <- 5

```

>Amazing! Just remember that completely automated solutions don't work every time.


TO DO  : Check whether frequency  
1 = Yearly  
12  = Monthly  
52.17 = Weekly 
There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845.
etc  


# Summary
* Dynamic regression allows us to use additional features that can impact the forecasts eg holidays, competitor activity, etc  
* The only change to how you used it previously is that you will now use the __xreg argument__ containing a matrix of regression variables.  
* When there are long seasonal periods, a dynamic regression with Fourier terms is often better than other models we have considered in this book.  
* The __fourier(data,K=n)__ makes it easy to generated the required harmonics, which can then be fed to the __xreg__ argument of __auto.arima()__  
* __tbats()__ is a fully automated time series model  


***
### End of the Course

Other DataCamp Course   

* ARIMA modeling with R  
* Introduction to Time Series Analysis  
* Manipulating Time Series Data in R with xts and zoo








