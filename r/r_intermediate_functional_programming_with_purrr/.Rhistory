library(purrr)
library(dplyr)
visit_a <- c(117, 147, 131, 73, 81, 134, 121)
visit_c <- c(57, 110, 68, 72, 87, 141, 67)
# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)
# Map the mean() function and output a numeric vector
map_dbl(all_visits_day, mean)
# Plot all_tests_day with map
map(all_tests_day, barplot)
# Get sum of all visits and class of sum_all
sum_all <- pmap(all_tests_day, sum)
# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) { x*24 })
# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) { x*24 })
# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)
# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)
# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
# Map to_ten on visit_b
map_dbl(visit_b, to_ten)
# Create a mapper that test if .x is more than 100
is_more_than_hundred <- as_mapper(~ .x > 100)
# Run this mapper on the all_visits object
map(all_visits, ~ keep(.x, is_more_than_hundred) )
full_visits_named <- map(all_visits, ~ set_names(.x, day))
library(purrr)
library(dplyr)
visit_a <- c(117, 147, 131, 73, 81, 134, 121)
visit_b <- c(180, 193, 116, 166, 131, 153, 146)
visit_c <- c(57, 110, 68, 72, 87, 141, 67)
# Create the to_day function
to_day <- function(x) {
x*24
}
# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)
# Convert to daily number of visits: all_visits_day
all_visits_day <- map(all_visits, to_day)
# Map the mean() function and output a numeric vector
map_dbl(all_visits_day, mean)
# Create all_tests list  and modify with to_day() function
all_tests <- list(visit_a, visit_b, visit_c)
all_tests_day <- map(all_tests, to_day)
# Plot all_tests_day with map
map(all_tests_day, barplot)
# Plot all_tests_day
walk(all_tests_day, barplot)
# Get sum of all visits and class of sum_all
sum_all <- pmap(all_tests_day, sum)
class(sum_all)
# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) { x*24 })
# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)
# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)
# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
map(visit_c, to_day)
# Round visit_a to the nearest tenth with a mapper
map_dbl(visit_a, ~ round(.x, -1))
# Create to_ten, a mapper that rounds to the nearest tenth
to_ten <- as_mapper(~ round(.x, -1))
# Map to_ten on visit_b
map_dbl(visit_b, to_ten)
# Map to_ten on visit_c
map_dbl(visit_c, to_ten)
# Create a mapper that test if .x is more than 100
is_more_than_hundred <- as_mapper(~ .x > 100)
# Run this mapper on the all_visits object
map(all_visits, ~ keep(.x, is_more_than_hundred) )
# Use the  day vector to set names to all_list
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
full_visits_named <- map(all_visits, ~ set_names(.x, day))
# Use this mapper with keep()
map(full_visits_named, ~ keep(.x, is_more_than_hundred))
# Set the name of each subvector
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
all_visits_named <- map(all_visits, ~ set_names(.x, day))
# Create a mapper that will test if .x is over 100
threshold <- as_mapper(~.x > 100)
# Run this mapper on the all_visits object: group_over
group_over <- map(all_visits, ~ keep(.x, threshold) )
# Run this mapper on the all_visits object: group_under
group_under <-  map(all_visits, ~ discard(.x, threshold) )
# Create a threshold variable, set it to 160
threshold <- 160
# Create a mapper that tests if .x is over the defined threshold
over_threshold <- as_mapper(~ .x > threshold)
# Are all elements in every all_visits vectors over the defined threshold?
map(all_visits, ~ every(.x, over_threshold))
# Are some elements in every all_visits vectors over the defined threshold?
map(all_visits, ~ some(.x, over_threshold))
# Launch Sys.time(), Sys.sleep(1), & Sys.time()
Sys.time()
Sys.sleep(1)
Sys.sleep(1)
Sys.time()
data(iris)
data(iris)
str(iris)
# Launch nrow(iris), Sys.sleep(1), & nrow(iris)
nrow(iris)
Sys.sleep(1)
Sys.sleep(1)
nrow(iris)
# Create a plot of the iris dataset
plot(iris)
Sys.sleep(1)
nrow(iris)
# Create a plot of the iris dataset
plot(iris)
urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Create a plot of the iris dataset
plot(iris)
urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Create a safe version of read_lines()
safe_read <- safely(readr::read_lines)
# Map it on the urls vector
res <- map(urls, safe_read)
# Map it on the urls vector
res <- map(urls, safe_read)
# Set the name of the results to `urls`
named_res <-  set_names(res, urls)
# Extract only the "error" part of each sublist
map(named_res, "error")
str(iris)
# Launch nrow(iris), Sys.sleep(1), & nrow(iris)
nrow(iris)
Sys.sleep(1)
nrow(iris)
# Create a plot of the iris dataset
plot(iris)
urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Create a safe version of read_lines()
safe_read <- safely(readr::read_lines)
# Map it on the urls vector
res <- map(urls, safe_read)
# Set the name of the results to `urls`
named_res <-  set_names(res, urls)
# Extract only the "error" part of each sublist
map(named_res, "error")
# Code a function that discard() the NULL from safe_read()
safe_read_discard <- function(url){
safe_read(url) %>%
discard(is.null)
}
# Map this function on the url list
res <- map(urls, safe_read_discard)
# Create a possibly() version of read_lines()
possible_read <- possibly(readr::read_lines, otherwise = 404)
# Create a possibly() version of read_lines()
possible_read <- possibly(readr::read_lines, otherwise = 404)
# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)
# Paste each element of the list
res_pasted <- map(res, paste, collapse=" ")
# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)
# Map this function on the url list
res <- map(urls, safe_read_discard)
# Create a possibly() version of read_lines()
possible_read <- possibly(readr::read_lines, otherwise = 404)
# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)
# Paste each element of the list
res_pasted <- map(res, paste, collapse=" ")
# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)
url_tester <- function(url_list){
url_list %>%
# Map a version of read_lines() that otherwise returns 404
map( possibly(readr::read_lines, otherwise = 404) ) %>%
# Set the names of the result
set_names( urls ) %>%
# paste() and collapse each element
map(paste, collapse =" ") %>%
# Remove the 404
discard(~.x==404) %>%
names() # Will return the names of the good ones
}
# Paste each element of the list
res_pasted <- map(res, paste, collapse=" ")
# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)
url_tester <- function(url_list){
url_list %>%
# Map a version of read_lines() that otherwise returns 404
map( possibly(readr::read_lines, otherwise = 404) ) %>%
# Set the names of the result
set_names( urls ) %>%
# paste() and collapse each element
map(paste, collapse =" ") %>%
# Remove the 404
discard(~.x==404) %>%
names() # Will return the names of the good ones
}
# Try this function on the urls object
url_tester(urls)
url_tester <- function(url_list){
url_list %>%
# Map a version of read_lines() that otherwise returns 404
map( possibly(readr::read_lines, otherwise = 404) ) %>%
# Set the names of the result
set_names( urls ) %>%
# paste() and collapse each element
map(paste, collapse =" ") %>%
# Remove the 404
discard(~.x==404) %>%
names() # Will return the names of the good ones
}
# Try this function on the urls object
url_tester(urls)
url_tester <- function(url_list, type = c("result", "error")){
res <- url_list %>%
# Create a safely() version of read_lines()
map( safely(readr::read_lines) ) %>%
set_names( url_list ) %>%
# Transpose into a list of $result and $error
transpose()
# Complete this if statement
if (type == "result") return( res$result )
if (type == "error") return( res$error )
}
# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)
url_tester <- function(url_list){
url_list %>%
# Map a version of read_lines() that otherwise returns 404
map( possibly(readr::read_lines, otherwise = 404) ) %>%
# Set the names of the result
set_names( urls ) %>%
# paste() and collapse each element
map(paste, collapse =" ") %>%
# Remove the 404
discard(~.x==404) %>%
names() # Will return the names of the good ones
}
# Try this function on the urls object
url_tester(urls)
url_tester <- function(url_list, type = c("result", "error")){
res <- url_list %>%
# Create a safely() version of read_lines()
map( safely(readr::read_lines) ) %>%
set_names( url_list ) %>%
# Transpose into a list of $result and $error
transpose()
# Complete this if statement
if (type == "result") return( res$result )
if (type == "error") return( res$error )
}
# Try this function on the urls object
url_tester(urls, type = "error")
url_tester <- function(url_list){
url_list %>%
# Map a version of GET() that would otherwise return NULL
map( possibly(httr::GET, otherwise=NULL) ) %>%
# Set the names of the result
set_names( urls ) %>%
# Remove the NULL
compact() %>%
# Extract all the "status_code" elements
map("status_code")
}
# Try this function on the urls object
url_tester(urls)
urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Compose a status extractor (compose is also an igraph function)
status_extract <- purrr::compose(httr::status_code, httr::GET)
# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")
status_extract("http://datacamp.com")
# Map it on the urls vector, return a vector of numbers
oldUrls <- urls
# Map it on the urls vector, return a vector of numbers
oldUrls <- urls
urls <- oldUrls[c(1, 2, 4, 5)]
map_dbl(urls, status_extract)
status_extract("http://datacamp.com")
# Map it on the urls vector, return a vector of numbers
oldUrls <- urls
urls <- oldUrls[c(1, 2, 4, 5)]
map_dbl(urls, status_extract)
urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Compose a status extractor (compose is also an igraph function)
status_extract <- purrr::compose(httr::status_code, httr::GET)
# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")
# Map it on the urls vector, return a vector of numbers
oldUrls <- urls
urls <- oldUrls[c(1, 2, 4, 5)]
map_dbl(urls, status_extract)
# Negate the %in% function
`%not_in%` <- negate(`%in%`)
# Compose a status extractor
status_extract <- purrr::compose(httr::status_code, httr::GET)
# Complete the function
strict_code <- function(url){
code <- status_extract(url)
if (code %not_in% c(200:203)){ return(NA) } else { return(code) }
}
# Map the strict_code function on the urls vector
res <- map(urls, strict_code)
# Map the strict_code function on the urls vector
res <- map(urls, strict_code)
# Set the names of the results using the urls vector
res_named <- set_names(res, urls)
# Set the names of the results using the urls vector
res_named <- set_names(res, urls)
# Negate the is.na function
is_not_na <- negate(is.na)
# Run is_not_na on the results
is_not_na(res_named)
# Prefill html_nodes() with the css param set to h2
get_h2 <- partial(rvest::html_nodes, css="h2")
# Combine the html_text, get_h2 and read_html functions
get_content <- purrr::compose(rvest::html_text, get_h2, xml2::read_html)
# Map get_content to the urls list
res <- map(urls, get_content) %>%
set_names(urls)
# Map get_content to the urls list
res <- map(urls, get_content) %>%
set_names(urls)
# Print the results to the console
res
# Print the results to the console
res
# Create a partial version of html_nodes(), with the css param set to "a"
a_node <- partial(rvest::html_nodes, css="a")
# Create href(), a partial version of html_attr()
href <- partial(rvest::html_attr, name = "href")
# Combine href(), a_node(), and read_html()
get_links <- purrr::compose(href, a_node, xml2::read_html)
# Map get_links() to the urls list
res <- map(urls, get_links) %>%
set_names(urls)
# Map get_links() to the urls list
res <- map(urls, get_links) %>%
set_names(urls)
df <- tibble::tibble(urls=urls)
df
# Create a "links" columns, by mapping get_links() on urls
df2 <- df %>%
mutate(links = map(urls, get_links))
# Create a "links" columns, by mapping get_links() on urls
df2 <- df %>%
mutate(links = map(urls, get_links))
# Print df2 to see what it looks like
df2
df
# Create a "links" columns, by mapping get_links() on urls
df2 <- df %>%
mutate(links = map(urls, get_links))
# Print df2 to see what it looks like
df2
# unnest() df2 to have a tidy dataframe
df2 %>%
tidyr::unnest()
rstudioconfDF <- readRDS("#RStudioConf.RDS")
dim(rstudioconfDF)
rstudioconf <- as.list(as.data.frame(t(rstudioconfDF)))
length(rstudioconf)
length(rstudioconf[[1]])
# Print the first element of the list to the console
rstudioconf[[1]]
# Print the first element of the list to the console
rstudioconf[[1]]
# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")
# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")
# Extract the favorite count element of each non_rt sublist
fav_count <- map_dbl(non_rt, "favorite_count")
# Get the median of favorite_count for non_rt
median(fav_count)
# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
map("user_id") %>%
unique()
# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")
# Extract the favorite count element of each non_rt sublist
fav_count <- map_dbl(non_rt, "favorite_count")
# Get the median of favorite_count for non_rt
median(fav_count)
# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
map("user_id") %>%
unique()
# Remove the RT, extract the user id, remove the duplicate
non_rt <- discard(rstudioconf, "is_retweet") %>%
map("user_id") %>%
unique()
# Determine the number of users who has just retweeted
setdiff(rt, non_rt) %>% length()
# Prefill mean() with na.rm, and round() with digits = 1
mean_na_rm <- partial(mean, na.rm=TRUE)
round_one <- partial(round, digits=1)
# Compose a rounded_mean function
rounded_mean <- purrr::compose(round_one, mean_na_rm)
# Extract the non retweet
non_rt <- discard(rstudioconf, "is_retweet")
# Extract "favorite_count", and pass it to rounded_mean()
map_dbl(non_rt, "favorite_count") %>%
rounded_mean()
# Combine as_vector(), compact(), and flatten()
flatten_to_vector <- purrr::compose(as_vector, compact, flatten)
# Complete the fonction
extractor <- function(list, what = "mentions_screen_name"){
map(list, what) %>%
flatten_to_vector()
}
# Create six_most, with tail(), sort(), and table()
six_most <- purrr::compose(tail, sort, table)
# Run extractor() on rstudioconf
extractor(rstudioconf) %>%
six_most()
# Extract the "urls_url" elements, and flatten() the result
urls_clean <- map(rstudioconf, "urls_url") %>%
flatten()
# Remove the NULL
compact_urls <- compact(urls_clean)
compact_urls <- discard(urls_clean, is.na)  # Due to creation of the list above, NULL became NA
# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x, "github"))
# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
sum()
# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
sum()
# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x, "github"))
library(stringr)
# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x, "github"))
# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
sum()
# Complete the function
ratio_pattern <- function(vec, pattern){
n_pattern <- str_detect(vec, pattern) %>% sum()
n_pattern / length(vec)
}
# Create flatten_and_compact()
extraDiscard <- function(x) { discard(x, is.na) }  # address same NA issue as above
flatten_and_compact <- purrr::compose(compact, extraDiscard, flatten)
# Complete the pipe to get the ratio of URLs with "github"
map(rstudioconf, "urls_url") %>%
flatten_and_compact() %>%
ratio_pattern("github")
# Create mean_above, a mapper that tests if .x is over 3.3
mean_above <- as_mapper(~ .x > 3.3)
# Prefil map_at() with "retweet_count", mean_above for above,
# and mean_above negation for below
above <- partial(map_at, .at = "retweet_count", .f = mean_above )
below <- partial(map_at, .at = "retweet_count", .f = negate(mean_above) )
# Get the max() of "retweet_count"
max_rt <- map_dbl(non_rt, "retweet_count") %>%
max()
