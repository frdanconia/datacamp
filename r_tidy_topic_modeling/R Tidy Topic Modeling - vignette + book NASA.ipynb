{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#How-can-we-restore-disorganized-chapters-to-their-original-books?\" data-toc-modified-id=\"How-can-we-restore-disorganized-chapters-to-their-original-books?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>How can we restore disorganized chapters to their original books?</a></span></li><li><span><a href=\"#Latent-Dirichlet-Allocation-with-the-topicmodels-package\" data-toc-modified-id=\"Latent-Dirichlet-Allocation-with-the-topicmodels-package-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Latent Dirichlet Allocation with the topicmodels package</a></span></li><li><span><a href=\"#Per-document-classification\" data-toc-modified-id=\"Per-document-classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Per-document classification</a></span></li><li><span><a href=\"#Which-chapters-were-misidentified?\" data-toc-modified-id=\"Which-chapters-were-misidentified?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Which chapters were misidentified?</a></span></li><li><span><a href=\"#By-word-assignments:-augment\" data-toc-modified-id=\"By-word-assignments:-augment-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>By word assignments: augment</a></span></li><li><span><a href=\"#What-were-the-most-commonly-mistaken-words?\" data-toc-modified-id=\"What-were-the-most-commonly-mistaken-words?-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>What were the most commonly mistaken words?</a></span></li><li><span><a href=\"#Case-study:-mining-NASA-metadata\" data-toc-modified-id=\"Case-study:-mining-NASA-metadata-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Case study: mining NASA metadata</a></span></li><li><span><a href=\"#How-data-is-organized-at-NASA\" data-toc-modified-id=\"How-data-is-organized-at-NASA-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>How data is organized at NASA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Wrangling-and-tidying-the-data\" data-toc-modified-id=\"Wrangling-and-tidying-the-data-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Wrangling and tidying the data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cran.r-project.org/web/packages/tidytext/vignettes/topic_modeling.html\n",
    "\n",
    "- Topic modeling is a method for unsupervised classification of documents, by modeling each document as a mixture of topics and each topic as a mixture of words. Latent Dirichlet allocation is a particularly popular method for fitting a topic model: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation.\n",
    "\n",
    "- We can use tidy text principles, as described in the main vignette (https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html), to approach topic modeling. In particular, we’ll be using **tidying functions for LDA objects** from the `topicmodels` package: https://cran.r-project.org/web/packages/topicmodels/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we restore disorganized chapters to their original books?\n",
    "- Great Expectations by Charles Dickens\n",
    "- The War of the Worlds by H.G. Wells\n",
    "- Twenty Thousand Leagues Under the Sea by Jules Verne\n",
    "- Pride and Prejudice by Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(gutenbergr)\n",
    "titles <- c(\"Twenty Thousand Leagues under the Sea\", \"The War of the Worlds\",\n",
    "            \"Pride and Prejudice\", \"Great Expectations\")\n",
    "books <- gutenberg_works(title %in% titles) %>% gutenberg_download(meta_fields = \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>gutenberg_id</th><th scope=col>text</th><th scope=col>title</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36                                                 </td><td>The War of the Worlds                              </td><td>The War of the Worlds                              </td></tr>\n",
       "\t<tr><td>36                                                 </td><td>                                                   </td><td>The War of the Worlds                              </td></tr>\n",
       "\t<tr><td>36                                                 </td><td>by H. G. Wells [1898]                              </td><td>The War of the Worlds                              </td></tr>\n",
       "\t<tr><td>36                                                 </td><td>                                                   </td><td>The War of the Worlds                              </td></tr>\n",
       "\t<tr><td>36                                                 </td><td>                                                   </td><td>The War of the Worlds                              </td></tr>\n",
       "\t<tr><td>36                                                 </td><td>     But who shall dwell in these worlds if they be</td><td>The War of the Worlds                              </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " gutenberg\\_id & text & title\\\\\n",
       "\\hline\n",
       "\t 36                                                  & The War of the Worlds                               & The War of the Worlds                              \\\\\n",
       "\t 36                                                  &                                                     & The War of the Worlds                              \\\\\n",
       "\t 36                                                      & by H. G. Wells {[}1898{]}                               & The War of the Worlds                                  \\\\\n",
       "\t 36                                                  &                                                     & The War of the Worlds                              \\\\\n",
       "\t 36                                                  &                                                     & The War of the Worlds                              \\\\\n",
       "\t 36                                                  &      But who shall dwell in these worlds if they be & The War of the Worlds                              \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "gutenberg_id | text | title | \n",
       "|---|---|---|---|---|---|\n",
       "| 36                                                  | The War of the Worlds                               | The War of the Worlds                               | \n",
       "| 36                                                  |                                                     | The War of the Worlds                               | \n",
       "| 36                                                  | by H. G. Wells [1898]                               | The War of the Worlds                               | \n",
       "| 36                                                  |                                                     | The War of the Worlds                               | \n",
       "| 36                                                  |                                                     | The War of the Worlds                               | \n",
       "| 36                                                  |      But who shall dwell in these worlds if they be | The War of the Worlds                               | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  gutenberg_id text                                               \n",
       "1 36           The War of the Worlds                              \n",
       "2 36                                                              \n",
       "3 36           by H. G. Wells [1898]                              \n",
       "4 36                                                              \n",
       "5 36                                                              \n",
       "6 36                But who shall dwell in these worlds if they be\n",
       "  title                \n",
       "1 The War of the Worlds\n",
       "2 The War of the Worlds\n",
       "3 The War of the Worlds\n",
       "4 The War of the Worlds\n",
       "5 The War of the Worlds\n",
       "6 The War of the Worlds"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>gutenberg_id</th><th scope=col>text</th><th scope=col>title</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1400                                                                    </td><td>\"And will continue friends apart,\" said Estella.                        </td><td>Great Expectations                                                      </td></tr>\n",
       "\t<tr><td>1400                                                                    </td><td>                                                                        </td><td>Great Expectations                                                      </td></tr>\n",
       "\t<tr><td>1400                                                                    </td><td>I took her hand in mine, and we went out of the ruined place; and, as   </td><td>Great Expectations                                                      </td></tr>\n",
       "\t<tr><td>1400                                                                    </td><td>the morning mists had risen long ago when I first left the forge, so the</td><td>Great Expectations                                                      </td></tr>\n",
       "\t<tr><td>1400                                                                    </td><td>evening mists were rising now, and in all the broad expanse of tranquil </td><td>Great Expectations                                                      </td></tr>\n",
       "\t<tr><td>1400                                                                    </td><td>light they showed to me, I saw no shadow of another parting from her.   </td><td>Great Expectations                                                      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " gutenberg\\_id & text & title\\\\\n",
       "\\hline\n",
       "\t 1400                                                                     & \"And will continue friends apart,\" said Estella.                         & Great Expectations                                                      \\\\\n",
       "\t 1400                                                                     &                                                                          & Great Expectations                                                      \\\\\n",
       "\t 1400                                                                     & I took her hand in mine, and we went out of the ruined place; and, as    & Great Expectations                                                      \\\\\n",
       "\t 1400                                                                     & the morning mists had risen long ago when I first left the forge, so the & Great Expectations                                                      \\\\\n",
       "\t 1400                                                                     & evening mists were rising now, and in all the broad expanse of tranquil  & Great Expectations                                                      \\\\\n",
       "\t 1400                                                                     & light they showed to me, I saw no shadow of another parting from her.    & Great Expectations                                                      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "gutenberg_id | text | title | \n",
       "|---|---|---|---|---|---|\n",
       "| 1400                                                                     | \"And will continue friends apart,\" said Estella.                         | Great Expectations                                                       | \n",
       "| 1400                                                                     |                                                                          | Great Expectations                                                       | \n",
       "| 1400                                                                     | I took her hand in mine, and we went out of the ruined place; and, as    | Great Expectations                                                       | \n",
       "| 1400                                                                     | the morning mists had risen long ago when I first left the forge, so the | Great Expectations                                                       | \n",
       "| 1400                                                                     | evening mists were rising now, and in all the broad expanse of tranquil  | Great Expectations                                                       | \n",
       "| 1400                                                                     | light they showed to me, I saw no shadow of another parting from her.    | Great Expectations                                                       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  gutenberg_id\n",
       "1 1400        \n",
       "2 1400        \n",
       "3 1400        \n",
       "4 1400        \n",
       "5 1400        \n",
       "6 1400        \n",
       "  text                                                                    \n",
       "1 \"And will continue friends apart,\" said Estella.                        \n",
       "2                                                                         \n",
       "3 I took her hand in mine, and we went out of the ruined place; and, as   \n",
       "4 the morning mists had risen long ago when I first left the forge, so the\n",
       "5 evening mists were rising now, and in all the broad expanse of tranquil \n",
       "6 light they showed to me, I saw no shadow of another parting from her.   \n",
       "  title             \n",
       "1 Great Expectations\n",
       "2 Great Expectations\n",
       "3 Great Expectations\n",
       "4 Great Expectations\n",
       "5 Great Expectations\n",
       "6 Great Expectations"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(books)\n",
    "tail(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t51663 obs. of  3 variables:\n",
      " $ gutenberg_id: int  36 36 36 36 36 36 36 36 36 36 ...\n",
      " $ text        : chr  \"The War of the Worlds\" \"\" \"by H. G. Wells [1898]\" \"\" ...\n",
      " $ title       : chr  \"The War of the Worlds\" \"The War of the Worlds\" \"The War of the Worlds\" \"The War of the Worlds\" ...\n"
     ]
    }
   ],
   "source": [
    "str(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pre-processing, we divide these into chapters, use tidytext’s `unnest_tokens` to separate them into words, then remove `stop_words`. We’re treating every chapter as a separate “document”, each with a name like Great Expectations_1 or Pride and Prejudice_11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>gutenberg_id</th><th scope=col>text</th><th scope=col>title</th><th scope=col>chapter</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36                   </td><td>The War of the Worlds</td><td>The War of the Worlds</td><td>0                    </td></tr>\n",
       "\t<tr><td>36                   </td><td>                     </td><td>The War of the Worlds</td><td>0                    </td></tr>\n",
       "\t<tr><td>36                   </td><td>by H. G. Wells [1898]</td><td>The War of the Worlds</td><td>0                    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " gutenberg\\_id & text & title & chapter\\\\\n",
       "\\hline\n",
       "\t 36                    & The War of the Worlds & The War of the Worlds & 0                    \\\\\n",
       "\t 36                    &                       & The War of the Worlds & 0                    \\\\\n",
       "\t 36                        & by H. G. Wells {[}1898{]} & The War of the Worlds     & 0                        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "gutenberg_id | text | title | chapter | \n",
       "|---|---|---|\n",
       "| 36                    | The War of the Worlds | The War of the Worlds | 0                     | \n",
       "| 36                    |                       | The War of the Worlds | 0                     | \n",
       "| 36                    | by H. G. Wells [1898] | The War of the Worlds | 0                     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  gutenberg_id text                  title                 chapter\n",
       "1 36           The War of the Worlds The War of the Worlds 0      \n",
       "2 36                                 The War of the Worlds 0      \n",
       "3 36           by H. G. Wells [1898] The War of the Worlds 0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidytext)\n",
    "library(stringr)\n",
    "library(tidyr)\n",
    "# add column indicating from which chapter each line of text comes from\n",
    "books %>% group_by(title) %>% mutate(chapter = cumsum(str_detect(text, regex(\"^chapter \", ignore_case = TRUE)))) %>% head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>gutenberg_id</th><th scope=col>text</th><th scope=col>title</th><th scope=col>chapter</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36                                                            </td><td>CHAPTER ONE                                                   </td><td>The War of the Worlds                                         </td><td>1                                                             </td></tr>\n",
       "\t<tr><td>36                                                            </td><td>                                                              </td><td>The War of the Worlds                                         </td><td>1                                                             </td></tr>\n",
       "\t<tr><td>36                                                            </td><td>THE EVE OF THE WAR                                            </td><td>The War of the Worlds                                         </td><td>1                                                             </td></tr>\n",
       "\t<tr><td>36                                                            </td><td>                                                              </td><td>The War of the Worlds                                         </td><td>1                                                             </td></tr>\n",
       "\t<tr><td>36                                                            </td><td>                                                              </td><td>The War of the Worlds                                         </td><td>1                                                             </td></tr>\n",
       "\t<tr><td>36                                                            </td><td>No one would have believed in the last years of the nineteenth</td><td>The War of the Worlds                                         </td><td>1                                                             </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " gutenberg\\_id & text & title & chapter\\\\\n",
       "\\hline\n",
       "\t 36                                                             & CHAPTER ONE                                                    & The War of the Worlds                                          & 1                                                             \\\\\n",
       "\t 36                                                             &                                                                & The War of the Worlds                                          & 1                                                             \\\\\n",
       "\t 36                                                             & THE EVE OF THE WAR                                             & The War of the Worlds                                          & 1                                                             \\\\\n",
       "\t 36                                                             &                                                                & The War of the Worlds                                          & 1                                                             \\\\\n",
       "\t 36                                                             &                                                                & The War of the Worlds                                          & 1                                                             \\\\\n",
       "\t 36                                                             & No one would have believed in the last years of the nineteenth & The War of the Worlds                                          & 1                                                             \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "gutenberg_id | text | title | chapter | \n",
       "|---|---|---|---|---|---|\n",
       "| 36                                                             | CHAPTER ONE                                                    | The War of the Worlds                                          | 1                                                              | \n",
       "| 36                                                             |                                                                | The War of the Worlds                                          | 1                                                              | \n",
       "| 36                                                             | THE EVE OF THE WAR                                             | The War of the Worlds                                          | 1                                                              | \n",
       "| 36                                                             |                                                                | The War of the Worlds                                          | 1                                                              | \n",
       "| 36                                                             |                                                                | The War of the Worlds                                          | 1                                                              | \n",
       "| 36                                                             | No one would have believed in the last years of the nineteenth | The War of the Worlds                                          | 1                                                              | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  gutenberg_id text                                                          \n",
       "1 36           CHAPTER ONE                                                   \n",
       "2 36                                                                         \n",
       "3 36           THE EVE OF THE WAR                                            \n",
       "4 36                                                                         \n",
       "5 36                                                                         \n",
       "6 36           No one would have believed in the last years of the nineteenth\n",
       "  title                 chapter\n",
       "1 The War of the Worlds 1      \n",
       "2 The War of the Worlds 1      \n",
       "3 The War of the Worlds 1      \n",
       "4 The War of the Worlds 1      \n",
       "5 The War of the Worlds 1      \n",
       "6 The War of the Worlds 1      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_chapter <- books %>%\n",
    "              group_by(title) %>%\n",
    "              mutate(chapter = cumsum(str_detect(text, regex(\"^chapter \", ignore_case = TRUE)))) %>%\n",
    "              ungroup() %>%\n",
    "              filter(chapter > 0) # to eliminate forword, introduction etc. and start with chapter 1\n",
    "head(by_chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>gutenberg_id</th><th scope=col>text</th><th scope=col>title_chapter</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36                     </td><td>CHAPTER ONE            </td><td>The War of the Worlds_1</td></tr>\n",
       "\t<tr><td>36                     </td><td>                       </td><td>The War of the Worlds_1</td></tr>\n",
       "\t<tr><td>36                     </td><td>THE EVE OF THE WAR     </td><td>The War of the Worlds_1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " gutenberg\\_id & text & title\\_chapter\\\\\n",
       "\\hline\n",
       "\t 36                        & CHAPTER ONE               & The War of the Worlds\\_1\\\\\n",
       "\t 36                        &                           & The War of the Worlds\\_1\\\\\n",
       "\t 36                        & THE EVE OF THE WAR        & The War of the Worlds\\_1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "gutenberg_id | text | title_chapter | \n",
       "|---|---|---|\n",
       "| 36                      | CHAPTER ONE             | The War of the Worlds_1 | \n",
       "| 36                      |                         | The War of the Worlds_1 | \n",
       "| 36                      | THE EVE OF THE WAR      | The War of the Worlds_1 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  gutenberg_id text               title_chapter          \n",
       "1 36           CHAPTER ONE        The War of the Worlds_1\n",
       "2 36                              The War of the Worlds_1\n",
       "3 36           THE EVE OF THE WAR The War of the Worlds_1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_chapter %>% unite(title_chapter, title, chapter) %>% head(3) # combine title + chapter into col title_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>gutenberg_id</th><th scope=col>title_chapter</th><th scope=col>word</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36                     </td><td>The War of the Worlds_1</td><td>chapter                </td></tr>\n",
       "\t<tr><td>36                     </td><td>The War of the Worlds_1</td><td>one                    </td></tr>\n",
       "\t<tr><td>36                     </td><td>The War of the Worlds_1</td><td>the                    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " gutenberg\\_id & title\\_chapter & word\\\\\n",
       "\\hline\n",
       "\t 36                        & The War of the Worlds\\_1 & chapter                  \\\\\n",
       "\t 36                        & The War of the Worlds\\_1 & one                      \\\\\n",
       "\t 36                        & The War of the Worlds\\_1 & the                      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "gutenberg_id | title_chapter | word | \n",
       "|---|---|---|\n",
       "| 36                      | The War of the Worlds_1 | chapter                 | \n",
       "| 36                      | The War of the Worlds_1 | one                     | \n",
       "| 36                      | The War of the Worlds_1 | the                     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  gutenberg_id title_chapter           word   \n",
       "1 36           The War of the Worlds_1 chapter\n",
       "2 36           The War of the Worlds_1 one    \n",
       "3 36           The War of the Worlds_1 the    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_chapter_word <- by_chapter %>% unite(title_chapter, title, chapter) %>%\n",
    "                        unnest_tokens(word, text) # unnest into word from text\n",
    "head(by_chapter_word, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title_chapter</th><th scope=col>word</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations_57</td><td>joe                  </td><td>88                   </td></tr>\n",
       "\t<tr><td>Great Expectations_7 </td><td>joe                  </td><td>70                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " title\\_chapter & word & n\\\\\n",
       "\\hline\n",
       "\t Great Expectations\\_57 & joe                     & 88                     \\\\\n",
       "\t Great Expectations\\_7  & joe                     & 70                     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title_chapter | word | n | \n",
       "|---|---|\n",
       "| Great Expectations_57 | joe                   | 88                    | \n",
       "| Great Expectations_7  | joe                   | 70                    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title_chapter         word n \n",
       "1 Great Expectations_57 joe  88\n",
       "2 Great Expectations_7  joe  70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_chapter_word %>% anti_join(stop_words) %>%\n",
    "              count(title_chapter, word, sort = TRUE) %>% head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title_chapter</th><th scope=col>word</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations_57</td><td>joe                  </td><td>88                   </td></tr>\n",
       "\t<tr><td>Great Expectations_7 </td><td>joe                  </td><td>70                   </td></tr>\n",
       "\t<tr><td>Great Expectations_17</td><td>biddy                </td><td>63                   </td></tr>\n",
       "\t<tr><td>Great Expectations_27</td><td>joe                  </td><td>58                   </td></tr>\n",
       "\t<tr><td>Great Expectations_38</td><td>estella              </td><td>58                   </td></tr>\n",
       "\t<tr><td>Great Expectations_2 </td><td>joe                  </td><td>56                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " title\\_chapter & word & n\\\\\n",
       "\\hline\n",
       "\t Great Expectations\\_57 & joe                     & 88                     \\\\\n",
       "\t Great Expectations\\_7  & joe                     & 70                     \\\\\n",
       "\t Great Expectations\\_17 & biddy                   & 63                     \\\\\n",
       "\t Great Expectations\\_27 & joe                     & 58                     \\\\\n",
       "\t Great Expectations\\_38 & estella                 & 58                     \\\\\n",
       "\t Great Expectations\\_2  & joe                     & 56                     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title_chapter | word | n | \n",
       "|---|---|---|---|---|---|\n",
       "| Great Expectations_57 | joe                   | 88                    | \n",
       "| Great Expectations_7  | joe                   | 70                    | \n",
       "| Great Expectations_17 | biddy                 | 63                    | \n",
       "| Great Expectations_27 | joe                   | 58                    | \n",
       "| Great Expectations_38 | estella               | 58                    | \n",
       "| Great Expectations_2  | joe                   | 56                    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title_chapter         word    n \n",
       "1 Great Expectations_57 joe     88\n",
       "2 Great Expectations_7  joe     70\n",
       "3 Great Expectations_17 biddy   63\n",
       "4 Great Expectations_27 joe     58\n",
       "5 Great Expectations_38 estella 58\n",
       "6 Great Expectations_2  joe     56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts <- by_chapter_word %>%\n",
    "                  anti_join(stop_words) %>%\n",
    "                  count(title_chapter, word, sort = TRUE) %>%\n",
    "                  ungroup() # no effect now, but important for later pipes\n",
    "head(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation with the topicmodels package\n",
    "Right now this data frame is in a tidy form, with `one-term-per-document-per-row`. However, the `topicmodels` package requires a `tm::DocumentTermMatrix`. We can **cast a one-token-per-row table into a DocumentTermMatrix** with `tidytext::cast_dtm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<DocumentTermMatrix (documents: 193, terms: 18215)>>\n",
       "Non-/sparse entries: 104722/3410773\n",
       "Sparsity           : 97%\n",
       "Maximal term length: 19\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chapters_dtm <- word_counts %>% cast_dtm(title_chapter, word, n)\n",
    "chapters_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use the topicmodels package to **create a four topic LDA model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'topicmodels' was built under R version 3.4.4\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "A LDA_VEM topic model with 4 topics."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(topicmodels)\n",
    "chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))\n",
    "chapters_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case **we know there are four topics because there are four books**; in practice **we may need to try a few different values of k**.\n",
    "- tidytext gives us the option of returning to a tidy analysis, using the `tidy` and `augment` verbs borrowed from the `broom` package. In particular, we start with the tidy verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>topic</th><th scope=col>term</th><th scope=col>beta</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1           </td><td>joe         </td><td>1.436612e-17</td></tr>\n",
       "\t<tr><td>2           </td><td>joe         </td><td>5.962111e-61</td></tr>\n",
       "\t<tr><td>3           </td><td>joe         </td><td>9.881855e-25</td></tr>\n",
       "\t<tr><td>4           </td><td>joe         </td><td>1.447329e-02</td></tr>\n",
       "\t<tr><td>1           </td><td>biddy       </td><td>5.139275e-28</td></tr>\n",
       "\t<tr><td>2           </td><td>biddy       </td><td>5.022015e-73</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " topic & term & beta\\\\\n",
       "\\hline\n",
       "\t 1            & joe          & 1.436612e-17\\\\\n",
       "\t 2            & joe          & 5.962111e-61\\\\\n",
       "\t 3            & joe          & 9.881855e-25\\\\\n",
       "\t 4            & joe          & 1.447329e-02\\\\\n",
       "\t 1            & biddy        & 5.139275e-28\\\\\n",
       "\t 2            & biddy        & 5.022015e-73\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "topic | term | beta | \n",
       "|---|---|---|---|---|---|\n",
       "| 1            | joe          | 1.436612e-17 | \n",
       "| 2            | joe          | 5.962111e-61 | \n",
       "| 3            | joe          | 9.881855e-25 | \n",
       "| 4            | joe          | 1.447329e-02 | \n",
       "| 1            | biddy        | 5.139275e-28 | \n",
       "| 2            | biddy        | 5.022015e-73 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  topic term  beta        \n",
       "1 1     joe   1.436612e-17\n",
       "2 2     joe   5.962111e-61\n",
       "3 3     joe   9.881855e-25\n",
       "4 4     joe   1.447329e-02\n",
       "5 1     biddy 5.139275e-28\n",
       "6 2     biddy 5.022015e-73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t72860 obs. of  3 variables:\n",
      " $ topic: int  1 2 3 4 1 2 3 4 1 2 ...\n",
      " $ term : chr  \"joe\" \"joe\" \"joe\" \"joe\" ...\n",
      " $ beta : num  1.44e-17 5.96e-61 9.88e-25 1.45e-02 5.14e-28 ...\n"
     ]
    }
   ],
   "source": [
    "chapters_lda_td <- tidy(chapters_lda)\n",
    "head(chapters_lda_td)\n",
    "str(chapters_lda_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this has turned the model into a `one-topic-per-term-per-row` format. For each combination the model has **β = probability of that term being generated from that topic**.\n",
    "- We could use dplyr’s `top_n` to find the top 5 terms within each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>topic</th><th scope=col>term</th><th scope=col>beta</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>4          </td><td>joe        </td><td>0.014473289</td></tr>\n",
       "\t<tr><td>1          </td><td>miss       </td><td>0.008708777</td></tr>\n",
       "\t<tr><td>4          </td><td>miss       </td><td>0.006232761</td></tr>\n",
       "\t<tr><td>1          </td><td>elizabeth  </td><td>0.014101270</td></tr>\n",
       "\t<tr><td>2          </td><td>captain    </td><td>0.015510635</td></tr>\n",
       "\t<tr><td>2          </td><td>nautilus   </td><td>0.013051927</td></tr>\n",
       "\t<tr><td>1          </td><td>darcy      </td><td>0.008810341</td></tr>\n",
       "\t<tr><td>2          </td><td>sea        </td><td>0.008843483</td></tr>\n",
       "\t<tr><td>4          </td><td>pip        </td><td>0.006828209</td></tr>\n",
       "\t<tr><td>2          </td><td>ned        </td><td>0.008031955</td></tr>\n",
       "\t<tr><td>3          </td><td>people     </td><td>0.006785987</td></tr>\n",
       "\t<tr><td>3          </td><td>night      </td><td>0.004491174</td></tr>\n",
       "\t<tr><td>3          </td><td>martians   </td><td>0.006456394</td></tr>\n",
       "\t<tr><td>2          </td><td>nemo       </td><td>0.008709651</td></tr>\n",
       "\t<tr><td>1          </td><td>jane       </td><td>0.006494613</td></tr>\n",
       "\t<tr><td>1          </td><td>bennet     </td><td>0.006944344</td></tr>\n",
       "\t<tr><td>4          </td><td>looked     </td><td>0.006366418</td></tr>\n",
       "\t<tr><td>3          </td><td>black      </td><td>0.005277449</td></tr>\n",
       "\t<tr><td>3          </td><td>time       </td><td>0.005343667</td></tr>\n",
       "\t<tr><td>4          </td><td>time       </td><td>0.006852889</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " topic & term & beta\\\\\n",
       "\\hline\n",
       "\t 4           & joe         & 0.014473289\\\\\n",
       "\t 1           & miss        & 0.008708777\\\\\n",
       "\t 4           & miss        & 0.006232761\\\\\n",
       "\t 1           & elizabeth   & 0.014101270\\\\\n",
       "\t 2           & captain     & 0.015510635\\\\\n",
       "\t 2           & nautilus    & 0.013051927\\\\\n",
       "\t 1           & darcy       & 0.008810341\\\\\n",
       "\t 2           & sea         & 0.008843483\\\\\n",
       "\t 4           & pip         & 0.006828209\\\\\n",
       "\t 2           & ned         & 0.008031955\\\\\n",
       "\t 3           & people      & 0.006785987\\\\\n",
       "\t 3           & night       & 0.004491174\\\\\n",
       "\t 3           & martians    & 0.006456394\\\\\n",
       "\t 2           & nemo        & 0.008709651\\\\\n",
       "\t 1           & jane        & 0.006494613\\\\\n",
       "\t 1           & bennet      & 0.006944344\\\\\n",
       "\t 4           & looked      & 0.006366418\\\\\n",
       "\t 3           & black       & 0.005277449\\\\\n",
       "\t 3           & time        & 0.005343667\\\\\n",
       "\t 4           & time        & 0.006852889\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "topic | term | beta | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 4           | joe         | 0.014473289 | \n",
       "| 1           | miss        | 0.008708777 | \n",
       "| 4           | miss        | 0.006232761 | \n",
       "| 1           | elizabeth   | 0.014101270 | \n",
       "| 2           | captain     | 0.015510635 | \n",
       "| 2           | nautilus    | 0.013051927 | \n",
       "| 1           | darcy       | 0.008810341 | \n",
       "| 2           | sea         | 0.008843483 | \n",
       "| 4           | pip         | 0.006828209 | \n",
       "| 2           | ned         | 0.008031955 | \n",
       "| 3           | people      | 0.006785987 | \n",
       "| 3           | night       | 0.004491174 | \n",
       "| 3           | martians    | 0.006456394 | \n",
       "| 2           | nemo        | 0.008709651 | \n",
       "| 1           | jane        | 0.006494613 | \n",
       "| 1           | bennet      | 0.006944344 | \n",
       "| 4           | looked      | 0.006366418 | \n",
       "| 3           | black       | 0.005277449 | \n",
       "| 3           | time        | 0.005343667 | \n",
       "| 4           | time        | 0.006852889 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   topic term      beta       \n",
       "1  4     joe       0.014473289\n",
       "2  1     miss      0.008708777\n",
       "3  4     miss      0.006232761\n",
       "4  1     elizabeth 0.014101270\n",
       "5  2     captain   0.015510635\n",
       "6  2     nautilus  0.013051927\n",
       "7  1     darcy     0.008810341\n",
       "8  2     sea       0.008843483\n",
       "9  4     pip       0.006828209\n",
       "10 2     ned       0.008031955\n",
       "11 3     people    0.006785987\n",
       "12 3     night     0.004491174\n",
       "13 3     martians  0.006456394\n",
       "14 2     nemo      0.008709651\n",
       "15 1     jane      0.006494613\n",
       "16 1     bennet    0.006944344\n",
       "17 4     looked    0.006366418\n",
       "18 3     black     0.005277449\n",
       "19 3     time      0.005343667\n",
       "20 4     time      0.006852889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chapters_lda_td %>% group_by(topic) %>% top_n(5, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>topic</th><th scope=col>term</th><th scope=col>beta</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1          </td><td>elizabeth  </td><td>0.014101270</td></tr>\n",
       "\t<tr><td>1          </td><td>darcy      </td><td>0.008810341</td></tr>\n",
       "\t<tr><td>1          </td><td>miss       </td><td>0.008708777</td></tr>\n",
       "\t<tr><td>1          </td><td>bennet     </td><td>0.006944344</td></tr>\n",
       "\t<tr><td>1          </td><td>jane       </td><td>0.006494613</td></tr>\n",
       "\t<tr><td>2          </td><td>captain    </td><td>0.015510635</td></tr>\n",
       "\t<tr><td>2          </td><td>nautilus   </td><td>0.013051927</td></tr>\n",
       "\t<tr><td>2          </td><td>sea        </td><td>0.008843483</td></tr>\n",
       "\t<tr><td>2          </td><td>nemo       </td><td>0.008709651</td></tr>\n",
       "\t<tr><td>2          </td><td>ned        </td><td>0.008031955</td></tr>\n",
       "\t<tr><td>3          </td><td>people     </td><td>0.006785987</td></tr>\n",
       "\t<tr><td>3          </td><td>martians   </td><td>0.006456394</td></tr>\n",
       "\t<tr><td>3          </td><td>time       </td><td>0.005343667</td></tr>\n",
       "\t<tr><td>3          </td><td>black      </td><td>0.005277449</td></tr>\n",
       "\t<tr><td>3          </td><td>night      </td><td>0.004491174</td></tr>\n",
       "\t<tr><td>4          </td><td>joe        </td><td>0.014473289</td></tr>\n",
       "\t<tr><td>4          </td><td>time       </td><td>0.006852889</td></tr>\n",
       "\t<tr><td>4          </td><td>pip        </td><td>0.006828209</td></tr>\n",
       "\t<tr><td>4          </td><td>looked     </td><td>0.006366418</td></tr>\n",
       "\t<tr><td>4          </td><td>miss       </td><td>0.006232761</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " topic & term & beta\\\\\n",
       "\\hline\n",
       "\t 1           & elizabeth   & 0.014101270\\\\\n",
       "\t 1           & darcy       & 0.008810341\\\\\n",
       "\t 1           & miss        & 0.008708777\\\\\n",
       "\t 1           & bennet      & 0.006944344\\\\\n",
       "\t 1           & jane        & 0.006494613\\\\\n",
       "\t 2           & captain     & 0.015510635\\\\\n",
       "\t 2           & nautilus    & 0.013051927\\\\\n",
       "\t 2           & sea         & 0.008843483\\\\\n",
       "\t 2           & nemo        & 0.008709651\\\\\n",
       "\t 2           & ned         & 0.008031955\\\\\n",
       "\t 3           & people      & 0.006785987\\\\\n",
       "\t 3           & martians    & 0.006456394\\\\\n",
       "\t 3           & time        & 0.005343667\\\\\n",
       "\t 3           & black       & 0.005277449\\\\\n",
       "\t 3           & night       & 0.004491174\\\\\n",
       "\t 4           & joe         & 0.014473289\\\\\n",
       "\t 4           & time        & 0.006852889\\\\\n",
       "\t 4           & pip         & 0.006828209\\\\\n",
       "\t 4           & looked      & 0.006366418\\\\\n",
       "\t 4           & miss        & 0.006232761\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "topic | term | beta | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1           | elizabeth   | 0.014101270 | \n",
       "| 1           | darcy       | 0.008810341 | \n",
       "| 1           | miss        | 0.008708777 | \n",
       "| 1           | bennet      | 0.006944344 | \n",
       "| 1           | jane        | 0.006494613 | \n",
       "| 2           | captain     | 0.015510635 | \n",
       "| 2           | nautilus    | 0.013051927 | \n",
       "| 2           | sea         | 0.008843483 | \n",
       "| 2           | nemo        | 0.008709651 | \n",
       "| 2           | ned         | 0.008031955 | \n",
       "| 3           | people      | 0.006785987 | \n",
       "| 3           | martians    | 0.006456394 | \n",
       "| 3           | time        | 0.005343667 | \n",
       "| 3           | black       | 0.005277449 | \n",
       "| 3           | night       | 0.004491174 | \n",
       "| 4           | joe         | 0.014473289 | \n",
       "| 4           | time        | 0.006852889 | \n",
       "| 4           | pip         | 0.006828209 | \n",
       "| 4           | looked      | 0.006366418 | \n",
       "| 4           | miss        | 0.006232761 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   topic term      beta       \n",
       "1  1     elizabeth 0.014101270\n",
       "2  1     darcy     0.008810341\n",
       "3  1     miss      0.008708777\n",
       "4  1     bennet    0.006944344\n",
       "5  1     jane      0.006494613\n",
       "6  2     captain   0.015510635\n",
       "7  2     nautilus  0.013051927\n",
       "8  2     sea       0.008843483\n",
       "9  2     nemo      0.008709651\n",
       "10 2     ned       0.008031955\n",
       "11 3     people    0.006785987\n",
       "12 3     martians  0.006456394\n",
       "13 3     time      0.005343667\n",
       "14 3     black     0.005277449\n",
       "15 3     night     0.004491174\n",
       "16 4     joe       0.014473289\n",
       "17 4     time      0.006852889\n",
       "18 4     pip       0.006828209\n",
       "19 4     looked    0.006366418\n",
       "20 4     miss      0.006232761"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_terms <- chapters_lda_td %>% group_by(topic) %>% top_n(5, beta) %>%\n",
    "              ungroup() %>% arrange(topic, -beta) # ORDER the output from above BY topic ASC a. beta DESC\n",
    "\n",
    "top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAZlBMVEUAAAAaGhozMzNNTU1Z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\n",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This model lends itself to a visualization:\n",
    "library(ggplot2)\n",
    "theme_set(theme_bw())\n",
    "\n",
    "top_terms %>% mutate(term = reorder(term, beta)) %>% # does nothing\n",
    "  ggplot(aes(term, beta)) +\n",
    "  geom_bar(stat = \"identity\") + # If you want heights of bars to represent values in the data \n",
    "# (you need to map a value to the y aesthetic) - without this stat = \"identity\" plot returns an error\n",
    "  facet_wrap(~ topic, scales = \"free\") + # free cause scales vary across rows and columns\n",
    "  theme(axis.text.x = element_text(size = 15, angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-document classification\n",
    "Each chapter was a “document” in this analysis. Thus, we may want to know which topics are associated with each document. **Can we put the chapters back together in the correct books?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>document</th><th scope=col>topic</th><th scope=col>gamma</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations_57</td><td>1                    </td><td>1.338547e-05         </td></tr>\n",
       "\t<tr><td>Great Expectations_7 </td><td>1                    </td><td>1.456215e-05         </td></tr>\n",
       "\t<tr><td>Great Expectations_17</td><td>1                    </td><td>2.096237e-05         </td></tr>\n",
       "\t<tr><td>Great Expectations_27</td><td>1                    </td><td>1.900804e-05         </td></tr>\n",
       "\t<tr><td>Great Expectations_38</td><td>1                    </td><td>3.552749e-01         </td></tr>\n",
       "\t<tr><td>Great Expectations_2 </td><td>1                    </td><td>1.706715e-05         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " document & topic & gamma\\\\\n",
       "\\hline\n",
       "\t Great Expectations\\_57 & 1                       & 1.338547e-05           \\\\\n",
       "\t Great Expectations\\_7  & 1                       & 1.456215e-05           \\\\\n",
       "\t Great Expectations\\_17 & 1                       & 2.096237e-05           \\\\\n",
       "\t Great Expectations\\_27 & 1                       & 1.900804e-05           \\\\\n",
       "\t Great Expectations\\_38 & 1                       & 3.552749e-01           \\\\\n",
       "\t Great Expectations\\_2  & 1                       & 1.706715e-05           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "document | topic | gamma | \n",
       "|---|---|---|---|---|---|\n",
       "| Great Expectations_57 | 1                     | 1.338547e-05          | \n",
       "| Great Expectations_7  | 1                     | 1.456215e-05          | \n",
       "| Great Expectations_17 | 1                     | 2.096237e-05          | \n",
       "| Great Expectations_27 | 1                     | 1.900804e-05          | \n",
       "| Great Expectations_38 | 1                     | 3.552749e-01          | \n",
       "| Great Expectations_2  | 1                     | 1.706715e-05          | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  document              topic gamma       \n",
       "1 Great Expectations_57 1     1.338547e-05\n",
       "2 Great Expectations_7  1     1.456215e-05\n",
       "3 Great Expectations_17 1     2.096237e-05\n",
       "4 Great Expectations_27 1     1.900804e-05\n",
       "5 Great Expectations_38 1     3.552749e-01\n",
       "6 Great Expectations_2  1     1.706715e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>document</th><th scope=col>topic</th><th scope=col>gamma</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Pride and Prejudice_5                   </td><td>4                                       </td><td>6.485469e-05                            </td></tr>\n",
       "\t<tr><td>Pride and Prejudice_60                  </td><td>4                                       </td><td>4.068034e-05                            </td></tr>\n",
       "\t<tr><td>Pride and Prejudice_12                  </td><td>4                                       </td><td>8.029132e-05                            </td></tr>\n",
       "\t<tr><td>The War of the Worlds_3                 </td><td>4                                       </td><td>5.096009e-05                            </td></tr>\n",
       "\t<tr><td>The War of the Worlds_7                 </td><td>4                                       </td><td>4.177962e-05                            </td></tr>\n",
       "\t<tr><td>Twenty Thousand Leagues under the Sea_46</td><td>4                                       </td><td>1.188746e-04                            </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " document & topic & gamma\\\\\n",
       "\\hline\n",
       "\t Pride and Prejudice\\_5                    & 4                                          & 6.485469e-05                              \\\\\n",
       "\t Pride and Prejudice\\_60                   & 4                                          & 4.068034e-05                              \\\\\n",
       "\t Pride and Prejudice\\_12                   & 4                                          & 8.029132e-05                              \\\\\n",
       "\t The War of the Worlds\\_3                  & 4                                          & 5.096009e-05                              \\\\\n",
       "\t The War of the Worlds\\_7                  & 4                                          & 4.177962e-05                              \\\\\n",
       "\t Twenty Thousand Leagues under the Sea\\_46 & 4                                          & 1.188746e-04                              \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "document | topic | gamma | \n",
       "|---|---|---|---|---|---|\n",
       "| Pride and Prejudice_5                    | 4                                        | 6.485469e-05                             | \n",
       "| Pride and Prejudice_60                   | 4                                        | 4.068034e-05                             | \n",
       "| Pride and Prejudice_12                   | 4                                        | 8.029132e-05                             | \n",
       "| The War of the Worlds_3                  | 4                                        | 5.096009e-05                             | \n",
       "| The War of the Worlds_7                  | 4                                        | 4.177962e-05                             | \n",
       "| Twenty Thousand Leagues under the Sea_46 | 4                                        | 1.188746e-04                             | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  document                                 topic gamma       \n",
       "1 Pride and Prejudice_5                    4     6.485469e-05\n",
       "2 Pride and Prejudice_60                   4     4.068034e-05\n",
       "3 Pride and Prejudice_12                   4     8.029132e-05\n",
       "4 The War of the Worlds_3                  4     5.096009e-05\n",
       "5 The War of the Worlds_7                  4     4.177962e-05\n",
       "6 Twenty Thousand Leagues under the Sea_46 4     1.188746e-04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chapters_lda_gamma <- tidy(chapters_lda, matrix = \"gamma\")\n",
    "head(chapters_lda_gamma)\n",
    "tail(chapters_lda_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `matrix = \"gamma\"` returns a tidied version with one-document-per-topic-per-row. \n",
    "- Now that we have these document classifications, we can see **how well our unsupervised learning did at distinguishing the four books**. \n",
    "- First we **re-separate** the document name **into title and chapter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>chapter</th><th scope=col>topic</th><th scope=col>gamma</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations</td><td>57                </td><td>1                 </td><td>1.338547e-05      </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td> 7                </td><td>1                 </td><td>1.456215e-05      </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>17                </td><td>1                 </td><td>2.096237e-05      </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>27                </td><td>1                 </td><td>1.900804e-05      </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>38                </td><td>1                 </td><td>3.552749e-01      </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td> 2                </td><td>1                 </td><td>1.706715e-05      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " title & chapter & topic & gamma\\\\\n",
       "\\hline\n",
       "\t Great Expectations & 57                 & 1                  & 1.338547e-05      \\\\\n",
       "\t Great Expectations &  7                 & 1                  & 1.456215e-05      \\\\\n",
       "\t Great Expectations & 17                 & 1                  & 2.096237e-05      \\\\\n",
       "\t Great Expectations & 27                 & 1                  & 1.900804e-05      \\\\\n",
       "\t Great Expectations & 38                 & 1                  & 3.552749e-01      \\\\\n",
       "\t Great Expectations &  2                 & 1                  & 1.706715e-05      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | chapter | topic | gamma | \n",
       "|---|---|---|---|---|---|\n",
       "| Great Expectations | 57                 | 1                  | 1.338547e-05       | \n",
       "| Great Expectations |  7                 | 1                  | 1.456215e-05       | \n",
       "| Great Expectations | 17                 | 1                  | 2.096237e-05       | \n",
       "| Great Expectations | 27                 | 1                  | 1.900804e-05       | \n",
       "| Great Expectations | 38                 | 1                  | 3.552749e-01       | \n",
       "| Great Expectations |  2                 | 1                  | 1.706715e-05       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title              chapter topic gamma       \n",
       "1 Great Expectations 57      1     1.338547e-05\n",
       "2 Great Expectations  7      1     1.456215e-05\n",
       "3 Great Expectations 17      1     2.096237e-05\n",
       "4 Great Expectations 27      1     1.900804e-05\n",
       "5 Great Expectations 38      1     3.552749e-01\n",
       "6 Great Expectations  2      1     1.706715e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t772 obs. of  4 variables:\n",
      " $ title  : chr  \"Great Expectations\" \"Great Expectations\" \"Great Expectations\" \"Great Expectations\" ...\n",
      " $ chapter: int  57 7 17 27 38 2 23 15 18 16 ...\n",
      " $ topic  : int  1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ gamma  : num  1.34e-05 1.46e-05 2.10e-05 1.90e-05 3.55e-01 ...\n"
     ]
    }
   ],
   "source": [
    "chapters_lda_gamma <- chapters_lda_gamma %>% separate(document, c(\"title\", \"chapter\"), sep = \"_\", convert = TRUE)\n",
    "head(chapters_lda_gamma)\n",
    "str(chapters_lda_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAARVBMVEUAAAAAv8QaGhozMzNN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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then we examine what fraction of chapters we got right for each:\n",
    "ggplot(chapters_lda_gamma, aes(gamma, fill = factor(topic))) + geom_histogram() + facet_wrap(~ title, nrow = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that almost all of the chapters from top right & both at the bottom were **uniquely identified as a single topic each**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>chapter</th><th scope=col>topic</th><th scope=col>gamma</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations</td><td>54                </td><td>3                 </td><td>0.4812041         </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>22                </td><td>4                 </td><td>0.5362582         </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>23                </td><td>1                 </td><td>0.5470853         </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>31                </td><td>4                 </td><td>0.5473876         </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>33                </td><td>4                 </td><td>0.5689223         </td></tr>\n",
       "\t<tr><td>Great Expectations</td><td>47                </td><td>4                 </td><td>0.5799048         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " title & chapter & topic & gamma\\\\\n",
       "\\hline\n",
       "\t Great Expectations & 54                 & 3                  & 0.4812041         \\\\\n",
       "\t Great Expectations & 22                 & 4                  & 0.5362582         \\\\\n",
       "\t Great Expectations & 23                 & 1                  & 0.5470853         \\\\\n",
       "\t Great Expectations & 31                 & 4                  & 0.5473876         \\\\\n",
       "\t Great Expectations & 33                 & 4                  & 0.5689223         \\\\\n",
       "\t Great Expectations & 47                 & 4                  & 0.5799048         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | chapter | topic | gamma | \n",
       "|---|---|---|---|---|---|\n",
       "| Great Expectations | 54                 | 3                  | 0.4812041          | \n",
       "| Great Expectations | 22                 | 4                  | 0.5362582          | \n",
       "| Great Expectations | 23                 | 1                  | 0.5470853          | \n",
       "| Great Expectations | 31                 | 4                  | 0.5473876          | \n",
       "| Great Expectations | 33                 | 4                  | 0.5689223          | \n",
       "| Great Expectations | 47                 | 4                  | 0.5799048          | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title              chapter topic gamma    \n",
       "1 Great Expectations 54      3     0.4812041\n",
       "2 Great Expectations 22      4     0.5362582\n",
       "3 Great Expectations 23      1     0.5470853\n",
       "4 Great Expectations 31      4     0.5473876\n",
       "5 Great Expectations 33      4     0.5689223\n",
       "6 Great Expectations 47      4     0.5799048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t193 obs. of  4 variables:\n",
      " $ title  : chr  \"Great Expectations\" \"Great Expectations\" \"Great Expectations\" \"Great Expectations\" ...\n",
      " $ chapter: int  54 22 23 31 33 47 56 38 3 11 ...\n",
      " $ topic  : int  3 4 1 4 4 4 4 4 4 4 ...\n",
      " $ gamma  : num  0.481 0.536 0.547 0.547 0.569 ...\n"
     ]
    }
   ],
   "source": [
    "chapter_classifications <- chapters_lda_gamma %>% group_by(title, chapter) %>%\n",
    "                              top_n(1, gamma) %>% ungroup() %>% arrange(gamma)\n",
    "head(chapter_classifications)\n",
    "str(chapter_classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine this by finding the consensus book for each, which we note is correct based on our earlier visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>topic</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Pride and Prejudice</td><td>1                  </td><td>61                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " title & topic & n\\\\\n",
       "\\hline\n",
       "\t Pride and Prejudice & 1                   & 61                 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | topic | n | \n",
       "|---|\n",
       "| Pride and Prejudice | 1                   | 61                  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title               topic n \n",
       "1 Pride and Prejudice 1     61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chapter_classifications %>% count(title, topic) %>% top_n(1, n) %>% ungroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>consensus</th><th scope=col>topic</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Pride and Prejudice</td><td>1                  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " consensus & topic\\\\\n",
       "\\hline\n",
       "\t Pride and Prejudice & 1                  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "consensus | topic | \n",
       "|---|\n",
       "| Pride and Prejudice | 1                   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  consensus           topic\n",
       "1 Pride and Prejudice 1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "book_topics <- chapter_classifications %>% count(title, topic) %>% top_n(1, n) %>% ungroup() %>%\n",
    "                transmute(consensus = title, topic) # Adds new variable consensus and drops title and topic \n",
    "book_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which chapters were misidentified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>consensus</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td> 1                 </td></tr>\n",
       "\t<tr><td>Pride and Prejudice</td><td>Pride and Prejudice</td><td>61                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " title & consensus & n\\\\\n",
       "\\hline\n",
       "\t Great Expectations  & Pride and Prejudice &  1                 \\\\\n",
       "\t Pride and Prejudice & Pride and Prejudice & 61                 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | consensus | n | \n",
       "|---|---|\n",
       "| Great Expectations  | Pride and Prejudice |  1                  | \n",
       "| Pride and Prejudice | Pride and Prejudice | 61                  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title               consensus           n \n",
       "1 Great Expectations  Pride and Prejudice  1\n",
       "2 Pride and Prejudice Pride and Prejudice 61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chapter_classifications %>% inner_join(book_topics, by = \"topic\") %>% count(title, consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only a few chapters from Great Expectations were misclassified. Not bad for unsupervised clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By word assignments: augment\n",
    "One important step in the topic modeling expectation-maximization algorithm is **assigning each word in each document to a topic**. \n",
    "- The more words in a document are assigned to a topic, the more weight (`gamma`) will go on that document-topic classification.\n",
    "- We may want to take the original document-word pairs and find which words in each document were assigned to which topic. This is the job of the `augment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t104722 obs. of  4 variables:\n",
      " $ document: chr  \"Great Expectations_57\" \"Great Expectations_7\" \"Great Expectations_17\" \"Great Expectations_27\" ...\n",
      " $ term    : chr  \"joe\" \"joe\" \"joe\" \"joe\" ...\n",
      " $ count   : num  88 70 5 58 56 1 50 50 44 40 ...\n",
      " $ .topic  : num  4 4 4 4 4 4 4 4 4 4 ...\n"
     ]
    }
   ],
   "source": [
    "assignments <- augment(chapters_lda, data = chapters_dtm)\n",
    "str(assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine this with the consensus book titles to find which words were incorrectly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>chapter</th><th scope=col>term</th><th scope=col>count</th><th scope=col>.topic</th><th scope=col>consensus</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Pride and Prejudice</td><td>47                 </td><td>pocket             </td><td>1                  </td><td>1                  </td><td>Pride and Prejudice</td></tr>\n",
       "\t<tr><td>Pride and Prejudice</td><td>50                 </td><td>pocket             </td><td>1                  </td><td>1                  </td><td>Pride and Prejudice</td></tr>\n",
       "\t<tr><td>Pride and Prejudice</td><td>49                 </td><td>pocket             </td><td>1                  </td><td>1                  </td><td>Pride and Prejudice</td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>38                 </td><td>brother            </td><td>2                  </td><td>1                  </td><td>Pride and Prejudice</td></tr>\n",
       "\t<tr><td>Pride and Prejudice</td><td>43                 </td><td>brother            </td><td>4                  </td><td>1                  </td><td>Pride and Prejudice</td></tr>\n",
       "\t<tr><td>Pride and Prejudice</td><td>18                 </td><td>brother            </td><td>1                  </td><td>1                  </td><td>Pride and Prejudice</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " title & chapter & term & count & .topic & consensus\\\\\n",
       "\\hline\n",
       "\t Pride and Prejudice & 47                  & pocket              & 1                   & 1                   & Pride and Prejudice\\\\\n",
       "\t Pride and Prejudice & 50                  & pocket              & 1                   & 1                   & Pride and Prejudice\\\\\n",
       "\t Pride and Prejudice & 49                  & pocket              & 1                   & 1                   & Pride and Prejudice\\\\\n",
       "\t Great Expectations  & 38                  & brother             & 2                   & 1                   & Pride and Prejudice\\\\\n",
       "\t Pride and Prejudice & 43                  & brother             & 4                   & 1                   & Pride and Prejudice\\\\\n",
       "\t Pride and Prejudice & 18                  & brother             & 1                   & 1                   & Pride and Prejudice\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | chapter | term | count | .topic | consensus | \n",
       "|---|---|---|---|---|---|\n",
       "| Pride and Prejudice | 47                  | pocket              | 1                   | 1                   | Pride and Prejudice | \n",
       "| Pride and Prejudice | 50                  | pocket              | 1                   | 1                   | Pride and Prejudice | \n",
       "| Pride and Prejudice | 49                  | pocket              | 1                   | 1                   | Pride and Prejudice | \n",
       "| Great Expectations  | 38                  | brother             | 2                   | 1                   | Pride and Prejudice | \n",
       "| Pride and Prejudice | 43                  | brother             | 4                   | 1                   | Pride and Prejudice | \n",
       "| Pride and Prejudice | 18                  | brother             | 1                   | 1                   | Pride and Prejudice | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title               chapter term    count .topic consensus          \n",
       "1 Pride and Prejudice 47      pocket  1     1      Pride and Prejudice\n",
       "2 Pride and Prejudice 50      pocket  1     1      Pride and Prejudice\n",
       "3 Pride and Prejudice 49      pocket  1     1      Pride and Prejudice\n",
       "4 Great Expectations  38      brother 2     1      Pride and Prejudice\n",
       "5 Pride and Prejudice 43      brother 4     1      Pride and Prejudice\n",
       "6 Pride and Prejudice 18      brother 1     1      Pride and Prejudice"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t29412 obs. of  6 variables:\n",
      " $ title    : chr  \"Pride and Prejudice\" \"Pride and Prejudice\" \"Pride and Prejudice\" \"Great Expectations\" ...\n",
      " $ chapter  : int  47 50 49 38 43 18 22 45 16 10 ...\n",
      " $ term     : chr  \"pocket\" \"pocket\" \"pocket\" \"brother\" ...\n",
      " $ count    : num  1 1 1 2 4 1 4 2 1 2 ...\n",
      " $ .topic   : num  1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ consensus: chr  \"Pride and Prejudice\" \"Pride and Prejudice\" \"Pride and Prejudice\" \"Pride and Prejudice\" ...\n"
     ]
    }
   ],
   "source": [
    "assignments <- assignments %>% separate(document, c(\"title\", \"chapter\"), sep = \"_\", convert = TRUE) %>%\n",
    "                      inner_join(book_topics, by = c(\".topic\" = \"topic\"))\n",
    "head(assignments)\n",
    "str(assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, for example, create a “confusion matrix” using `dplyr::count` and `tidyr::spread`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>Pride and Prejudice</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations                   </td><td> 3908                                </td></tr>\n",
       "\t<tr><td>Pride and Prejudice                  </td><td>37231                                </td></tr>\n",
       "\t<tr><td>Twenty Thousand Leagues under the Sea</td><td>    5                                </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " title & Pride and Prejudice\\\\\n",
       "\\hline\n",
       "\t Great Expectations                    &  3908                                \\\\\n",
       "\t Pride and Prejudice                   & 37231                                \\\\\n",
       "\t Twenty Thousand Leagues under the Sea &     5                                \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | Pride and Prejudice | \n",
       "|---|---|---|\n",
       "| Great Expectations                    |  3908                                 | \n",
       "| Pride and Prejudice                   | 37231                                 | \n",
       "| Twenty Thousand Leagues under the Sea |     5                                 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title                                 Pride and Prejudice\n",
       "1 Great Expectations                     3908              \n",
       "2 Pride and Prejudice                   37231              \n",
       "3 Twenty Thousand Leagues under the Sea     5              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assignments %>% count(title, consensus, wt = count) %>% spread(consensus, n, fill = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that almost all the words for Pride and Prejudice, Twenty Thousand (*only 5 misclassifications*), and War of the Worlds (*perfectly assigned*) were **correctly assigned**, while Great Expectations was 3908 times misassigned as Pride and Prejudice.\n",
    "\n",
    "## What were the most commonly mistaken words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>chapter</th><th scope=col>term</th><th scope=col>count</th><th scope=col>.topic</th><th scope=col>consensus</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations                   </td><td>38                                   </td><td>brother                              </td><td> 2                                   </td><td>1                                    </td><td>Pride and Prejudice                  </td></tr>\n",
       "\t<tr><td>Great Expectations                   </td><td>22                                   </td><td>brother                              </td><td> 4                                   </td><td>1                                    </td><td>Pride and Prejudice                  </td></tr>\n",
       "\t<tr><td>Great Expectations                   </td><td>23                                   </td><td>miss                                 </td><td> 2                                   </td><td>1                                    </td><td>Pride and Prejudice                  </td></tr>\n",
       "\t<tr><td>Great Expectations                   </td><td>22                                   </td><td>miss                                 </td><td>23                                   </td><td>1                                    </td><td>Pride and Prejudice                  </td></tr>\n",
       "\t<tr><td>Twenty Thousand Leagues under the Sea</td><td> 8                                   </td><td>miss                                 </td><td> 1                                   </td><td>1                                    </td><td>Pride and Prejudice                  </td></tr>\n",
       "\t<tr><td>Great Expectations                   </td><td>31                                   </td><td>miss                                 </td><td> 1                                   </td><td>1                                    </td><td>Pride and Prejudice                  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " title & chapter & term & count & .topic & consensus\\\\\n",
       "\\hline\n",
       "\t Great Expectations                    & 38                                    & brother                               &  2                                    & 1                                     & Pride and Prejudice                  \\\\\n",
       "\t Great Expectations                    & 22                                    & brother                               &  4                                    & 1                                     & Pride and Prejudice                  \\\\\n",
       "\t Great Expectations                    & 23                                    & miss                                  &  2                                    & 1                                     & Pride and Prejudice                  \\\\\n",
       "\t Great Expectations                    & 22                                    & miss                                  & 23                                    & 1                                     & Pride and Prejudice                  \\\\\n",
       "\t Twenty Thousand Leagues under the Sea &  8                                    & miss                                  &  1                                    & 1                                     & Pride and Prejudice                  \\\\\n",
       "\t Great Expectations                    & 31                                    & miss                                  &  1                                    & 1                                     & Pride and Prejudice                  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | chapter | term | count | .topic | consensus | \n",
       "|---|---|---|---|---|---|\n",
       "| Great Expectations                    | 38                                    | brother                               |  2                                    | 1                                     | Pride and Prejudice                   | \n",
       "| Great Expectations                    | 22                                    | brother                               |  4                                    | 1                                     | Pride and Prejudice                   | \n",
       "| Great Expectations                    | 23                                    | miss                                  |  2                                    | 1                                     | Pride and Prejudice                   | \n",
       "| Great Expectations                    | 22                                    | miss                                  | 23                                    | 1                                     | Pride and Prejudice                   | \n",
       "| Twenty Thousand Leagues under the Sea |  8                                    | miss                                  |  1                                    | 1                                     | Pride and Prejudice                   | \n",
       "| Great Expectations                    | 31                                    | miss                                  |  1                                    | 1                                     | Pride and Prejudice                   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title                                 chapter term    count .topic\n",
       "1 Great Expectations                    38      brother  2    1     \n",
       "2 Great Expectations                    22      brother  4    1     \n",
       "3 Great Expectations                    23      miss     2    1     \n",
       "4 Great Expectations                    22      miss    23    1     \n",
       "5 Twenty Thousand Leagues under the Sea  8      miss     1    1     \n",
       "6 Great Expectations                    31      miss     1    1     \n",
       "  consensus          \n",
       "1 Pride and Prejudice\n",
       "2 Pride and Prejudice\n",
       "3 Pride and Prejudice\n",
       "4 Pride and Prejudice\n",
       "5 Pride and Prejudice\n",
       "6 Pride and Prejudice"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t3140 obs. of  6 variables:\n",
      " $ title    : chr  \"Great Expectations\" \"Great Expectations\" \"Great Expectations\" \"Great Expectations\" ...\n",
      " $ chapter  : int  38 22 23 22 8 31 5 22 31 23 ...\n",
      " $ term     : chr  \"brother\" \"brother\" \"miss\" \"miss\" ...\n",
      " $ count    : num  2 4 2 23 1 1 37 3 2 10 ...\n",
      " $ .topic   : num  1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ consensus: chr  \"Pride and Prejudice\" \"Pride and Prejudice\" \"Pride and Prejudice\" \"Pride and Prejudice\" ...\n"
     ]
    }
   ],
   "source": [
    "wrong_words <- assignments %>% filter(title != consensus)\n",
    "head(wrong_words)\n",
    "str(wrong_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>consensus</th><th scope=col>term</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>love               </td><td>44                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>sergeant           </td><td>37                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>lady               </td><td>32                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>miss               </td><td>26                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>father             </td><td>19                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>baby               </td><td>18                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>flopson            </td><td>18                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>family             </td><td>16                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>replied            </td><td>16                 </td></tr>\n",
       "\t<tr><td>Great Expectations </td><td>Pride and Prejudice</td><td>attention          </td><td>14                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " title & consensus & term & n\\\\\n",
       "\\hline\n",
       "\t Great Expectations  & Pride and Prejudice & love                & 44                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & sergeant            & 37                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & lady                & 32                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & miss                & 26                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & father              & 19                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & baby                & 18                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & flopson             & 18                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & family              & 16                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & replied             & 16                 \\\\\n",
       "\t Great Expectations  & Pride and Prejudice & attention           & 14                 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title | consensus | term | n | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| Great Expectations  | Pride and Prejudice | love                | 44                  | \n",
       "| Great Expectations  | Pride and Prejudice | sergeant            | 37                  | \n",
       "| Great Expectations  | Pride and Prejudice | lady                | 32                  | \n",
       "| Great Expectations  | Pride and Prejudice | miss                | 26                  | \n",
       "| Great Expectations  | Pride and Prejudice | father              | 19                  | \n",
       "| Great Expectations  | Pride and Prejudice | baby                | 18                  | \n",
       "| Great Expectations  | Pride and Prejudice | flopson             | 18                  | \n",
       "| Great Expectations  | Pride and Prejudice | family              | 16                  | \n",
       "| Great Expectations  | Pride and Prejudice | replied             | 16                  | \n",
       "| Great Expectations  | Pride and Prejudice | attention           | 14                  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   title              consensus           term      n \n",
       "1  Great Expectations Pride and Prejudice love      44\n",
       "2  Great Expectations Pride and Prejudice sergeant  37\n",
       "3  Great Expectations Pride and Prejudice lady      32\n",
       "4  Great Expectations Pride and Prejudice miss      26\n",
       "5  Great Expectations Pride and Prejudice father    19\n",
       "6  Great Expectations Pride and Prejudice baby      18\n",
       "7  Great Expectations Pride and Prejudice flopson   18\n",
       "8  Great Expectations Pride and Prejudice family    16\n",
       "9  Great Expectations Pride and Prejudice replied   16\n",
       "10 Great Expectations Pride and Prejudice attention 14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_words %>% count(title, consensus, term, wt = count) %>% ungroup() %>% arrange(desc(n)) %>% head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the word **“flopson”** here; these **wrong words do not necessarily appear in the novels they were misassigned to**. Indeed, we can confirm “flopson” appears only in Great Expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title_chapter</th><th scope=col>word</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Great Expectations_22</td><td>flopson              </td><td>10                   </td></tr>\n",
       "\t<tr><td>Great Expectations_23</td><td>flopson              </td><td> 7                   </td></tr>\n",
       "\t<tr><td>Great Expectations_33</td><td>flopson              </td><td> 1                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " title\\_chapter & word & n\\\\\n",
       "\\hline\n",
       "\t Great Expectations\\_22 & flopson                 & 10                     \\\\\n",
       "\t Great Expectations\\_23 & flopson                 &  7                     \\\\\n",
       "\t Great Expectations\\_33 & flopson                 &  1                     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "title_chapter | word | n | \n",
       "|---|---|---|\n",
       "| Great Expectations_22 | flopson               | 10                    | \n",
       "| Great Expectations_23 | flopson               |  7                    | \n",
       "| Great Expectations_33 | flopson               |  1                    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  title_chapter         word    n \n",
       "1 Great Expectations_22 flopson 10\n",
       "2 Great Expectations_23 flopson  7\n",
       "3 Great Expectations_33 flopson  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts %>% filter(word == \"flopson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is stochastic and iterative, and it can accidentally land on a topic that spans multiple books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: mining NASA metadata\n",
    "https://www.tidytextmining.com/nasa.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(methods)\n",
    "theme_set(theme_light())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 32,000 datasets hosted and/or maintained by [NASA](https://www.nasa.gov/); \n",
    "- these datasets cover **topics from Earth science to aerospace engineering to management of NASA itself**. We can use the metadata for these datasets to understand the connections between them. \n",
    "- The metadata includes information like the title of the dataset, a description field, what **organization(s)** within NASA is responsible for the dataset, **keywords** for the dataset that have been assigned by a human being, and so forth. \n",
    "- NASA places a high priority on making its data open and accessible, even requiring all NASA-funded research to be [openly accessible online](https://www.nasa.gov/press-release/nasa-unveils-new-public-web-portal-for-research-results). The metadata for all its datasets is [publicly available online in JSON format](https://data.nasa.gov/data.json).\n",
    "- **We will treat the NASA metadata as a text dataset**: \n",
    "    - word co-occurrences \n",
    "    - correlations, \n",
    "    - tf-idf,\n",
    "    - topic modeling **to explore the connections between the datasets**: find datasets that are related to each other (=find clusters of similar datasets)\n",
    "- text fields: \n",
    "    - title, description, keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How data is organized at NASA\n",
    "let's download the JSON file and take a look at the names of what is stored in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'_id'</li>\n",
       "\t<li>'@type'</li>\n",
       "\t<li>'accessLevel'</li>\n",
       "\t<li>'accrualPeriodicity'</li>\n",
       "\t<li>'bureauCode'</li>\n",
       "\t<li>'contactPoint'</li>\n",
       "\t<li>'description'</li>\n",
       "\t<li>'distribution'</li>\n",
       "\t<li>'identifier'</li>\n",
       "\t<li>'issued'</li>\n",
       "\t<li>'keyword'</li>\n",
       "\t<li>'landingPage'</li>\n",
       "\t<li>'language'</li>\n",
       "\t<li>'modified'</li>\n",
       "\t<li>'programCode'</li>\n",
       "\t<li>'publisher'</li>\n",
       "\t<li>'spatial'</li>\n",
       "\t<li>'temporal'</li>\n",
       "\t<li>'theme'</li>\n",
       "\t<li>'title'</li>\n",
       "\t<li>'license'</li>\n",
       "\t<li>'isPartOf'</li>\n",
       "\t<li>'references'</li>\n",
       "\t<li>'rights'</li>\n",
       "\t<li>'describedBy'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '\\_id'\n",
       "\\item '@type'\n",
       "\\item 'accessLevel'\n",
       "\\item 'accrualPeriodicity'\n",
       "\\item 'bureauCode'\n",
       "\\item 'contactPoint'\n",
       "\\item 'description'\n",
       "\\item 'distribution'\n",
       "\\item 'identifier'\n",
       "\\item 'issued'\n",
       "\\item 'keyword'\n",
       "\\item 'landingPage'\n",
       "\\item 'language'\n",
       "\\item 'modified'\n",
       "\\item 'programCode'\n",
       "\\item 'publisher'\n",
       "\\item 'spatial'\n",
       "\\item 'temporal'\n",
       "\\item 'theme'\n",
       "\\item 'title'\n",
       "\\item 'license'\n",
       "\\item 'isPartOf'\n",
       "\\item 'references'\n",
       "\\item 'rights'\n",
       "\\item 'describedBy'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '_id'\n",
       "2. '@type'\n",
       "3. 'accessLevel'\n",
       "4. 'accrualPeriodicity'\n",
       "5. 'bureauCode'\n",
       "6. 'contactPoint'\n",
       "7. 'description'\n",
       "8. 'distribution'\n",
       "9. 'identifier'\n",
       "10. 'issued'\n",
       "11. 'keyword'\n",
       "12. 'landingPage'\n",
       "13. 'language'\n",
       "14. 'modified'\n",
       "15. 'programCode'\n",
       "16. 'publisher'\n",
       "17. 'spatial'\n",
       "18. 'temporal'\n",
       "19. 'theme'\n",
       "20. 'title'\n",
       "21. 'license'\n",
       "22. 'isPartOf'\n",
       "23. 'references'\n",
       "24. 'rights'\n",
       "25. 'describedBy'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"_id\"                \"@type\"              \"accessLevel\"       \n",
       " [4] \"accrualPeriodicity\" \"bureauCode\"         \"contactPoint\"      \n",
       " [7] \"description\"        \"distribution\"       \"identifier\"        \n",
       "[10] \"issued\"             \"keyword\"            \"landingPage\"       \n",
       "[13] \"language\"           \"modified\"           \"programCode\"       \n",
       "[16] \"publisher\"          \"spatial\"            \"temporal\"          \n",
       "[19] \"theme\"              \"title\"              \"license\"           \n",
       "[22] \"isPartOf\"           \"references\"         \"rights\"            \n",
       "[25] \"describedBy\"       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#library(jsonlite)\n",
    "#metadata <- fromJSON(\"https://data.nasa.gov/data.json\")\n",
    "load(\"data/metadata.rda\")\n",
    "names(metadata$dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we could extract information from **who publishes each dataset** to what license they are released under. \n",
    "- title, description, keywords are best for drawing connections between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(metadata$dataset$title)\n",
    "class(metadata$dataset$description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title & description - stored as character vectors, keywords - stored as a list of character vectors.\n",
    "### Wrangling and tidying the data\n",
    "Let's set up separate tidy data frames for title, description, and keyword, keeping the dataset ids for each so that we can connect them later if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>id</th><th scope=col>title</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>55942a57c63a7fe59b495a77                           </td><td>15 Minute Stream Flow Data: USGS (FIFE)            </td></tr>\n",
       "\t<tr><td>55942a57c63a7fe59b495a78                           </td><td>15 Minute Stream Flow Data: USGS (FIFE)            </td></tr>\n",
       "\t<tr><td>55942a58c63a7fe59b495a79                           </td><td>15 Minute Stream Flow Data: USGS (FIFE)            </td></tr>\n",
       "\t<tr><td>55942a58c63a7fe59b495a7a                           </td><td>2000 Pilot Environmental Sustainability Index (ESI)</td></tr>\n",
       "\t<tr><td>55942a58c63a7fe59b495a7b                           </td><td>2000 Pilot Environmental Sustainability Index (ESI)</td></tr>\n",
       "\t<tr><td>55942a58c63a7fe59b495a7c                           </td><td>2000 Pilot Environmental Sustainability Index (ESI)</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " id & title\\\\\n",
       "\\hline\n",
       "\t 55942a57c63a7fe59b495a77                            & 15 Minute Stream Flow Data: USGS (FIFE)            \\\\\n",
       "\t 55942a57c63a7fe59b495a78                            & 15 Minute Stream Flow Data: USGS (FIFE)            \\\\\n",
       "\t 55942a58c63a7fe59b495a79                            & 15 Minute Stream Flow Data: USGS (FIFE)            \\\\\n",
       "\t 55942a58c63a7fe59b495a7a                            & 2000 Pilot Environmental Sustainability Index (ESI)\\\\\n",
       "\t 55942a58c63a7fe59b495a7b                            & 2000 Pilot Environmental Sustainability Index (ESI)\\\\\n",
       "\t 55942a58c63a7fe59b495a7c                            & 2000 Pilot Environmental Sustainability Index (ESI)\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "id | title | \n",
       "|---|---|---|---|---|---|\n",
       "| 55942a57c63a7fe59b495a77                            | 15 Minute Stream Flow Data: USGS (FIFE)             | \n",
       "| 55942a57c63a7fe59b495a78                            | 15 Minute Stream Flow Data: USGS (FIFE)             | \n",
       "| 55942a58c63a7fe59b495a79                            | 15 Minute Stream Flow Data: USGS (FIFE)             | \n",
       "| 55942a58c63a7fe59b495a7a                            | 2000 Pilot Environmental Sustainability Index (ESI) | \n",
       "| 55942a58c63a7fe59b495a7b                            | 2000 Pilot Environmental Sustainability Index (ESI) | \n",
       "| 55942a58c63a7fe59b495a7c                            | 2000 Pilot Environmental Sustainability Index (ESI) | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  id                       title                                              \n",
       "1 55942a57c63a7fe59b495a77 15 Minute Stream Flow Data: USGS (FIFE)            \n",
       "2 55942a57c63a7fe59b495a78 15 Minute Stream Flow Data: USGS (FIFE)            \n",
       "3 55942a58c63a7fe59b495a79 15 Minute Stream Flow Data: USGS (FIFE)            \n",
       "4 55942a58c63a7fe59b495a7a 2000 Pilot Environmental Sustainability Index (ESI)\n",
       "5 55942a58c63a7fe59b495a7b 2000 Pilot Environmental Sustainability Index (ESI)\n",
       "6 55942a58c63a7fe59b495a7c 2000 Pilot Environmental Sustainability Index (ESI)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t32089 obs. of  2 variables:\n",
      " $ id   : chr  \"55942a57c63a7fe59b495a77\" \"55942a57c63a7fe59b495a78\" \"55942a58c63a7fe59b495a79\" \"55942a58c63a7fe59b495a7a\" ...\n",
      " $ title: chr  \"15 Minute Stream Flow Data: USGS (FIFE)\" \"15 Minute Stream Flow Data: USGS (FIFE)\" \"15 Minute Stream Flow Data: USGS (FIFE)\" \"2000 Pilot Environmental Sustainability Index (ESI)\" ...\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "nasa_title <- data_frame(id = metadata$dataset$`_id`$`$oid`, title = metadata$dataset$title)\n",
    "head(nasa_title) # there are duplicate titles on separate datasets\n",
    "str(nasa_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>desc</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Coherent Logix, Incorporated proposes the Software Defined Common Processing System (SDCPS) program to facilitate the development of a Software Defined Radio development kit based on HyperX Technology with an accompanying software development flow to support rapid development and fielding of this technology to NASA and high reliability system integrators.  NASA's exploration, science, and space operations systems are critically dependent on the hardware technologies used in their implementation.  Specifically, the performance and deployment of autonomous and computationally-intensive capabilities for space based observatories, orbiters, autonomous landing and hazard avoidance, autonomous rendezvous and capture, robotic, relative navigation, command, control and communications systems are directly dependent on the availability of radiation-tolerant, high-performance, reconfigurable and adaptable, modern communications and underlying energy-efficient processor technology.  The HyperX Technology will simultaneously enable order of magnitude improvement in power savings while reducing chipset count, thus size and weight of the radio.  The HyperX processor technology is fully programmable and reconfigurable on the fly and is supported by industry standards based [hardware agnostic software development flow and] programming model using ANSI-C and MPI (message passing interface) API.  This provides reduced life-cycle costs and future proofing of hardware through fully portable software code.                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr>\n",
       "\t<tr><td>The land water content data used to create these images were generated from The Global Land Data Assimilation System (GLDAS).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr>\n",
       "\t<tr><td>MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the\n",
       "Terra (EOS AM) and Aqua (EOS PM) satellites. Terra's orbit around the Earth is timed so\n",
       "that it passes from north to south across the equator in the morning, while Aqua passes\n",
       "south to north over the equator in the afternoon. Terra MODIS and Aqua MODIS are viewing\n",
       "the entire Earth's surface every 1 to 2 days, acquiring data in 36 spectral bands, or\n",
       "groups of wavelengths (see MODIS Technical Specifications). These data will improve our\n",
       "understanding of global dynamics and processes occurring on the land, in the oceans,\n",
       "and in the lower atmosphere. MODIS is playing a vital role in the development of\n",
       "validated, global, interactive Earth system models able to predict global change\n",
       "accurately enough to assist policy makers in making sound decisions concerning the\n",
       "protection of our environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr>\n",
       "\t<tr><td>The Coastal Zone Color Scanner Experiment (CZCS) was the first instrument devoted to the measurement of ocean color and flown on a spacecraft. Although other instruments flown on other spacecraft had sensed ocean color, their spectral bands, spatial resolution and dynamic range were optimized for land or meteorological use and had limited sensitivity in this area, whereas in CZCS, every parameter was optimized for use over water to the exclusion of any other type of sensing. CZCS had six spectral bands, four of which were used primarily for ocean color. These were of a 20 nanometer bandwidth centered at 443, 520, 550, and 670 nm. Band 5 had a 100 nm bandwidth centered at 750 nm and a dynamic range more suited to land. Band 6 operated in the 10.5 to 12.5 micrometer region and sensed emitted thermal radiance for derivation of equivalent black body temperature. (This thermal band failed within the first year of the mission, and so was not used in the global processing effort.) Bands 1-4 were preset to view water only and saturated when the IFOV was over most types of land surfaces, or clouds.\n",
       "        \n",
       "        The most important objective of the Coastal Zone Color Scanner mission was to determine if satellite remote sensing of color could be used to identify and quantify material suspended or dissolved in ocean waters. Specifically CZCS attempted to discriminate between organic and inorganic materials in the water, determine the quantity of material and discriminate between different organic particulate types.\n",
       "        \n",
       "        This product contains monthly climatological Level-3 Standard Mapped Image (Equal-Angular Cylindrical grid) data products.  Each product contains one of the following parameters at 4 or 9 km resolution:\n",
       "        Remote sensing reflectance at 443, 520, 555 and 670nm\n",
       "        Diffuse attenuation at 490nm\n",
       "        Aerosol opticatl thickness at 670nm\n",
       "        Chlorophyll a concentration\n",
       "    </td></tr>\n",
       "\t<tr><td>The Aura-OMI Daily Gridded Surface UV Irradiance Product (OMUVBd) is now available from the NASA Goddard Earth Sciences Data and Information Services Center (GES DISC) for the public access.\n",
       "\n",
       "(The shortname for this Level-3 OMI Surface UVB product is OMUVBd_V003)\n",
       "\n",
       "The algorithm team consists of FMI scientists Drs. J. Hovila, A. Arola and J. Tamminen.\n",
       "\n",
       "The OMUVBd product contains global erythemally weighted daily dose and erythemal dose rate at local solar noon at 1.0x1.0 deg grids.\n",
       "\n",
       "OMUVBd files are available in EOS Hierarchical Data Format (HDF-EOS) and TOMS-Like ASCII Format. Each file contains daily data from the day lit portion of the globe. The maximum file size for the OMUVBd data product is about 2 MBytes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " desc\\\\\n",
       "\\hline\n",
       "\t Coherent Logix, Incorporated proposes the Software Defined Common Processing System (SDCPS) program to facilitate the development of a Software Defined Radio development kit based on HyperX Technology with an accompanying software development flow to support rapid development and fielding of this technology to NASA and high reliability system integrators.  NASA's exploration, science, and space operations systems are critically dependent on the hardware technologies used in their implementation.  Specifically, the performance and deployment of autonomous and computationally-intensive capabilities for space based observatories, orbiters, autonomous landing and hazard avoidance, autonomous rendezvous and capture, robotic, relative navigation, command, control and communications systems are directly dependent on the availability of radiation-tolerant, high-performance, reconfigurable and adaptable, modern communications and underlying energy-efficient processor technology.  The HyperX Technology will simultaneously enable order of magnitude improvement in power savings while reducing chipset count, thus size and weight of the radio.  The HyperX processor technology is fully programmable and reconfigurable on the fly and is supported by industry standards based {[}hardware agnostic software development flow and{]} programming model using ANSI-C and MPI (message passing interface) API.  This provides reduced life-cycle costs and future proofing of hardware through fully portable software code.                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\\\\n",
       "\t The land water content data used to create these images were generated from The Global Land Data Assimilation System (GLDAS).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\\\\n",
       "\t MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the\n",
       "Terra (EOS AM) and Aqua (EOS PM) satellites. Terra's orbit around the Earth is timed so\n",
       "that it passes from north to south across the equator in the morning, while Aqua passes\n",
       "south to north over the equator in the afternoon. Terra MODIS and Aqua MODIS are viewing\n",
       "the entire Earth's surface every 1 to 2 days, acquiring data in 36 spectral bands, or\n",
       "groups of wavelengths (see MODIS Technical Specifications). These data will improve our\n",
       "understanding of global dynamics and processes occurring on the land, in the oceans,\n",
       "and in the lower atmosphere. MODIS is playing a vital role in the development of\n",
       "validated, global, interactive Earth system models able to predict global change\n",
       "accurately enough to assist policy makers in making sound decisions concerning the\n",
       "protection of our environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\\\\n",
       "\t The Coastal Zone Color Scanner Experiment (CZCS) was the first instrument devoted to the measurement of ocean color and flown on a spacecraft. Although other instruments flown on other spacecraft had sensed ocean color, their spectral bands, spatial resolution and dynamic range were optimized for land or meteorological use and had limited sensitivity in this area, whereas in CZCS, every parameter was optimized for use over water to the exclusion of any other type of sensing. CZCS had six spectral bands, four of which were used primarily for ocean color. These were of a 20 nanometer bandwidth centered at 443, 520, 550, and 670 nm. Band 5 had a 100 nm bandwidth centered at 750 nm and a dynamic range more suited to land. Band 6 operated in the 10.5 to 12.5 micrometer region and sensed emitted thermal radiance for derivation of equivalent black body temperature. (This thermal band failed within the first year of the mission, and so was not used in the global processing effort.) Bands 1-4 were preset to view water only and saturated when the IFOV was over most types of land surfaces, or clouds.\n",
       "        \n",
       "        The most important objective of the Coastal Zone Color Scanner mission was to determine if satellite remote sensing of color could be used to identify and quantify material suspended or dissolved in ocean waters. Specifically CZCS attempted to discriminate between organic and inorganic materials in the water, determine the quantity of material and discriminate between different organic particulate types.\n",
       "        \n",
       "        This product contains monthly climatological Level-3 Standard Mapped Image (Equal-Angular Cylindrical grid) data products.  Each product contains one of the following parameters at 4 or 9 km resolution:\n",
       "        Remote sensing reflectance at 443, 520, 555 and 670nm\n",
       "        Diffuse attenuation at 490nm\n",
       "        Aerosol opticatl thickness at 670nm\n",
       "        Chlorophyll a concentration\n",
       "    \\\\\n",
       "\t The Aura-OMI Daily Gridded Surface UV Irradiance Product (OMUVBd) is now available from the NASA Goddard Earth Sciences Data and Information Services Center (GES DISC) for the public access.\n",
       "\n",
       "(The shortname for this Level-3 OMI Surface UVB product is OMUVBd\\_V003)\n",
       "\n",
       "The algorithm team consists of FMI scientists Drs. J. Hovila, A. Arola and J. Tamminen.\n",
       "\n",
       "The OMUVBd product contains global erythemally weighted daily dose and erythemal dose rate at local solar noon at 1.0x1.0 deg grids.\n",
       "\n",
       "OMUVBd files are available in EOS Hierarchical Data Format (HDF-EOS) and TOMS-Like ASCII Format. Each file contains daily data from the day lit portion of the globe. The maximum file size for the OMUVBd data product is about 2 MBytes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "desc | \n",
       "|---|---|---|---|---|\n",
       "| Coherent Logix, Incorporated proposes the Software Defined Common Processing System (SDCPS) program to facilitate the development of a Software Defined Radio development kit based on HyperX Technology with an accompanying software development flow to support rapid development and fielding of this technology to NASA and high reliability system integrators.  NASA's exploration, science, and space operations systems are critically dependent on the hardware technologies used in their implementation.  Specifically, the performance and deployment of autonomous and computationally-intensive capabilities for space based observatories, orbiters, autonomous landing and hazard avoidance, autonomous rendezvous and capture, robotic, relative navigation, command, control and communications systems are directly dependent on the availability of radiation-tolerant, high-performance, reconfigurable and adaptable, modern communications and underlying energy-efficient processor technology.  The HyperX Technology will simultaneously enable order of magnitude improvement in power savings while reducing chipset count, thus size and weight of the radio.  The HyperX processor technology is fully programmable and reconfigurable on the fly and is supported by industry standards based [hardware agnostic software development flow and] programming model using ANSI-C and MPI (message passing interface) API.  This provides reduced life-cycle costs and future proofing of hardware through fully portable software code.                                                                                                                                                                                                                                                                                                                                                                                                                                                    | \n",
       "| The land water content data used to create these images were generated from The Global Land Data Assimilation System (GLDAS).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | \n",
       "| MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the\n",
       "Terra (EOS AM) and Aqua (EOS PM) satellites. Terra's orbit around the Earth is timed so\n",
       "that it passes from north to south across the equator in the morning, while Aqua passes\n",
       "south to north over the equator in the afternoon. Terra MODIS and Aqua MODIS are viewing\n",
       "the entire Earth's surface every 1 to 2 days, acquiring data in 36 spectral bands, or\n",
       "groups of wavelengths (see MODIS Technical Specifications). These data will improve our\n",
       "understanding of global dynamics and processes occurring on the land, in the oceans,\n",
       "and in the lower atmosphere. MODIS is playing a vital role in the development of\n",
       "validated, global, interactive Earth system models able to predict global change\n",
       "accurately enough to assist policy makers in making sound decisions concerning the\n",
       "protection of our environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | \n",
       "| The Coastal Zone Color Scanner Experiment (CZCS) was the first instrument devoted to the measurement of ocean color and flown on a spacecraft. Although other instruments flown on other spacecraft had sensed ocean color, their spectral bands, spatial resolution and dynamic range were optimized for land or meteorological use and had limited sensitivity in this area, whereas in CZCS, every parameter was optimized for use over water to the exclusion of any other type of sensing. CZCS had six spectral bands, four of which were used primarily for ocean color. These were of a 20 nanometer bandwidth centered at 443, 520, 550, and 670 nm. Band 5 had a 100 nm bandwidth centered at 750 nm and a dynamic range more suited to land. Band 6 operated in the 10.5 to 12.5 micrometer region and sensed emitted thermal radiance for derivation of equivalent black body temperature. (This thermal band failed within the first year of the mission, and so was not used in the global processing effort.) Bands 1-4 were preset to view water only and saturated when the IFOV was over most types of land surfaces, or clouds.\n",
       "        \n",
       "        The most important objective of the Coastal Zone Color Scanner mission was to determine if satellite remote sensing of color could be used to identify and quantify material suspended or dissolved in ocean waters. Specifically CZCS attempted to discriminate between organic and inorganic materials in the water, determine the quantity of material and discriminate between different organic particulate types.\n",
       "        \n",
       "        This product contains monthly climatological Level-3 Standard Mapped Image (Equal-Angular Cylindrical grid) data products.  Each product contains one of the following parameters at 4 or 9 km resolution:\n",
       "        Remote sensing reflectance at 443, 520, 555 and 670nm\n",
       "        Diffuse attenuation at 490nm\n",
       "        Aerosol opticatl thickness at 670nm\n",
       "        Chlorophyll a concentration\n",
       "     | \n",
       "| The Aura-OMI Daily Gridded Surface UV Irradiance Product (OMUVBd) is now available from the NASA Goddard Earth Sciences Data and Information Services Center (GES DISC) for the public access.\n",
       "\n",
       "(The shortname for this Level-3 OMI Surface UVB product is OMUVBd_V003)\n",
       "\n",
       "The algorithm team consists of FMI scientists Drs. J. Hovila, A. Arola and J. Tamminen.\n",
       "\n",
       "The OMUVBd product contains global erythemally weighted daily dose and erythemal dose rate at local solar noon at 1.0x1.0 deg grids.\n",
       "\n",
       "OMUVBd files are available in EOS Hierarchical Data Format (HDF-EOS) and TOMS-Like ASCII Format. Each file contains daily data from the day lit portion of the globe. The maximum file size for the OMUVBd data product is about 2 MBytes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "1 Coherent Logix, Incorporated proposes the Software Defined Common Processing System (SDCPS) program to facilitate the development of a Software Defined Radio development kit based on HyperX Technology with an accompanying software development flow to support rapid development and fielding of this technology to NASA and high reliability system integrators.  NASA's exploration, science, and space operations systems are critically dependent on the hardware technologies used in their implementation.  Specifically, the performance and deployment of autonomous and computationally-intensive capabilities for space based observatories, orbiters, autonomous landing and hazard avoidance, autonomous rendezvous and capture, robotic, relative navigation, command, control and communications systems are directly dependent on the availability of radiation-tolerant, high-performance, reconfigurable and adaptable, modern communications and underlying energy-efficient processor technology.  The HyperX Technology will simultaneously enable order of magnitude improvement in power savings while reducing chipset count, thus size and weight of the radio.  The HyperX processor technology is fully programmable and reconfigurable on the fly and is supported by industry standards based [hardware agnostic software development flow and] programming model using ANSI-C and MPI (message passing interface) API.  This provides reduced life-cycle costs and future proofing of hardware through fully portable software code.                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2 The land water content data used to create these images were generated from The Global Land Data Assimilation System (GLDAS).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3 MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the\\nTerra (EOS AM) and Aqua (EOS PM) satellites. Terra's orbit around the Earth is timed so\\nthat it passes from north to south across the equator in the morning, while Aqua passes\\nsouth to north over the equator in the afternoon. Terra MODIS and Aqua MODIS are viewing\\nthe entire Earth's surface every 1 to 2 days, acquiring data in 36 spectral bands, or\\ngroups of wavelengths (see MODIS Technical Specifications). These data will improve our\\nunderstanding of global dynamics and processes occurring on the land, in the oceans,\\nand in the lower atmosphere. MODIS is playing a vital role in the development of\\nvalidated, global, interactive Earth system models able to predict global change\\naccurately enough to assist policy makers in making sound decisions concerning the\\nprotection of our environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4 The Coastal Zone Color Scanner Experiment (CZCS) was the first instrument devoted to the measurement of ocean color and flown on a spacecraft. Although other instruments flown on other spacecraft had sensed ocean color, their spectral bands, spatial resolution and dynamic range were optimized for land or meteorological use and had limited sensitivity in this area, whereas in CZCS, every parameter was optimized for use over water to the exclusion of any other type of sensing. CZCS had six spectral bands, four of which were used primarily for ocean color. These were of a 20 nanometer bandwidth centered at 443, 520, 550, and 670 nm. Band 5 had a 100 nm bandwidth centered at 750 nm and a dynamic range more suited to land. Band 6 operated in the 10.5 to 12.5 micrometer region and sensed emitted thermal radiance for derivation of equivalent black body temperature. (This thermal band failed within the first year of the mission, and so was not used in the global processing effort.) Bands 1-4 were preset to view water only and saturated when the IFOV was over most types of land surfaces, or clouds.\\n        \\n        The most important objective of the Coastal Zone Color Scanner mission was to determine if satellite remote sensing of color could be used to identify and quantify material suspended or dissolved in ocean waters. Specifically CZCS attempted to discriminate between organic and inorganic materials in the water, determine the quantity of material and discriminate between different organic particulate types.\\n        \\n        This product contains monthly climatological Level-3 Standard Mapped Image (Equal-Angular Cylindrical grid) data products.  Each product contains one of the following parameters at 4 or 9 km resolution:\\n        Remote sensing reflectance at 443, 520, 555 and 670nm\\n        Diffuse attenuation at 490nm\\n        Aerosol opticatl thickness at 670nm\\n        Chlorophyll a concentration\\n    \n",
       "5 The Aura-OMI Daily Gridded Surface UV Irradiance Product (OMUVBd) is now available from the NASA Goddard Earth Sciences Data and Information Services Center (GES DISC) for the public access.\\n\\n(The shortname for this Level-3 OMI Surface UVB product is OMUVBd_V003)\\n\\nThe algorithm team consists of FMI scientists Drs. J. Hovila, A. Arola and J. Tamminen.\\n\\nThe OMUVBd product contains global erythemally weighted daily dose and erythemal dose rate at local solar noon at 1.0x1.0 deg grids.\\n\\nOMUVBd files are available in EOS Hierarchical Data Format (HDF-EOS) and TOMS-Like ASCII Format. Each file contains daily data from the day lit portion of the globe. The maximum file size for the OMUVBd data product is about 2 MBytes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nasa_desc <- data_frame(id = metadata$dataset$`_id`$`$oid`, desc = metadata$dataset$description)\n",
    "nasa_desc %>% select(desc) %>% sample_n(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the first part of several selected description fields from the metadata.\n",
    "Now we can build the tidy data frame for the keywords. For this one, we need to use `unnest()` from tidyr, because they are in a list-column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_keyword <- data_frame(id = metadata$dataset$`_id`$`$oid`, keyword = metadata$dataset$keyword) %>% unnest(keyword)\n",
    "head(nasa_keyword)\n",
    "str(nasa_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tidy data frame because we have one row for each keyword; this means we will have multiple rows for each dataset because a dataset can have more than one keyword.\n",
    "Now it is time to use tidytext's `unnest_tokens()` for the title and description fields so we can do the text analysis. Let's also remove stop words from the titles and descriptions. We will not remove stop words from the keywords, because those are short, human-assigned keywords like \"RADIATION\" or \"CLIMATE INDICATORS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidytext)\n",
    "nasa_title <- nasa_title %>% unnest_tokens(word, title) %>% anti_join(stop_words)\n",
    "head(nasa_title)\n",
    "str(nasa_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are now in the tidy text format that we have been working with throughout this book, with one token (word, in this case) per row; let's take a look before we move on in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_desc <- nasa_desc %>% unnest_tokens(word, desc) %>% anti_join(stop_words)\n",
    "nasa_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some initial simple exploration\n",
    "What are the most common words in the NASA dataset titles? We can use `count()` from dplyr to check this out.\n",
    "\n",
    "```{r dependson = \"unnest\"}\n",
    "nasa_title %>% count(word, sort = TRUE)\n",
    "```\n",
    "\n",
    "What about the descriptions?\n",
    "\n",
    "```{r dependson = \"unnest\"}\n",
    "nasa_desc %>% count(word, sort = TRUE)\n",
    "```\n",
    "\n",
    "Words like \"data\" and \"global\" are used very often in NASA titles and descriptions. We may want to remove digits and some \"words\" like \"v1\" from these data frames for many types of analyses; they are not too meaningful for most audiences. \n",
    "\n",
    "```{block, type = \"rmdtip\"}\n",
    "We can do this by making a list of custom stop words and using `anti_join()` to remove them from the data frame, just like we removed the default stop words that are in the tidytext package. This approach can be used in many instances and is a great tool to bear in mind.\n",
    "```\n",
    "\n",
    "```{r my_stopwords, dependson = \"unnest\"}\n",
    "my_stopwords <- data_frame(word = c(as.character(1:10), \n",
    "                                    \"v1\", \"v03\", \"l2\", \"l3\", \"l4\", \"v5.2.0\", \n",
    "                                    \"v003\", \"v004\", \"v005\", \"v006\", \"v7\"))\n",
    "nasa_title <- nasa_title %>% anti_join(my_stopwords)\n",
    "nasa_desc <- nasa_desc %>% anti_join(my_stopwords)\n",
    "```\n",
    "\n",
    "What are the most common keywords?\n",
    "\n",
    "```{r dependson = \"keyword\"}\n",
    "nasa_keyword %>% \n",
    "  group_by(keyword) %>% \n",
    "  count(sort = TRUE)\n",
    "```\n",
    "\n",
    "We likely want to change all of the keywords to either lower or upper case to get rid of duplicates like \"OCEANS\" and \"Oceans\". Let's do that here.\n",
    "\n",
    "```{r toupper, dependson = \"keyword\"}\n",
    "nasa_keyword <- nasa_keyword %>% \n",
    "  mutate(keyword = toupper(keyword))\n",
    "```\n",
    "\n",
    "## Word co-ocurrences and correlations\n",
    "\n",
    "As a next step, let's examine which words commonly occur together in the titles, descriptions, and keywords of NASA datasets, as described in Chapter \\@ref(ngrams). We can then examine word networks for these fields; this may help us see, for example, which datasets are related to each other. \n",
    "\n",
    "### Networks of Description and Title Words\n",
    "\n",
    "We can use `pairwise_count()` from the widyr package to count how many times each pair of words occurs together in a title or description field.\n",
    "\n",
    "```{r title_word_pairs, dependson = \"my_stopwords\"}\n",
    "library(widyr)\n",
    "\n",
    "title_word_pairs <- nasa_title %>% \n",
    "  pairwise_count(word, id, sort = TRUE, upper = FALSE)\n",
    "\n",
    "title_word_pairs\n",
    "```\n",
    "\n",
    "These are the pairs of words that occur together most often in title fields. Some of these words are obviously acronyms used within NASA, and we see how often words like \"project\" and \"system\" are used.\n",
    "\n",
    "```{r desc_word_pairs, dependson = \"my_stopwords\"}\n",
    "desc_word_pairs <- nasa_desc %>% \n",
    "  pairwise_count(word, id, sort = TRUE, upper = FALSE)\n",
    "\n",
    "desc_word_pairs\n",
    "```\n",
    "\n",
    "These are the pairs of words that occur together most often in descripton fields. \"Data\" is a very common word in description fields; there is no shortage of data in the datasets at NASA!\n",
    "\n",
    "Let's plot networks of these co-occurring words so we can see these relationships better in Figure \\@ref(fig:plottitle). We will again use the ggraph package for visualizing our networks.\n",
    "\n",
    "```{r plottitle, dependson = \"title_word_pairs\", fig.height=6, fig.width=9, fig.cap=\"Word network in NASA dataset titles\"}\n",
    "library(ggplot2)\n",
    "library(igraph)\n",
    "library(ggraph)\n",
    "\n",
    "set.seed(1234)\n",
    "title_word_pairs %>%\n",
    "  filter(n >= 250) %>%\n",
    "  graph_from_data_frame() %>%\n",
    "  ggraph(layout = \"fr\") +\n",
    "  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"cyan4\") +\n",
    "  geom_node_point(size = 5) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE, \n",
    "                 point.padding = unit(0.2, \"lines\")) +\n",
    "  theme_void()\n",
    "```\n",
    "\n",
    "We see some clear clustering in this network of title words; words in NASA dataset titles are largely organized into several families of words that tend to go together.\n",
    "\n",
    "What about the words from the description fields?\n",
    "\n",
    "```{r plotdesc, dependson = \"desc_word_pairs\", fig.height=6, fig.width=9, fig.cap=\"Word network in NASA dataset descriptions\"}\n",
    "set.seed(1234)\n",
    "desc_word_pairs %>%\n",
    "  filter(n >= 5000) %>%\n",
    "  graph_from_data_frame() %>%\n",
    "  ggraph(layout = \"fr\") +\n",
    "  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"darkred\") +\n",
    "  geom_node_point(size = 5) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE,\n",
    "                 point.padding = unit(0.2, \"lines\")) +\n",
    "  theme_void()\n",
    "\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:plotdesc) shows such *strong* connections between the top dozen or so words (words like \"data\", \"global\", \"resolution\", and \"instrument\") that we do not see clear clustering structure in the network. We may want to use tf-idf (as described in detail in Chapter \\@ref(tfidf)) as a metric to find characteristic words for each description field, instead of looking at counts of words. \n",
    "\n",
    "### Networks of Keywords\n",
    "\n",
    "Next, let's make a network of the keywords in Figure \\@ref(fig:plotcounts) to see which keywords commonly occur together in the same datasets.\n",
    "\n",
    "```{r plotcounts, dependson = \"toupper\", fig.height=7, fig.width=9, fig.cap=\"Co-occurrence network in NASA dataset keywords\"}\n",
    "keyword_pairs <- nasa_keyword %>% \n",
    "  pairwise_count(keyword, id, sort = TRUE, upper = FALSE)\n",
    "\n",
    "keyword_pairs\n",
    "\n",
    "set.seed(1234)\n",
    "keyword_pairs %>%\n",
    "  filter(n >= 700) %>%\n",
    "  graph_from_data_frame() %>%\n",
    "  ggraph(layout = \"fr\") +\n",
    "  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"royalblue\") +\n",
    "  geom_node_point(size = 5) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE,\n",
    "                 point.padding = unit(0.2, \"lines\")) +\n",
    "  theme_void()\n",
    "```\n",
    "\n",
    "We definitely see clustering here, and strong connections between keywords like \"OCEANS\", \"OCEAN OPTICS\", and \"OCEAN COLOR\", or \"PROJECT\" and \"COMPLETED\". \n",
    "\n",
    "```{block, type = \"rmdwarning\"}\n",
    "These are the most commonly co-occurring words, but also just the most common keywords in general. \n",
    "```\n",
    "\n",
    "To examine the relationships among keywords in a different way, we can find the correlation among the keywords as described in Chapter \\@ref(ngrams). This looks for those keywords that are more likely to occur together than with other keywords in a description field.\n",
    "\n",
    "```{r keyword_cors, dependson = \"toupper\"}\n",
    "keyword_cors <- nasa_keyword %>% \n",
    "  group_by(keyword) %>%\n",
    "  filter(n() >= 50) %>%\n",
    "  pairwise_cor(keyword, id, sort = TRUE, upper = FALSE)\n",
    "\n",
    "keyword_cors\n",
    "```\n",
    "\n",
    "Notice that these keywords at the top of this sorted data frame have correlation coefficients equal to 1; they always occur together. This means these are redundant keywords. It may not make sense to continue to use both of the keywords in these sets of pairs; instead, just one keyword could be used.\n",
    "\n",
    "Let's visualize the network of keyword correlations, just as we did for keyword co-occurences.\n",
    "\n",
    "```{r plotcors, dependson = \"keyword_cors\", fig.height=8, fig.width=12, fig.cap=\"Correlation network in NASA dataset keywords\"}\n",
    "set.seed(1234)\n",
    "keyword_cors %>%\n",
    "  filter(correlation > .6) %>%\n",
    "  graph_from_data_frame() %>%\n",
    "  ggraph(layout = \"fr\") +\n",
    "  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = \"royalblue\") +\n",
    "  geom_node_point(size = 5) +\n",
    "  geom_node_text(aes(label = name), repel = TRUE,\n",
    "                 point.padding = unit(0.2, \"lines\")) +\n",
    "  theme_void()\n",
    "```\n",
    "\n",
    "This network in Figure \\@ref(fig:plotcors) appears much different than the co-occurence network. The difference is that the co-occurrence network asks a question about which keyword pairs occur most often, and the correlation network asks a question about which keywordsoccur more often together than with other keywords. Notice here the high number of small clusters of keywords; the network structure can be extracted (for further analysis) from the `graph_from_data_frame()` function above.\n",
    "\n",
    "## Calculating tf-idf for the description fields\n",
    "\n",
    "The network graph in Figure \\@ref(fig:plotdesc) showed us that the description fields are dominated by a few common words like \"data\", \"global\", and \"resolution\"; this would be an excellent opportunity to use tf-idf as a statistic to find characteristic words for individual description fields. As discussed in Chapter \\@ref(tfidf), we can use tf-idf, the term frequency times inverse document frequency, to identify words that are especially important to a document within a collection of documents. Let's apply that approach to the description fields of these NASA datasets. \n",
    "\n",
    "### What is tf-idf for the description field words?\n",
    "\n",
    "We will consider each description field a document, and the whole set of description fields the collection or corpus of documents. We have already used `unnest_tokens()` earlier in this chapter to make a tidy data frame of the words in the description fields, so now we can use `bind_tf_idf()` to calculate tf-idf for each word.\n",
    "\n",
    "```{r desc_tf_idf, dependson = \"my_stopwords\"}\n",
    "desc_tf_idf <- nasa_desc %>% \n",
    "  count(id, word, sort = TRUE) %>%\n",
    "  ungroup() %>%\n",
    "  bind_tf_idf(word, id, n)\n",
    "```\n",
    "\n",
    "What are the highest tf-idf words in the NASA description fields?\n",
    "\n",
    "```{r dependson = \"desc_tf_idf\"}\n",
    "desc_tf_idf %>% \n",
    "  arrange(-tf_idf)\n",
    "```\n",
    "\n",
    "These are the most important words in the description fields as measured by tf-idf, meaning they are common but not too common. \n",
    "\n",
    "```{block, type = \"rmdwarning\"}\n",
    "Notice we have run into an issue here; both $n$ and term frequency are equal to 1 for these terms, meaning that these were description fields that only had a single word in them. If a description field only contains one word, the tf-idf algorithm will think that is a very important word. \n",
    "```\n",
    "\n",
    "Depending on our analytic goals, it might be a good idea to throw out all description fields that have very few words.\n",
    "\n",
    "### Connecting description fields to keywords\n",
    "\n",
    "We now know which words in the descriptions have high tf-idf, and we also have labels for these descriptions in the keywords. Letâs do a full join of the keyword data frame and the data frame of description words with tf-idf, and then find the highest tf-idf words for a given keyword.\n",
    "\n",
    "```{r full_join, dependson = c(\"desc_tf_idf\", \"toupper\")}\n",
    "desc_tf_idf <- full_join(desc_tf_idf, nasa_keyword, by = \"id\")\n",
    "```\n",
    "\n",
    "Let's plot some of the most important words, as measured by tf-idf, for a few example keywords used on NASA datasets. First, let's use dplyr operations to filter for the keywords we want to examine and take just the top 15 words for each keyword. Then, let's plot those words in Figure \\@ref(fig:plottfidf).\n",
    "\n",
    "```{r plottfidf, dependson = \"full_join\", fig.width=10, fig.height=7, fig.cap=\"Distribution of tf-idf for words from datasets labeled with select keywords\"}\n",
    "desc_tf_idf %>% \n",
    "  filter(!near(tf, 1)) %>%\n",
    "  filter(keyword %in% c(\"SOLAR ACTIVITY\", \"CLOUDS\", \n",
    "                        \"SEISMOLOGY\", \"ASTROPHYSICS\",\n",
    "                        \"HUMAN HEALTH\", \"BUDGET\")) %>%\n",
    "  arrange(desc(tf_idf)) %>%\n",
    "  group_by(keyword) %>%\n",
    "  distinct(word, keyword, .keep_all = TRUE) %>%\n",
    "  top_n(15, tf_idf) %>% \n",
    "  ungroup() %>%\n",
    "  mutate(word = factor(word, levels = rev(unique(word)))) %>%\n",
    "  ggplot(aes(word, tf_idf, fill = keyword)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~keyword, ncol = 3, scales = \"free\") +\n",
    "  coord_flip() +\n",
    "  labs(title = \"Highest tf-idf words in NASA metadata description fields\",\n",
    "       caption = \"NASA metadata from https://data.nasa.gov/data.json\",\n",
    "       x = NULL, y = \"tf-idf\")\n",
    "```\n",
    "\n",
    "Using tf-idf has allowed us to identify important description words for each of these keywords. Datasets labeled with the keyword \"SEISMOLOGY\" have words like \"earthquake\", \"risk\", and \"hazard\" in their description, while those labeled with \"HUMAN HEALTH\" have descriptions characterized by words like \"wellbeing\", \"vulnerability\", and \"children.\" Most of the combinations of letters that are not English words are certainly acronyms (like OMB for the Office of Management and Budget), and the examples of years and numbers are important for these topics. The tf-idf statistic has identified the kinds of words it is intended to, important words for individual documents within a collection of documents.\n",
    "\n",
    "## Topic modeling\n",
    "\n",
    "Using tf-idf as a statistic has already given us insight into the content of NASA description fields, but let's try an additional approach to the question of what the NASA descriptions fields are about. We can use topic modeling as described in Chapter \\@ref(topicmodeling) to model each document (description field) as a mixture of topics and each topic as a mixture of words. As in earlier chapters, we will use [latent Dirichlet allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) for our topic modeling; there are other possible approaches for topic modeling.\n",
    "\n",
    "### Casting to a document-term matrix\n",
    "\n",
    "To do the topic modeling as implemented here, we need to make a `DocumentTermMatrix`, a special kind of matrix from the tm package (of course, this is just a specific implementation of the general concept of a \"document-term matrix\"). Rows correspond to documents (description texts in our case) and columns correspond to terms (i.e., words); it is a sparse matrix and the values are word counts.\n",
    "\n",
    "Letâs clean up the text a bit using stop words to remove some of the nonsense \"words\" leftover from HTML or other character encoding. We can use `bind_rows()` to add our custom stop words to the list of default stop words from the tidytext package, and then all at once use `anti_join()` to remove them all from our data frame.\n",
    "\n",
    "```{r word_counts, dependson = \"my_stopwords\"}\n",
    "my_stop_words <- bind_rows(stop_words, \n",
    "                           data_frame(word = c(\"nbsp\", \"amp\", \"gt\", \"lt\",\n",
    "                                               \"timesnewromanpsmt\", \"font\",\n",
    "                                               \"td\", \"li\", \"br\", \"tr\", \"quot\",\n",
    "                                               \"st\", \"img\", \"src\", \"strong\",\n",
    "                                               \"http\", \"file\", \"files\",\n",
    "                                               as.character(1:12)), \n",
    "                                      lexicon = rep(\"custom\", 30)))\n",
    "\n",
    "word_counts <- nasa_desc %>%\n",
    "  anti_join(my_stop_words) %>%\n",
    "  count(id, word, sort = TRUE) %>%\n",
    "  ungroup()\n",
    "\n",
    "word_counts\n",
    "```\n",
    "\n",
    "This is the information we need, the number of times each word is used in each document, to make a `DocumentTermMatrix`. We can `cast()` from our tidy text format to this non-tidy format as described in detail in Chapter \\@ref(dtm).\n",
    "\n",
    "```{r desc_dtm, dependson = \"word_counts\"}\n",
    "desc_dtm <- word_counts %>%\n",
    "  cast_dtm(id, word, n)\n",
    "\n",
    "desc_dtm\n",
    "```\n",
    "\n",
    "We see that this dataset contains documents (each of them a NASA description field) and terms (words). Notice that this example document-term matrix is (very close to) 100% sparse, meaning that almost all of the entries in this matrix are zero. Each non-zero entry corresponds to a certain word appearing in a certain document.\n",
    "\n",
    "### Ready for topic modeling\n",
    "\n",
    "Now letâs use the [topicmodels](https://cran.r-project.org/package=topicmodels) package to create an LDA model. How many topics will we tell the algorithm to make? This is a question much like in $k$-means clustering; we donât really know ahead of time. We tried the following modeling procedure using 8, 16, 24, 32, and 64 topics; we found that at 24 topics, documents are still getting sorted into topics cleanly but going much beyond that caused the distributions of $\\gamma$, the probability that each document belongs in each topic, to look worrisome. We will show more details on this later.\n",
    "\n",
    "```{r, eval = FALSE}\n",
    "library(topicmodels)\n",
    "\n",
    "# be aware that running this model is time intensive\n",
    "desc_lda <- LDA(desc_dtm, k = 24, control = list(seed = 1234))\n",
    "desc_lda\n",
    "```\n",
    "\n",
    "```{r desc_lda, echo=FALSE}\n",
    "library(topicmodels)\n",
    "load(\"data/desc_lda.rda\")\n",
    "desc_lda\n",
    "```\n",
    "\n",
    "This is a stochastic algorithm that could have different results depending on where the algorithm starts, so we need to specify a `seed` for reproducibility as shown here.\n",
    "\n",
    "### Interpreting the topic model\n",
    "\n",
    "Now that we have built the model, let's `tidy()` the results of the model, i.e., construct a tidy data frame that summarizes the results of the model. The tidytext package includes a tidying method for LDA models from the topicmodels package.\n",
    "\n",
    "```{r tidy_lda, dependson = \"desc_lda\"}\n",
    "tidy_lda <- tidy(desc_lda)\n",
    "\n",
    "tidy_lda\n",
    "```\n",
    "\n",
    "The column $\\beta$ tells us the probability of that term being generated from that topic for that document. It is the probability of that term (word) belonging to that topic. Notice that some of the values for $\\beta$ are very, very low, and some are not so low.\n",
    "\n",
    "What is each topic about? Let's examine the top 10 terms for each topic.\n",
    "\n",
    "```{r top_terms, dependson = \"tidy_lda\"}\n",
    "top_terms <- tidy_lda %>%\n",
    "  group_by(topic) %>%\n",
    "  top_n(10, beta) %>%\n",
    "  ungroup() %>%\n",
    "  arrange(topic, -beta)\n",
    "\n",
    "top_terms\n",
    "```\n",
    "\n",
    "It is not very easy to interpret what the topics are about from a data frame like this so letâs look at this information visually in Figure \\@ref(fig:plotbeta).\n",
    "\n",
    "```{r plotbeta, dependson = \"top_terms\", fig.width=12, fig.height=16, fig.cap=\"Top terms in topic modeling of NASA metadata description field texts\"}\n",
    "top_terms %>%\n",
    "  mutate(term = reorder(term, beta)) %>%\n",
    "  group_by(topic, term) %>%    \n",
    "  arrange(desc(beta)) %>%  \n",
    "  ungroup() %>%\n",
    "  mutate(term = factor(paste(term, topic, sep = \"__\"), \n",
    "                       levels = rev(paste(term, topic, sep = \"__\")))) %>%\n",
    "  ggplot(aes(term, beta, fill = as.factor(topic))) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  coord_flip() +\n",
    "  scale_x_discrete(labels = function(x) gsub(\"__.+$\", \"\", x)) +\n",
    "  labs(title = \"Top 10 terms in each LDA topic\",\n",
    "       x = NULL, y = expression(beta)) +\n",
    "  facet_wrap(~ topic, ncol = 4, scales = \"free\")\n",
    "```\n",
    "\n",
    "We can see what a dominant word \"data\" is in these description texts. In addition, there are meaningful differences between these collections of terms, from terms about soil, forests, and biomass in topic 12 to terms about design, systems, and technology in topic 21. The topic modeling process has identified groupings of terms that we can understand as human readers of these description fields.\n",
    "\n",
    "We just explored which words are associated with which topics. Next, letâs examine which topics are associated with which description fields (i.e., documents). We will look at a different probability for this, $\\gamma$, the probability that each document belongs in each topic, again using the `tidy` verb.\n",
    "\n",
    "```{r lda_gamma, dependson = \"desc_lda\"}\n",
    "lda_gamma <- tidy(desc_lda, matrix = \"gamma\")\n",
    "\n",
    "lda_gamma\n",
    "```\n",
    "\n",
    "Notice that some of the probabilites visible at the top of the data frame are low and some are higher. Our model has assigned a probability to each description belonging to each of the topics we constructed from the sets of words. How are the probabilities distributed? Let's visualize them (Figure \\@ref(fig:plotgammaall)).\n",
    "\n",
    "```{r plotgammaall, dependson = \"lda_gamma\", fig.width=7, fig.height=5, fig.cap=\"Probability distribution in topic modeling of NASA metadata description field texts\"}\n",
    "ggplot(lda_gamma, aes(gamma)) +\n",
    "  geom_histogram() +\n",
    "  scale_y_log10() +\n",
    "  labs(title = \"Distribution of probabilities for all topics\",\n",
    "       y = \"Number of documents\", x = expression(gamma))\n",
    "```\n",
    "\n",
    "First notice that the y-axis is plotted on a log scale; otherwise it is difficult to make out any detail in the plot. Next, notice that $\\gamma$ runs from 0 to 1; remember that this is the probability that a given document belongs in a given topic. There are many values near zero, which means there are many documents that do not belong in each topic. Also, there are many values near $\\gamma = 1$; these are the documents that *do* belong in those topics. This distribution shows that documents are being well discriminated as belonging to a topic or not. We can also look at how the probabilities are distributed within each topic, as shown in Figure \\@ref(fig:plotgamma).\n",
    "\n",
    "```{r plotgamma, dependson = \"lda_gamma\", fig.width=10, fig.height=12, fig.cap=\"Probability distribution for each topic in topic modeling of NASA metadata description field texts\"}\n",
    "ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +\n",
    "  geom_histogram(show.legend = FALSE) +\n",
    "  facet_wrap(~ topic, ncol = 4) +\n",
    "  scale_y_log10() +\n",
    "  labs(title = \"Distribution of probability for each topic\",\n",
    "       y = \"Number of documents\", x = expression(gamma))\n",
    "```\n",
    "\n",
    "Let's look specifically at topic 18 in Figure \\@ref(fig:plotgammaall), a topic that had documents cleanly sorted in and out of it. There are many documents with $\\gamma$ close to 1; these are the documents that *do* belong to topic 18 according to the model. There are also many documents with $\\gamma$ close to 0; these are the documents that do *not* belong to topic 18. Each document appears in each panel in this plot, and its $\\gamma$ for that topic tells us that document's probability of belonging in that topic.\n",
    "\n",
    "This plot displays the type of information we used to choose how many topics for our topic modeling procedure. When we tried options higher than 24 (such as 32 or 64), the distributions for $\\gamma$ started to look very flat toward $\\gamma = 1$; documents were not getting sorted into topics very well.\n",
    "\n",
    "### Connecting topic modeling with keywords\n",
    "\n",
    "Letâs connect these topic models with the keywords and see what relationships we can find. We can `full_join()` this to the human-tagged keywords and discover which keywords are associated with which topic.\n",
    "\n",
    "```{r lda_join, dependson = c(\"lda_gamma\", \"toupper\")}\n",
    "lda_gamma <- full_join(lda_gamma, nasa_keyword, by = c(\"document\" = \"id\"))\n",
    "\n",
    "lda_gamma\n",
    "```\n",
    "\n",
    "Now we can use `filter()` to keep only the document-topic entries that have probabilities ($\\gamma$) greater than some cut-off value; let's use 0.9.\n",
    "\n",
    "```{r top_keywords, dependson = \"lda_join\"}\n",
    "top_keywords <- lda_gamma %>% \n",
    "  filter(gamma > 0.9) %>% \n",
    "  count(topic, keyword, sort = TRUE)\n",
    "\n",
    "top_keywords\n",
    "```\n",
    "\n",
    "What are the top keywords for each topic?\n",
    "\n",
    "```{r plottopkeywords, dependson = \"top_keywords\", fig.width=16, fig.height=16, fig.cap=\"Top keywords in topic modeling of NASA metadata description field texts\"}\n",
    "top_keywords %>%\n",
    "  group_by(topic) %>%\n",
    "  top_n(5, n) %>%\n",
    "  group_by(topic, keyword) %>%\n",
    "  arrange(desc(n)) %>%  \n",
    "  ungroup() %>%\n",
    "  mutate(keyword = factor(paste(keyword, topic, sep = \"__\"), \n",
    "                          levels = rev(paste(keyword, topic, sep = \"__\")))) %>%\n",
    "  ggplot(aes(keyword, n, fill = as.factor(topic))) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  labs(title = \"Top keywords for each LDA topic\",\n",
    "       x = NULL, y = \"Number of documents\") +\n",
    "  coord_flip() +\n",
    "  scale_x_discrete(labels = function(x) gsub(\"__.+$\", \"\", x)) +\n",
    "  facet_wrap(~ topic, ncol = 4, scales = \"free\")\n",
    "```\n",
    "\n",
    "Let's take a step back and remind ourselves what Figure \\@ref(fig:plottopkeywords) is telling us. NASA datasets are tagged with keywords by human beings, and we have built an LDA topic model (with 24 topics) for the description fields of the NASA datasets. This plot answers the question, \"For the datasets with description fields that have a high probability of belonging to a given topic, what are the most common human-assigned keywords?\"\n",
    "\n",
    "Itâs interesting that the keywords for topics 13, 16, and 18 are essentially duplicates of each other (\"OCEAN COLOR\", \"OCEAN OPTICS\", \"OCEANS\"), because the top words in those topics do exhibit meaningful differences, as shown in Figure \\@ref(fig:plotbeta). Also note that by number of documents, the combination of 13, 16, and 18 is quite a large percentage of the total number of datasets represented in this plot, and even more if we were to include topic 11. By number, there are *many* datasets at NASA that deal with oceans, ocean color, and ocean optics. We see \"PROJECT COMPLETED\" in topics 9, 10, and 21, along with the names of NASA laboratories and research centers. Other important subject areas that stand out are groups of keywords about atmospheric science, budget/finance, and population/human dimensions. We can go back to Figure \\@ref(fig:plotbeta) on terms and topics to see which words in the description fields are driving datasets being assigned to these topics. For example, topic 4 is associated with keywords about population and human dimensions, and some of the top terms for that topic are \"population\", \"international\", \"center\", and \"university\".\n",
    "\n",
    "## Summary\n",
    "\n",
    "By using a combination of network analysis, tf-idf, and topic modeling, we have come to a greater understanding of how datasets are related at NASA. Specifically, we have more information now about how keywords are connected to each other and which datasets are likely to be related. The topic model could be used to suggest keywords based on the words in the description field, or the work on the keywords could suggest the most important combination of keywords for certain areas of study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "238px",
    "left": "1066px",
    "right": "20px",
    "top": "138px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
